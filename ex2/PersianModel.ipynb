{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e37dcc2-c3bc-4a74-a1bb-bc92eb07fb90",
   "metadata": {},
   "source": [
    "# Persian Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725a2d7-bfad-4627-8d78-e4a3260e349b",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4399cd45-4657-45db-8d2e-bc68114309af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#! pip install tokenizers scikit-learn --user \n",
    "#! pip install hazm --user \n",
    "#! pip install tiktoken --user \n",
    "#! pip install transformers --user\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import trainers\n",
    "from tokenizers.normalizers import StripAccents, Lowercase, Sequence\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer\n",
    "from tokenizers.models import BPE, Unigram\n",
    "from transformers import  AutoTokenizer  #pipeline, GPT2LMHeadModel\n",
    "\n",
    "from hazm import * \n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b04b71-e698-4f30-aa08-3c6ebf89a799",
   "metadata": {},
   "source": [
    "## Main Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47420bce-2049-48ce-95bb-be757e2bcf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:  100000  lines seperators replaced\n",
      "Total lines replaced 95195\n",
      "Total lines replaced 40842\n",
      "Total lines replaced 1591\n"
     ]
    }
   ],
   "source": [
    "from src.helper  import clean_pers_text_replace, get_cleaned_text\n",
    "text_path = \"content/fas_news_2020_100K/fas_news_2020_100K-sentences.txt\"\n",
    "path_to_save_folder= \"model/train_data_pers\"\n",
    "\n",
    "raw_text = get_cleaned_text(text_path,clean_pers_text_replace)\n",
    "#enc_text = tokenizer.encode(raw_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea314dc7-1e1e-44b6-8629-c25e067e8b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create Dataset 2720000 / 2733504"
     ]
    }
   ],
   "source": [
    "from src.dataset import GPTDataset\n",
    "from src.dataset import create_dataloader\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "\n",
    "# Parameters\n",
    "batch_size = 128\n",
    "context_length = 32  # Context size for training\n",
    "vocab_size =  30000#tokenizer.n_vocab\n",
    "embedding_dim = 128\n",
    "\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    raw_text,tokenizer = tokenizer,allowed_special=False, batch_size=batch_size, \n",
    "    context_length=context_length, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d7700-93a5-4454-a67e-797ff70ea489",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f39c1374-f0b7-4bf1-9919-406e2302922c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create Dataset 2720000 / 2733504Epoch [1/4], Step [0/34168], Loss: 10.7367\n",
      "Validation perplexity: 41141.40421065184\n",
      "Epoch [1/4], Step [75/34168], Loss: 8.7246\n",
      "Epoch [1/4], Step [150/34168], Loss: 7.3658\n",
      "Epoch [1/4], Step [225/34168], Loss: 6.9710\n",
      "Epoch [1/4], Step [300/34168], Loss: 6.9824\n",
      "Epoch [1/4], Step [375/34168], Loss: 6.8438\n",
      "Epoch [1/4], Step [450/34168], Loss: 6.6495\n",
      "Epoch [1/4], Step [525/34168], Loss: 6.7453\n",
      "Epoch [1/4], Step [600/34168], Loss: 6.5350\n",
      "Epoch [1/4], Step [675/34168], Loss: 6.5044\n",
      "Epoch [1/4], Step [750/34168], Loss: 6.4446\n",
      "Epoch [1/4], Step [825/34168], Loss: 6.2616\n",
      "Epoch [1/4], Step [900/34168], Loss: 6.2418\n",
      "Epoch [1/4], Step [975/34168], Loss: 6.2203\n",
      "Epoch [1/4], Step [1050/34168], Loss: 6.5285\n",
      "Epoch [1/4], Step [1125/34168], Loss: 6.3570\n",
      "Epoch [1/4], Step [1200/34168], Loss: 6.2309\n",
      "Epoch [1/4], Step [1275/34168], Loss: 6.3496\n",
      "Epoch [1/4], Step [1350/34168], Loss: 6.2279\n",
      "Epoch [1/4], Step [1425/34168], Loss: 6.1301\n",
      "Epoch [1/4], Step [1500/34168], Loss: 6.1322\n",
      "Epoch [1/4], Step [1575/34168], Loss: 6.3483\n",
      "Epoch [1/4], Step [1650/34168], Loss: 6.1233\n",
      "Epoch [1/4], Step [1725/34168], Loss: 6.2407\n",
      "Epoch [1/4], Step [1800/34168], Loss: 6.1672\n",
      "Epoch [1/4], Step [1875/34168], Loss: 6.1677\n",
      "Epoch [1/4], Step [1950/34168], Loss: 6.0636\n",
      "Epoch [1/4], Step [2025/34168], Loss: 6.1071\n",
      "Epoch [1/4], Step [2100/34168], Loss: 5.9848\n",
      "Epoch [1/4], Step [2175/34168], Loss: 6.0776\n",
      "Epoch [1/4], Step [2250/34168], Loss: 6.0378\n",
      "Epoch [1/4], Step [2325/34168], Loss: 5.9805\n",
      "Epoch [1/4], Step [2400/34168], Loss: 6.1542\n",
      "Epoch [1/4], Step [2475/34168], Loss: 5.9365\n",
      "Epoch [1/4], Step [2550/34168], Loss: 6.1303\n",
      "Epoch [1/4], Step [2625/34168], Loss: 5.9388\n",
      "Epoch [1/4], Step [2700/34168], Loss: 5.9239\n",
      "Epoch [1/4], Step [2775/34168], Loss: 6.0454\n",
      "Epoch [1/4], Step [2850/34168], Loss: 6.0632\n",
      "Epoch [1/4], Step [2925/34168], Loss: 5.8490\n",
      "Epoch [1/4], Step [3000/34168], Loss: 6.1297\n",
      "Epoch [1/4], Step [3075/34168], Loss: 6.0216\n",
      "Epoch [1/4], Step [3150/34168], Loss: 5.9516\n",
      "Epoch [1/4], Step [3225/34168], Loss: 5.8731\n",
      "Epoch [1/4], Step [3300/34168], Loss: 5.9357\n",
      "Epoch [1/4], Step [3375/34168], Loss: 5.8084\n",
      "Epoch [1/4], Step [3450/34168], Loss: 5.9297\n",
      "Epoch [1/4], Step [3525/34168], Loss: 5.9803\n",
      "Epoch [1/4], Step [3600/34168], Loss: 5.9013\n",
      "Epoch [1/4], Step [3675/34168], Loss: 5.7792\n",
      "Epoch [1/4], Step [3750/34168], Loss: 5.7774\n",
      "Epoch [1/4], Step [3825/34168], Loss: 5.7965\n",
      "Epoch [1/4], Step [3900/34168], Loss: 5.7105\n",
      "Epoch [1/4], Step [3975/34168], Loss: 5.7814\n",
      "Epoch [1/4], Step [4050/34168], Loss: 5.7455\n",
      "Epoch [1/4], Step [4125/34168], Loss: 5.7822\n",
      "Epoch [1/4], Step [4200/34168], Loss: 5.8201\n",
      "Epoch [1/4], Step [4275/34168], Loss: 5.8659\n",
      "Epoch [1/4], Step [4350/34168], Loss: 5.8456\n",
      "Epoch [1/4], Step [4425/34168], Loss: 5.8130\n",
      "Epoch [1/4], Step [4500/34168], Loss: 5.7484\n",
      "Epoch [1/4], Step [4575/34168], Loss: 5.7897\n",
      "Epoch [1/4], Step [4650/34168], Loss: 5.6154\n",
      "Epoch [1/4], Step [4725/34168], Loss: 5.7238\n",
      "Epoch [1/4], Step [4800/34168], Loss: 5.9072\n",
      "Epoch [1/4], Step [4875/34168], Loss: 5.7391\n",
      "Epoch [1/4], Step [4950/34168], Loss: 5.7466\n",
      "Epoch [1/4], Step [5025/34168], Loss: 5.7087\n",
      "Epoch [1/4], Step [5100/34168], Loss: 5.6801\n",
      "Epoch [1/4], Step [5175/34168], Loss: 5.9167\n",
      "Epoch [1/4], Step [5250/34168], Loss: 5.6523\n",
      "Epoch [1/4], Step [5325/34168], Loss: 5.6306\n",
      "Epoch [1/4], Step [5400/34168], Loss: 5.6909\n",
      "Epoch [1/4], Step [5475/34168], Loss: 5.6096\n",
      "Epoch [1/4], Step [5550/34168], Loss: 5.6804\n",
      "Epoch [1/4], Step [5625/34168], Loss: 5.6982\n",
      "Epoch [1/4], Step [5700/34168], Loss: 5.6536\n",
      "Epoch [1/4], Step [5775/34168], Loss: 5.6804\n",
      "Epoch [1/4], Step [5850/34168], Loss: 5.5984\n",
      "Epoch [1/4], Step [5925/34168], Loss: 5.6359\n",
      "Epoch [1/4], Step [6000/34168], Loss: 5.5612\n",
      "Epoch [1/4], Step [6075/34168], Loss: 5.6444\n",
      "Epoch [1/4], Step [6150/34168], Loss: 5.7729\n",
      "Epoch [1/4], Step [6225/34168], Loss: 5.6328\n",
      "Epoch [1/4], Step [6300/34168], Loss: 5.6614\n",
      "Epoch [1/4], Step [6375/34168], Loss: 5.5569\n",
      "Epoch [1/4], Step [6450/34168], Loss: 5.7405\n",
      "Epoch [1/4], Step [6525/34168], Loss: 5.6071\n",
      "Epoch [1/4], Step [6600/34168], Loss: 5.6876\n",
      "Epoch [1/4], Step [6675/34168], Loss: 5.6124\n",
      "Epoch [1/4], Step [6750/34168], Loss: 5.7278\n",
      "Epoch [1/4], Step [6825/34168], Loss: 5.6422\n",
      "Epoch [1/4], Step [6900/34168], Loss: 5.6705\n",
      "Epoch [1/4], Step [6975/34168], Loss: 5.6143\n",
      "Epoch [1/4], Step [7050/34168], Loss: 5.7339\n",
      "Epoch [1/4], Step [7125/34168], Loss: 5.7328\n",
      "Epoch [1/4], Step [7200/34168], Loss: 5.4984\n",
      "Epoch [1/4], Step [7275/34168], Loss: 5.5772\n",
      "Epoch [1/4], Step [7350/34168], Loss: 5.3622\n",
      "Epoch [1/4], Step [7425/34168], Loss: 5.6140\n",
      "Epoch [1/4], Step [7500/34168], Loss: 5.7015\n",
      "Epoch [1/4], Step [7575/34168], Loss: 5.5342\n",
      "Epoch [1/4], Step [7650/34168], Loss: 5.6348\n",
      "Epoch [1/4], Step [7725/34168], Loss: 5.6718\n",
      "Epoch [1/4], Step [7800/34168], Loss: 5.7507\n",
      "Epoch [1/4], Step [7875/34168], Loss: 5.5197\n",
      "Epoch [1/4], Step [7950/34168], Loss: 5.6062\n",
      "Epoch [1/4], Step [8025/34168], Loss: 5.5694\n",
      "Epoch [1/4], Step [8100/34168], Loss: 5.5321\n",
      "Epoch [1/4], Step [8175/34168], Loss: 5.5623\n",
      "Epoch [1/4], Step [8250/34168], Loss: 5.5078\n",
      "Epoch [1/4], Step [8325/34168], Loss: 5.5500\n",
      "Epoch [1/4], Step [8400/34168], Loss: 5.5201\n",
      "Epoch [1/4], Step [8475/34168], Loss: 5.4808\n",
      "Epoch [1/4], Step [8550/34168], Loss: 5.6246\n",
      "Epoch [1/4], Step [8625/34168], Loss: 5.5904\n",
      "Epoch [1/4], Step [8700/34168], Loss: 5.5165\n",
      "Epoch [1/4], Step [8775/34168], Loss: 5.6192\n",
      "Epoch [1/4], Step [8850/34168], Loss: 5.5009\n",
      "Epoch [1/4], Step [8925/34168], Loss: 5.4122\n",
      "Epoch [1/4], Step [9000/34168], Loss: 5.6615\n",
      "Epoch [1/4], Step [9075/34168], Loss: 5.5914\n",
      "Epoch [1/4], Step [9150/34168], Loss: 5.5305\n",
      "Epoch [1/4], Step [9225/34168], Loss: 5.5399\n",
      "Epoch [1/4], Step [9300/34168], Loss: 5.4598\n",
      "Epoch [1/4], Step [9375/34168], Loss: 5.5825\n",
      "Epoch [1/4], Step [9450/34168], Loss: 5.5130\n",
      "Epoch [1/4], Step [9525/34168], Loss: 5.4659\n",
      "Epoch [1/4], Step [9600/34168], Loss: 5.4749\n",
      "Epoch [1/4], Step [9675/34168], Loss: 5.5976\n",
      "Epoch [1/4], Step [9750/34168], Loss: 5.6744\n",
      "Epoch [1/4], Step [9825/34168], Loss: 5.4797\n",
      "Epoch [1/4], Step [9900/34168], Loss: 5.4750\n",
      "Epoch [1/4], Step [9975/34168], Loss: 5.5354\n",
      "Validation perplexity: 215.21554170683504\n",
      "Epoch [1/4], Step [10050/34168], Loss: 5.5723\n",
      "Epoch [1/4], Step [10125/34168], Loss: 5.4651\n",
      "Epoch [1/4], Step [10200/34168], Loss: 5.4724\n",
      "Epoch [1/4], Step [10275/34168], Loss: 5.3954\n",
      "Epoch [1/4], Step [10350/34168], Loss: 5.4722\n",
      "Epoch [1/4], Step [10425/34168], Loss: 5.3801\n",
      "Epoch [1/4], Step [10500/34168], Loss: 5.5532\n",
      "Epoch [1/4], Step [10575/34168], Loss: 5.4608\n",
      "Epoch [1/4], Step [10650/34168], Loss: 5.4411\n",
      "Epoch [1/4], Step [10725/34168], Loss: 5.4966\n",
      "Epoch [1/4], Step [10800/34168], Loss: 5.5695\n",
      "Epoch [1/4], Step [10875/34168], Loss: 5.5583\n",
      "Epoch [1/4], Step [10950/34168], Loss: 5.4429\n",
      "Epoch [1/4], Step [11025/34168], Loss: 5.4995\n",
      "Epoch [1/4], Step [11100/34168], Loss: 5.4280\n",
      "Epoch [1/4], Step [11175/34168], Loss: 5.4233\n",
      "Epoch [1/4], Step [11250/34168], Loss: 5.5598\n",
      "Epoch [1/4], Step [11325/34168], Loss: 5.4954\n",
      "Epoch [1/4], Step [11400/34168], Loss: 5.4036\n",
      "Epoch [1/4], Step [11475/34168], Loss: 5.5223\n",
      "Epoch [1/4], Step [11550/34168], Loss: 5.4192\n",
      "Epoch [1/4], Step [11625/34168], Loss: 5.4151\n",
      "Epoch [1/4], Step [11700/34168], Loss: 5.4140\n",
      "Epoch [1/4], Step [11775/34168], Loss: 5.4311\n",
      "Epoch [1/4], Step [11850/34168], Loss: 5.4135\n",
      "Epoch [1/4], Step [11925/34168], Loss: 5.5470\n",
      "Epoch [1/4], Step [12000/34168], Loss: 5.3801\n",
      "Epoch [1/4], Step [12075/34168], Loss: 5.5249\n",
      "Epoch [1/4], Step [12150/34168], Loss: 5.4127\n",
      "Epoch [1/4], Step [12225/34168], Loss: 5.3639\n",
      "Epoch [1/4], Step [12300/34168], Loss: 5.4398\n",
      "Epoch [1/4], Step [12375/34168], Loss: 5.4439\n",
      "Epoch [1/4], Step [12450/34168], Loss: 5.5291\n",
      "Epoch [1/4], Step [12525/34168], Loss: 5.4487\n",
      "Epoch [1/4], Step [12600/34168], Loss: 5.4613\n",
      "Epoch [1/4], Step [12675/34168], Loss: 5.5428\n",
      "Epoch [1/4], Step [12750/34168], Loss: 5.3420\n",
      "Epoch [1/4], Step [12825/34168], Loss: 5.3716\n",
      "Epoch [1/4], Step [12900/34168], Loss: 5.2897\n",
      "Epoch [1/4], Step [12975/34168], Loss: 5.4273\n",
      "Epoch [1/4], Step [13050/34168], Loss: 5.3576\n",
      "Epoch [1/4], Step [13125/34168], Loss: 5.3528\n",
      "Epoch [1/4], Step [13200/34168], Loss: 5.3592\n",
      "Epoch [1/4], Step [13275/34168], Loss: 5.4578\n",
      "Epoch [1/4], Step [13350/34168], Loss: 5.4592\n",
      "Epoch [1/4], Step [13425/34168], Loss: 5.3596\n",
      "Epoch [1/4], Step [13500/34168], Loss: 5.1899\n",
      "Epoch [1/4], Step [13575/34168], Loss: 5.3912\n",
      "Epoch [1/4], Step [13650/34168], Loss: 5.4068\n",
      "Epoch [1/4], Step [13725/34168], Loss: 5.2827\n",
      "Epoch [1/4], Step [13800/34168], Loss: 5.5363\n",
      "Epoch [1/4], Step [13875/34168], Loss: 5.4742\n",
      "Epoch [1/4], Step [13950/34168], Loss: 5.4125\n",
      "Epoch [1/4], Step [14025/34168], Loss: 5.3885\n",
      "Epoch [1/4], Step [14100/34168], Loss: 5.4561\n",
      "Epoch [1/4], Step [14175/34168], Loss: 5.4719\n",
      "Epoch [1/4], Step [14250/34168], Loss: 5.3974\n",
      "Epoch [1/4], Step [14325/34168], Loss: 5.4121\n",
      "Epoch [1/4], Step [14400/34168], Loss: 5.4059\n",
      "Epoch [1/4], Step [14475/34168], Loss: 5.3158\n",
      "Epoch [1/4], Step [14550/34168], Loss: 5.5193\n",
      "Epoch [1/4], Step [14625/34168], Loss: 5.4104\n",
      "Epoch [1/4], Step [14700/34168], Loss: 5.4807\n",
      "Epoch [1/4], Step [14775/34168], Loss: 5.3968\n",
      "Epoch [1/4], Step [14850/34168], Loss: 5.4210\n",
      "Epoch [1/4], Step [14925/34168], Loss: 5.4936\n",
      "Epoch [1/4], Step [15000/34168], Loss: 5.4613\n",
      "Epoch [1/4], Step [15075/34168], Loss: 5.2463\n",
      "Epoch [1/4], Step [15150/34168], Loss: 5.5226\n",
      "Epoch [1/4], Step [15225/34168], Loss: 5.4185\n",
      "Epoch [1/4], Step [15300/34168], Loss: 5.3749\n",
      "Epoch [1/4], Step [15375/34168], Loss: 5.3883\n",
      "Epoch [1/4], Step [15450/34168], Loss: 5.3912\n",
      "Epoch [1/4], Step [15525/34168], Loss: 5.4582\n",
      "Epoch [1/4], Step [15600/34168], Loss: 5.5229\n",
      "Epoch [1/4], Step [15675/34168], Loss: 5.3820\n",
      "Epoch [1/4], Step [15750/34168], Loss: 5.3988\n",
      "Epoch [1/4], Step [15825/34168], Loss: 5.4609\n",
      "Epoch [1/4], Step [15900/34168], Loss: 5.4850\n",
      "Epoch [1/4], Step [15975/34168], Loss: 5.2675\n",
      "Epoch [1/4], Step [16050/34168], Loss: 5.4596\n",
      "Epoch [1/4], Step [16125/34168], Loss: 5.4914\n",
      "Epoch [1/4], Step [16200/34168], Loss: 5.5036\n",
      "Epoch [1/4], Step [16275/34168], Loss: 5.3732\n",
      "Epoch [1/4], Step [16350/34168], Loss: 5.4427\n",
      "Epoch [1/4], Step [16425/34168], Loss: 5.3316\n",
      "Epoch [1/4], Step [16500/34168], Loss: 5.3184\n",
      "Epoch [1/4], Step [16575/34168], Loss: 5.4106\n",
      "Epoch [1/4], Step [16650/34168], Loss: 5.4834\n",
      "Epoch [1/4], Step [16725/34168], Loss: 5.3997\n",
      "Epoch [1/4], Step [16800/34168], Loss: 5.2941\n",
      "Epoch [1/4], Step [16875/34168], Loss: 5.3977\n",
      "Epoch [1/4], Step [16950/34168], Loss: 5.3534\n",
      "Epoch [1/4], Step [17025/34168], Loss: 5.3752\n",
      "Epoch [1/4], Step [17100/34168], Loss: 5.2923\n",
      "Epoch [1/4], Step [17175/34168], Loss: 5.2746\n",
      "Epoch [1/4], Step [17250/34168], Loss: 5.4343\n",
      "Epoch [1/4], Step [17325/34168], Loss: 5.4900\n",
      "Epoch [1/4], Step [17400/34168], Loss: 5.3922\n",
      "Epoch [1/4], Step [17475/34168], Loss: 5.4373\n",
      "Epoch [1/4], Step [17550/34168], Loss: 5.3269\n",
      "Epoch [1/4], Step [17625/34168], Loss: 5.3606\n",
      "Epoch [1/4], Step [17700/34168], Loss: 5.3569\n",
      "Epoch [1/4], Step [17775/34168], Loss: 5.4448\n",
      "Epoch [1/4], Step [17850/34168], Loss: 5.3099\n",
      "Epoch [1/4], Step [17925/34168], Loss: 5.2946\n",
      "Epoch [1/4], Step [18000/34168], Loss: 5.3100\n",
      "Epoch [1/4], Step [18075/34168], Loss: 5.4101\n",
      "Epoch [1/4], Step [18150/34168], Loss: 5.3261\n",
      "Epoch [1/4], Step [18225/34168], Loss: 5.3585\n",
      "Epoch [1/4], Step [18300/34168], Loss: 5.2154\n",
      "Epoch [1/4], Step [18375/34168], Loss: 5.2557\n",
      "Epoch [1/4], Step [18450/34168], Loss: 5.3546\n",
      "Epoch [1/4], Step [18525/34168], Loss: 5.2783\n",
      "Epoch [1/4], Step [18600/34168], Loss: 5.3078\n",
      "Epoch [1/4], Step [18675/34168], Loss: 5.3334\n",
      "Epoch [1/4], Step [18750/34168], Loss: 5.4282\n",
      "Epoch [1/4], Step [18825/34168], Loss: 5.3490\n",
      "Epoch [1/4], Step [18900/34168], Loss: 5.4481\n",
      "Epoch [1/4], Step [18975/34168], Loss: 5.3467\n",
      "Epoch [1/4], Step [19050/34168], Loss: 5.1877\n",
      "Epoch [1/4], Step [19125/34168], Loss: 5.4087\n",
      "Epoch [1/4], Step [19200/34168], Loss: 5.2854\n",
      "Epoch [1/4], Step [19275/34168], Loss: 5.3853\n",
      "Epoch [1/4], Step [19350/34168], Loss: 5.3553\n",
      "Epoch [1/4], Step [19425/34168], Loss: 5.3982\n",
      "Epoch [1/4], Step [19500/34168], Loss: 5.3651\n",
      "Epoch [1/4], Step [19575/34168], Loss: 5.2992\n",
      "Epoch [1/4], Step [19650/34168], Loss: 5.3981\n",
      "Epoch [1/4], Step [19725/34168], Loss: 5.1741\n",
      "Epoch [1/4], Step [19800/34168], Loss: 5.3674\n",
      "Epoch [1/4], Step [19875/34168], Loss: 5.4261\n",
      "Epoch [1/4], Step [19950/34168], Loss: 5.2487\n",
      "Validation perplexity: 181.03024137404043\n",
      "Epoch [1/4], Step [20025/34168], Loss: 5.2820\n",
      "Epoch [1/4], Step [20100/34168], Loss: 5.1966\n",
      "Epoch [1/4], Step [20175/34168], Loss: 5.3213\n",
      "Epoch [1/4], Step [20250/34168], Loss: 5.3401\n",
      "Epoch [1/4], Step [20325/34168], Loss: 5.4656\n",
      "Epoch [1/4], Step [20400/34168], Loss: 5.3196\n",
      "Epoch [1/4], Step [20475/34168], Loss: 5.4157\n",
      "Epoch [1/4], Step [20550/34168], Loss: 5.3342\n",
      "Epoch [1/4], Step [20625/34168], Loss: 5.1910\n",
      "Epoch [1/4], Step [20700/34168], Loss: 5.3032\n",
      "Epoch [1/4], Step [20775/34168], Loss: 5.2361\n",
      "Epoch [1/4], Step [20850/34168], Loss: 5.1465\n",
      "Epoch [1/4], Step [20925/34168], Loss: 5.3691\n",
      "Epoch [1/4], Step [21000/34168], Loss: 5.3447\n",
      "Epoch [1/4], Step [21075/34168], Loss: 5.1904\n",
      "Epoch [1/4], Step [21150/34168], Loss: 5.2922\n",
      "Epoch [1/4], Step [21225/34168], Loss: 5.3920\n",
      "Epoch [1/4], Step [21300/34168], Loss: 5.2704\n",
      "Epoch [1/4], Step [21375/34168], Loss: 5.3218\n",
      "Epoch [1/4], Step [21450/34168], Loss: 5.2372\n",
      "Epoch [1/4], Step [21525/34168], Loss: 5.2990\n",
      "Epoch [1/4], Step [21600/34168], Loss: 5.4527\n",
      "Epoch [1/4], Step [21675/34168], Loss: 5.2242\n",
      "Epoch [1/4], Step [21750/34168], Loss: 5.4573\n",
      "Epoch [1/4], Step [21825/34168], Loss: 5.3493\n",
      "Epoch [1/4], Step [21900/34168], Loss: 5.2518\n",
      "Epoch [1/4], Step [21975/34168], Loss: 5.3401\n",
      "Epoch [1/4], Step [22050/34168], Loss: 5.3894\n",
      "Epoch [1/4], Step [22125/34168], Loss: 5.2620\n",
      "Epoch [1/4], Step [22200/34168], Loss: 5.3735\n",
      "Epoch [1/4], Step [22275/34168], Loss: 5.3724\n",
      "Epoch [1/4], Step [22350/34168], Loss: 5.3599\n",
      "Epoch [1/4], Step [22425/34168], Loss: 5.2984\n",
      "Epoch [1/4], Step [22500/34168], Loss: 5.3028\n",
      "Epoch [1/4], Step [22575/34168], Loss: 5.3078\n",
      "Epoch [1/4], Step [22650/34168], Loss: 5.3463\n",
      "Epoch [1/4], Step [22725/34168], Loss: 5.3495\n",
      "Epoch [1/4], Step [22800/34168], Loss: 5.2891\n",
      "Epoch [1/4], Step [22875/34168], Loss: 5.2480\n",
      "Epoch [1/4], Step [22950/34168], Loss: 5.2248\n",
      "Epoch [1/4], Step [23025/34168], Loss: 5.3302\n",
      "Epoch [1/4], Step [23100/34168], Loss: 5.2722\n",
      "Epoch [1/4], Step [23175/34168], Loss: 5.4387\n",
      "Epoch [1/4], Step [23250/34168], Loss: 5.4490\n",
      "Epoch [1/4], Step [23325/34168], Loss: 5.3290\n",
      "Epoch [1/4], Step [23400/34168], Loss: 5.5129\n",
      "Epoch [1/4], Step [23475/34168], Loss: 5.2870\n",
      "Epoch [1/4], Step [23550/34168], Loss: 5.3848\n",
      "Epoch [1/4], Step [23625/34168], Loss: 5.2772\n",
      "Epoch [1/4], Step [23700/34168], Loss: 5.4971\n",
      "Epoch [1/4], Step [23775/34168], Loss: 5.3901\n",
      "Epoch [1/4], Step [23850/34168], Loss: 5.3376\n",
      "Epoch [1/4], Step [23925/34168], Loss: 5.1498\n",
      "Epoch [1/4], Step [24000/34168], Loss: 5.2990\n",
      "Epoch [1/4], Step [24075/34168], Loss: 5.3196\n",
      "Epoch [1/4], Step [24150/34168], Loss: 5.2852\n",
      "Epoch [1/4], Step [24225/34168], Loss: 5.3493\n",
      "Epoch [1/4], Step [24300/34168], Loss: 5.3014\n",
      "Epoch [1/4], Step [24375/34168], Loss: 5.2424\n",
      "Epoch [1/4], Step [24450/34168], Loss: 5.2109\n",
      "Epoch [1/4], Step [24525/34168], Loss: 5.3459\n",
      "Epoch [1/4], Step [24600/34168], Loss: 5.3615\n",
      "Epoch [1/4], Step [24675/34168], Loss: 5.2685\n",
      "Epoch [1/4], Step [24750/34168], Loss: 5.4595\n",
      "Epoch [1/4], Step [24825/34168], Loss: 5.2319\n",
      "Epoch [1/4], Step [24900/34168], Loss: 5.3132\n",
      "Epoch [1/4], Step [24975/34168], Loss: 5.3765\n",
      "Epoch [1/4], Step [25050/34168], Loss: 5.2550\n",
      "Epoch [1/4], Step [25125/34168], Loss: 5.3458\n",
      "Epoch [1/4], Step [25200/34168], Loss: 5.2789\n",
      "Epoch [1/4], Step [25275/34168], Loss: 5.3485\n",
      "Epoch [1/4], Step [25350/34168], Loss: 5.2908\n",
      "Epoch [1/4], Step [25425/34168], Loss: 5.2774\n",
      "Epoch [1/4], Step [25500/34168], Loss: 5.2409\n",
      "Epoch [1/4], Step [25575/34168], Loss: 5.3822\n",
      "Epoch [1/4], Step [25650/34168], Loss: 5.1050\n",
      "Epoch [1/4], Step [25725/34168], Loss: 5.2847\n",
      "Epoch [1/4], Step [25800/34168], Loss: 5.3135\n",
      "Epoch [1/4], Step [25875/34168], Loss: 5.2224\n",
      "Epoch [1/4], Step [25950/34168], Loss: 5.2457\n",
      "Epoch [1/4], Step [26025/34168], Loss: 5.2439\n",
      "Epoch [1/4], Step [26100/34168], Loss: 5.3214\n",
      "Epoch [1/4], Step [26175/34168], Loss: 5.2587\n",
      "Epoch [1/4], Step [26250/34168], Loss: 5.2269\n",
      "Epoch [1/4], Step [26325/34168], Loss: 5.2643\n",
      "Epoch [1/4], Step [26400/34168], Loss: 5.3596\n",
      "Epoch [1/4], Step [26475/34168], Loss: 5.2627\n",
      "Epoch [1/4], Step [26550/34168], Loss: 5.3090\n",
      "Epoch [1/4], Step [26625/34168], Loss: 5.3508\n",
      "Epoch [1/4], Step [26700/34168], Loss: 5.1963\n",
      "Epoch [1/4], Step [26775/34168], Loss: 5.2299\n",
      "Epoch [1/4], Step [26850/34168], Loss: 5.2890\n",
      "Epoch [1/4], Step [26925/34168], Loss: 5.1931\n",
      "Epoch [1/4], Step [27000/34168], Loss: 5.3662\n",
      "Epoch [1/4], Step [27075/34168], Loss: 5.3408\n",
      "Epoch [1/4], Step [27150/34168], Loss: 5.3866\n",
      "Epoch [1/4], Step [27225/34168], Loss: 5.3926\n",
      "Epoch [1/4], Step [27300/34168], Loss: 5.1980\n",
      "Epoch [1/4], Step [27375/34168], Loss: 5.2774\n",
      "Epoch [1/4], Step [27450/34168], Loss: 5.2278\n",
      "Epoch [1/4], Step [27525/34168], Loss: 5.1424\n",
      "Epoch [1/4], Step [27600/34168], Loss: 5.2625\n",
      "Epoch [1/4], Step [27675/34168], Loss: 5.3911\n",
      "Epoch [1/4], Step [27750/34168], Loss: 5.2883\n",
      "Epoch [1/4], Step [27825/34168], Loss: 5.2253\n",
      "Epoch [1/4], Step [27900/34168], Loss: 5.1884\n",
      "Epoch [1/4], Step [27975/34168], Loss: 5.2615\n",
      "Epoch [1/4], Step [28050/34168], Loss: 5.2971\n",
      "Epoch [1/4], Step [28125/34168], Loss: 5.1292\n",
      "Epoch [1/4], Step [28200/34168], Loss: 5.2125\n",
      "Epoch [1/4], Step [28275/34168], Loss: 5.1787\n",
      "Epoch [1/4], Step [28350/34168], Loss: 5.3809\n",
      "Epoch [1/4], Step [28425/34168], Loss: 5.1952\n",
      "Epoch [1/4], Step [28500/34168], Loss: 5.2765\n",
      "Epoch [1/4], Step [28575/34168], Loss: 5.3178\n",
      "Epoch [1/4], Step [28650/34168], Loss: 5.2744\n",
      "Epoch [1/4], Step [28725/34168], Loss: 5.1540\n",
      "Epoch [1/4], Step [28800/34168], Loss: 5.3521\n",
      "Epoch [1/4], Step [28875/34168], Loss: 5.3514\n",
      "Epoch [1/4], Step [28950/34168], Loss: 5.3433\n",
      "Epoch [1/4], Step [29025/34168], Loss: 5.1730\n",
      "Epoch [1/4], Step [29100/34168], Loss: 5.2447\n",
      "Epoch [1/4], Step [29175/34168], Loss: 5.2410\n",
      "Epoch [1/4], Step [29250/34168], Loss: 5.2722\n",
      "Epoch [1/4], Step [29325/34168], Loss: 5.3410\n",
      "Epoch [1/4], Step [29400/34168], Loss: 5.2931\n",
      "Epoch [1/4], Step [29475/34168], Loss: 5.1571\n",
      "Epoch [1/4], Step [29550/34168], Loss: 5.1621\n",
      "Epoch [1/4], Step [29625/34168], Loss: 5.1649\n",
      "Epoch [1/4], Step [29700/34168], Loss: 5.2101\n",
      "Epoch [1/4], Step [29775/34168], Loss: 5.2131\n",
      "Epoch [1/4], Step [29850/34168], Loss: 5.3226\n",
      "Epoch [1/4], Step [29925/34168], Loss: 5.2462\n",
      "Epoch [1/4], Step [30000/34168], Loss: 5.3586\n",
      "Validation perplexity: 166.5700793640042\n",
      "Epoch [1/4], Step [30075/34168], Loss: 5.2228\n",
      "Epoch [1/4], Step [30150/34168], Loss: 5.1820\n",
      "Epoch [1/4], Step [30225/34168], Loss: 5.2678\n",
      "Epoch [1/4], Step [30300/34168], Loss: 5.1647\n",
      "Epoch [1/4], Step [30375/34168], Loss: 5.3165\n",
      "Epoch [1/4], Step [30450/34168], Loss: 5.2904\n",
      "Epoch [1/4], Step [30525/34168], Loss: 5.2565\n",
      "Epoch [1/4], Step [30600/34168], Loss: 5.2030\n",
      "Epoch [1/4], Step [30675/34168], Loss: 5.1848\n",
      "Epoch [1/4], Step [30750/34168], Loss: 5.2475\n",
      "Epoch [1/4], Step [30825/34168], Loss: 5.2679\n",
      "Epoch [1/4], Step [30900/34168], Loss: 5.3040\n",
      "Epoch [1/4], Step [30975/34168], Loss: 5.2755\n",
      "Epoch [1/4], Step [31050/34168], Loss: 5.1817\n",
      "Epoch [1/4], Step [31125/34168], Loss: 5.2171\n",
      "Epoch [1/4], Step [31200/34168], Loss: 5.2584\n",
      "Epoch [1/4], Step [31275/34168], Loss: 5.2985\n",
      "Epoch [1/4], Step [31350/34168], Loss: 5.1833\n",
      "Epoch [1/4], Step [31425/34168], Loss: 5.2576\n",
      "Epoch [1/4], Step [31500/34168], Loss: 5.2386\n",
      "Epoch [1/4], Step [31575/34168], Loss: 5.3655\n",
      "Epoch [1/4], Step [31650/34168], Loss: 5.3623\n",
      "Epoch [1/4], Step [31725/34168], Loss: 5.3112\n",
      "Epoch [1/4], Step [31800/34168], Loss: 5.3131\n",
      "Epoch [1/4], Step [31875/34168], Loss: 5.1758\n",
      "Epoch [1/4], Step [31950/34168], Loss: 5.2883\n",
      "Epoch [1/4], Step [32025/34168], Loss: 5.2255\n",
      "Epoch [1/4], Step [32100/34168], Loss: 5.1720\n",
      "Epoch [1/4], Step [32175/34168], Loss: 5.0958\n",
      "Epoch [1/4], Step [32250/34168], Loss: 5.2880\n",
      "Epoch [1/4], Step [32325/34168], Loss: 5.2371\n",
      "Epoch [1/4], Step [32400/34168], Loss: 5.1639\n",
      "Epoch [1/4], Step [32475/34168], Loss: 5.1177\n",
      "Epoch [1/4], Step [32550/34168], Loss: 5.2937\n",
      "Epoch [1/4], Step [32625/34168], Loss: 5.2862\n",
      "Epoch [1/4], Step [32700/34168], Loss: 5.1345\n",
      "Epoch [1/4], Step [32775/34168], Loss: 5.4028\n",
      "Epoch [1/4], Step [32850/34168], Loss: 5.2541\n",
      "Epoch [1/4], Step [32925/34168], Loss: 5.2252\n",
      "Epoch [1/4], Step [33000/34168], Loss: 5.3740\n",
      "Epoch [1/4], Step [33075/34168], Loss: 5.3600\n",
      "Epoch [1/4], Step [33150/34168], Loss: 5.2856\n",
      "Epoch [1/4], Step [33225/34168], Loss: 5.3739\n",
      "Epoch [1/4], Step [33300/34168], Loss: 5.3235\n",
      "Epoch [1/4], Step [33375/34168], Loss: 5.1760\n",
      "Epoch [1/4], Step [33450/34168], Loss: 5.1581\n",
      "Epoch [1/4], Step [33525/34168], Loss: 5.3093\n",
      "Epoch [1/4], Step [33600/34168], Loss: 5.3011\n",
      "Epoch [1/4], Step [33675/34168], Loss: 5.2314\n",
      "Epoch [1/4], Step [33750/34168], Loss: 5.3023\n",
      "Epoch [1/4], Step [33825/34168], Loss: 5.2423\n",
      "Epoch [1/4], Step [33900/34168], Loss: 5.2988\n",
      "Epoch [1/4], Step [33975/34168], Loss: 5.2185\n",
      "Epoch [1/4], Step [34050/34168], Loss: 5.2050\n",
      "Epoch [1/4], Step [34125/34168], Loss: 5.2170\n",
      "Epoch [1/4] Average Loss: 5.4938, Perplexity: 243.17\n",
      "Epoch [2/4], Step [0/34168], Loss: 5.0952\n",
      "Validation perplexity: 162.54871579599399\n",
      "Epoch [2/4], Step [75/34168], Loss: 5.1782\n",
      "Epoch [2/4], Step [150/34168], Loss: 5.1976\n",
      "Epoch [2/4], Step [225/34168], Loss: 5.4072\n",
      "Epoch [2/4], Step [300/34168], Loss: 5.2184\n",
      "Epoch [2/4], Step [375/34168], Loss: 5.2401\n",
      "Epoch [2/4], Step [450/34168], Loss: 5.1600\n",
      "Epoch [2/4], Step [525/34168], Loss: 5.2559\n",
      "Epoch [2/4], Step [600/34168], Loss: 5.2495\n",
      "Epoch [2/4], Step [675/34168], Loss: 5.3301\n",
      "Epoch [2/4], Step [750/34168], Loss: 5.1534\n",
      "Epoch [2/4], Step [825/34168], Loss: 5.2430\n",
      "Epoch [2/4], Step [900/34168], Loss: 5.1962\n",
      "Epoch [2/4], Step [975/34168], Loss: 5.3119\n",
      "Epoch [2/4], Step [1050/34168], Loss: 5.1527\n",
      "Epoch [2/4], Step [1125/34168], Loss: 5.2218\n",
      "Epoch [2/4], Step [1200/34168], Loss: 5.2428\n",
      "Epoch [2/4], Step [1275/34168], Loss: 5.2528\n",
      "Epoch [2/4], Step [1350/34168], Loss: 5.2073\n",
      "Epoch [2/4], Step [1425/34168], Loss: 5.3261\n",
      "Epoch [2/4], Step [1500/34168], Loss: 5.0478\n",
      "Epoch [2/4], Step [1575/34168], Loss: 5.3581\n",
      "Epoch [2/4], Step [1650/34168], Loss: 5.3121\n",
      "Epoch [2/4], Step [1725/34168], Loss: 5.1984\n",
      "Epoch [2/4], Step [1800/34168], Loss: 5.2103\n",
      "Epoch [2/4], Step [1875/34168], Loss: 5.2097\n",
      "Epoch [2/4], Step [1950/34168], Loss: 5.1240\n",
      "Epoch [2/4], Step [2025/34168], Loss: 5.2889\n",
      "Epoch [2/4], Step [2100/34168], Loss: 5.1919\n",
      "Epoch [2/4], Step [2175/34168], Loss: 5.2526\n",
      "Epoch [2/4], Step [2250/34168], Loss: 5.3478\n",
      "Epoch [2/4], Step [2325/34168], Loss: 5.1791\n",
      "Epoch [2/4], Step [2400/34168], Loss: 5.1792\n",
      "Epoch [2/4], Step [2475/34168], Loss: 5.1621\n",
      "Epoch [2/4], Step [2550/34168], Loss: 5.2457\n",
      "Epoch [2/4], Step [2625/34168], Loss: 5.1667\n",
      "Epoch [2/4], Step [2700/34168], Loss: 5.1421\n",
      "Epoch [2/4], Step [2775/34168], Loss: 5.1787\n",
      "Epoch [2/4], Step [2850/34168], Loss: 5.1685\n",
      "Epoch [2/4], Step [2925/34168], Loss: 5.3031\n",
      "Epoch [2/4], Step [3000/34168], Loss: 5.3242\n",
      "Epoch [2/4], Step [3075/34168], Loss: 5.1034\n",
      "Epoch [2/4], Step [3150/34168], Loss: 5.2086\n",
      "Epoch [2/4], Step [3225/34168], Loss: 5.2211\n",
      "Epoch [2/4], Step [3300/34168], Loss: 5.0945\n",
      "Epoch [2/4], Step [3375/34168], Loss: 5.1381\n",
      "Epoch [2/4], Step [3450/34168], Loss: 5.1291\n",
      "Epoch [2/4], Step [3525/34168], Loss: 5.1602\n",
      "Epoch [2/4], Step [3600/34168], Loss: 5.2249\n",
      "Epoch [2/4], Step [3675/34168], Loss: 5.1327\n",
      "Epoch [2/4], Step [3750/34168], Loss: 5.1827\n",
      "Epoch [2/4], Step [3825/34168], Loss: 5.3208\n",
      "Epoch [2/4], Step [3900/34168], Loss: 5.2971\n",
      "Epoch [2/4], Step [3975/34168], Loss: 5.2649\n",
      "Epoch [2/4], Step [4050/34168], Loss: 5.2120\n",
      "Epoch [2/4], Step [4125/34168], Loss: 5.2839\n",
      "Epoch [2/4], Step [4200/34168], Loss: 5.1252\n",
      "Epoch [2/4], Step [4275/34168], Loss: 5.0649\n",
      "Epoch [2/4], Step [4350/34168], Loss: 5.2577\n",
      "Epoch [2/4], Step [4425/34168], Loss: 5.0972\n",
      "Epoch [2/4], Step [4500/34168], Loss: 5.3263\n",
      "Epoch [2/4], Step [4575/34168], Loss: 5.2881\n",
      "Epoch [2/4], Step [4650/34168], Loss: 5.2740\n",
      "Epoch [2/4], Step [4725/34168], Loss: 5.2303\n",
      "Epoch [2/4], Step [4800/34168], Loss: 5.3558\n",
      "Epoch [2/4], Step [4875/34168], Loss: 5.1380\n",
      "Epoch [2/4], Step [4950/34168], Loss: 5.3191\n",
      "Epoch [2/4], Step [5025/34168], Loss: 5.2103\n",
      "Epoch [2/4], Step [5100/34168], Loss: 5.3295\n",
      "Epoch [2/4], Step [5175/34168], Loss: 5.0577\n",
      "Epoch [2/4], Step [5250/34168], Loss: 5.2146\n",
      "Epoch [2/4], Step [5325/34168], Loss: 5.2836\n",
      "Epoch [2/4], Step [5400/34168], Loss: 5.2401\n",
      "Epoch [2/4], Step [5475/34168], Loss: 5.2799\n",
      "Epoch [2/4], Step [5550/34168], Loss: 5.1677\n",
      "Epoch [2/4], Step [5625/34168], Loss: 5.2824\n",
      "Epoch [2/4], Step [5700/34168], Loss: 5.2709\n",
      "Epoch [2/4], Step [5775/34168], Loss: 5.0796\n",
      "Epoch [2/4], Step [5850/34168], Loss: 5.2340\n",
      "Epoch [2/4], Step [5925/34168], Loss: 5.1816\n",
      "Epoch [2/4], Step [6000/34168], Loss: 5.3209\n",
      "Epoch [2/4], Step [6075/34168], Loss: 5.2120\n",
      "Epoch [2/4], Step [6150/34168], Loss: 5.2929\n",
      "Epoch [2/4], Step [6225/34168], Loss: 5.2908\n",
      "Epoch [2/4], Step [6300/34168], Loss: 5.2359\n",
      "Epoch [2/4], Step [6375/34168], Loss: 5.1859\n",
      "Epoch [2/4], Step [6450/34168], Loss: 5.1967\n",
      "Epoch [2/4], Step [6525/34168], Loss: 5.1663\n",
      "Epoch [2/4], Step [6600/34168], Loss: 5.2515\n",
      "Epoch [2/4], Step [6675/34168], Loss: 5.1541\n",
      "Epoch [2/4], Step [6750/34168], Loss: 5.1661\n",
      "Epoch [2/4], Step [6825/34168], Loss: 5.1872\n",
      "Epoch [2/4], Step [6900/34168], Loss: 5.2024\n",
      "Epoch [2/4], Step [6975/34168], Loss: 5.1711\n",
      "Epoch [2/4], Step [7050/34168], Loss: 5.1665\n",
      "Epoch [2/4], Step [7125/34168], Loss: 5.1478\n",
      "Epoch [2/4], Step [7200/34168], Loss: 5.1360\n",
      "Epoch [2/4], Step [7275/34168], Loss: 5.1989\n",
      "Epoch [2/4], Step [7350/34168], Loss: 5.2005\n",
      "Epoch [2/4], Step [7425/34168], Loss: 5.1131\n",
      "Epoch [2/4], Step [7500/34168], Loss: 5.2329\n",
      "Epoch [2/4], Step [7575/34168], Loss: 5.2044\n",
      "Epoch [2/4], Step [7650/34168], Loss: 5.2657\n",
      "Epoch [2/4], Step [7725/34168], Loss: 5.1681\n",
      "Epoch [2/4], Step [7800/34168], Loss: 5.2158\n",
      "Epoch [2/4], Step [7875/34168], Loss: 5.1167\n",
      "Epoch [2/4], Step [7950/34168], Loss: 5.2120\n",
      "Epoch [2/4], Step [8025/34168], Loss: 5.3752\n",
      "Epoch [2/4], Step [8100/34168], Loss: 5.1798\n",
      "Epoch [2/4], Step [8175/34168], Loss: 5.2012\n",
      "Epoch [2/4], Step [8250/34168], Loss: 5.2233\n",
      "Epoch [2/4], Step [8325/34168], Loss: 5.2245\n",
      "Epoch [2/4], Step [8400/34168], Loss: 5.1979\n",
      "Epoch [2/4], Step [8475/34168], Loss: 5.2549\n",
      "Epoch [2/4], Step [8550/34168], Loss: 5.0854\n",
      "Epoch [2/4], Step [8625/34168], Loss: 5.1832\n",
      "Epoch [2/4], Step [8700/34168], Loss: 5.2690\n",
      "Epoch [2/4], Step [8775/34168], Loss: 5.2077\n",
      "Epoch [2/4], Step [8850/34168], Loss: 5.1855\n",
      "Epoch [2/4], Step [8925/34168], Loss: 5.2268\n",
      "Epoch [2/4], Step [9000/34168], Loss: 5.1140\n",
      "Epoch [2/4], Step [9075/34168], Loss: 5.1569\n",
      "Epoch [2/4], Step [9150/34168], Loss: 5.2084\n",
      "Epoch [2/4], Step [9225/34168], Loss: 5.2030\n",
      "Epoch [2/4], Step [9300/34168], Loss: 5.0756\n",
      "Epoch [2/4], Step [9375/34168], Loss: 5.1547\n",
      "Epoch [2/4], Step [9450/34168], Loss: 5.1730\n",
      "Epoch [2/4], Step [9525/34168], Loss: 5.1563\n",
      "Epoch [2/4], Step [9600/34168], Loss: 5.2103\n",
      "Epoch [2/4], Step [9675/34168], Loss: 5.0991\n",
      "Epoch [2/4], Step [9750/34168], Loss: 5.2744\n",
      "Epoch [2/4], Step [9825/34168], Loss: 5.1278\n",
      "Epoch [2/4], Step [9900/34168], Loss: 5.1418\n",
      "Epoch [2/4], Step [9975/34168], Loss: 5.3098\n",
      "Validation perplexity: 155.2416874743791\n",
      "Epoch [2/4], Step [10050/34168], Loss: 5.2849\n",
      "Epoch [2/4], Step [10125/34168], Loss: 5.1724\n",
      "Epoch [2/4], Step [10200/34168], Loss: 5.0900\n",
      "Epoch [2/4], Step [10275/34168], Loss: 5.1104\n",
      "Epoch [2/4], Step [10350/34168], Loss: 5.1780\n",
      "Epoch [2/4], Step [10425/34168], Loss: 5.2314\n",
      "Epoch [2/4], Step [10500/34168], Loss: 5.0161\n",
      "Epoch [2/4], Step [10575/34168], Loss: 5.2498\n",
      "Epoch [2/4], Step [10650/34168], Loss: 5.0721\n",
      "Epoch [2/4], Step [10725/34168], Loss: 5.1695\n",
      "Epoch [2/4], Step [10800/34168], Loss: 5.2797\n",
      "Epoch [2/4], Step [10875/34168], Loss: 5.1607\n",
      "Epoch [2/4], Step [10950/34168], Loss: 5.0394\n",
      "Epoch [2/4], Step [11025/34168], Loss: 5.1740\n",
      "Epoch [2/4], Step [11100/34168], Loss: 5.2402\n",
      "Epoch [2/4], Step [11175/34168], Loss: 5.1661\n",
      "Epoch [2/4], Step [11250/34168], Loss: 5.2431\n",
      "Epoch [2/4], Step [11325/34168], Loss: 5.1702\n",
      "Epoch [2/4], Step [11400/34168], Loss: 5.1629\n",
      "Epoch [2/4], Step [11475/34168], Loss: 5.2512\n",
      "Epoch [2/4], Step [11550/34168], Loss: 5.1398\n",
      "Epoch [2/4], Step [11625/34168], Loss: 5.1290\n",
      "Epoch [2/4], Step [11700/34168], Loss: 5.0670\n",
      "Epoch [2/4], Step [11775/34168], Loss: 5.1709\n",
      "Epoch [2/4], Step [11850/34168], Loss: 5.2433\n",
      "Epoch [2/4], Step [11925/34168], Loss: 5.1324\n",
      "Epoch [2/4], Step [12000/34168], Loss: 5.2191\n",
      "Epoch [2/4], Step [12075/34168], Loss: 5.3042\n",
      "Epoch [2/4], Step [12150/34168], Loss: 5.1467\n",
      "Epoch [2/4], Step [12225/34168], Loss: 5.1685\n",
      "Epoch [2/4], Step [12300/34168], Loss: 5.1850\n",
      "Epoch [2/4], Step [12375/34168], Loss: 5.1430\n",
      "Epoch [2/4], Step [12450/34168], Loss: 5.0184\n",
      "Epoch [2/4], Step [12525/34168], Loss: 5.2126\n",
      "Epoch [2/4], Step [12600/34168], Loss: 5.1418\n",
      "Epoch [2/4], Step [12675/34168], Loss: 5.1707\n",
      "Epoch [2/4], Step [12750/34168], Loss: 5.0643\n",
      "Epoch [2/4], Step [12825/34168], Loss: 5.1943\n",
      "Epoch [2/4], Step [12900/34168], Loss: 5.1075\n",
      "Epoch [2/4], Step [12975/34168], Loss: 5.1548\n",
      "Epoch [2/4], Step [13050/34168], Loss: 5.2014\n",
      "Epoch [2/4], Step [13125/34168], Loss: 5.2365\n",
      "Epoch [2/4], Step [13200/34168], Loss: 5.1265\n",
      "Epoch [2/4], Step [13275/34168], Loss: 5.1642\n",
      "Epoch [2/4], Step [13350/34168], Loss: 5.2010\n",
      "Epoch [2/4], Step [13425/34168], Loss: 5.2266\n",
      "Epoch [2/4], Step [13500/34168], Loss: 5.3016\n",
      "Epoch [2/4], Step [13575/34168], Loss: 5.2700\n",
      "Epoch [2/4], Step [13650/34168], Loss: 5.2517\n",
      "Epoch [2/4], Step [13725/34168], Loss: 5.2731\n",
      "Epoch [2/4], Step [13800/34168], Loss: 5.1467\n",
      "Epoch [2/4], Step [13875/34168], Loss: 5.2818\n",
      "Epoch [2/4], Step [13950/34168], Loss: 5.2768\n",
      "Epoch [2/4], Step [14025/34168], Loss: 5.0999\n",
      "Epoch [2/4], Step [14100/34168], Loss: 5.2246\n",
      "Epoch [2/4], Step [14175/34168], Loss: 5.0968\n",
      "Epoch [2/4], Step [14250/34168], Loss: 5.1688\n",
      "Epoch [2/4], Step [14325/34168], Loss: 5.0817\n",
      "Epoch [2/4], Step [14400/34168], Loss: 5.2488\n",
      "Epoch [2/4], Step [14475/34168], Loss: 5.2262\n",
      "Epoch [2/4], Step [14550/34168], Loss: 5.1336\n",
      "Epoch [2/4], Step [14625/34168], Loss: 5.1484\n",
      "Epoch [2/4], Step [14700/34168], Loss: 5.1288\n",
      "Epoch [2/4], Step [14775/34168], Loss: 5.1980\n",
      "Epoch [2/4], Step [14850/34168], Loss: 5.1715\n",
      "Epoch [2/4], Step [14925/34168], Loss: 5.2351\n",
      "Epoch [2/4], Step [15000/34168], Loss: 5.1221\n",
      "Epoch [2/4], Step [15075/34168], Loss: 5.1710\n",
      "Epoch [2/4], Step [15150/34168], Loss: 5.1080\n",
      "Epoch [2/4], Step [15225/34168], Loss: 5.1065\n",
      "Epoch [2/4], Step [15300/34168], Loss: 5.1266\n",
      "Epoch [2/4], Step [15375/34168], Loss: 5.1346\n",
      "Epoch [2/4], Step [15450/34168], Loss: 5.1634\n",
      "Epoch [2/4], Step [15525/34168], Loss: 5.1767\n",
      "Epoch [2/4], Step [15600/34168], Loss: 5.1501\n",
      "Epoch [2/4], Step [15675/34168], Loss: 5.0800\n",
      "Epoch [2/4], Step [15750/34168], Loss: 5.2161\n",
      "Epoch [2/4], Step [15825/34168], Loss: 5.1902\n",
      "Epoch [2/4], Step [15900/34168], Loss: 5.1688\n",
      "Epoch [2/4], Step [15975/34168], Loss: 5.2104\n",
      "Epoch [2/4], Step [16050/34168], Loss: 5.1945\n",
      "Epoch [2/4], Step [16125/34168], Loss: 5.2331\n",
      "Epoch [2/4], Step [16200/34168], Loss: 5.1209\n",
      "Epoch [2/4], Step [16275/34168], Loss: 5.1095\n",
      "Epoch [2/4], Step [16350/34168], Loss: 5.2243\n",
      "Epoch [2/4], Step [16425/34168], Loss: 5.1826\n",
      "Epoch [2/4], Step [16500/34168], Loss: 5.1315\n",
      "Epoch [2/4], Step [16575/34168], Loss: 5.1326\n",
      "Epoch [2/4], Step [16650/34168], Loss: 5.1494\n",
      "Epoch [2/4], Step [16725/34168], Loss: 5.1330\n",
      "Epoch [2/4], Step [16800/34168], Loss: 5.1359\n",
      "Epoch [2/4], Step [16875/34168], Loss: 5.3177\n",
      "Epoch [2/4], Step [16950/34168], Loss: 5.2639\n",
      "Epoch [2/4], Step [17025/34168], Loss: 5.2245\n",
      "Epoch [2/4], Step [17100/34168], Loss: 5.2278\n",
      "Epoch [2/4], Step [17175/34168], Loss: 5.2386\n",
      "Epoch [2/4], Step [17250/34168], Loss: 5.2129\n",
      "Epoch [2/4], Step [17325/34168], Loss: 5.1012\n",
      "Epoch [2/4], Step [17400/34168], Loss: 5.1079\n",
      "Epoch [2/4], Step [17475/34168], Loss: 5.1718\n",
      "Epoch [2/4], Step [17550/34168], Loss: 5.0945\n",
      "Epoch [2/4], Step [17625/34168], Loss: 5.1966\n",
      "Epoch [2/4], Step [17700/34168], Loss: 5.1708\n",
      "Epoch [2/4], Step [17775/34168], Loss: 5.1621\n",
      "Epoch [2/4], Step [17850/34168], Loss: 5.1334\n",
      "Epoch [2/4], Step [17925/34168], Loss: 5.0184\n",
      "Epoch [2/4], Step [18000/34168], Loss: 5.1284\n",
      "Epoch [2/4], Step [18075/34168], Loss: 5.0967\n",
      "Epoch [2/4], Step [18150/34168], Loss: 5.1377\n",
      "Epoch [2/4], Step [18225/34168], Loss: 5.1685\n",
      "Epoch [2/4], Step [18300/34168], Loss: 5.2022\n",
      "Epoch [2/4], Step [18375/34168], Loss: 5.2268\n",
      "Epoch [2/4], Step [18450/34168], Loss: 5.1909\n",
      "Epoch [2/4], Step [18525/34168], Loss: 5.0968\n",
      "Epoch [2/4], Step [18600/34168], Loss: 5.0951\n",
      "Epoch [2/4], Step [18675/34168], Loss: 5.1387\n",
      "Epoch [2/4], Step [18750/34168], Loss: 5.2029\n",
      "Epoch [2/4], Step [18825/34168], Loss: 5.0887\n",
      "Epoch [2/4], Step [18900/34168], Loss: 5.1375\n",
      "Epoch [2/4], Step [18975/34168], Loss: 5.1798\n",
      "Epoch [2/4], Step [19050/34168], Loss: 5.1474\n",
      "Epoch [2/4], Step [19125/34168], Loss: 5.0550\n",
      "Epoch [2/4], Step [19200/34168], Loss: 5.1344\n",
      "Epoch [2/4], Step [19275/34168], Loss: 5.0074\n",
      "Epoch [2/4], Step [19350/34168], Loss: 5.1554\n",
      "Epoch [2/4], Step [19425/34168], Loss: 5.1290\n",
      "Epoch [2/4], Step [19500/34168], Loss: 5.1290\n",
      "Epoch [2/4], Step [19575/34168], Loss: 5.1359\n",
      "Epoch [2/4], Step [19650/34168], Loss: 5.0875\n",
      "Epoch [2/4], Step [19725/34168], Loss: 5.0924\n",
      "Epoch [2/4], Step [19800/34168], Loss: 5.1440\n",
      "Epoch [2/4], Step [19875/34168], Loss: 5.1623\n",
      "Epoch [2/4], Step [19950/34168], Loss: 5.1305\n",
      "Validation perplexity: 149.97461685594035\n",
      "Epoch [2/4], Step [20025/34168], Loss: 5.1018\n",
      "Epoch [2/4], Step [20100/34168], Loss: 5.1757\n",
      "Epoch [2/4], Step [20175/34168], Loss: 5.2470\n",
      "Epoch [2/4], Step [20250/34168], Loss: 5.2366\n",
      "Epoch [2/4], Step [20325/34168], Loss: 5.0861\n",
      "Epoch [2/4], Step [20400/34168], Loss: 5.1156\n",
      "Epoch [2/4], Step [20475/34168], Loss: 5.1631\n",
      "Epoch [2/4], Step [20550/34168], Loss: 5.1421\n",
      "Epoch [2/4], Step [20625/34168], Loss: 5.2718\n",
      "Epoch [2/4], Step [20700/34168], Loss: 5.1781\n",
      "Epoch [2/4], Step [20775/34168], Loss: 5.2179\n",
      "Epoch [2/4], Step [20850/34168], Loss: 5.1453\n",
      "Epoch [2/4], Step [20925/34168], Loss: 5.1738\n",
      "Epoch [2/4], Step [21000/34168], Loss: 5.1509\n",
      "Epoch [2/4], Step [21075/34168], Loss: 5.0856\n",
      "Epoch [2/4], Step [21150/34168], Loss: 5.2173\n",
      "Epoch [2/4], Step [21225/34168], Loss: 5.1120\n",
      "Epoch [2/4], Step [21300/34168], Loss: 5.1461\n",
      "Epoch [2/4], Step [21375/34168], Loss: 5.1563\n",
      "Epoch [2/4], Step [21450/34168], Loss: 5.1560\n",
      "Epoch [2/4], Step [21525/34168], Loss: 5.1147\n",
      "Epoch [2/4], Step [21600/34168], Loss: 4.9911\n",
      "Epoch [2/4], Step [21675/34168], Loss: 5.2269\n",
      "Epoch [2/4], Step [21750/34168], Loss: 5.1659\n",
      "Epoch [2/4], Step [21825/34168], Loss: 5.2045\n",
      "Epoch [2/4], Step [21900/34168], Loss: 5.0965\n",
      "Epoch [2/4], Step [21975/34168], Loss: 5.1818\n",
      "Epoch [2/4], Step [22050/34168], Loss: 5.3038\n",
      "Epoch [2/4], Step [22125/34168], Loss: 5.2342\n",
      "Epoch [2/4], Step [22200/34168], Loss: 5.2840\n",
      "Epoch [2/4], Step [22275/34168], Loss: 5.2656\n",
      "Epoch [2/4], Step [22350/34168], Loss: 5.0944\n",
      "Epoch [2/4], Step [22425/34168], Loss: 5.1683\n",
      "Epoch [2/4], Step [22500/34168], Loss: 4.9582\n",
      "Epoch [2/4], Step [22575/34168], Loss: 5.1506\n",
      "Epoch [2/4], Step [22650/34168], Loss: 5.1106\n",
      "Epoch [2/4], Step [22725/34168], Loss: 5.2219\n",
      "Epoch [2/4], Step [22800/34168], Loss: 5.2164\n",
      "Epoch [2/4], Step [22875/34168], Loss: 5.2760\n",
      "Epoch [2/4], Step [22950/34168], Loss: 5.1953\n",
      "Epoch [2/4], Step [23025/34168], Loss: 5.0734\n",
      "Epoch [2/4], Step [23100/34168], Loss: 5.0065\n",
      "Epoch [2/4], Step [23175/34168], Loss: 5.1839\n",
      "Epoch [2/4], Step [23250/34168], Loss: 5.1806\n",
      "Epoch [2/4], Step [23325/34168], Loss: 5.1180\n",
      "Epoch [2/4], Step [23400/34168], Loss: 5.0531\n",
      "Epoch [2/4], Step [23475/34168], Loss: 5.0400\n",
      "Epoch [2/4], Step [23550/34168], Loss: 5.0812\n",
      "Epoch [2/4], Step [23625/34168], Loss: 5.1432\n",
      "Epoch [2/4], Step [23700/34168], Loss: 5.1674\n",
      "Epoch [2/4], Step [23775/34168], Loss: 5.1037\n",
      "Epoch [2/4], Step [23850/34168], Loss: 5.0969\n",
      "Epoch [2/4], Step [23925/34168], Loss: 5.1206\n",
      "Epoch [2/4], Step [24000/34168], Loss: 5.1711\n",
      "Epoch [2/4], Step [24075/34168], Loss: 5.0981\n",
      "Epoch [2/4], Step [24150/34168], Loss: 5.1739\n",
      "Epoch [2/4], Step [24225/34168], Loss: 5.2135\n",
      "Epoch [2/4], Step [24300/34168], Loss: 5.0787\n",
      "Epoch [2/4], Step [24375/34168], Loss: 5.1364\n",
      "Epoch [2/4], Step [24450/34168], Loss: 5.1073\n",
      "Epoch [2/4], Step [24525/34168], Loss: 5.0626\n",
      "Epoch [2/4], Step [24600/34168], Loss: 5.1775\n",
      "Epoch [2/4], Step [24675/34168], Loss: 5.0684\n",
      "Epoch [2/4], Step [24750/34168], Loss: 5.0821\n",
      "Epoch [2/4], Step [24825/34168], Loss: 5.0467\n",
      "Epoch [2/4], Step [24900/34168], Loss: 5.2082\n",
      "Epoch [2/4], Step [24975/34168], Loss: 5.1817\n",
      "Epoch [2/4], Step [25050/34168], Loss: 5.1483\n",
      "Epoch [2/4], Step [25125/34168], Loss: 5.1597\n",
      "Epoch [2/4], Step [25200/34168], Loss: 5.1844\n",
      "Epoch [2/4], Step [25275/34168], Loss: 5.0908\n",
      "Epoch [2/4], Step [25350/34168], Loss: 5.1025\n",
      "Epoch [2/4], Step [25425/34168], Loss: 5.2038\n",
      "Epoch [2/4], Step [25500/34168], Loss: 5.1326\n",
      "Epoch [2/4], Step [25575/34168], Loss: 5.0830\n",
      "Epoch [2/4], Step [25650/34168], Loss: 5.2232\n",
      "Epoch [2/4], Step [25725/34168], Loss: 5.0929\n",
      "Epoch [2/4], Step [25800/34168], Loss: 5.1493\n",
      "Epoch [2/4], Step [25875/34168], Loss: 5.1252\n",
      "Epoch [2/4], Step [25950/34168], Loss: 5.2730\n",
      "Epoch [2/4], Step [26025/34168], Loss: 5.1226\n",
      "Epoch [2/4], Step [26100/34168], Loss: 5.1050\n",
      "Epoch [2/4], Step [26175/34168], Loss: 5.1758\n",
      "Epoch [2/4], Step [26250/34168], Loss: 5.1981\n",
      "Epoch [2/4], Step [26325/34168], Loss: 5.1336\n",
      "Epoch [2/4], Step [26400/34168], Loss: 5.1058\n",
      "Epoch [2/4], Step [26475/34168], Loss: 5.2125\n",
      "Epoch [2/4], Step [26550/34168], Loss: 5.1193\n",
      "Epoch [2/4], Step [26625/34168], Loss: 5.1074\n",
      "Epoch [2/4], Step [26700/34168], Loss: 5.0802\n",
      "Epoch [2/4], Step [26775/34168], Loss: 5.1182\n",
      "Epoch [2/4], Step [26850/34168], Loss: 5.1613\n",
      "Epoch [2/4], Step [26925/34168], Loss: 5.1226\n",
      "Epoch [2/4], Step [27000/34168], Loss: 5.1515\n",
      "Epoch [2/4], Step [27075/34168], Loss: 5.2952\n",
      "Epoch [2/4], Step [27150/34168], Loss: 5.2354\n",
      "Epoch [2/4], Step [27225/34168], Loss: 5.1299\n",
      "Epoch [2/4], Step [27300/34168], Loss: 5.1023\n",
      "Epoch [2/4], Step [27375/34168], Loss: 5.1591\n",
      "Epoch [2/4], Step [27450/34168], Loss: 5.1125\n",
      "Epoch [2/4], Step [27525/34168], Loss: 5.2020\n",
      "Epoch [2/4], Step [27600/34168], Loss: 5.2036\n",
      "Epoch [2/4], Step [27675/34168], Loss: 5.1874\n",
      "Epoch [2/4], Step [27750/34168], Loss: 5.2265\n",
      "Epoch [2/4], Step [27825/34168], Loss: 5.0954\n",
      "Epoch [2/4], Step [27900/34168], Loss: 5.0948\n",
      "Epoch [2/4], Step [27975/34168], Loss: 5.0820\n",
      "Epoch [2/4], Step [28050/34168], Loss: 5.1380\n",
      "Epoch [2/4], Step [28125/34168], Loss: 5.1311\n",
      "Epoch [2/4], Step [28200/34168], Loss: 5.1338\n",
      "Epoch [2/4], Step [28275/34168], Loss: 5.0997\n",
      "Epoch [2/4], Step [28350/34168], Loss: 5.1217\n",
      "Epoch [2/4], Step [28425/34168], Loss: 5.2017\n",
      "Epoch [2/4], Step [28500/34168], Loss: 4.9872\n",
      "Epoch [2/4], Step [28575/34168], Loss: 5.0941\n",
      "Epoch [2/4], Step [28650/34168], Loss: 5.1424\n",
      "Epoch [2/4], Step [28725/34168], Loss: 5.1012\n",
      "Epoch [2/4], Step [28800/34168], Loss: 5.1306\n",
      "Epoch [2/4], Step [28875/34168], Loss: 5.1883\n",
      "Epoch [2/4], Step [28950/34168], Loss: 5.1089\n",
      "Epoch [2/4], Step [29025/34168], Loss: 5.0808\n",
      "Epoch [2/4], Step [29100/34168], Loss: 5.3001\n",
      "Epoch [2/4], Step [29175/34168], Loss: 5.1350\n",
      "Epoch [2/4], Step [29250/34168], Loss: 5.0523\n",
      "Epoch [2/4], Step [29325/34168], Loss: 5.1496\n",
      "Epoch [2/4], Step [29400/34168], Loss: 5.0996\n",
      "Epoch [2/4], Step [29475/34168], Loss: 5.1612\n",
      "Epoch [2/4], Step [29550/34168], Loss: 5.1636\n",
      "Epoch [2/4], Step [29625/34168], Loss: 5.1107\n",
      "Epoch [2/4], Step [29700/34168], Loss: 5.0516\n",
      "Epoch [2/4], Step [29775/34168], Loss: 5.1455\n",
      "Epoch [2/4], Step [29850/34168], Loss: 5.0975\n",
      "Epoch [2/4], Step [29925/34168], Loss: 5.1109\n",
      "Epoch [2/4], Step [30000/34168], Loss: 5.1686\n",
      "Validation perplexity: 146.4056862254221\n",
      "Epoch [2/4], Step [30075/34168], Loss: 5.0572\n",
      "Epoch [2/4], Step [30150/34168], Loss: 5.1226\n",
      "Epoch [2/4], Step [30225/34168], Loss: 5.3097\n",
      "Epoch [2/4], Step [30300/34168], Loss: 5.2286\n",
      "Epoch [2/4], Step [30375/34168], Loss: 5.1480\n",
      "Epoch [2/4], Step [30450/34168], Loss: 5.1250\n",
      "Epoch [2/4], Step [30525/34168], Loss: 5.1086\n",
      "Epoch [2/4], Step [30600/34168], Loss: 5.0939\n",
      "Epoch [2/4], Step [30675/34168], Loss: 5.1617\n",
      "Epoch [2/4], Step [30750/34168], Loss: 5.1291\n",
      "Epoch [2/4], Step [30825/34168], Loss: 5.0791\n",
      "Epoch [2/4], Step [30900/34168], Loss: 5.1690\n",
      "Epoch [2/4], Step [30975/34168], Loss: 5.1873\n",
      "Epoch [2/4], Step [31050/34168], Loss: 5.2058\n",
      "Epoch [2/4], Step [31125/34168], Loss: 5.1380\n",
      "Epoch [2/4], Step [31200/34168], Loss: 5.0734\n",
      "Epoch [2/4], Step [31275/34168], Loss: 5.1727\n",
      "Epoch [2/4], Step [31350/34168], Loss: 5.1299\n",
      "Epoch [2/4], Step [31425/34168], Loss: 5.0536\n",
      "Epoch [2/4], Step [31500/34168], Loss: 5.1037\n",
      "Epoch [2/4], Step [31575/34168], Loss: 5.1594\n",
      "Epoch [2/4], Step [31650/34168], Loss: 5.0663\n",
      "Epoch [2/4], Step [31725/34168], Loss: 5.2157\n",
      "Epoch [2/4], Step [31800/34168], Loss: 5.0333\n",
      "Epoch [2/4], Step [31875/34168], Loss: 5.0032\n",
      "Epoch [2/4], Step [31950/34168], Loss: 5.0942\n",
      "Epoch [2/4], Step [32025/34168], Loss: 5.1756\n",
      "Epoch [2/4], Step [32100/34168], Loss: 5.0051\n",
      "Epoch [2/4], Step [32175/34168], Loss: 5.1927\n",
      "Epoch [2/4], Step [32250/34168], Loss: 4.9197\n",
      "Epoch [2/4], Step [32325/34168], Loss: 5.1529\n",
      "Epoch [2/4], Step [32400/34168], Loss: 5.1286\n",
      "Epoch [2/4], Step [32475/34168], Loss: 5.1319\n",
      "Epoch [2/4], Step [32550/34168], Loss: 5.0354\n",
      "Epoch [2/4], Step [32625/34168], Loss: 5.1088\n",
      "Epoch [2/4], Step [32700/34168], Loss: 5.1189\n",
      "Epoch [2/4], Step [32775/34168], Loss: 5.0775\n",
      "Epoch [2/4], Step [32850/34168], Loss: 5.1230\n",
      "Epoch [2/4], Step [32925/34168], Loss: 5.1033\n",
      "Epoch [2/4], Step [33000/34168], Loss: 5.1472\n",
      "Epoch [2/4], Step [33075/34168], Loss: 5.2039\n",
      "Epoch [2/4], Step [33150/34168], Loss: 5.2024\n",
      "Epoch [2/4], Step [33225/34168], Loss: 5.0458\n",
      "Epoch [2/4], Step [33300/34168], Loss: 5.1602\n",
      "Epoch [2/4], Step [33375/34168], Loss: 5.2233\n",
      "Epoch [2/4], Step [33450/34168], Loss: 5.1160\n",
      "Epoch [2/4], Step [33525/34168], Loss: 5.1211\n",
      "Epoch [2/4], Step [33600/34168], Loss: 5.1886\n",
      "Epoch [2/4], Step [33675/34168], Loss: 5.2480\n",
      "Epoch [2/4], Step [33750/34168], Loss: 5.0697\n",
      "Epoch [2/4], Step [33825/34168], Loss: 5.1157\n",
      "Epoch [2/4], Step [33900/34168], Loss: 5.1475\n",
      "Epoch [2/4], Step [33975/34168], Loss: 5.1273\n",
      "Epoch [2/4], Step [34050/34168], Loss: 5.1740\n",
      "Epoch [2/4], Step [34125/34168], Loss: 5.0688\n",
      "Epoch [2/4] Average Loss: 5.1672, Perplexity: 175.42\n",
      "Epoch [3/4], Step [0/34168], Loss: 5.2147\n",
      "Validation perplexity: 145.05207147817373\n",
      "Epoch [3/4], Step [75/34168], Loss: 5.1653\n",
      "Epoch [3/4], Step [150/34168], Loss: 5.0545\n",
      "Epoch [3/4], Step [225/34168], Loss: 5.2115\n",
      "Epoch [3/4], Step [300/34168], Loss: 5.1261\n",
      "Epoch [3/4], Step [375/34168], Loss: 5.0430\n",
      "Epoch [3/4], Step [450/34168], Loss: 5.0599\n",
      "Epoch [3/4], Step [525/34168], Loss: 5.1794\n",
      "Epoch [3/4], Step [600/34168], Loss: 5.0108\n",
      "Epoch [3/4], Step [675/34168], Loss: 5.0753\n",
      "Epoch [3/4], Step [750/34168], Loss: 5.0270\n",
      "Epoch [3/4], Step [825/34168], Loss: 5.0432\n",
      "Epoch [3/4], Step [900/34168], Loss: 5.1697\n",
      "Epoch [3/4], Step [975/34168], Loss: 5.0060\n",
      "Epoch [3/4], Step [1050/34168], Loss: 5.0927\n",
      "Epoch [3/4], Step [1125/34168], Loss: 5.1691\n",
      "Epoch [3/4], Step [1200/34168], Loss: 5.0744\n",
      "Epoch [3/4], Step [1275/34168], Loss: 5.1777\n",
      "Epoch [3/4], Step [1350/34168], Loss: 5.1332\n",
      "Epoch [3/4], Step [1425/34168], Loss: 5.1442\n",
      "Epoch [3/4], Step [1500/34168], Loss: 5.1209\n",
      "Epoch [3/4], Step [1575/34168], Loss: 5.0732\n",
      "Epoch [3/4], Step [1650/34168], Loss: 5.0870\n",
      "Epoch [3/4], Step [1725/34168], Loss: 5.0623\n",
      "Epoch [3/4], Step [1800/34168], Loss: 4.9718\n",
      "Epoch [3/4], Step [1875/34168], Loss: 5.0191\n",
      "Epoch [3/4], Step [1950/34168], Loss: 5.1959\n",
      "Epoch [3/4], Step [2025/34168], Loss: 5.1269\n",
      "Epoch [3/4], Step [2100/34168], Loss: 5.0225\n",
      "Epoch [3/4], Step [2175/34168], Loss: 4.9764\n",
      "Epoch [3/4], Step [2250/34168], Loss: 5.1466\n",
      "Epoch [3/4], Step [2325/34168], Loss: 5.2010\n",
      "Epoch [3/4], Step [2400/34168], Loss: 4.9998\n",
      "Epoch [3/4], Step [2475/34168], Loss: 5.1562\n",
      "Epoch [3/4], Step [2550/34168], Loss: 5.0698\n",
      "Epoch [3/4], Step [2625/34168], Loss: 5.1357\n",
      "Epoch [3/4], Step [2700/34168], Loss: 5.1103\n",
      "Epoch [3/4], Step [2775/34168], Loss: 5.1541\n",
      "Epoch [3/4], Step [2850/34168], Loss: 5.1473\n",
      "Epoch [3/4], Step [2925/34168], Loss: 5.1712\n",
      "Epoch [3/4], Step [3000/34168], Loss: 5.1409\n",
      "Epoch [3/4], Step [3075/34168], Loss: 5.0645\n",
      "Epoch [3/4], Step [3150/34168], Loss: 5.0508\n",
      "Epoch [3/4], Step [3225/34168], Loss: 5.1878\n",
      "Epoch [3/4], Step [3300/34168], Loss: 5.0316\n",
      "Epoch [3/4], Step [3375/34168], Loss: 5.1363\n",
      "Epoch [3/4], Step [3450/34168], Loss: 5.1288\n",
      "Epoch [3/4], Step [3525/34168], Loss: 5.2102\n",
      "Epoch [3/4], Step [3600/34168], Loss: 5.1107\n",
      "Epoch [3/4], Step [3675/34168], Loss: 5.1443\n",
      "Epoch [3/4], Step [3750/34168], Loss: 5.1102\n",
      "Epoch [3/4], Step [3825/34168], Loss: 5.1635\n",
      "Epoch [3/4], Step [3900/34168], Loss: 5.1274\n",
      "Epoch [3/4], Step [3975/34168], Loss: 5.0613\n",
      "Epoch [3/4], Step [4050/34168], Loss: 5.1761\n",
      "Epoch [3/4], Step [4125/34168], Loss: 5.0404\n",
      "Epoch [3/4], Step [4200/34168], Loss: 5.2728\n",
      "Epoch [3/4], Step [4275/34168], Loss: 5.0089\n",
      "Epoch [3/4], Step [4350/34168], Loss: 5.1775\n",
      "Epoch [3/4], Step [4425/34168], Loss: 5.1857\n",
      "Epoch [3/4], Step [4500/34168], Loss: 5.1189\n",
      "Epoch [3/4], Step [4575/34168], Loss: 5.0714\n",
      "Epoch [3/4], Step [4650/34168], Loss: 5.0510\n",
      "Epoch [3/4], Step [4725/34168], Loss: 5.0916\n",
      "Epoch [3/4], Step [4800/34168], Loss: 5.1315\n",
      "Epoch [3/4], Step [4875/34168], Loss: 5.0620\n",
      "Epoch [3/4], Step [4950/34168], Loss: 5.1310\n",
      "Epoch [3/4], Step [5025/34168], Loss: 5.1002\n",
      "Epoch [3/4], Step [5100/34168], Loss: 5.2209\n",
      "Epoch [3/4], Step [5175/34168], Loss: 5.1597\n",
      "Epoch [3/4], Step [5250/34168], Loss: 5.2081\n",
      "Epoch [3/4], Step [5325/34168], Loss: 5.0652\n",
      "Epoch [3/4], Step [5400/34168], Loss: 5.1468\n",
      "Epoch [3/4], Step [5475/34168], Loss: 5.1769\n",
      "Epoch [3/4], Step [5550/34168], Loss: 5.0954\n",
      "Epoch [3/4], Step [5625/34168], Loss: 5.1269\n",
      "Epoch [3/4], Step [5700/34168], Loss: 5.1766\n",
      "Epoch [3/4], Step [5775/34168], Loss: 5.0357\n",
      "Epoch [3/4], Step [5850/34168], Loss: 5.2108\n",
      "Epoch [3/4], Step [5925/34168], Loss: 5.1170\n",
      "Epoch [3/4], Step [6000/34168], Loss: 5.1090\n",
      "Epoch [3/4], Step [6075/34168], Loss: 5.1305\n",
      "Epoch [3/4], Step [6150/34168], Loss: 5.1544\n",
      "Epoch [3/4], Step [6225/34168], Loss: 5.1160\n",
      "Epoch [3/4], Step [6300/34168], Loss: 5.0247\n",
      "Epoch [3/4], Step [6375/34168], Loss: 5.0459\n",
      "Epoch [3/4], Step [6450/34168], Loss: 5.0773\n",
      "Epoch [3/4], Step [6525/34168], Loss: 5.0573\n",
      "Epoch [3/4], Step [6600/34168], Loss: 4.9797\n",
      "Epoch [3/4], Step [6675/34168], Loss: 5.1436\n",
      "Epoch [3/4], Step [6750/34168], Loss: 5.1536\n",
      "Epoch [3/4], Step [6825/34168], Loss: 5.1144\n",
      "Epoch [3/4], Step [6900/34168], Loss: 5.1130\n",
      "Epoch [3/4], Step [6975/34168], Loss: 5.1978\n",
      "Epoch [3/4], Step [7050/34168], Loss: 5.0579\n",
      "Epoch [3/4], Step [7125/34168], Loss: 5.0919\n",
      "Epoch [3/4], Step [7200/34168], Loss: 5.1939\n",
      "Epoch [3/4], Step [7275/34168], Loss: 5.1578\n",
      "Epoch [3/4], Step [7350/34168], Loss: 5.0924\n",
      "Epoch [3/4], Step [7425/34168], Loss: 5.0760\n",
      "Epoch [3/4], Step [7500/34168], Loss: 5.0000\n",
      "Epoch [3/4], Step [7575/34168], Loss: 5.1716\n",
      "Epoch [3/4], Step [7650/34168], Loss: 4.9455\n",
      "Epoch [3/4], Step [7725/34168], Loss: 5.0654\n",
      "Epoch [3/4], Step [7800/34168], Loss: 5.1566\n",
      "Epoch [3/4], Step [7875/34168], Loss: 5.1758\n",
      "Epoch [3/4], Step [7950/34168], Loss: 5.1544\n",
      "Epoch [3/4], Step [8025/34168], Loss: 5.1303\n",
      "Epoch [3/4], Step [8100/34168], Loss: 5.0690\n",
      "Epoch [3/4], Step [8175/34168], Loss: 5.0722\n",
      "Epoch [3/4], Step [8250/34168], Loss: 5.0771\n",
      "Epoch [3/4], Step [8325/34168], Loss: 5.1291\n",
      "Epoch [3/4], Step [8400/34168], Loss: 5.0936\n",
      "Epoch [3/4], Step [8475/34168], Loss: 5.1429\n",
      "Epoch [3/4], Step [8550/34168], Loss: 5.1798\n",
      "Epoch [3/4], Step [8625/34168], Loss: 5.0652\n",
      "Epoch [3/4], Step [8700/34168], Loss: 5.1167\n",
      "Epoch [3/4], Step [8775/34168], Loss: 5.1248\n",
      "Epoch [3/4], Step [8850/34168], Loss: 4.9810\n",
      "Epoch [3/4], Step [8925/34168], Loss: 5.1968\n",
      "Epoch [3/4], Step [9000/34168], Loss: 5.0236\n",
      "Epoch [3/4], Step [9075/34168], Loss: 5.0397\n",
      "Epoch [3/4], Step [9150/34168], Loss: 5.1801\n",
      "Epoch [3/4], Step [9225/34168], Loss: 5.0519\n",
      "Epoch [3/4], Step [9300/34168], Loss: 5.2002\n",
      "Epoch [3/4], Step [9375/34168], Loss: 5.1493\n",
      "Epoch [3/4], Step [9450/34168], Loss: 5.1088\n",
      "Epoch [3/4], Step [9525/34168], Loss: 5.1710\n",
      "Epoch [3/4], Step [9600/34168], Loss: 5.0359\n",
      "Epoch [3/4], Step [9675/34168], Loss: 4.9949\n",
      "Epoch [3/4], Step [9750/34168], Loss: 5.1652\n",
      "Epoch [3/4], Step [9825/34168], Loss: 5.1224\n",
      "Epoch [3/4], Step [9900/34168], Loss: 5.0578\n",
      "Epoch [3/4], Step [9975/34168], Loss: 5.2255\n",
      "Validation perplexity: 142.7568170888426\n",
      "Epoch [3/4], Step [10050/34168], Loss: 5.0077\n",
      "Epoch [3/4], Step [10125/34168], Loss: 5.1688\n",
      "Epoch [3/4], Step [10200/34168], Loss: 5.2501\n",
      "Epoch [3/4], Step [10275/34168], Loss: 5.0454\n",
      "Epoch [3/4], Step [10350/34168], Loss: 5.2726\n",
      "Epoch [3/4], Step [10425/34168], Loss: 5.1465\n",
      "Epoch [3/4], Step [10500/34168], Loss: 5.0403\n",
      "Epoch [3/4], Step [10575/34168], Loss: 5.0254\n",
      "Epoch [3/4], Step [10650/34168], Loss: 5.0242\n",
      "Epoch [3/4], Step [10725/34168], Loss: 5.0787\n",
      "Epoch [3/4], Step [10800/34168], Loss: 5.0394\n",
      "Epoch [3/4], Step [10875/34168], Loss: 5.0566\n",
      "Epoch [3/4], Step [10950/34168], Loss: 5.0320\n",
      "Epoch [3/4], Step [11025/34168], Loss: 5.0707\n",
      "Epoch [3/4], Step [11100/34168], Loss: 5.0920\n",
      "Epoch [3/4], Step [11175/34168], Loss: 5.1317\n",
      "Epoch [3/4], Step [11250/34168], Loss: 5.1075\n",
      "Epoch [3/4], Step [11325/34168], Loss: 5.0747\n",
      "Epoch [3/4], Step [11400/34168], Loss: 5.1805\n",
      "Epoch [3/4], Step [11475/34168], Loss: 5.0708\n",
      "Epoch [3/4], Step [11550/34168], Loss: 5.1265\n",
      "Epoch [3/4], Step [11625/34168], Loss: 5.1155\n",
      "Epoch [3/4], Step [11700/34168], Loss: 5.0745\n",
      "Epoch [3/4], Step [11775/34168], Loss: 5.1887\n",
      "Epoch [3/4], Step [11850/34168], Loss: 5.1229\n",
      "Epoch [3/4], Step [11925/34168], Loss: 5.1766\n",
      "Epoch [3/4], Step [12000/34168], Loss: 5.0911\n",
      "Epoch [3/4], Step [12075/34168], Loss: 5.1115\n",
      "Epoch [3/4], Step [12150/34168], Loss: 5.1308\n",
      "Epoch [3/4], Step [12225/34168], Loss: 5.0031\n",
      "Epoch [3/4], Step [12300/34168], Loss: 5.1117\n",
      "Epoch [3/4], Step [12375/34168], Loss: 5.0693\n",
      "Epoch [3/4], Step [12450/34168], Loss: 4.9686\n",
      "Epoch [3/4], Step [12525/34168], Loss: 5.1311\n",
      "Epoch [3/4], Step [12600/34168], Loss: 5.1937\n",
      "Epoch [3/4], Step [12675/34168], Loss: 5.0522\n",
      "Epoch [3/4], Step [12750/34168], Loss: 5.1123\n",
      "Epoch [3/4], Step [12825/34168], Loss: 5.0414\n",
      "Epoch [3/4], Step [12900/34168], Loss: 5.1251\n",
      "Epoch [3/4], Step [12975/34168], Loss: 5.1325\n",
      "Epoch [3/4], Step [13050/34168], Loss: 5.1279\n",
      "Epoch [3/4], Step [13125/34168], Loss: 5.0803\n",
      "Epoch [3/4], Step [13200/34168], Loss: 5.0290\n",
      "Epoch [3/4], Step [13275/34168], Loss: 5.0344\n",
      "Epoch [3/4], Step [13350/34168], Loss: 5.0962\n",
      "Epoch [3/4], Step [13425/34168], Loss: 4.9717\n",
      "Epoch [3/4], Step [13500/34168], Loss: 5.1734\n",
      "Epoch [3/4], Step [13575/34168], Loss: 5.0958\n",
      "Epoch [3/4], Step [13650/34168], Loss: 5.0828\n",
      "Epoch [3/4], Step [13725/34168], Loss: 5.1072\n",
      "Epoch [3/4], Step [13800/34168], Loss: 5.1113\n",
      "Epoch [3/4], Step [13875/34168], Loss: 4.9405\n",
      "Epoch [3/4], Step [13950/34168], Loss: 5.0055\n",
      "Epoch [3/4], Step [14025/34168], Loss: 5.1433\n",
      "Epoch [3/4], Step [14100/34168], Loss: 5.0843\n",
      "Epoch [3/4], Step [14175/34168], Loss: 5.1699\n",
      "Epoch [3/4], Step [14250/34168], Loss: 5.1009\n",
      "Epoch [3/4], Step [14325/34168], Loss: 5.1688\n",
      "Epoch [3/4], Step [14400/34168], Loss: 5.0587\n",
      "Epoch [3/4], Step [14475/34168], Loss: 5.0453\n",
      "Epoch [3/4], Step [14550/34168], Loss: 5.0966\n",
      "Epoch [3/4], Step [14625/34168], Loss: 5.0739\n",
      "Epoch [3/4], Step [14700/34168], Loss: 5.1955\n",
      "Epoch [3/4], Step [14775/34168], Loss: 5.1468\n",
      "Epoch [3/4], Step [14850/34168], Loss: 5.0786\n",
      "Epoch [3/4], Step [14925/34168], Loss: 5.0110\n",
      "Epoch [3/4], Step [15000/34168], Loss: 5.1192\n",
      "Epoch [3/4], Step [15075/34168], Loss: 5.0986\n",
      "Epoch [3/4], Step [15150/34168], Loss: 5.1579\n",
      "Epoch [3/4], Step [15225/34168], Loss: 5.0797\n",
      "Epoch [3/4], Step [15300/34168], Loss: 5.0861\n",
      "Epoch [3/4], Step [15375/34168], Loss: 5.1888\n",
      "Epoch [3/4], Step [15450/34168], Loss: 5.0778\n",
      "Epoch [3/4], Step [15525/34168], Loss: 5.0995\n",
      "Epoch [3/4], Step [15600/34168], Loss: 5.1867\n",
      "Epoch [3/4], Step [15675/34168], Loss: 5.2023\n",
      "Epoch [3/4], Step [15750/34168], Loss: 5.0308\n",
      "Epoch [3/4], Step [15825/34168], Loss: 5.0150\n",
      "Epoch [3/4], Step [15900/34168], Loss: 5.0522\n",
      "Epoch [3/4], Step [15975/34168], Loss: 5.0688\n",
      "Epoch [3/4], Step [16050/34168], Loss: 5.0963\n",
      "Epoch [3/4], Step [16125/34168], Loss: 5.1120\n",
      "Epoch [3/4], Step [16200/34168], Loss: 5.1319\n",
      "Epoch [3/4], Step [16275/34168], Loss: 5.1913\n",
      "Epoch [3/4], Step [16350/34168], Loss: 5.1168\n",
      "Epoch [3/4], Step [16425/34168], Loss: 5.2256\n",
      "Epoch [3/4], Step [16500/34168], Loss: 5.0932\n",
      "Epoch [3/4], Step [16575/34168], Loss: 5.1547\n",
      "Epoch [3/4], Step [16650/34168], Loss: 5.0923\n",
      "Epoch [3/4], Step [16725/34168], Loss: 5.2393\n",
      "Epoch [3/4], Step [16800/34168], Loss: 5.1024\n",
      "Epoch [3/4], Step [16875/34168], Loss: 5.0614\n",
      "Epoch [3/4], Step [16950/34168], Loss: 5.1191\n",
      "Epoch [3/4], Step [17025/34168], Loss: 5.0179\n",
      "Epoch [3/4], Step [17100/34168], Loss: 5.0656\n",
      "Epoch [3/4], Step [17175/34168], Loss: 5.1647\n",
      "Epoch [3/4], Step [17250/34168], Loss: 5.0834\n",
      "Epoch [3/4], Step [17325/34168], Loss: 5.1568\n",
      "Epoch [3/4], Step [17400/34168], Loss: 5.0568\n",
      "Epoch [3/4], Step [17475/34168], Loss: 5.0301\n",
      "Epoch [3/4], Step [17550/34168], Loss: 5.1727\n",
      "Epoch [3/4], Step [17625/34168], Loss: 5.1017\n",
      "Epoch [3/4], Step [17700/34168], Loss: 5.0963\n",
      "Epoch [3/4], Step [17775/34168], Loss: 5.0997\n",
      "Epoch [3/4], Step [17850/34168], Loss: 5.3297\n",
      "Epoch [3/4], Step [17925/34168], Loss: 5.0712\n",
      "Epoch [3/4], Step [18000/34168], Loss: 5.1828\n",
      "Epoch [3/4], Step [18075/34168], Loss: 5.0159\n",
      "Epoch [3/4], Step [18150/34168], Loss: 5.0537\n",
      "Epoch [3/4], Step [18225/34168], Loss: 5.1331\n",
      "Epoch [3/4], Step [18300/34168], Loss: 5.1163\n",
      "Epoch [3/4], Step [18375/34168], Loss: 5.0634\n",
      "Epoch [3/4], Step [18450/34168], Loss: 4.9752\n",
      "Epoch [3/4], Step [18525/34168], Loss: 5.2152\n",
      "Epoch [3/4], Step [18600/34168], Loss: 5.0834\n",
      "Epoch [3/4], Step [18675/34168], Loss: 5.1766\n",
      "Epoch [3/4], Step [18750/34168], Loss: 5.0760\n",
      "Epoch [3/4], Step [18825/34168], Loss: 5.1867\n",
      "Epoch [3/4], Step [18900/34168], Loss: 5.0949\n",
      "Epoch [3/4], Step [18975/34168], Loss: 5.0476\n",
      "Epoch [3/4], Step [19050/34168], Loss: 4.9651\n",
      "Epoch [3/4], Step [19125/34168], Loss: 5.0394\n",
      "Epoch [3/4], Step [19200/34168], Loss: 5.2210\n",
      "Epoch [3/4], Step [19275/34168], Loss: 5.1022\n",
      "Epoch [3/4], Step [19350/34168], Loss: 5.1557\n",
      "Epoch [3/4], Step [19425/34168], Loss: 5.0400\n",
      "Epoch [3/4], Step [19500/34168], Loss: 5.0269\n",
      "Epoch [3/4], Step [19575/34168], Loss: 5.1553\n",
      "Epoch [3/4], Step [19650/34168], Loss: 5.1137\n",
      "Epoch [3/4], Step [19725/34168], Loss: 5.0688\n",
      "Epoch [3/4], Step [19800/34168], Loss: 5.1603\n",
      "Epoch [3/4], Step [19875/34168], Loss: 5.0808\n",
      "Epoch [3/4], Step [19950/34168], Loss: 5.0928\n",
      "Validation perplexity: 140.7358683712936\n",
      "Epoch [3/4], Step [20025/34168], Loss: 5.1668\n",
      "Epoch [3/4], Step [20100/34168], Loss: 5.0158\n",
      "Epoch [3/4], Step [20175/34168], Loss: 5.0232\n",
      "Epoch [3/4], Step [20250/34168], Loss: 5.0420\n",
      "Epoch [3/4], Step [20325/34168], Loss: 4.9978\n",
      "Epoch [3/4], Step [20400/34168], Loss: 5.0727\n",
      "Epoch [3/4], Step [20475/34168], Loss: 5.0515\n",
      "Epoch [3/4], Step [20550/34168], Loss: 5.2001\n",
      "Epoch [3/4], Step [20625/34168], Loss: 5.1037\n",
      "Epoch [3/4], Step [20700/34168], Loss: 5.1160\n",
      "Epoch [3/4], Step [20775/34168], Loss: 5.0891\n",
      "Epoch [3/4], Step [20850/34168], Loss: 5.0524\n",
      "Epoch [3/4], Step [20925/34168], Loss: 5.1528\n",
      "Epoch [3/4], Step [21000/34168], Loss: 5.0665\n",
      "Epoch [3/4], Step [21075/34168], Loss: 5.0824\n",
      "Epoch [3/4], Step [21150/34168], Loss: 5.1482\n",
      "Epoch [3/4], Step [21225/34168], Loss: 5.0366\n",
      "Epoch [3/4], Step [21300/34168], Loss: 5.0542\n",
      "Epoch [3/4], Step [21375/34168], Loss: 5.0844\n",
      "Epoch [3/4], Step [21450/34168], Loss: 5.0126\n",
      "Epoch [3/4], Step [21525/34168], Loss: 5.0742\n",
      "Epoch [3/4], Step [21600/34168], Loss: 5.0501\n",
      "Epoch [3/4], Step [21675/34168], Loss: 5.0155\n",
      "Epoch [3/4], Step [21750/34168], Loss: 5.1339\n",
      "Epoch [3/4], Step [21825/34168], Loss: 5.1730\n",
      "Epoch [3/4], Step [21900/34168], Loss: 5.0767\n",
      "Epoch [3/4], Step [21975/34168], Loss: 5.1964\n",
      "Epoch [3/4], Step [22050/34168], Loss: 5.1356\n",
      "Epoch [3/4], Step [22125/34168], Loss: 5.1191\n",
      "Epoch [3/4], Step [22200/34168], Loss: 5.0206\n",
      "Epoch [3/4], Step [22275/34168], Loss: 5.1425\n",
      "Epoch [3/4], Step [22350/34168], Loss: 5.1967\n",
      "Epoch [3/4], Step [22425/34168], Loss: 5.0532\n",
      "Epoch [3/4], Step [22500/34168], Loss: 5.0759\n",
      "Epoch [3/4], Step [22575/34168], Loss: 5.0855\n",
      "Epoch [3/4], Step [22650/34168], Loss: 4.9892\n",
      "Epoch [3/4], Step [22725/34168], Loss: 5.0391\n",
      "Epoch [3/4], Step [22800/34168], Loss: 5.1822\n",
      "Epoch [3/4], Step [22875/34168], Loss: 4.9415\n",
      "Epoch [3/4], Step [22950/34168], Loss: 5.0671\n",
      "Epoch [3/4], Step [23025/34168], Loss: 5.0430\n",
      "Epoch [3/4], Step [23100/34168], Loss: 5.1306\n",
      "Epoch [3/4], Step [23175/34168], Loss: 5.0259\n",
      "Epoch [3/4], Step [23250/34168], Loss: 5.0809\n",
      "Epoch [3/4], Step [23325/34168], Loss: 5.1857\n",
      "Epoch [3/4], Step [23400/34168], Loss: 5.0308\n",
      "Epoch [3/4], Step [23475/34168], Loss: 5.1465\n",
      "Epoch [3/4], Step [23550/34168], Loss: 5.1618\n",
      "Epoch [3/4], Step [23625/34168], Loss: 5.1836\n",
      "Epoch [3/4], Step [23700/34168], Loss: 5.1017\n",
      "Epoch [3/4], Step [23775/34168], Loss: 5.1977\n",
      "Epoch [3/4], Step [23850/34168], Loss: 5.0948\n",
      "Epoch [3/4], Step [23925/34168], Loss: 5.1205\n",
      "Epoch [3/4], Step [24000/34168], Loss: 5.1298\n",
      "Epoch [3/4], Step [24075/34168], Loss: 5.0496\n",
      "Epoch [3/4], Step [24150/34168], Loss: 5.1140\n",
      "Epoch [3/4], Step [24225/34168], Loss: 5.0657\n",
      "Epoch [3/4], Step [24300/34168], Loss: 5.1679\n",
      "Epoch [3/4], Step [24375/34168], Loss: 5.0698\n",
      "Epoch [3/4], Step [24450/34168], Loss: 5.0591\n",
      "Epoch [3/4], Step [24525/34168], Loss: 5.2272\n",
      "Epoch [3/4], Step [24600/34168], Loss: 5.1018\n",
      "Epoch [3/4], Step [24675/34168], Loss: 5.1909\n",
      "Epoch [3/4], Step [24750/34168], Loss: 5.0758\n",
      "Epoch [3/4], Step [24825/34168], Loss: 5.0567\n",
      "Epoch [3/4], Step [24900/34168], Loss: 5.0807\n",
      "Epoch [3/4], Step [24975/34168], Loss: 5.1019\n",
      "Epoch [3/4], Step [25050/34168], Loss: 4.9865\n",
      "Epoch [3/4], Step [25125/34168], Loss: 4.9335\n",
      "Epoch [3/4], Step [25200/34168], Loss: 5.0922\n",
      "Epoch [3/4], Step [25275/34168], Loss: 5.1216\n",
      "Epoch [3/4], Step [25350/34168], Loss: 5.1403\n",
      "Epoch [3/4], Step [25425/34168], Loss: 5.1119\n",
      "Epoch [3/4], Step [25500/34168], Loss: 4.9937\n",
      "Epoch [3/4], Step [25575/34168], Loss: 5.0257\n",
      "Epoch [3/4], Step [25650/34168], Loss: 5.0124\n",
      "Epoch [3/4], Step [25725/34168], Loss: 5.0853\n",
      "Epoch [3/4], Step [25800/34168], Loss: 5.0359\n",
      "Epoch [3/4], Step [25875/34168], Loss: 5.1302\n",
      "Epoch [3/4], Step [25950/34168], Loss: 4.9914\n",
      "Epoch [3/4], Step [26025/34168], Loss: 5.0318\n",
      "Epoch [3/4], Step [26100/34168], Loss: 5.0557\n",
      "Epoch [3/4], Step [26175/34168], Loss: 5.1538\n",
      "Epoch [3/4], Step [26250/34168], Loss: 5.0260\n",
      "Epoch [3/4], Step [26325/34168], Loss: 5.1621\n",
      "Epoch [3/4], Step [26400/34168], Loss: 5.0812\n",
      "Epoch [3/4], Step [26475/34168], Loss: 5.0774\n",
      "Epoch [3/4], Step [26550/34168], Loss: 5.1128\n",
      "Epoch [3/4], Step [26625/34168], Loss: 5.0183\n",
      "Epoch [3/4], Step [26700/34168], Loss: 4.9165\n",
      "Epoch [3/4], Step [26775/34168], Loss: 5.0408\n",
      "Epoch [3/4], Step [26850/34168], Loss: 5.2327\n",
      "Epoch [3/4], Step [26925/34168], Loss: 5.0775\n",
      "Epoch [3/4], Step [27000/34168], Loss: 5.0557\n",
      "Epoch [3/4], Step [27075/34168], Loss: 5.0265\n",
      "Epoch [3/4], Step [27150/34168], Loss: 5.1016\n",
      "Epoch [3/4], Step [27225/34168], Loss: 5.1031\n",
      "Epoch [3/4], Step [27300/34168], Loss: 5.1602\n",
      "Epoch [3/4], Step [27375/34168], Loss: 5.1234\n",
      "Epoch [3/4], Step [27450/34168], Loss: 5.1397\n",
      "Epoch [3/4], Step [27525/34168], Loss: 5.0746\n",
      "Epoch [3/4], Step [27600/34168], Loss: 5.1918\n",
      "Epoch [3/4], Step [27675/34168], Loss: 5.0303\n",
      "Epoch [3/4], Step [27750/34168], Loss: 5.0372\n",
      "Epoch [3/4], Step [27825/34168], Loss: 5.0520\n",
      "Epoch [3/4], Step [27900/34168], Loss: 5.0276\n",
      "Epoch [3/4], Step [27975/34168], Loss: 4.9979\n",
      "Epoch [3/4], Step [28050/34168], Loss: 5.1098\n",
      "Epoch [3/4], Step [28125/34168], Loss: 5.1747\n",
      "Epoch [3/4], Step [28200/34168], Loss: 5.1258\n",
      "Epoch [3/4], Step [28275/34168], Loss: 5.0866\n",
      "Epoch [3/4], Step [28350/34168], Loss: 5.0569\n",
      "Epoch [3/4], Step [28425/34168], Loss: 4.9316\n",
      "Epoch [3/4], Step [28500/34168], Loss: 5.0761\n",
      "Epoch [3/4], Step [28575/34168], Loss: 5.0621\n",
      "Epoch [3/4], Step [28650/34168], Loss: 4.9954\n",
      "Epoch [3/4], Step [28725/34168], Loss: 5.1136\n",
      "Epoch [3/4], Step [28800/34168], Loss: 5.1019\n",
      "Epoch [3/4], Step [28875/34168], Loss: 5.0848\n",
      "Epoch [3/4], Step [28950/34168], Loss: 5.0937\n",
      "Epoch [3/4], Step [29025/34168], Loss: 5.1552\n",
      "Epoch [3/4], Step [29100/34168], Loss: 4.9839\n",
      "Epoch [3/4], Step [29175/34168], Loss: 5.0226\n",
      "Epoch [3/4], Step [29250/34168], Loss: 4.9416\n",
      "Epoch [3/4], Step [29325/34168], Loss: 5.1719\n",
      "Epoch [3/4], Step [29400/34168], Loss: 5.1387\n",
      "Epoch [3/4], Step [29475/34168], Loss: 5.0471\n",
      "Epoch [3/4], Step [29550/34168], Loss: 5.0756\n",
      "Epoch [3/4], Step [29625/34168], Loss: 5.1367\n",
      "Epoch [3/4], Step [29700/34168], Loss: 5.1224\n",
      "Epoch [3/4], Step [29775/34168], Loss: 5.0300\n",
      "Epoch [3/4], Step [29850/34168], Loss: 5.2275\n",
      "Epoch [3/4], Step [29925/34168], Loss: 5.1201\n",
      "Epoch [3/4], Step [30000/34168], Loss: 5.0889\n",
      "Validation perplexity: 139.20678251424636\n",
      "Epoch [3/4], Step [30075/34168], Loss: 5.1162\n",
      "Epoch [3/4], Step [30150/34168], Loss: 5.0570\n",
      "Epoch [3/4], Step [30225/34168], Loss: 5.0538\n",
      "Epoch [3/4], Step [30300/34168], Loss: 5.1044\n",
      "Epoch [3/4], Step [30375/34168], Loss: 5.0774\n",
      "Epoch [3/4], Step [30450/34168], Loss: 5.1453\n",
      "Epoch [3/4], Step [30525/34168], Loss: 5.0996\n",
      "Epoch [3/4], Step [30600/34168], Loss: 5.0241\n",
      "Epoch [3/4], Step [30675/34168], Loss: 5.0550\n",
      "Epoch [3/4], Step [30750/34168], Loss: 5.1124\n",
      "Epoch [3/4], Step [30825/34168], Loss: 5.0674\n",
      "Epoch [3/4], Step [30900/34168], Loss: 5.0843\n",
      "Epoch [3/4], Step [30975/34168], Loss: 5.1039\n",
      "Epoch [3/4], Step [31050/34168], Loss: 5.0868\n",
      "Epoch [3/4], Step [31125/34168], Loss: 5.1466\n",
      "Epoch [3/4], Step [31200/34168], Loss: 5.0082\n",
      "Epoch [3/4], Step [31275/34168], Loss: 5.0503\n",
      "Epoch [3/4], Step [31350/34168], Loss: 5.1746\n",
      "Epoch [3/4], Step [31425/34168], Loss: 5.1334\n",
      "Epoch [3/4], Step [31500/34168], Loss: 5.0187\n",
      "Epoch [3/4], Step [31575/34168], Loss: 5.0798\n",
      "Epoch [3/4], Step [31650/34168], Loss: 5.0665\n",
      "Epoch [3/4], Step [31725/34168], Loss: 5.0711\n",
      "Epoch [3/4], Step [31800/34168], Loss: 5.0730\n",
      "Epoch [3/4], Step [31875/34168], Loss: 4.9737\n",
      "Epoch [3/4], Step [31950/34168], Loss: 5.0798\n",
      "Epoch [3/4], Step [32025/34168], Loss: 5.1660\n",
      "Epoch [3/4], Step [32100/34168], Loss: 5.1129\n",
      "Epoch [3/4], Step [32175/34168], Loss: 5.0727\n",
      "Epoch [3/4], Step [32250/34168], Loss: 5.0400\n",
      "Epoch [3/4], Step [32325/34168], Loss: 5.0997\n",
      "Epoch [3/4], Step [32400/34168], Loss: 5.0110\n",
      "Epoch [3/4], Step [32475/34168], Loss: 5.1582\n",
      "Epoch [3/4], Step [32550/34168], Loss: 5.1737\n",
      "Epoch [3/4], Step [32625/34168], Loss: 5.0859\n",
      "Epoch [3/4], Step [32700/34168], Loss: 5.0617\n",
      "Epoch [3/4], Step [32775/34168], Loss: 5.0707\n",
      "Epoch [3/4], Step [32850/34168], Loss: 5.0064\n",
      "Epoch [3/4], Step [32925/34168], Loss: 4.9745\n",
      "Epoch [3/4], Step [33000/34168], Loss: 5.0364\n",
      "Epoch [3/4], Step [33075/34168], Loss: 5.1032\n",
      "Epoch [3/4], Step [33150/34168], Loss: 5.2511\n",
      "Epoch [3/4], Step [33225/34168], Loss: 5.0996\n",
      "Epoch [3/4], Step [33300/34168], Loss: 5.0229\n",
      "Epoch [3/4], Step [33375/34168], Loss: 4.9876\n",
      "Epoch [3/4], Step [33450/34168], Loss: 5.0584\n",
      "Epoch [3/4], Step [33525/34168], Loss: 5.1163\n",
      "Epoch [3/4], Step [33600/34168], Loss: 5.1881\n",
      "Epoch [3/4], Step [33675/34168], Loss: 5.1128\n",
      "Epoch [3/4], Step [33750/34168], Loss: 5.0655\n",
      "Epoch [3/4], Step [33825/34168], Loss: 5.1421\n",
      "Epoch [3/4], Step [33900/34168], Loss: 5.1510\n",
      "Epoch [3/4], Step [33975/34168], Loss: 5.0679\n",
      "Epoch [3/4], Step [34050/34168], Loss: 5.2143\n",
      "Epoch [3/4], Step [34125/34168], Loss: 5.1622\n",
      "Epoch [3/4] Average Loss: 5.0970, Perplexity: 163.52\n",
      "Epoch [4/4], Step [0/34168], Loss: 5.1784\n",
      "Validation perplexity: 138.502434950226\n",
      "Epoch [4/4], Step [75/34168], Loss: 5.0118\n",
      "Epoch [4/4], Step [150/34168], Loss: 5.1431\n",
      "Epoch [4/4], Step [225/34168], Loss: 5.0987\n",
      "Epoch [4/4], Step [300/34168], Loss: 4.9667\n",
      "Epoch [4/4], Step [375/34168], Loss: 5.0608\n",
      "Epoch [4/4], Step [450/34168], Loss: 5.1261\n",
      "Epoch [4/4], Step [525/34168], Loss: 5.0531\n",
      "Epoch [4/4], Step [600/34168], Loss: 5.0826\n",
      "Epoch [4/4], Step [675/34168], Loss: 5.0928\n",
      "Epoch [4/4], Step [750/34168], Loss: 5.1074\n",
      "Epoch [4/4], Step [825/34168], Loss: 5.0667\n",
      "Epoch [4/4], Step [900/34168], Loss: 5.0167\n",
      "Epoch [4/4], Step [975/34168], Loss: 4.9652\n",
      "Epoch [4/4], Step [1050/34168], Loss: 5.2410\n",
      "Epoch [4/4], Step [1125/34168], Loss: 5.0222\n",
      "Epoch [4/4], Step [1200/34168], Loss: 5.1337\n",
      "Epoch [4/4], Step [1275/34168], Loss: 5.1149\n",
      "Epoch [4/4], Step [1350/34168], Loss: 5.0813\n",
      "Epoch [4/4], Step [1425/34168], Loss: 5.1071\n",
      "Epoch [4/4], Step [1500/34168], Loss: 5.0147\n",
      "Epoch [4/4], Step [1575/34168], Loss: 5.0494\n",
      "Epoch [4/4], Step [1650/34168], Loss: 5.1531\n",
      "Epoch [4/4], Step [1725/34168], Loss: 5.0158\n",
      "Epoch [4/4], Step [1800/34168], Loss: 5.0267\n",
      "Epoch [4/4], Step [1875/34168], Loss: 5.2597\n",
      "Epoch [4/4], Step [1950/34168], Loss: 5.0048\n",
      "Epoch [4/4], Step [2025/34168], Loss: 5.1634\n",
      "Epoch [4/4], Step [2100/34168], Loss: 5.0674\n",
      "Epoch [4/4], Step [2175/34168], Loss: 5.1793\n",
      "Epoch [4/4], Step [2250/34168], Loss: 5.0634\n",
      "Epoch [4/4], Step [2325/34168], Loss: 5.0611\n",
      "Epoch [4/4], Step [2400/34168], Loss: 5.1445\n",
      "Epoch [4/4], Step [2475/34168], Loss: 5.0519\n",
      "Epoch [4/4], Step [2550/34168], Loss: 5.0375\n",
      "Epoch [4/4], Step [2625/34168], Loss: 5.1222\n",
      "Epoch [4/4], Step [2700/34168], Loss: 5.1545\n",
      "Epoch [4/4], Step [2775/34168], Loss: 5.0539\n",
      "Epoch [4/4], Step [2850/34168], Loss: 5.1698\n",
      "Epoch [4/4], Step [2925/34168], Loss: 5.0780\n",
      "Epoch [4/4], Step [3000/34168], Loss: 5.0064\n",
      "Epoch [4/4], Step [3075/34168], Loss: 5.1154\n",
      "Epoch [4/4], Step [3150/34168], Loss: 5.1426\n",
      "Epoch [4/4], Step [3225/34168], Loss: 5.1414\n",
      "Epoch [4/4], Step [3300/34168], Loss: 5.1975\n",
      "Epoch [4/4], Step [3375/34168], Loss: 5.1170\n",
      "Epoch [4/4], Step [3450/34168], Loss: 5.0647\n",
      "Epoch [4/4], Step [3525/34168], Loss: 5.0944\n",
      "Epoch [4/4], Step [3600/34168], Loss: 5.0570\n",
      "Epoch [4/4], Step [3675/34168], Loss: 5.0754\n",
      "Epoch [4/4], Step [3750/34168], Loss: 5.0470\n",
      "Epoch [4/4], Step [3825/34168], Loss: 4.9356\n",
      "Epoch [4/4], Step [3900/34168], Loss: 5.0644\n",
      "Epoch [4/4], Step [3975/34168], Loss: 5.1698\n",
      "Epoch [4/4], Step [4050/34168], Loss: 5.0336\n",
      "Epoch [4/4], Step [4125/34168], Loss: 5.1352\n",
      "Epoch [4/4], Step [4200/34168], Loss: 5.2145\n",
      "Epoch [4/4], Step [4275/34168], Loss: 5.1101\n",
      "Epoch [4/4], Step [4350/34168], Loss: 5.1268\n",
      "Epoch [4/4], Step [4425/34168], Loss: 4.9994\n",
      "Epoch [4/4], Step [4500/34168], Loss: 5.0409\n",
      "Epoch [4/4], Step [4575/34168], Loss: 5.0386\n",
      "Epoch [4/4], Step [4650/34168], Loss: 5.0114\n",
      "Epoch [4/4], Step [4725/34168], Loss: 5.1647\n",
      "Epoch [4/4], Step [4800/34168], Loss: 5.0411\n",
      "Epoch [4/4], Step [4875/34168], Loss: 5.0002\n",
      "Epoch [4/4], Step [4950/34168], Loss: 5.0794\n",
      "Epoch [4/4], Step [5025/34168], Loss: 5.0355\n",
      "Epoch [4/4], Step [5100/34168], Loss: 5.0644\n",
      "Epoch [4/4], Step [5175/34168], Loss: 5.0918\n",
      "Epoch [4/4], Step [5250/34168], Loss: 5.0623\n",
      "Epoch [4/4], Step [5325/34168], Loss: 5.1415\n",
      "Epoch [4/4], Step [5400/34168], Loss: 5.0235\n",
      "Epoch [4/4], Step [5475/34168], Loss: 5.0717\n",
      "Epoch [4/4], Step [5550/34168], Loss: 4.9944\n",
      "Epoch [4/4], Step [5625/34168], Loss: 5.0952\n",
      "Epoch [4/4], Step [5700/34168], Loss: 5.2320\n",
      "Epoch [4/4], Step [5775/34168], Loss: 5.0929\n",
      "Epoch [4/4], Step [5850/34168], Loss: 5.0383\n",
      "Epoch [4/4], Step [5925/34168], Loss: 5.1308\n",
      "Epoch [4/4], Step [6000/34168], Loss: 5.0714\n",
      "Epoch [4/4], Step [6075/34168], Loss: 5.0907\n",
      "Epoch [4/4], Step [6150/34168], Loss: 5.0372\n",
      "Epoch [4/4], Step [6225/34168], Loss: 5.0776\n",
      "Epoch [4/4], Step [6300/34168], Loss: 5.0538\n",
      "Epoch [4/4], Step [6375/34168], Loss: 5.0481\n",
      "Epoch [4/4], Step [6450/34168], Loss: 5.0363\n",
      "Epoch [4/4], Step [6525/34168], Loss: 5.0768\n",
      "Epoch [4/4], Step [6600/34168], Loss: 4.9519\n",
      "Epoch [4/4], Step [6675/34168], Loss: 5.0587\n",
      "Epoch [4/4], Step [6750/34168], Loss: 5.1137\n",
      "Epoch [4/4], Step [6825/34168], Loss: 5.1507\n",
      "Epoch [4/4], Step [6900/34168], Loss: 5.1395\n",
      "Epoch [4/4], Step [6975/34168], Loss: 5.1697\n",
      "Epoch [4/4], Step [7050/34168], Loss: 5.1460\n",
      "Epoch [4/4], Step [7125/34168], Loss: 5.1133\n",
      "Epoch [4/4], Step [7200/34168], Loss: 5.0682\n",
      "Epoch [4/4], Step [7275/34168], Loss: 4.9380\n",
      "Epoch [4/4], Step [7350/34168], Loss: 5.0936\n",
      "Epoch [4/4], Step [7425/34168], Loss: 5.1084\n",
      "Epoch [4/4], Step [7500/34168], Loss: 5.0734\n",
      "Epoch [4/4], Step [7575/34168], Loss: 5.0854\n",
      "Epoch [4/4], Step [7650/34168], Loss: 5.1051\n",
      "Epoch [4/4], Step [7725/34168], Loss: 5.0570\n",
      "Epoch [4/4], Step [7800/34168], Loss: 4.9370\n",
      "Epoch [4/4], Step [7875/34168], Loss: 5.0591\n",
      "Epoch [4/4], Step [7950/34168], Loss: 5.0113\n",
      "Epoch [4/4], Step [8025/34168], Loss: 5.0380\n",
      "Epoch [4/4], Step [8100/34168], Loss: 5.1092\n",
      "Epoch [4/4], Step [8175/34168], Loss: 5.0322\n",
      "Epoch [4/4], Step [8250/34168], Loss: 5.0892\n",
      "Epoch [4/4], Step [8325/34168], Loss: 5.0834\n",
      "Epoch [4/4], Step [8400/34168], Loss: 5.1106\n",
      "Epoch [4/4], Step [8475/34168], Loss: 5.0236\n",
      "Epoch [4/4], Step [8550/34168], Loss: 5.0391\n",
      "Epoch [4/4], Step [8625/34168], Loss: 4.9671\n",
      "Epoch [4/4], Step [8700/34168], Loss: 5.0402\n",
      "Epoch [4/4], Step [8775/34168], Loss: 4.9222\n",
      "Epoch [4/4], Step [8850/34168], Loss: 5.0538\n",
      "Epoch [4/4], Step [8925/34168], Loss: 5.1113\n",
      "Epoch [4/4], Step [9000/34168], Loss: 5.0998\n",
      "Epoch [4/4], Step [9075/34168], Loss: 5.0550\n",
      "Epoch [4/4], Step [9150/34168], Loss: 5.0162\n",
      "Epoch [4/4], Step [9225/34168], Loss: 5.0944\n",
      "Epoch [4/4], Step [9300/34168], Loss: 5.1386\n",
      "Epoch [4/4], Step [9375/34168], Loss: 5.0386\n",
      "Epoch [4/4], Step [9450/34168], Loss: 5.0977\n",
      "Epoch [4/4], Step [9525/34168], Loss: 5.0290\n",
      "Epoch [4/4], Step [9600/34168], Loss: 5.1884\n",
      "Epoch [4/4], Step [9675/34168], Loss: 5.0342\n",
      "Epoch [4/4], Step [9750/34168], Loss: 5.0333\n",
      "Epoch [4/4], Step [9825/34168], Loss: 5.2042\n",
      "Epoch [4/4], Step [9900/34168], Loss: 5.1642\n",
      "Epoch [4/4], Step [9975/34168], Loss: 5.1077\n",
      "Validation perplexity: 137.58285387369963\n",
      "Epoch [4/4], Step [10050/34168], Loss: 5.1998\n",
      "Epoch [4/4], Step [10125/34168], Loss: 4.9644\n",
      "Epoch [4/4], Step [10200/34168], Loss: 5.0927\n",
      "Epoch [4/4], Step [10275/34168], Loss: 5.0627\n",
      "Epoch [4/4], Step [10350/34168], Loss: 5.1300\n",
      "Epoch [4/4], Step [10425/34168], Loss: 4.9887\n",
      "Epoch [4/4], Step [10500/34168], Loss: 5.1174\n",
      "Epoch [4/4], Step [10575/34168], Loss: 5.0902\n",
      "Epoch [4/4], Step [10650/34168], Loss: 5.0651\n",
      "Epoch [4/4], Step [10725/34168], Loss: 4.9153\n",
      "Epoch [4/4], Step [10800/34168], Loss: 5.1017\n",
      "Epoch [4/4], Step [10875/34168], Loss: 5.1942\n",
      "Epoch [4/4], Step [10950/34168], Loss: 5.2020\n",
      "Epoch [4/4], Step [11025/34168], Loss: 4.9494\n",
      "Epoch [4/4], Step [11100/34168], Loss: 5.0268\n",
      "Epoch [4/4], Step [11175/34168], Loss: 5.0440\n",
      "Epoch [4/4], Step [11250/34168], Loss: 4.9621\n",
      "Epoch [4/4], Step [11325/34168], Loss: 5.0536\n",
      "Epoch [4/4], Step [11400/34168], Loss: 5.1795\n",
      "Epoch [4/4], Step [11475/34168], Loss: 5.1381\n",
      "Epoch [4/4], Step [11550/34168], Loss: 4.9971\n",
      "Epoch [4/4], Step [11625/34168], Loss: 5.1984\n",
      "Epoch [4/4], Step [11700/34168], Loss: 4.9439\n",
      "Epoch [4/4], Step [11775/34168], Loss: 5.0496\n",
      "Epoch [4/4], Step [11850/34168], Loss: 5.0372\n",
      "Epoch [4/4], Step [11925/34168], Loss: 5.1420\n",
      "Epoch [4/4], Step [12000/34168], Loss: 5.0670\n",
      "Epoch [4/4], Step [12075/34168], Loss: 5.0289\n",
      "Epoch [4/4], Step [12150/34168], Loss: 5.0769\n",
      "Epoch [4/4], Step [12225/34168], Loss: 5.2142\n",
      "Epoch [4/4], Step [12300/34168], Loss: 4.9731\n",
      "Epoch [4/4], Step [12375/34168], Loss: 5.1142\n",
      "Epoch [4/4], Step [12450/34168], Loss: 4.9964\n",
      "Epoch [4/4], Step [12525/34168], Loss: 5.1027\n",
      "Epoch [4/4], Step [12600/34168], Loss: 5.1115\n",
      "Epoch [4/4], Step [12675/34168], Loss: 5.0726\n",
      "Epoch [4/4], Step [12750/34168], Loss: 5.1934\n",
      "Epoch [4/4], Step [12825/34168], Loss: 5.1580\n",
      "Epoch [4/4], Step [12900/34168], Loss: 4.9819\n",
      "Epoch [4/4], Step [12975/34168], Loss: 5.1479\n",
      "Epoch [4/4], Step [13050/34168], Loss: 5.0455\n",
      "Epoch [4/4], Step [13125/34168], Loss: 5.0287\n",
      "Epoch [4/4], Step [13200/34168], Loss: 5.0841\n",
      "Epoch [4/4], Step [13275/34168], Loss: 5.1456\n",
      "Epoch [4/4], Step [13350/34168], Loss: 5.0677\n",
      "Epoch [4/4], Step [13425/34168], Loss: 4.9781\n",
      "Epoch [4/4], Step [13500/34168], Loss: 5.0098\n",
      "Epoch [4/4], Step [13575/34168], Loss: 4.9902\n",
      "Epoch [4/4], Step [13650/34168], Loss: 5.0351\n",
      "Epoch [4/4], Step [13725/34168], Loss: 5.1147\n",
      "Epoch [4/4], Step [13800/34168], Loss: 5.1105\n",
      "Epoch [4/4], Step [13875/34168], Loss: 5.0641\n",
      "Epoch [4/4], Step [13950/34168], Loss: 5.0707\n",
      "Epoch [4/4], Step [14025/34168], Loss: 4.9888\n",
      "Epoch [4/4], Step [14100/34168], Loss: 5.0674\n",
      "Epoch [4/4], Step [14175/34168], Loss: 5.0891\n",
      "Epoch [4/4], Step [14250/34168], Loss: 5.0558\n",
      "Epoch [4/4], Step [14325/34168], Loss: 5.0082\n",
      "Epoch [4/4], Step [14400/34168], Loss: 5.0456\n",
      "Epoch [4/4], Step [14475/34168], Loss: 5.0489\n",
      "Epoch [4/4], Step [14550/34168], Loss: 5.1620\n",
      "Epoch [4/4], Step [14625/34168], Loss: 5.0142\n",
      "Epoch [4/4], Step [14700/34168], Loss: 5.0256\n",
      "Epoch [4/4], Step [14775/34168], Loss: 5.0544\n",
      "Epoch [4/4], Step [14850/34168], Loss: 5.0987\n",
      "Epoch [4/4], Step [14925/34168], Loss: 5.0963\n",
      "Epoch [4/4], Step [15000/34168], Loss: 5.0624\n",
      "Epoch [4/4], Step [15075/34168], Loss: 5.0990\n",
      "Epoch [4/4], Step [15150/34168], Loss: 5.1546\n",
      "Epoch [4/4], Step [15225/34168], Loss: 5.0248\n",
      "Epoch [4/4], Step [15300/34168], Loss: 5.1218\n",
      "Epoch [4/4], Step [15375/34168], Loss: 5.0462\n",
      "Epoch [4/4], Step [15450/34168], Loss: 5.1340\n",
      "Epoch [4/4], Step [15525/34168], Loss: 5.1058\n",
      "Epoch [4/4], Step [15600/34168], Loss: 5.0802\n",
      "Epoch [4/4], Step [15675/34168], Loss: 5.1546\n",
      "Epoch [4/4], Step [15750/34168], Loss: 5.0144\n",
      "Epoch [4/4], Step [15825/34168], Loss: 5.0282\n",
      "Epoch [4/4], Step [15900/34168], Loss: 5.1377\n",
      "Epoch [4/4], Step [15975/34168], Loss: 5.0965\n",
      "Epoch [4/4], Step [16050/34168], Loss: 5.0858\n",
      "Epoch [4/4], Step [16125/34168], Loss: 5.0104\n",
      "Epoch [4/4], Step [16200/34168], Loss: 5.0448\n",
      "Epoch [4/4], Step [16275/34168], Loss: 5.1297\n",
      "Epoch [4/4], Step [16350/34168], Loss: 5.0124\n",
      "Epoch [4/4], Step [16425/34168], Loss: 5.0944\n",
      "Epoch [4/4], Step [16500/34168], Loss: 5.1261\n",
      "Epoch [4/4], Step [16575/34168], Loss: 5.0171\n",
      "Epoch [4/4], Step [16650/34168], Loss: 4.9804\n",
      "Epoch [4/4], Step [16725/34168], Loss: 5.0272\n",
      "Epoch [4/4], Step [16800/34168], Loss: 5.1461\n",
      "Epoch [4/4], Step [16875/34168], Loss: 5.0063\n",
      "Epoch [4/4], Step [16950/34168], Loss: 4.9964\n",
      "Epoch [4/4], Step [17025/34168], Loss: 5.1942\n",
      "Epoch [4/4], Step [17100/34168], Loss: 5.0225\n",
      "Epoch [4/4], Step [17175/34168], Loss: 4.9662\n",
      "Epoch [4/4], Step [17250/34168], Loss: 5.0225\n",
      "Epoch [4/4], Step [17325/34168], Loss: 5.0979\n",
      "Epoch [4/4], Step [17400/34168], Loss: 5.0474\n",
      "Epoch [4/4], Step [17475/34168], Loss: 4.9844\n",
      "Epoch [4/4], Step [17550/34168], Loss: 5.1238\n",
      "Epoch [4/4], Step [17625/34168], Loss: 5.0654\n",
      "Epoch [4/4], Step [17700/34168], Loss: 5.0783\n",
      "Epoch [4/4], Step [17775/34168], Loss: 5.0979\n",
      "Epoch [4/4], Step [17850/34168], Loss: 5.0458\n",
      "Epoch [4/4], Step [17925/34168], Loss: 5.0653\n",
      "Epoch [4/4], Step [18000/34168], Loss: 5.0972\n",
      "Epoch [4/4], Step [18075/34168], Loss: 5.0493\n",
      "Epoch [4/4], Step [18150/34168], Loss: 5.0951\n",
      "Epoch [4/4], Step [18225/34168], Loss: 5.0372\n",
      "Epoch [4/4], Step [18300/34168], Loss: 5.0323\n",
      "Epoch [4/4], Step [18375/34168], Loss: 5.0368\n",
      "Epoch [4/4], Step [18450/34168], Loss: 5.0025\n",
      "Epoch [4/4], Step [18525/34168], Loss: 5.0279\n",
      "Epoch [4/4], Step [18600/34168], Loss: 5.1314\n",
      "Epoch [4/4], Step [18675/34168], Loss: 5.0836\n",
      "Epoch [4/4], Step [18750/34168], Loss: 5.0826\n",
      "Epoch [4/4], Step [18825/34168], Loss: 5.0368\n",
      "Epoch [4/4], Step [18900/34168], Loss: 5.0000\n",
      "Epoch [4/4], Step [18975/34168], Loss: 5.1134\n",
      "Epoch [4/4], Step [19050/34168], Loss: 5.0946\n",
      "Epoch [4/4], Step [19125/34168], Loss: 5.0443\n",
      "Epoch [4/4], Step [19200/34168], Loss: 5.0180\n",
      "Epoch [4/4], Step [19275/34168], Loss: 5.1109\n",
      "Epoch [4/4], Step [19350/34168], Loss: 5.0181\n",
      "Epoch [4/4], Step [19425/34168], Loss: 5.1007\n",
      "Epoch [4/4], Step [19500/34168], Loss: 4.9704\n",
      "Epoch [4/4], Step [19575/34168], Loss: 5.1248\n",
      "Epoch [4/4], Step [19650/34168], Loss: 5.0869\n",
      "Epoch [4/4], Step [19725/34168], Loss: 4.9701\n",
      "Epoch [4/4], Step [19800/34168], Loss: 5.1846\n",
      "Epoch [4/4], Step [19875/34168], Loss: 5.1046\n",
      "Epoch [4/4], Step [19950/34168], Loss: 5.0624\n",
      "Validation perplexity: 136.53127723490437\n",
      "Epoch [4/4], Step [20025/34168], Loss: 5.1442\n",
      "Epoch [4/4], Step [20100/34168], Loss: 5.0885\n",
      "Epoch [4/4], Step [20175/34168], Loss: 5.0216\n",
      "Epoch [4/4], Step [20250/34168], Loss: 4.9994\n",
      "Epoch [4/4], Step [20325/34168], Loss: 5.1062\n",
      "Epoch [4/4], Step [20400/34168], Loss: 5.1780\n",
      "Epoch [4/4], Step [20475/34168], Loss: 5.0836\n",
      "Epoch [4/4], Step [20550/34168], Loss: 5.0816\n",
      "Epoch [4/4], Step [20625/34168], Loss: 5.1252\n",
      "Epoch [4/4], Step [20700/34168], Loss: 5.0575\n",
      "Epoch [4/4], Step [20775/34168], Loss: 5.0401\n",
      "Epoch [4/4], Step [20850/34168], Loss: 5.0059\n",
      "Epoch [4/4], Step [20925/34168], Loss: 5.0294\n",
      "Epoch [4/4], Step [21000/34168], Loss: 5.1600\n",
      "Epoch [4/4], Step [21075/34168], Loss: 5.0763\n",
      "Epoch [4/4], Step [21150/34168], Loss: 5.1168\n",
      "Epoch [4/4], Step [21225/34168], Loss: 5.0230\n",
      "Epoch [4/4], Step [21300/34168], Loss: 4.9977\n",
      "Epoch [4/4], Step [21375/34168], Loss: 5.0225\n",
      "Epoch [4/4], Step [21450/34168], Loss: 4.9066\n",
      "Epoch [4/4], Step [21525/34168], Loss: 5.1348\n",
      "Epoch [4/4], Step [21600/34168], Loss: 5.0930\n",
      "Epoch [4/4], Step [21675/34168], Loss: 5.1177\n",
      "Epoch [4/4], Step [21750/34168], Loss: 5.0646\n",
      "Epoch [4/4], Step [21825/34168], Loss: 5.1734\n",
      "Epoch [4/4], Step [21900/34168], Loss: 5.0575\n",
      "Epoch [4/4], Step [21975/34168], Loss: 5.0212\n",
      "Epoch [4/4], Step [22050/34168], Loss: 5.0865\n",
      "Epoch [4/4], Step [22125/34168], Loss: 4.9354\n",
      "Epoch [4/4], Step [22200/34168], Loss: 5.1375\n",
      "Epoch [4/4], Step [22275/34168], Loss: 5.1039\n",
      "Epoch [4/4], Step [22350/34168], Loss: 5.1023\n",
      "Epoch [4/4], Step [22425/34168], Loss: 5.0826\n",
      "Epoch [4/4], Step [22500/34168], Loss: 5.0749\n",
      "Epoch [4/4], Step [22575/34168], Loss: 4.9499\n",
      "Epoch [4/4], Step [22650/34168], Loss: 5.1260\n",
      "Epoch [4/4], Step [22725/34168], Loss: 5.0743\n",
      "Epoch [4/4], Step [22800/34168], Loss: 5.0335\n",
      "Epoch [4/4], Step [22875/34168], Loss: 5.0426\n",
      "Epoch [4/4], Step [22950/34168], Loss: 5.1427\n",
      "Epoch [4/4], Step [23025/34168], Loss: 5.0119\n",
      "Epoch [4/4], Step [23100/34168], Loss: 5.0223\n",
      "Epoch [4/4], Step [23175/34168], Loss: 5.0705\n",
      "Epoch [4/4], Step [23250/34168], Loss: 5.0528\n",
      "Epoch [4/4], Step [23325/34168], Loss: 5.0506\n",
      "Epoch [4/4], Step [23400/34168], Loss: 5.0463\n",
      "Epoch [4/4], Step [23475/34168], Loss: 4.9950\n",
      "Epoch [4/4], Step [23550/34168], Loss: 5.0074\n",
      "Epoch [4/4], Step [23625/34168], Loss: 5.1882\n",
      "Epoch [4/4], Step [23700/34168], Loss: 4.9735\n",
      "Epoch [4/4], Step [23775/34168], Loss: 5.0112\n",
      "Epoch [4/4], Step [23850/34168], Loss: 5.2687\n",
      "Epoch [4/4], Step [23925/34168], Loss: 5.0582\n",
      "Epoch [4/4], Step [24000/34168], Loss: 5.0371\n",
      "Epoch [4/4], Step [24075/34168], Loss: 5.0857\n",
      "Epoch [4/4], Step [24150/34168], Loss: 5.0395\n",
      "Epoch [4/4], Step [24225/34168], Loss: 4.9099\n",
      "Epoch [4/4], Step [24300/34168], Loss: 4.9989\n",
      "Epoch [4/4], Step [24375/34168], Loss: 5.0814\n",
      "Epoch [4/4], Step [24450/34168], Loss: 4.9583\n",
      "Epoch [4/4], Step [24525/34168], Loss: 4.9857\n",
      "Epoch [4/4], Step [24600/34168], Loss: 5.0679\n",
      "Epoch [4/4], Step [24675/34168], Loss: 5.1425\n",
      "Epoch [4/4], Step [24750/34168], Loss: 5.0426\n",
      "Epoch [4/4], Step [24825/34168], Loss: 4.9993\n",
      "Epoch [4/4], Step [24900/34168], Loss: 5.0259\n",
      "Epoch [4/4], Step [24975/34168], Loss: 5.1679\n",
      "Epoch [4/4], Step [25050/34168], Loss: 5.1262\n",
      "Epoch [4/4], Step [25125/34168], Loss: 5.0820\n",
      "Epoch [4/4], Step [25200/34168], Loss: 4.9554\n",
      "Epoch [4/4], Step [25275/34168], Loss: 5.1212\n",
      "Epoch [4/4], Step [25350/34168], Loss: 4.8962\n",
      "Epoch [4/4], Step [25425/34168], Loss: 5.1507\n",
      "Epoch [4/4], Step [25500/34168], Loss: 5.0763\n",
      "Epoch [4/4], Step [25575/34168], Loss: 5.0434\n",
      "Epoch [4/4], Step [25650/34168], Loss: 4.9990\n",
      "Epoch [4/4], Step [25725/34168], Loss: 5.0832\n",
      "Epoch [4/4], Step [25800/34168], Loss: 5.0904\n",
      "Epoch [4/4], Step [25875/34168], Loss: 4.9631\n",
      "Epoch [4/4], Step [25950/34168], Loss: 5.0611\n",
      "Epoch [4/4], Step [26025/34168], Loss: 5.0467\n",
      "Epoch [4/4], Step [26100/34168], Loss: 5.1038\n",
      "Epoch [4/4], Step [26175/34168], Loss: 5.0337\n",
      "Epoch [4/4], Step [26250/34168], Loss: 4.8935\n",
      "Epoch [4/4], Step [26325/34168], Loss: 5.0104\n",
      "Epoch [4/4], Step [26400/34168], Loss: 4.9825\n",
      "Epoch [4/4], Step [26475/34168], Loss: 5.0622\n",
      "Epoch [4/4], Step [26550/34168], Loss: 5.1053\n",
      "Epoch [4/4], Step [26625/34168], Loss: 5.1056\n",
      "Epoch [4/4], Step [26700/34168], Loss: 5.0886\n",
      "Epoch [4/4], Step [26775/34168], Loss: 4.9678\n",
      "Epoch [4/4], Step [26850/34168], Loss: 5.1236\n",
      "Epoch [4/4], Step [26925/34168], Loss: 5.1327\n",
      "Epoch [4/4], Step [27000/34168], Loss: 4.9406\n",
      "Epoch [4/4], Step [27075/34168], Loss: 5.1008\n",
      "Epoch [4/4], Step [27150/34168], Loss: 5.0904\n",
      "Epoch [4/4], Step [27225/34168], Loss: 5.0366\n",
      "Epoch [4/4], Step [27300/34168], Loss: 5.0350\n",
      "Epoch [4/4], Step [27375/34168], Loss: 5.0225\n",
      "Epoch [4/4], Step [27450/34168], Loss: 4.9995\n",
      "Epoch [4/4], Step [27525/34168], Loss: 5.1055\n",
      "Epoch [4/4], Step [27600/34168], Loss: 5.0286\n",
      "Epoch [4/4], Step [27675/34168], Loss: 5.0074\n",
      "Epoch [4/4], Step [27750/34168], Loss: 5.1448\n",
      "Epoch [4/4], Step [27825/34168], Loss: 5.0795\n",
      "Epoch [4/4], Step [27900/34168], Loss: 4.9941\n",
      "Epoch [4/4], Step [27975/34168], Loss: 5.0608\n",
      "Epoch [4/4], Step [28050/34168], Loss: 5.0948\n",
      "Epoch [4/4], Step [28125/34168], Loss: 5.0696\n",
      "Epoch [4/4], Step [28200/34168], Loss: 5.1127\n",
      "Epoch [4/4], Step [28275/34168], Loss: 4.9085\n",
      "Epoch [4/4], Step [28350/34168], Loss: 5.0979\n",
      "Epoch [4/4], Step [28425/34168], Loss: 5.0996\n",
      "Epoch [4/4], Step [28500/34168], Loss: 5.1438\n",
      "Epoch [4/4], Step [28575/34168], Loss: 5.0395\n",
      "Epoch [4/4], Step [28650/34168], Loss: 4.9816\n",
      "Epoch [4/4], Step [28725/34168], Loss: 5.0169\n",
      "Epoch [4/4], Step [28800/34168], Loss: 5.0761\n",
      "Epoch [4/4], Step [28875/34168], Loss: 5.0317\n",
      "Epoch [4/4], Step [28950/34168], Loss: 5.0764\n",
      "Epoch [4/4], Step [29025/34168], Loss: 5.1258\n",
      "Epoch [4/4], Step [29100/34168], Loss: 5.1393\n",
      "Epoch [4/4], Step [29175/34168], Loss: 5.0227\n",
      "Epoch [4/4], Step [29250/34168], Loss: 5.0453\n",
      "Epoch [4/4], Step [29325/34168], Loss: 5.1647\n",
      "Epoch [4/4], Step [29400/34168], Loss: 4.9774\n",
      "Epoch [4/4], Step [29475/34168], Loss: 5.1628\n",
      "Epoch [4/4], Step [29550/34168], Loss: 4.9887\n",
      "Epoch [4/4], Step [29625/34168], Loss: 5.0878\n",
      "Epoch [4/4], Step [29700/34168], Loss: 5.0390\n",
      "Epoch [4/4], Step [29775/34168], Loss: 5.0137\n",
      "Epoch [4/4], Step [29850/34168], Loss: 5.0372\n",
      "Epoch [4/4], Step [29925/34168], Loss: 4.9709\n",
      "Epoch [4/4], Step [30000/34168], Loss: 5.1739\n",
      "Validation perplexity: 135.6875644190647\n",
      "Epoch [4/4], Step [30075/34168], Loss: 4.9512\n",
      "Epoch [4/4], Step [30150/34168], Loss: 5.0970\n",
      "Epoch [4/4], Step [30225/34168], Loss: 4.9831\n",
      "Epoch [4/4], Step [30300/34168], Loss: 5.1349\n",
      "Epoch [4/4], Step [30375/34168], Loss: 5.0506\n",
      "Epoch [4/4], Step [30450/34168], Loss: 5.0247\n",
      "Epoch [4/4], Step [30525/34168], Loss: 5.0887\n",
      "Epoch [4/4], Step [30600/34168], Loss: 5.0501\n",
      "Epoch [4/4], Step [30675/34168], Loss: 5.0118\n",
      "Epoch [4/4], Step [30750/34168], Loss: 5.0504\n",
      "Epoch [4/4], Step [30825/34168], Loss: 5.0751\n",
      "Epoch [4/4], Step [30900/34168], Loss: 5.0131\n",
      "Epoch [4/4], Step [30975/34168], Loss: 5.1383\n",
      "Epoch [4/4], Step [31050/34168], Loss: 5.0899\n",
      "Epoch [4/4], Step [31125/34168], Loss: 5.1927\n",
      "Epoch [4/4], Step [31200/34168], Loss: 5.0618\n",
      "Epoch [4/4], Step [31275/34168], Loss: 4.9365\n",
      "Epoch [4/4], Step [31350/34168], Loss: 5.0804\n",
      "Epoch [4/4], Step [31425/34168], Loss: 5.0791\n",
      "Epoch [4/4], Step [31500/34168], Loss: 4.9787\n",
      "Epoch [4/4], Step [31575/34168], Loss: 5.0938\n",
      "Epoch [4/4], Step [31650/34168], Loss: 4.9889\n",
      "Epoch [4/4], Step [31725/34168], Loss: 5.0446\n",
      "Epoch [4/4], Step [31800/34168], Loss: 5.0253\n",
      "Epoch [4/4], Step [31875/34168], Loss: 5.0363\n",
      "Epoch [4/4], Step [31950/34168], Loss: 5.0037\n",
      "Epoch [4/4], Step [32025/34168], Loss: 5.0288\n",
      "Epoch [4/4], Step [32100/34168], Loss: 5.0184\n",
      "Epoch [4/4], Step [32175/34168], Loss: 5.1236\n",
      "Epoch [4/4], Step [32250/34168], Loss: 5.0338\n",
      "Epoch [4/4], Step [32325/34168], Loss: 5.0584\n",
      "Epoch [4/4], Step [32400/34168], Loss: 4.9617\n",
      "Epoch [4/4], Step [32475/34168], Loss: 5.1386\n",
      "Epoch [4/4], Step [32550/34168], Loss: 5.0693\n",
      "Epoch [4/4], Step [32625/34168], Loss: 4.9742\n",
      "Epoch [4/4], Step [32700/34168], Loss: 5.0566\n",
      "Epoch [4/4], Step [32775/34168], Loss: 4.9753\n",
      "Epoch [4/4], Step [32850/34168], Loss: 5.0174\n",
      "Epoch [4/4], Step [32925/34168], Loss: 5.0087\n",
      "Epoch [4/4], Step [33000/34168], Loss: 5.1132\n",
      "Epoch [4/4], Step [33075/34168], Loss: 5.0895\n",
      "Epoch [4/4], Step [33150/34168], Loss: 4.9828\n",
      "Epoch [4/4], Step [33225/34168], Loss: 4.9460\n",
      "Epoch [4/4], Step [33300/34168], Loss: 5.0679\n",
      "Epoch [4/4], Step [33375/34168], Loss: 5.0151\n",
      "Epoch [4/4], Step [33450/34168], Loss: 5.0443\n",
      "Epoch [4/4], Step [33525/34168], Loss: 5.0927\n",
      "Epoch [4/4], Step [33600/34168], Loss: 5.0442\n",
      "Epoch [4/4], Step [33675/34168], Loss: 5.1040\n",
      "Epoch [4/4], Step [33750/34168], Loss: 5.0790\n",
      "Epoch [4/4], Step [33825/34168], Loss: 4.9365\n",
      "Epoch [4/4], Step [33900/34168], Loss: 5.1710\n",
      "Epoch [4/4], Step [33975/34168], Loss: 5.0515\n",
      "Epoch [4/4], Step [34050/34168], Loss: 5.1785\n",
      "Epoch [4/4], Step [34125/34168], Loss: 5.1090\n",
      "Epoch [4/4] Average Loss: 5.0657, Perplexity: 158.49\n"
     ]
    }
   ],
   "source": [
    "from src.model import RegularizedLanguageModel\n",
    "from src.trainComplete import TrainComplete\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,allowed_special=False)\n",
    "\n",
    "model = RegularizedLanguageModel(vocab_size, embedding_dim, context_length, dropout=0.2).to(device)\n",
    "\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_standardLinear_ep4_batchsize64\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 64,\n",
    "              embedding_dim = 128,\n",
    "              context_length = 32,\n",
    "              num_epochs = 4\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095bcd72-1159-45df-86ca-dc74a9a9bddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b114ef8-aba8-478e-9e48-19bfb82a4559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e56383a9-cfa9-4a08-ac9b-dd478cd634eb",
   "metadata": {},
   "source": [
    "## Generate Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9481004-79e1-46bc-9981-809f17576d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import generate_text\n",
    "start_text = \" من در راه\"\n",
    "for x in range(10):\n",
    "    generated_text = generate_text(model, tokenizer, start_text, device=device, context_length=20)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c60278-566c-454d-b078-e9d07de941c9",
   "metadata": {},
   "source": [
    "# More Training Cells "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224e67e-4246-4e77-8463-5e80cb2de3d8",
   "metadata": {},
   "source": [
    "##  Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8a220-0d91-4bfb-9096-5e7f847fb103",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Evening Training RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e72aec-83d6-4104-be20-e3970605fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import RegularizedLanguageModel\n",
    "from src.trainComplete import TrainComplete\n",
    "from src.helper  import clean_pers_text_replace, get_cleaned_text,clean_pers_remove,clean_text_pers_both\n",
    "\n",
    "raw_text = get_cleaned_text(text_path,clean_pers_remove)\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,allowed_special=False)\n",
    "\n",
    "model = RegularizedLanguageModel(vocab_size, embedding_dim, context_length, dropout=0.2).to(device)\n",
    "\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_standardLinearNotRelu_ep4_evaluate10000_preprocessingRemove\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 32,\n",
    "              embedding_dim = 128,\n",
    "              context_length = 32,\n",
    "              num_epochs = 4\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbebfe0-d4ad-449c-a8d3-c9dd7c347c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import RegularizedLanguageModel\n",
    "from src.trainComplete import TrainComplete\n",
    "from src.helper  import clean_pers_text_replace, get_cleaned_text,clean_pers_remove,clean_text_pers_both\n",
    "\n",
    "raw_text = get_cleaned_text(text_path,clean_text_pers_both)\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,allowed_special=False)\n",
    "\n",
    "model = RegularizedLanguageModel(vocab_size, embedding_dim, context_length, dropout=0.2).to(device)\n",
    "\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_standardLinearNotRelu_ep4_evaluate10000_preprocessingBoth\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 32,\n",
    "              embedding_dim = 128,\n",
    "              context_length = 32,\n",
    "              num_epochs = 4\n",
    "             )\n",
    "\n",
    "\n",
    "raw_text = get_cleaned_text(text_path,clean_pers_text_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d456d47-5f5f-4c89-922a-a3ef7d6c10b0",
   "metadata": {},
   "source": [
    "### Current Training Runn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f7291-5dae-4d59-a41f-258b3c138751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create Dataset 2720000 / 2733504 2200000 / 2733504Started Training\n",
      "Epoch [1/10], Step [0/68337], Loss: 10.2985\n",
      "Validation perplexity: 29479.28083287785\n",
      "Epoch [1/10], Step [75/68337], Loss: 7.2926\n",
      "Epoch [1/10], Step [150/68337], Loss: 7.2566\n",
      "Epoch [1/10], Step [225/68337], Loss: 7.1984\n",
      "Epoch [1/10], Step [300/68337], Loss: 7.0778\n",
      "Epoch [1/10], Step [375/68337], Loss: 7.0588\n",
      "Epoch [1/10], Step [450/68337], Loss: 7.0533\n",
      "Epoch [1/10], Step [525/68337], Loss: 7.0211\n",
      "Epoch [1/10], Step [600/68337], Loss: 6.9933\n",
      "Epoch [1/10], Step [675/68337], Loss: 7.1365\n",
      "Epoch [1/10], Step [750/68337], Loss: 6.8697\n",
      "Epoch [1/10], Step [825/68337], Loss: 6.8120\n",
      "Epoch [1/10], Step [900/68337], Loss: 6.8621\n",
      "Epoch [1/10], Step [975/68337], Loss: 6.9619\n",
      "Epoch [1/10], Step [1050/68337], Loss: 6.8166\n",
      "Epoch [1/10], Step [1125/68337], Loss: 6.7020\n",
      "Epoch [1/10], Step [1200/68337], Loss: 6.5050\n",
      "Epoch [1/10], Step [1275/68337], Loss: 6.8385\n",
      "Epoch [1/10], Step [1350/68337], Loss: 6.6040\n",
      "Epoch [1/10], Step [1425/68337], Loss: 6.6414\n",
      "Epoch [1/10], Step [1500/68337], Loss: 6.6489\n",
      "Epoch [1/10], Step [1575/68337], Loss: 6.6165\n",
      "Epoch [1/10], Step [1650/68337], Loss: 6.7131\n",
      "Epoch [1/10], Step [1725/68337], Loss: 6.4411\n",
      "Epoch [1/10], Step [1800/68337], Loss: 6.5363\n",
      "Epoch [1/10], Step [1875/68337], Loss: 6.6730\n",
      "Epoch [1/10], Step [1950/68337], Loss: 6.5924\n",
      "Epoch [1/10], Step [2025/68337], Loss: 6.5660\n",
      "Epoch [1/10], Step [2100/68337], Loss: 6.1287\n",
      "Epoch [1/10], Step [2175/68337], Loss: 6.5807\n",
      "Epoch [1/10], Step [2250/68337], Loss: 6.5679\n",
      "Epoch [1/10], Step [2325/68337], Loss: 6.4673\n",
      "Epoch [1/10], Step [2400/68337], Loss: 6.4206\n",
      "Epoch [1/10], Step [2475/68337], Loss: 6.5361\n",
      "Epoch [1/10], Step [2550/68337], Loss: 6.5440\n",
      "Epoch [1/10], Step [2625/68337], Loss: 6.4473\n",
      "Epoch [1/10], Step [2700/68337], Loss: 6.4252\n",
      "Epoch [1/10], Step [2775/68337], Loss: 6.5757\n",
      "Epoch [1/10], Step [2850/68337], Loss: 6.1471\n",
      "Epoch [1/10], Step [2925/68337], Loss: 6.3481\n",
      "Epoch [1/10], Step [3000/68337], Loss: 6.5638\n",
      "Epoch [1/10], Step [3075/68337], Loss: 6.3883\n",
      "Epoch [1/10], Step [3150/68337], Loss: 6.1295\n",
      "Epoch [1/10], Step [3225/68337], Loss: 6.2916\n",
      "Epoch [1/10], Step [3300/68337], Loss: 6.0696\n",
      "Epoch [1/10], Step [3375/68337], Loss: 6.3261\n",
      "Epoch [1/10], Step [3450/68337], Loss: 6.1794\n",
      "Epoch [1/10], Step [3525/68337], Loss: 6.1608\n",
      "Epoch [1/10], Step [3600/68337], Loss: 6.1771\n",
      "Epoch [1/10], Step [3675/68337], Loss: 6.3879\n",
      "Epoch [1/10], Step [3750/68337], Loss: 6.0653\n",
      "Epoch [1/10], Step [3825/68337], Loss: 6.2139\n",
      "Epoch [1/10], Step [3900/68337], Loss: 6.3565\n",
      "Epoch [1/10], Step [3975/68337], Loss: 6.2947\n",
      "Epoch [1/10], Step [4050/68337], Loss: 6.1711\n",
      "Epoch [1/10], Step [4125/68337], Loss: 6.2012\n",
      "Epoch [1/10], Step [4200/68337], Loss: 6.2445\n",
      "Epoch [1/10], Step [4275/68337], Loss: 6.2135\n",
      "Epoch [1/10], Step [4350/68337], Loss: 6.0775\n",
      "Epoch [1/10], Step [4425/68337], Loss: 6.3161\n",
      "Epoch [1/10], Step [4500/68337], Loss: 6.0744\n",
      "Epoch [1/10], Step [4575/68337], Loss: 6.1642\n",
      "Epoch [1/10], Step [4650/68337], Loss: 6.1547\n",
      "Epoch [1/10], Step [4725/68337], Loss: 6.4178\n",
      "Epoch [1/10], Step [4800/68337], Loss: 5.7811\n",
      "Epoch [1/10], Step [4875/68337], Loss: 6.2694\n",
      "Epoch [1/10], Step [4950/68337], Loss: 5.8884\n",
      "Epoch [1/10], Step [5025/68337], Loss: 6.1381\n",
      "Epoch [1/10], Step [5100/68337], Loss: 6.2241\n",
      "Epoch [1/10], Step [5175/68337], Loss: 6.0833\n",
      "Epoch [1/10], Step [5250/68337], Loss: 5.8779\n",
      "Epoch [1/10], Step [5325/68337], Loss: 6.2247\n",
      "Epoch [1/10], Step [5400/68337], Loss: 5.8744\n",
      "Epoch [1/10], Step [5475/68337], Loss: 5.9671\n",
      "Epoch [1/10], Step [5550/68337], Loss: 6.0124\n",
      "Epoch [1/10], Step [5625/68337], Loss: 6.4525\n",
      "Epoch [1/10], Step [5700/68337], Loss: 6.1101\n",
      "Epoch [1/10], Step [5775/68337], Loss: 6.1475\n",
      "Epoch [1/10], Step [5850/68337], Loss: 6.1716\n",
      "Epoch [1/10], Step [5925/68337], Loss: 6.0508\n",
      "Epoch [1/10], Step [6000/68337], Loss: 5.9427\n",
      "Epoch [1/10], Step [6075/68337], Loss: 6.1663\n",
      "Epoch [1/10], Step [6150/68337], Loss: 6.0938\n",
      "Epoch [1/10], Step [6225/68337], Loss: 6.0758\n",
      "Epoch [1/10], Step [6300/68337], Loss: 5.9182\n",
      "Epoch [1/10], Step [6375/68337], Loss: 6.1725\n",
      "Epoch [1/10], Step [6450/68337], Loss: 5.9096\n",
      "Epoch [1/10], Step [6525/68337], Loss: 6.1603\n",
      "Epoch [1/10], Step [6600/68337], Loss: 5.8501\n",
      "Epoch [1/10], Step [6675/68337], Loss: 6.0648\n",
      "Epoch [1/10], Step [6750/68337], Loss: 6.2515\n",
      "Epoch [1/10], Step [6825/68337], Loss: 6.1661\n",
      "Epoch [1/10], Step [6900/68337], Loss: 6.1460\n",
      "Epoch [1/10], Step [6975/68337], Loss: 5.9264\n",
      "Epoch [1/10], Step [7050/68337], Loss: 5.8197\n",
      "Epoch [1/10], Step [7125/68337], Loss: 6.0218\n",
      "Epoch [1/10], Step [7200/68337], Loss: 5.8838\n",
      "Epoch [1/10], Step [7275/68337], Loss: 5.8854\n",
      "Epoch [1/10], Step [7350/68337], Loss: 6.0896\n",
      "Epoch [1/10], Step [7425/68337], Loss: 6.0957\n",
      "Epoch [1/10], Step [7500/68337], Loss: 5.9534\n",
      "Epoch [1/10], Step [7575/68337], Loss: 6.0092\n",
      "Epoch [1/10], Step [7650/68337], Loss: 5.9032\n",
      "Epoch [1/10], Step [7725/68337], Loss: 6.0608\n",
      "Epoch [1/10], Step [7800/68337], Loss: 5.7657\n",
      "Epoch [1/10], Step [7875/68337], Loss: 6.1023\n",
      "Epoch [1/10], Step [7950/68337], Loss: 5.8231\n",
      "Epoch [1/10], Step [8025/68337], Loss: 5.8499\n",
      "Epoch [1/10], Step [8100/68337], Loss: 5.8074\n",
      "Epoch [1/10], Step [8175/68337], Loss: 6.1093\n",
      "Epoch [1/10], Step [8250/68337], Loss: 5.9692\n",
      "Epoch [1/10], Step [8325/68337], Loss: 5.7392\n",
      "Epoch [1/10], Step [8400/68337], Loss: 5.8399\n",
      "Epoch [1/10], Step [8475/68337], Loss: 5.9263\n",
      "Epoch [1/10], Step [8550/68337], Loss: 5.7996\n",
      "Epoch [1/10], Step [8625/68337], Loss: 5.8246\n",
      "Epoch [1/10], Step [8700/68337], Loss: 5.8554\n",
      "Epoch [1/10], Step [8775/68337], Loss: 5.8449\n",
      "Epoch [1/10], Step [8850/68337], Loss: 5.8791\n",
      "Epoch [1/10], Step [8925/68337], Loss: 5.7814\n",
      "Epoch [1/10], Step [9000/68337], Loss: 5.9455\n",
      "Epoch [1/10], Step [9075/68337], Loss: 5.7227\n",
      "Epoch [1/10], Step [9150/68337], Loss: 5.8522\n",
      "Epoch [1/10], Step [9225/68337], Loss: 5.8933\n",
      "Epoch [1/10], Step [9300/68337], Loss: 6.1243\n",
      "Epoch [1/10], Step [9375/68337], Loss: 5.8355\n",
      "Epoch [1/10], Step [9450/68337], Loss: 5.7687\n",
      "Epoch [1/10], Step [9525/68337], Loss: 5.9998\n",
      "Epoch [1/10], Step [9600/68337], Loss: 5.7693\n",
      "Epoch [1/10], Step [9675/68337], Loss: 5.9322\n",
      "Epoch [1/10], Step [9750/68337], Loss: 6.0477\n",
      "Epoch [1/10], Step [9825/68337], Loss: 5.9529\n",
      "Epoch [1/10], Step [9900/68337], Loss: 5.8013\n",
      "Epoch [1/10], Step [9975/68337], Loss: 5.9209\n",
      "Validation perplexity: 299.2474180836743\n",
      "Epoch [1/10], Step [10050/68337], Loss: 5.8178\n",
      "Epoch [1/10], Step [10125/68337], Loss: 5.8635\n",
      "Epoch [1/10], Step [10200/68337], Loss: 5.7421\n",
      "Epoch [1/10], Step [10275/68337], Loss: 5.8353\n",
      "Epoch [1/10], Step [10350/68337], Loss: 5.8573\n",
      "Epoch [1/10], Step [10425/68337], Loss: 5.5958\n",
      "Epoch [1/10], Step [10500/68337], Loss: 5.8751\n",
      "Epoch [1/10], Step [10575/68337], Loss: 5.9198\n",
      "Epoch [1/10], Step [10650/68337], Loss: 5.9411\n",
      "Epoch [1/10], Step [10725/68337], Loss: 5.8032\n",
      "Epoch [1/10], Step [10800/68337], Loss: 5.8690\n",
      "Epoch [1/10], Step [10875/68337], Loss: 5.8274\n",
      "Epoch [1/10], Step [10950/68337], Loss: 5.7605\n",
      "Epoch [1/10], Step [11025/68337], Loss: 5.8270\n",
      "Epoch [1/10], Step [11100/68337], Loss: 5.9302\n",
      "Epoch [1/10], Step [11175/68337], Loss: 5.7855\n",
      "Epoch [1/10], Step [11250/68337], Loss: 5.9419\n",
      "Epoch [1/10], Step [11325/68337], Loss: 5.8260\n",
      "Epoch [1/10], Step [11400/68337], Loss: 5.9107\n",
      "Epoch [1/10], Step [11475/68337], Loss: 5.6698\n",
      "Epoch [1/10], Step [11550/68337], Loss: 5.9481\n",
      "Epoch [1/10], Step [11625/68337], Loss: 5.8222\n",
      "Epoch [1/10], Step [11700/68337], Loss: 5.8000\n",
      "Epoch [1/10], Step [11775/68337], Loss: 5.6175\n",
      "Epoch [1/10], Step [11850/68337], Loss: 5.7421\n",
      "Epoch [1/10], Step [11925/68337], Loss: 5.8546\n",
      "Epoch [1/10], Step [12000/68337], Loss: 5.8012\n",
      "Epoch [1/10], Step [12075/68337], Loss: 5.7167\n",
      "Epoch [1/10], Step [12150/68337], Loss: 5.7116\n",
      "Epoch [1/10], Step [12225/68337], Loss: 5.6973\n",
      "Epoch [1/10], Step [12300/68337], Loss: 5.8199\n",
      "Epoch [1/10], Step [12375/68337], Loss: 6.0714\n",
      "Epoch [1/10], Step [12450/68337], Loss: 5.6166\n",
      "Epoch [1/10], Step [12525/68337], Loss: 5.8311\n",
      "Epoch [1/10], Step [12600/68337], Loss: 5.7805\n",
      "Epoch [1/10], Step [12675/68337], Loss: 5.7407\n",
      "Epoch [1/10], Step [12750/68337], Loss: 5.6867\n",
      "Epoch [1/10], Step [12825/68337], Loss: 5.5826\n",
      "Epoch [1/10], Step [12900/68337], Loss: 5.7712\n",
      "Epoch [1/10], Step [12975/68337], Loss: 5.8045\n",
      "Epoch [1/10], Step [13050/68337], Loss: 5.8343\n",
      "Epoch [1/10], Step [13125/68337], Loss: 5.5969\n",
      "Epoch [1/10], Step [13200/68337], Loss: 5.8012\n",
      "Epoch [1/10], Step [13275/68337], Loss: 5.8616\n",
      "Epoch [1/10], Step [13350/68337], Loss: 5.7329\n",
      "Epoch [1/10], Step [13425/68337], Loss: 5.7891\n",
      "Epoch [1/10], Step [13500/68337], Loss: 5.9489\n",
      "Epoch [1/10], Step [13575/68337], Loss: 5.8498\n",
      "Epoch [1/10], Step [13650/68337], Loss: 5.6800\n",
      "Epoch [1/10], Step [13725/68337], Loss: 5.7550\n",
      "Epoch [1/10], Step [13800/68337], Loss: 5.4887\n",
      "Epoch [1/10], Step [13875/68337], Loss: 5.8188\n",
      "Epoch [1/10], Step [13950/68337], Loss: 5.7311\n",
      "Epoch [1/10], Step [14025/68337], Loss: 5.8081\n",
      "Epoch [1/10], Step [14100/68337], Loss: 5.8523\n",
      "Epoch [1/10], Step [14175/68337], Loss: 5.8056\n",
      "Epoch [1/10], Step [14250/68337], Loss: 5.5894\n",
      "Epoch [1/10], Step [14325/68337], Loss: 5.9483\n",
      "Epoch [1/10], Step [14400/68337], Loss: 5.7899\n",
      "Epoch [1/10], Step [14475/68337], Loss: 6.1024\n",
      "Epoch [1/10], Step [14550/68337], Loss: 5.8000\n",
      "Epoch [1/10], Step [14625/68337], Loss: 6.0293\n",
      "Epoch [1/10], Step [14700/68337], Loss: 5.7071\n",
      "Epoch [1/10], Step [14775/68337], Loss: 5.7056\n",
      "Epoch [1/10], Step [14850/68337], Loss: 5.7120\n",
      "Epoch [1/10], Step [14925/68337], Loss: 5.5933\n",
      "Epoch [1/10], Step [15000/68337], Loss: 5.7222\n",
      "Epoch [1/10], Step [15075/68337], Loss: 5.5147\n",
      "Epoch [1/10], Step [15150/68337], Loss: 5.7115\n",
      "Epoch [1/10], Step [15225/68337], Loss: 5.4620\n",
      "Epoch [1/10], Step [15300/68337], Loss: 5.7573\n",
      "Epoch [1/10], Step [15375/68337], Loss: 5.7660\n",
      "Epoch [1/10], Step [15450/68337], Loss: 5.5769\n",
      "Epoch [1/10], Step [15525/68337], Loss: 5.6267\n",
      "Epoch [1/10], Step [15600/68337], Loss: 5.6010\n",
      "Epoch [1/10], Step [15675/68337], Loss: 5.6438\n",
      "Epoch [1/10], Step [15750/68337], Loss: 5.8237\n",
      "Epoch [1/10], Step [15825/68337], Loss: 5.7613\n",
      "Epoch [1/10], Step [15900/68337], Loss: 5.5864\n",
      "Epoch [1/10], Step [15975/68337], Loss: 5.8468\n",
      "Epoch [1/10], Step [16050/68337], Loss: 5.4334\n",
      "Epoch [1/10], Step [16125/68337], Loss: 5.6204\n",
      "Epoch [1/10], Step [16200/68337], Loss: 5.7380\n",
      "Epoch [1/10], Step [16275/68337], Loss: 5.6624\n",
      "Epoch [1/10], Step [16350/68337], Loss: 5.8680\n",
      "Epoch [1/10], Step [16425/68337], Loss: 5.6464\n",
      "Epoch [1/10], Step [16500/68337], Loss: 5.7861\n",
      "Epoch [1/10], Step [16575/68337], Loss: 5.3929\n",
      "Epoch [1/10], Step [16650/68337], Loss: 5.6824\n",
      "Epoch [1/10], Step [16725/68337], Loss: 5.6877\n",
      "Epoch [1/10], Step [16800/68337], Loss: 5.6084\n",
      "Epoch [1/10], Step [16875/68337], Loss: 5.6325\n",
      "Epoch [1/10], Step [16950/68337], Loss: 5.4349\n",
      "Epoch [1/10], Step [17025/68337], Loss: 5.6853\n",
      "Epoch [1/10], Step [17100/68337], Loss: 5.6891\n",
      "Epoch [1/10], Step [17175/68337], Loss: 5.7559\n",
      "Epoch [1/10], Step [17250/68337], Loss: 5.6695\n",
      "Epoch [1/10], Step [17325/68337], Loss: 5.7861\n",
      "Epoch [1/10], Step [17400/68337], Loss: 5.8020\n",
      "Epoch [1/10], Step [17475/68337], Loss: 5.6912\n",
      "Epoch [1/10], Step [17550/68337], Loss: 5.9193\n",
      "Epoch [1/10], Step [17625/68337], Loss: 5.6302\n",
      "Epoch [1/10], Step [17700/68337], Loss: 5.6137\n",
      "Epoch [1/10], Step [17775/68337], Loss: 5.5526\n",
      "Epoch [1/10], Step [17850/68337], Loss: 5.6536\n",
      "Epoch [1/10], Step [17925/68337], Loss: 5.8508\n",
      "Epoch [1/10], Step [18000/68337], Loss: 5.8671\n",
      "Epoch [1/10], Step [18075/68337], Loss: 5.7095\n",
      "Epoch [1/10], Step [18150/68337], Loss: 5.8521\n",
      "Epoch [1/10], Step [18225/68337], Loss: 5.6687\n",
      "Epoch [1/10], Step [18300/68337], Loss: 5.7350\n",
      "Epoch [1/10], Step [18375/68337], Loss: 5.5172\n",
      "Epoch [1/10], Step [18450/68337], Loss: 5.6250\n",
      "Epoch [1/10], Step [18525/68337], Loss: 5.6032\n",
      "Epoch [1/10], Step [18600/68337], Loss: 5.6276\n",
      "Epoch [1/10], Step [18675/68337], Loss: 5.8269\n",
      "Epoch [1/10], Step [18750/68337], Loss: 5.4349\n",
      "Epoch [1/10], Step [18825/68337], Loss: 5.6978\n",
      "Epoch [1/10], Step [18900/68337], Loss: 5.7074\n",
      "Epoch [1/10], Step [18975/68337], Loss: 5.8260\n",
      "Epoch [1/10], Step [19050/68337], Loss: 5.6835\n",
      "Epoch [1/10], Step [19125/68337], Loss: 5.5838\n",
      "Epoch [1/10], Step [19200/68337], Loss: 5.5370\n",
      "Epoch [1/10], Step [19275/68337], Loss: 5.6131\n",
      "Epoch [1/10], Step [19350/68337], Loss: 5.6383\n",
      "Epoch [1/10], Step [19425/68337], Loss: 5.5642\n",
      "Epoch [1/10], Step [19500/68337], Loss: 5.7158\n",
      "Epoch [1/10], Step [19575/68337], Loss: 5.4221\n",
      "Epoch [1/10], Step [19650/68337], Loss: 5.5404\n",
      "Epoch [1/10], Step [19725/68337], Loss: 5.5966\n",
      "Epoch [1/10], Step [19800/68337], Loss: 5.4861\n",
      "Epoch [1/10], Step [19875/68337], Loss: 5.5605\n",
      "Epoch [1/10], Step [19950/68337], Loss: 5.6022\n",
      "Validation perplexity: 232.53113341797106\n",
      "Epoch [1/10], Step [20025/68337], Loss: 5.5391\n",
      "Epoch [1/10], Step [20100/68337], Loss: 5.3555\n",
      "Epoch [1/10], Step [20175/68337], Loss: 5.8519\n",
      "Epoch [1/10], Step [20250/68337], Loss: 5.6611\n",
      "Epoch [1/10], Step [20325/68337], Loss: 5.5970\n",
      "Epoch [1/10], Step [20400/68337], Loss: 5.6865\n",
      "Epoch [1/10], Step [20475/68337], Loss: 5.4992\n",
      "Epoch [1/10], Step [20550/68337], Loss: 5.6901\n",
      "Epoch [1/10], Step [20625/68337], Loss: 5.6044\n",
      "Epoch [1/10], Step [20700/68337], Loss: 5.6967\n",
      "Epoch [1/10], Step [20775/68337], Loss: 5.8219\n",
      "Epoch [1/10], Step [20850/68337], Loss: 5.4395\n",
      "Epoch [1/10], Step [20925/68337], Loss: 5.5498\n",
      "Epoch [1/10], Step [21000/68337], Loss: 5.5038\n",
      "Epoch [1/10], Step [21075/68337], Loss: 5.5851\n",
      "Epoch [1/10], Step [21150/68337], Loss: 5.5442\n",
      "Epoch [1/10], Step [21225/68337], Loss: 5.7665\n",
      "Epoch [1/10], Step [21300/68337], Loss: 5.5154\n",
      "Epoch [1/10], Step [21375/68337], Loss: 5.7467\n",
      "Epoch [1/10], Step [21450/68337], Loss: 5.6567\n",
      "Epoch [1/10], Step [21525/68337], Loss: 5.6456\n",
      "Epoch [1/10], Step [21600/68337], Loss: 5.8510\n",
      "Epoch [1/10], Step [21675/68337], Loss: 5.6947\n",
      "Epoch [1/10], Step [21750/68337], Loss: 5.6830\n",
      "Epoch [1/10], Step [21825/68337], Loss: 5.4402\n",
      "Epoch [1/10], Step [21900/68337], Loss: 5.6416\n",
      "Epoch [1/10], Step [21975/68337], Loss: 5.5111\n",
      "Epoch [1/10], Step [22050/68337], Loss: 5.5142\n",
      "Epoch [1/10], Step [22125/68337], Loss: 5.8165\n",
      "Epoch [1/10], Step [22200/68337], Loss: 5.5905\n",
      "Epoch [1/10], Step [22275/68337], Loss: 5.8224\n",
      "Epoch [1/10], Step [22350/68337], Loss: 5.6111\n",
      "Epoch [1/10], Step [22425/68337], Loss: 5.4947\n",
      "Epoch [1/10], Step [22500/68337], Loss: 5.8351\n",
      "Epoch [1/10], Step [22575/68337], Loss: 5.6281\n",
      "Epoch [1/10], Step [22650/68337], Loss: 5.5278\n",
      "Epoch [1/10], Step [22725/68337], Loss: 5.7346\n",
      "Epoch [1/10], Step [22800/68337], Loss: 5.6896\n",
      "Epoch [1/10], Step [22875/68337], Loss: 5.6701\n",
      "Epoch [1/10], Step [22950/68337], Loss: 5.5775\n",
      "Epoch [1/10], Step [23025/68337], Loss: 5.6276\n",
      "Epoch [1/10], Step [23100/68337], Loss: 5.7080\n",
      "Epoch [1/10], Step [23175/68337], Loss: 5.3656\n",
      "Epoch [1/10], Step [23250/68337], Loss: 5.5306\n",
      "Epoch [1/10], Step [23325/68337], Loss: 5.5468\n",
      "Epoch [1/10], Step [23400/68337], Loss: 5.4012\n",
      "Epoch [1/10], Step [23475/68337], Loss: 5.6585\n",
      "Epoch [1/10], Step [23550/68337], Loss: 5.8337\n",
      "Epoch [1/10], Step [23625/68337], Loss: 5.6490\n",
      "Epoch [1/10], Step [23700/68337], Loss: 5.7246\n",
      "Epoch [1/10], Step [23775/68337], Loss: 5.7095\n",
      "Epoch [1/10], Step [23850/68337], Loss: 5.4866\n",
      "Epoch [1/10], Step [23925/68337], Loss: 5.5977\n",
      "Epoch [1/10], Step [24000/68337], Loss: 5.3741\n",
      "Epoch [1/10], Step [24075/68337], Loss: 5.5433\n",
      "Epoch [1/10], Step [24150/68337], Loss: 5.3602\n",
      "Epoch [1/10], Step [24225/68337], Loss: 5.3272\n",
      "Epoch [1/10], Step [24300/68337], Loss: 5.4026\n",
      "Epoch [1/10], Step [24375/68337], Loss: 5.5741\n",
      "Epoch [1/10], Step [24450/68337], Loss: 5.7924\n",
      "Epoch [1/10], Step [24525/68337], Loss: 5.7797\n",
      "Epoch [1/10], Step [24600/68337], Loss: 5.5140\n",
      "Epoch [1/10], Step [24675/68337], Loss: 5.6395\n",
      "Epoch [1/10], Step [24750/68337], Loss: 5.5594\n",
      "Epoch [1/10], Step [24825/68337], Loss: 5.4891\n",
      "Epoch [1/10], Step [24900/68337], Loss: 5.5462\n",
      "Epoch [1/10], Step [24975/68337], Loss: 5.7881\n",
      "Epoch [1/10], Step [25050/68337], Loss: 5.4655\n",
      "Epoch [1/10], Step [25125/68337], Loss: 5.6220\n",
      "Epoch [1/10], Step [25200/68337], Loss: 5.5851\n",
      "Epoch [1/10], Step [25275/68337], Loss: 5.4390\n",
      "Epoch [1/10], Step [25350/68337], Loss: 5.4280\n",
      "Epoch [1/10], Step [25425/68337], Loss: 5.3878\n",
      "Epoch [1/10], Step [25500/68337], Loss: 5.4501\n",
      "Epoch [1/10], Step [25575/68337], Loss: 5.6225\n",
      "Epoch [1/10], Step [25650/68337], Loss: 5.7351\n",
      "Epoch [1/10], Step [25725/68337], Loss: 5.7637\n",
      "Epoch [1/10], Step [25800/68337], Loss: 5.4883\n",
      "Epoch [1/10], Step [25875/68337], Loss: 5.6845\n",
      "Epoch [1/10], Step [25950/68337], Loss: 5.4504\n",
      "Epoch [1/10], Step [26025/68337], Loss: 5.5834\n",
      "Epoch [1/10], Step [26100/68337], Loss: 5.5741\n",
      "Epoch [1/10], Step [26175/68337], Loss: 5.5480\n",
      "Epoch [1/10], Step [26250/68337], Loss: 5.3262\n",
      "Epoch [1/10], Step [26325/68337], Loss: 5.4250\n",
      "Epoch [1/10], Step [26400/68337], Loss: 5.7052\n",
      "Epoch [1/10], Step [26475/68337], Loss: 5.5943\n",
      "Epoch [1/10], Step [26550/68337], Loss: 5.6667\n",
      "Epoch [1/10], Step [26625/68337], Loss: 5.7882\n",
      "Epoch [1/10], Step [26700/68337], Loss: 5.4771\n",
      "Epoch [1/10], Step [26775/68337], Loss: 5.3883\n",
      "Epoch [1/10], Step [26850/68337], Loss: 5.5678\n",
      "Epoch [1/10], Step [26925/68337], Loss: 5.6966\n",
      "Epoch [1/10], Step [27000/68337], Loss: 5.5760\n",
      "Epoch [1/10], Step [27075/68337], Loss: 5.6414\n",
      "Epoch [1/10], Step [27150/68337], Loss: 5.6695\n",
      "Epoch [1/10], Step [27225/68337], Loss: 5.5407\n",
      "Epoch [1/10], Step [27300/68337], Loss: 5.5078\n",
      "Epoch [1/10], Step [27375/68337], Loss: 5.3392\n",
      "Epoch [1/10], Step [27450/68337], Loss: 5.4641\n",
      "Epoch [1/10], Step [27525/68337], Loss: 5.4364\n",
      "Epoch [1/10], Step [27600/68337], Loss: 5.4840\n",
      "Epoch [1/10], Step [27675/68337], Loss: 5.2965\n",
      "Epoch [1/10], Step [27750/68337], Loss: 5.4391\n",
      "Epoch [1/10], Step [27825/68337], Loss: 5.4331\n",
      "Epoch [1/10], Step [27900/68337], Loss: 5.6530\n",
      "Epoch [1/10], Step [27975/68337], Loss: 5.5731\n",
      "Epoch [1/10], Step [28050/68337], Loss: 5.5575\n",
      "Epoch [1/10], Step [28125/68337], Loss: 5.7010\n",
      "Epoch [1/10], Step [28200/68337], Loss: 5.3238\n",
      "Epoch [1/10], Step [28275/68337], Loss: 5.5834\n",
      "Epoch [1/10], Step [28350/68337], Loss: 5.5036\n",
      "Epoch [1/10], Step [28425/68337], Loss: 5.4080\n",
      "Epoch [1/10], Step [28500/68337], Loss: 5.5462\n",
      "Epoch [1/10], Step [28575/68337], Loss: 5.5989\n",
      "Epoch [1/10], Step [28650/68337], Loss: 5.7336\n",
      "Epoch [1/10], Step [28725/68337], Loss: 5.6000\n",
      "Epoch [1/10], Step [28800/68337], Loss: 5.5422\n",
      "Epoch [1/10], Step [28875/68337], Loss: 5.8153\n",
      "Epoch [1/10], Step [28950/68337], Loss: 5.7251\n",
      "Epoch [1/10], Step [29025/68337], Loss: 5.4984\n",
      "Epoch [1/10], Step [29100/68337], Loss: 5.5073\n",
      "Epoch [1/10], Step [29175/68337], Loss: 5.4223\n",
      "Epoch [1/10], Step [29250/68337], Loss: 5.3893\n",
      "Epoch [1/10], Step [29325/68337], Loss: 5.6771\n",
      "Epoch [1/10], Step [29400/68337], Loss: 5.4047\n",
      "Epoch [1/10], Step [29475/68337], Loss: 5.4994\n",
      "Epoch [1/10], Step [29550/68337], Loss: 5.4967\n",
      "Epoch [1/10], Step [29625/68337], Loss: 5.5201\n",
      "Epoch [1/10], Step [29700/68337], Loss: 5.6094\n",
      "Epoch [1/10], Step [29775/68337], Loss: 5.6431\n",
      "Epoch [1/10], Step [29850/68337], Loss: 5.3626\n",
      "Epoch [1/10], Step [29925/68337], Loss: 5.4391\n",
      "Epoch [1/10], Step [30000/68337], Loss: 5.7260\n",
      "Validation perplexity: 203.7642616359582\n",
      "Epoch [1/10], Step [30075/68337], Loss: 5.3937\n",
      "Epoch [1/10], Step [30150/68337], Loss: 5.4812\n",
      "Epoch [1/10], Step [30225/68337], Loss: 5.6189\n",
      "Epoch [1/10], Step [30300/68337], Loss: 5.5779\n",
      "Epoch [1/10], Step [30375/68337], Loss: 5.6125\n",
      "Epoch [1/10], Step [30450/68337], Loss: 5.5492\n",
      "Epoch [1/10], Step [30525/68337], Loss: 5.5861\n",
      "Epoch [1/10], Step [30600/68337], Loss: 5.2924\n",
      "Epoch [1/10], Step [30675/68337], Loss: 5.5401\n",
      "Epoch [1/10], Step [30750/68337], Loss: 5.4727\n",
      "Epoch [1/10], Step [30825/68337], Loss: 5.6607\n",
      "Epoch [1/10], Step [30900/68337], Loss: 5.6063\n",
      "Epoch [1/10], Step [30975/68337], Loss: 5.4622\n",
      "Epoch [1/10], Step [31050/68337], Loss: 5.3639\n",
      "Epoch [1/10], Step [31125/68337], Loss: 5.4447\n",
      "Epoch [1/10], Step [31200/68337], Loss: 5.4639\n",
      "Epoch [1/10], Step [31275/68337], Loss: 5.4722\n",
      "Epoch [1/10], Step [31350/68337], Loss: 5.5649\n",
      "Epoch [1/10], Step [31425/68337], Loss: 5.4312\n",
      "Epoch [1/10], Step [31500/68337], Loss: 5.3423\n",
      "Epoch [1/10], Step [31575/68337], Loss: 5.4080\n",
      "Epoch [1/10], Step [31650/68337], Loss: 5.2639\n",
      "Epoch [1/10], Step [31725/68337], Loss: 5.3145\n",
      "Epoch [1/10], Step [31800/68337], Loss: 5.2750\n",
      "Epoch [1/10], Step [31875/68337], Loss: 5.5059\n",
      "Epoch [1/10], Step [31950/68337], Loss: 5.3513\n",
      "Epoch [1/10], Step [32025/68337], Loss: 5.3691\n",
      "Epoch [1/10], Step [32100/68337], Loss: 5.6071\n",
      "Epoch [1/10], Step [32175/68337], Loss: 5.5746\n",
      "Epoch [1/10], Step [32250/68337], Loss: 4.9727\n",
      "Epoch [1/10], Step [32325/68337], Loss: 5.3912\n",
      "Epoch [1/10], Step [32400/68337], Loss: 5.1817\n",
      "Epoch [1/10], Step [32475/68337], Loss: 5.5617\n",
      "Epoch [1/10], Step [32550/68337], Loss: 5.6173\n",
      "Epoch [1/10], Step [32625/68337], Loss: 5.4769\n",
      "Epoch [1/10], Step [32700/68337], Loss: 5.2091\n",
      "Epoch [1/10], Step [32775/68337], Loss: 5.8018\n",
      "Epoch [1/10], Step [32850/68337], Loss: 5.3221\n",
      "Epoch [1/10], Step [32925/68337], Loss: 5.8247\n",
      "Epoch [1/10], Step [33000/68337], Loss: 5.5240\n",
      "Epoch [1/10], Step [33075/68337], Loss: 5.6185\n",
      "Epoch [1/10], Step [33150/68337], Loss: 5.4877\n",
      "Epoch [1/10], Step [33225/68337], Loss: 5.4638\n",
      "Epoch [1/10], Step [33300/68337], Loss: 5.4011\n",
      "Epoch [1/10], Step [33375/68337], Loss: 5.5212\n",
      "Epoch [1/10], Step [33450/68337], Loss: 5.4408\n",
      "Epoch [1/10], Step [33525/68337], Loss: 5.2993\n",
      "Epoch [1/10], Step [33600/68337], Loss: 5.2540\n",
      "Epoch [1/10], Step [33675/68337], Loss: 5.6601\n",
      "Epoch [1/10], Step [33750/68337], Loss: 5.4284\n",
      "Epoch [1/10], Step [33825/68337], Loss: 5.4870\n",
      "Epoch [1/10], Step [33900/68337], Loss: 5.5378\n",
      "Epoch [1/10], Step [33975/68337], Loss: 5.4134\n",
      "Epoch [1/10], Step [34050/68337], Loss: 5.4677\n",
      "Epoch [1/10], Step [34125/68337], Loss: 5.5031\n",
      "Epoch [1/10], Step [34200/68337], Loss: 5.5143\n",
      "Epoch [1/10], Step [34275/68337], Loss: 5.4890\n",
      "Epoch [1/10], Step [34350/68337], Loss: 5.3478\n",
      "Epoch [1/10], Step [34425/68337], Loss: 5.5421\n",
      "Epoch [1/10], Step [34500/68337], Loss: 5.5541\n",
      "Epoch [1/10], Step [34575/68337], Loss: 5.2958\n",
      "Epoch [1/10], Step [34650/68337], Loss: 5.4932\n",
      "Epoch [1/10], Step [34725/68337], Loss: 5.4306\n",
      "Epoch [1/10], Step [34800/68337], Loss: 5.5040\n",
      "Epoch [1/10], Step [34875/68337], Loss: 5.4864\n",
      "Epoch [1/10], Step [34950/68337], Loss: 5.3116\n",
      "Epoch [1/10], Step [35025/68337], Loss: 5.5679\n",
      "Epoch [1/10], Step [35100/68337], Loss: 5.6118\n",
      "Epoch [1/10], Step [35175/68337], Loss: 5.4665\n",
      "Epoch [1/10], Step [35250/68337], Loss: 5.5419\n",
      "Epoch [1/10], Step [35325/68337], Loss: 5.3837\n",
      "Epoch [1/10], Step [35400/68337], Loss: 5.5014\n",
      "Epoch [1/10], Step [35475/68337], Loss: 5.3749\n",
      "Epoch [1/10], Step [35550/68337], Loss: 5.3291\n",
      "Epoch [1/10], Step [35625/68337], Loss: 5.5129\n",
      "Epoch [1/10], Step [35700/68337], Loss: 5.3696\n",
      "Epoch [1/10], Step [35775/68337], Loss: 5.4216\n",
      "Epoch [1/10], Step [35850/68337], Loss: 5.2358\n",
      "Epoch [1/10], Step [35925/68337], Loss: 5.3230\n",
      "Epoch [1/10], Step [36000/68337], Loss: 5.4484\n",
      "Epoch [1/10], Step [36075/68337], Loss: 5.5881\n",
      "Epoch [1/10], Step [36150/68337], Loss: 5.5462\n",
      "Epoch [1/10], Step [36225/68337], Loss: 5.3433\n",
      "Epoch [1/10], Step [36300/68337], Loss: 5.4317\n",
      "Epoch [1/10], Step [36375/68337], Loss: 5.4841\n",
      "Epoch [1/10], Step [36450/68337], Loss: 5.2936\n",
      "Epoch [1/10], Step [36525/68337], Loss: 5.3021\n",
      "Epoch [1/10], Step [36600/68337], Loss: 5.2394\n",
      "Epoch [1/10], Step [36675/68337], Loss: 5.3239\n",
      "Epoch [1/10], Step [36750/68337], Loss: 5.4899\n",
      "Epoch [1/10], Step [36825/68337], Loss: 5.3578\n",
      "Epoch [1/10], Step [36900/68337], Loss: 5.3942\n",
      "Epoch [1/10], Step [36975/68337], Loss: 5.6802\n",
      "Epoch [1/10], Step [37050/68337], Loss: 5.4688\n",
      "Epoch [1/10], Step [37125/68337], Loss: 5.2986\n",
      "Epoch [1/10], Step [37200/68337], Loss: 5.2416\n",
      "Epoch [1/10], Step [37275/68337], Loss: 5.3687\n",
      "Epoch [1/10], Step [37350/68337], Loss: 5.1423\n",
      "Epoch [1/10], Step [37425/68337], Loss: 5.2577\n",
      "Epoch [1/10], Step [37500/68337], Loss: 5.3159\n",
      "Epoch [1/10], Step [37575/68337], Loss: 5.4877\n",
      "Epoch [1/10], Step [37650/68337], Loss: 5.5207\n",
      "Epoch [1/10], Step [37725/68337], Loss: 5.2622\n",
      "Epoch [1/10], Step [37800/68337], Loss: 5.2699\n",
      "Epoch [1/10], Step [37875/68337], Loss: 5.4171\n",
      "Epoch [1/10], Step [37950/68337], Loss: 5.4966\n",
      "Epoch [1/10], Step [38025/68337], Loss: 5.5181\n",
      "Epoch [1/10], Step [38100/68337], Loss: 5.5005\n",
      "Epoch [1/10], Step [38175/68337], Loss: 5.6001\n",
      "Epoch [1/10], Step [38250/68337], Loss: 5.3374\n",
      "Epoch [1/10], Step [38325/68337], Loss: 5.4660\n",
      "Epoch [1/10], Step [38400/68337], Loss: 5.4084\n",
      "Epoch [1/10], Step [38475/68337], Loss: 5.1569\n",
      "Epoch [1/10], Step [38550/68337], Loss: 5.3342\n",
      "Epoch [1/10], Step [38625/68337], Loss: 5.5736\n",
      "Epoch [1/10], Step [38700/68337], Loss: 5.4991\n",
      "Epoch [1/10], Step [38775/68337], Loss: 5.5337\n",
      "Epoch [1/10], Step [38850/68337], Loss: 5.4386\n",
      "Epoch [1/10], Step [38925/68337], Loss: 5.2841\n",
      "Epoch [1/10], Step [39000/68337], Loss: 5.2426\n",
      "Epoch [1/10], Step [39075/68337], Loss: 5.4531\n",
      "Epoch [1/10], Step [39150/68337], Loss: 5.5696\n",
      "Epoch [1/10], Step [39225/68337], Loss: 5.4457\n",
      "Epoch [1/10], Step [39300/68337], Loss: 5.0874\n",
      "Epoch [1/10], Step [39375/68337], Loss: 5.3448\n",
      "Epoch [1/10], Step [39450/68337], Loss: 5.2903\n",
      "Epoch [1/10], Step [39525/68337], Loss: 5.5798\n",
      "Epoch [1/10], Step [39600/68337], Loss: 5.3222\n",
      "Epoch [1/10], Step [39675/68337], Loss: 5.4569\n",
      "Epoch [1/10], Step [39750/68337], Loss: 5.4710\n",
      "Epoch [1/10], Step [39825/68337], Loss: 5.3793\n",
      "Epoch [1/10], Step [39900/68337], Loss: 5.5471\n",
      "Epoch [1/10], Step [39975/68337], Loss: 5.4555\n",
      "Validation perplexity: 187.3973258537753\n",
      "Epoch [1/10], Step [40050/68337], Loss: 5.2093\n",
      "Epoch [1/10], Step [40125/68337], Loss: 5.4757\n",
      "Epoch [1/10], Step [40200/68337], Loss: 5.6632\n",
      "Epoch [1/10], Step [40275/68337], Loss: 5.1885\n",
      "Epoch [1/10], Step [40350/68337], Loss: 5.5110\n",
      "Epoch [1/10], Step [40425/68337], Loss: 5.4238\n",
      "Epoch [1/10], Step [40500/68337], Loss: 5.4969\n",
      "Epoch [1/10], Step [40575/68337], Loss: 5.5624\n",
      "Epoch [1/10], Step [40650/68337], Loss: 5.4904\n",
      "Epoch [1/10], Step [40725/68337], Loss: 5.4313\n",
      "Epoch [1/10], Step [40800/68337], Loss: 5.4758\n",
      "Epoch [1/10], Step [40875/68337], Loss: 5.4887\n",
      "Epoch [1/10], Step [40950/68337], Loss: 5.2872\n",
      "Epoch [1/10], Step [41025/68337], Loss: 5.3726\n",
      "Epoch [1/10], Step [41100/68337], Loss: 5.3115\n",
      "Epoch [1/10], Step [41175/68337], Loss: 5.3576\n",
      "Epoch [1/10], Step [41250/68337], Loss: 5.4585\n",
      "Epoch [1/10], Step [41325/68337], Loss: 5.3916\n",
      "Epoch [1/10], Step [41400/68337], Loss: 5.2516\n",
      "Epoch [1/10], Step [41475/68337], Loss: 5.4159\n",
      "Epoch [1/10], Step [41550/68337], Loss: 5.2131\n",
      "Epoch [1/10], Step [41625/68337], Loss: 5.4026\n",
      "Epoch [1/10], Step [41700/68337], Loss: 5.3315\n",
      "Epoch [1/10], Step [41775/68337], Loss: 5.3717\n",
      "Epoch [1/10], Step [41850/68337], Loss: 5.4990\n",
      "Epoch [1/10], Step [41925/68337], Loss: 5.6092\n",
      "Epoch [1/10], Step [42000/68337], Loss: 5.4829\n",
      "Epoch [1/10], Step [42075/68337], Loss: 5.5521\n",
      "Epoch [1/10], Step [42150/68337], Loss: 5.4295\n",
      "Epoch [1/10], Step [42225/68337], Loss: 5.4803\n",
      "Epoch [1/10], Step [42300/68337], Loss: 5.5245\n",
      "Epoch [1/10], Step [42375/68337], Loss: 5.3524\n",
      "Epoch [1/10], Step [42450/68337], Loss: 5.5068\n",
      "Epoch [1/10], Step [42525/68337], Loss: 5.3116\n",
      "Epoch [1/10], Step [42600/68337], Loss: 5.2165\n",
      "Epoch [1/10], Step [42675/68337], Loss: 5.2620\n",
      "Epoch [1/10], Step [42750/68337], Loss: 5.4530\n",
      "Epoch [1/10], Step [42825/68337], Loss: 5.3654\n",
      "Epoch [1/10], Step [42900/68337], Loss: 5.4016\n",
      "Epoch [1/10], Step [42975/68337], Loss: 5.3816\n",
      "Epoch [1/10], Step [43050/68337], Loss: 5.4758\n",
      "Epoch [1/10], Step [43125/68337], Loss: 5.3543\n",
      "Epoch [1/10], Step [43200/68337], Loss: 5.5508\n",
      "Epoch [1/10], Step [43275/68337], Loss: 5.3197\n",
      "Epoch [1/10], Step [43350/68337], Loss: 5.1422\n",
      "Epoch [1/10], Step [43425/68337], Loss: 5.2709\n",
      "Epoch [1/10], Step [43500/68337], Loss: 5.3434\n",
      "Epoch [1/10], Step [43575/68337], Loss: 5.3681\n",
      "Epoch [1/10], Step [43650/68337], Loss: 5.4895\n",
      "Epoch [1/10], Step [43725/68337], Loss: 5.4235\n",
      "Epoch [1/10], Step [43800/68337], Loss: 5.4847\n",
      "Epoch [1/10], Step [43875/68337], Loss: 5.4697\n",
      "Epoch [1/10], Step [43950/68337], Loss: 5.2723\n",
      "Epoch [1/10], Step [44025/68337], Loss: 5.3451\n",
      "Epoch [1/10], Step [44100/68337], Loss: 5.4530\n",
      "Epoch [1/10], Step [44175/68337], Loss: 5.2954\n",
      "Epoch [1/10], Step [44250/68337], Loss: 5.4264\n",
      "Epoch [1/10], Step [44325/68337], Loss: 5.4860\n",
      "Epoch [1/10], Step [44400/68337], Loss: 5.4200\n",
      "Epoch [1/10], Step [44475/68337], Loss: 5.1449\n",
      "Epoch [1/10], Step [44550/68337], Loss: 5.5515\n",
      "Epoch [1/10], Step [44625/68337], Loss: 5.6113\n",
      "Epoch [1/10], Step [44700/68337], Loss: 5.4934\n",
      "Epoch [1/10], Step [44775/68337], Loss: 5.4018\n",
      "Epoch [1/10], Step [44850/68337], Loss: 5.3044\n",
      "Epoch [1/10], Step [44925/68337], Loss: 5.3592\n",
      "Epoch [1/10], Step [45000/68337], Loss: 5.3832\n",
      "Epoch [1/10], Step [45075/68337], Loss: 5.5619\n",
      "Epoch [1/10], Step [45150/68337], Loss: 5.3432\n",
      "Epoch [1/10], Step [45225/68337], Loss: 5.4459\n",
      "Epoch [1/10], Step [45300/68337], Loss: 5.4076\n",
      "Epoch [1/10], Step [45375/68337], Loss: 5.4378\n",
      "Epoch [1/10], Step [45450/68337], Loss: 5.1798\n",
      "Epoch [1/10], Step [45525/68337], Loss: 5.3035\n",
      "Epoch [1/10], Step [45600/68337], Loss: 5.3988\n",
      "Epoch [1/10], Step [45675/68337], Loss: 5.3498\n",
      "Epoch [1/10], Step [45750/68337], Loss: 5.6665\n",
      "Epoch [1/10], Step [45825/68337], Loss: 5.3421\n",
      "Epoch [1/10], Step [45900/68337], Loss: 5.3887\n",
      "Epoch [1/10], Step [45975/68337], Loss: 5.4665\n",
      "Epoch [1/10], Step [46050/68337], Loss: 5.5068\n",
      "Epoch [1/10], Step [46125/68337], Loss: 5.2890\n",
      "Epoch [1/10], Step [46200/68337], Loss: 5.3098\n",
      "Epoch [1/10], Step [46275/68337], Loss: 5.5283\n",
      "Epoch [1/10], Step [46350/68337], Loss: 5.1682\n",
      "Epoch [1/10], Step [46425/68337], Loss: 5.3567\n",
      "Epoch [1/10], Step [46500/68337], Loss: 5.3483\n",
      "Epoch [1/10], Step [46575/68337], Loss: 5.3531\n",
      "Epoch [1/10], Step [46650/68337], Loss: 5.2616\n",
      "Epoch [1/10], Step [46725/68337], Loss: 5.5820\n",
      "Epoch [1/10], Step [46800/68337], Loss: 5.3567\n",
      "Epoch [1/10], Step [46875/68337], Loss: 5.3018\n",
      "Epoch [1/10], Step [46950/68337], Loss: 5.5756\n",
      "Epoch [1/10], Step [47025/68337], Loss: 5.4818\n",
      "Epoch [1/10], Step [47100/68337], Loss: 5.3895\n",
      "Epoch [1/10], Step [47175/68337], Loss: 5.2517\n",
      "Epoch [1/10], Step [47250/68337], Loss: 5.4272\n",
      "Epoch [1/10], Step [47325/68337], Loss: 5.6021\n",
      "Epoch [1/10], Step [47400/68337], Loss: 5.3000\n",
      "Epoch [1/10], Step [47475/68337], Loss: 5.3152\n",
      "Epoch [1/10], Step [47550/68337], Loss: 5.4154\n",
      "Epoch [1/10], Step [47625/68337], Loss: 5.2505\n",
      "Epoch [1/10], Step [47700/68337], Loss: 5.4985\n",
      "Epoch [1/10], Step [47775/68337], Loss: 5.4861\n",
      "Epoch [1/10], Step [47850/68337], Loss: 5.3085\n",
      "Epoch [1/10], Step [47925/68337], Loss: 5.3559\n",
      "Epoch [1/10], Step [48000/68337], Loss: 5.4481\n",
      "Epoch [1/10], Step [48075/68337], Loss: 5.0793\n",
      "Epoch [1/10], Step [48150/68337], Loss: 5.5522\n",
      "Epoch [1/10], Step [48225/68337], Loss: 5.1270\n",
      "Epoch [1/10], Step [48300/68337], Loss: 5.4188\n",
      "Epoch [1/10], Step [48375/68337], Loss: 5.4764\n",
      "Epoch [1/10], Step [48450/68337], Loss: 5.4564\n",
      "Epoch [1/10], Step [48525/68337], Loss: 5.3433\n",
      "Epoch [1/10], Step [48600/68337], Loss: 5.6037\n",
      "Epoch [1/10], Step [48675/68337], Loss: 5.3308\n",
      "Epoch [1/10], Step [48750/68337], Loss: 5.3147\n",
      "Epoch [1/10], Step [48825/68337], Loss: 5.2626\n",
      "Epoch [1/10], Step [48900/68337], Loss: 5.1283\n",
      "Epoch [1/10], Step [48975/68337], Loss: 5.3768\n",
      "Epoch [1/10], Step [49050/68337], Loss: 5.2825\n",
      "Epoch [1/10], Step [49125/68337], Loss: 5.3383\n",
      "Epoch [1/10], Step [49200/68337], Loss: 5.3602\n",
      "Epoch [1/10], Step [49275/68337], Loss: 5.2436\n",
      "Epoch [1/10], Step [49350/68337], Loss: 5.4407\n",
      "Epoch [1/10], Step [49425/68337], Loss: 5.4535\n",
      "Epoch [1/10], Step [49500/68337], Loss: 5.4688\n",
      "Epoch [1/10], Step [49575/68337], Loss: 5.6007\n",
      "Epoch [1/10], Step [49650/68337], Loss: 5.1784\n",
      "Epoch [1/10], Step [49725/68337], Loss: 5.5369\n",
      "Epoch [1/10], Step [49800/68337], Loss: 5.5606\n",
      "Epoch [1/10], Step [49875/68337], Loss: 5.5756\n",
      "Epoch [1/10], Step [49950/68337], Loss: 5.5075\n",
      "Validation perplexity: 177.58468724311984\n",
      "Epoch [1/10], Step [50025/68337], Loss: 5.4960\n",
      "Epoch [1/10], Step [50100/68337], Loss: 5.3583\n",
      "Epoch [1/10], Step [50175/68337], Loss: 5.3245\n",
      "Epoch [1/10], Step [50250/68337], Loss: 5.3878\n",
      "Epoch [1/10], Step [50325/68337], Loss: 5.5542\n",
      "Epoch [1/10], Step [50400/68337], Loss: 5.5389\n",
      "Epoch [1/10], Step [50475/68337], Loss: 5.3344\n",
      "Epoch [1/10], Step [50550/68337], Loss: 5.5037\n",
      "Epoch [1/10], Step [50625/68337], Loss: 5.3033\n",
      "Epoch [1/10], Step [50700/68337], Loss: 5.2838\n",
      "Epoch [1/10], Step [50775/68337], Loss: 5.3292\n",
      "Epoch [1/10], Step [50850/68337], Loss: 5.6184\n",
      "Epoch [1/10], Step [50925/68337], Loss: 5.5234\n",
      "Epoch [1/10], Step [51000/68337], Loss: 5.2861\n",
      "Epoch [1/10], Step [51075/68337], Loss: 5.2689\n",
      "Epoch [1/10], Step [51150/68337], Loss: 5.3824\n",
      "Epoch [1/10], Step [51225/68337], Loss: 5.2544\n",
      "Epoch [1/10], Step [51300/68337], Loss: 5.2371\n",
      "Epoch [1/10], Step [51375/68337], Loss: 5.2750\n",
      "Epoch [1/10], Step [51450/68337], Loss: 5.0689\n",
      "Epoch [1/10], Step [51525/68337], Loss: 5.4150\n",
      "Epoch [1/10], Step [51600/68337], Loss: 5.3637\n",
      "Epoch [1/10], Step [51675/68337], Loss: 5.2970\n",
      "Epoch [1/10], Step [51750/68337], Loss: 5.3155\n",
      "Epoch [1/10], Step [51825/68337], Loss: 5.4923\n",
      "Epoch [1/10], Step [51900/68337], Loss: 5.5791\n",
      "Epoch [1/10], Step [51975/68337], Loss: 5.2915\n",
      "Epoch [1/10], Step [52050/68337], Loss: 5.4950\n",
      "Epoch [1/10], Step [52125/68337], Loss: 5.2636\n",
      "Epoch [1/10], Step [52200/68337], Loss: 5.5692\n",
      "Epoch [1/10], Step [52275/68337], Loss: 5.4005\n",
      "Epoch [1/10], Step [52350/68337], Loss: 5.4182\n",
      "Epoch [1/10], Step [52425/68337], Loss: 5.2855\n",
      "Epoch [1/10], Step [52500/68337], Loss: 5.2305\n",
      "Epoch [1/10], Step [52575/68337], Loss: 5.1969\n",
      "Epoch [1/10], Step [52650/68337], Loss: 5.3174\n",
      "Epoch [1/10], Step [52725/68337], Loss: 5.4229\n",
      "Epoch [1/10], Step [52800/68337], Loss: 5.1725\n",
      "Epoch [1/10], Step [52875/68337], Loss: 5.2824\n",
      "Epoch [1/10], Step [52950/68337], Loss: 5.3969\n",
      "Epoch [1/10], Step [53025/68337], Loss: 5.4470\n",
      "Epoch [1/10], Step [53100/68337], Loss: 5.2552\n",
      "Epoch [1/10], Step [53175/68337], Loss: 5.2438\n",
      "Epoch [1/10], Step [53250/68337], Loss: 5.6427\n",
      "Epoch [1/10], Step [53325/68337], Loss: 5.4824\n",
      "Epoch [1/10], Step [53400/68337], Loss: 5.5952\n",
      "Epoch [1/10], Step [53475/68337], Loss: 5.4461\n",
      "Epoch [1/10], Step [53550/68337], Loss: 5.1655\n",
      "Epoch [1/10], Step [53625/68337], Loss: 5.3754\n",
      "Epoch [1/10], Step [53700/68337], Loss: 5.4388\n",
      "Epoch [1/10], Step [53775/68337], Loss: 5.3764\n",
      "Epoch [1/10], Step [53850/68337], Loss: 5.3622\n",
      "Epoch [1/10], Step [53925/68337], Loss: 5.3824\n",
      "Epoch [1/10], Step [54000/68337], Loss: 5.4384\n",
      "Epoch [1/10], Step [54075/68337], Loss: 5.5119\n",
      "Epoch [1/10], Step [54150/68337], Loss: 5.0595\n",
      "Epoch [1/10], Step [54225/68337], Loss: 5.2932\n",
      "Epoch [1/10], Step [54300/68337], Loss: 5.3771\n",
      "Epoch [1/10], Step [54375/68337], Loss: 5.3420\n",
      "Epoch [1/10], Step [54450/68337], Loss: 5.5531\n",
      "Epoch [1/10], Step [54525/68337], Loss: 5.4967\n",
      "Epoch [1/10], Step [54600/68337], Loss: 5.3531\n",
      "Epoch [1/10], Step [54675/68337], Loss: 5.7046\n",
      "Epoch [1/10], Step [54750/68337], Loss: 5.2691\n",
      "Epoch [1/10], Step [54825/68337], Loss: 5.2569\n",
      "Epoch [1/10], Step [54900/68337], Loss: 5.5210\n",
      "Epoch [1/10], Step [54975/68337], Loss: 5.1955\n",
      "Epoch [1/10], Step [55050/68337], Loss: 5.3931\n",
      "Epoch [1/10], Step [55125/68337], Loss: 5.3428\n",
      "Epoch [1/10], Step [55200/68337], Loss: 5.6312\n",
      "Epoch [1/10], Step [55275/68337], Loss: 5.3342\n",
      "Epoch [1/10], Step [55350/68337], Loss: 5.2748\n",
      "Epoch [1/10], Step [55425/68337], Loss: 5.3540\n",
      "Epoch [1/10], Step [55500/68337], Loss: 5.2589\n",
      "Epoch [1/10], Step [55575/68337], Loss: 5.4545\n",
      "Epoch [1/10], Step [55650/68337], Loss: 5.4123\n",
      "Epoch [1/10], Step [55725/68337], Loss: 5.2622\n",
      "Epoch [1/10], Step [55800/68337], Loss: 5.4347\n",
      "Epoch [1/10], Step [55875/68337], Loss: 5.4790\n",
      "Epoch [1/10], Step [55950/68337], Loss: 5.3710\n",
      "Epoch [1/10], Step [56025/68337], Loss: 5.2407\n",
      "Epoch [1/10], Step [56100/68337], Loss: 5.3902\n",
      "Epoch [1/10], Step [56175/68337], Loss: 5.5034\n",
      "Epoch [1/10], Step [56250/68337], Loss: 5.4393\n",
      "Epoch [1/10], Step [56325/68337], Loss: 5.3003\n",
      "Epoch [1/10], Step [56400/68337], Loss: 5.5292\n",
      "Epoch [1/10], Step [56475/68337], Loss: 5.4418\n",
      "Epoch [1/10], Step [56550/68337], Loss: 5.3447\n",
      "Epoch [1/10], Step [56625/68337], Loss: 5.4702\n",
      "Epoch [1/10], Step [56700/68337], Loss: 5.1084\n",
      "Epoch [1/10], Step [56775/68337], Loss: 5.1568\n",
      "Epoch [1/10], Step [56850/68337], Loss: 5.3384\n",
      "Epoch [1/10], Step [56925/68337], Loss: 5.5327\n",
      "Epoch [1/10], Step [57000/68337], Loss: 5.2832\n",
      "Epoch [1/10], Step [57075/68337], Loss: 5.3612\n",
      "Epoch [1/10], Step [57150/68337], Loss: 5.3293\n",
      "Epoch [1/10], Step [57225/68337], Loss: 5.2462\n",
      "Epoch [1/10], Step [57300/68337], Loss: 5.2234\n",
      "Epoch [1/10], Step [57375/68337], Loss: 5.3412\n",
      "Epoch [1/10], Step [57450/68337], Loss: 5.2525\n",
      "Epoch [1/10], Step [57525/68337], Loss: 5.3581\n",
      "Epoch [1/10], Step [57600/68337], Loss: 5.3730\n",
      "Epoch [1/10], Step [57675/68337], Loss: 5.5068\n",
      "Epoch [1/10], Step [57750/68337], Loss: 5.4424\n",
      "Epoch [1/10], Step [57825/68337], Loss: 5.5196\n",
      "Epoch [1/10], Step [57900/68337], Loss: 5.3117\n",
      "Epoch [1/10], Step [57975/68337], Loss: 5.4344\n",
      "Epoch [1/10], Step [58050/68337], Loss: 5.3377\n",
      "Epoch [1/10], Step [58125/68337], Loss: 5.5011\n",
      "Epoch [1/10], Step [58200/68337], Loss: 5.2927\n",
      "Epoch [1/10], Step [58275/68337], Loss: 5.5272\n",
      "Epoch [1/10], Step [58350/68337], Loss: 5.3286\n",
      "Epoch [1/10], Step [58425/68337], Loss: 5.5283\n",
      "Epoch [1/10], Step [58500/68337], Loss: 5.3976\n",
      "Epoch [1/10], Step [58575/68337], Loss: 5.3077\n",
      "Epoch [1/10], Step [58650/68337], Loss: 5.1426\n",
      "Epoch [1/10], Step [58725/68337], Loss: 5.2581\n",
      "Epoch [1/10], Step [58800/68337], Loss: 5.1741\n",
      "Epoch [1/10], Step [58875/68337], Loss: 5.2653\n",
      "Epoch [1/10], Step [58950/68337], Loss: 5.4544\n",
      "Epoch [1/10], Step [59025/68337], Loss: 5.4660\n",
      "Epoch [1/10], Step [59100/68337], Loss: 5.2952\n",
      "Epoch [1/10], Step [59175/68337], Loss: 5.4713\n",
      "Epoch [1/10], Step [59250/68337], Loss: 5.2806\n",
      "Epoch [1/10], Step [59325/68337], Loss: 5.5059\n",
      "Epoch [1/10], Step [59400/68337], Loss: 5.3817\n",
      "Epoch [1/10], Step [59475/68337], Loss: 5.2570\n",
      "Epoch [1/10], Step [59550/68337], Loss: 5.3699\n",
      "Epoch [1/10], Step [59625/68337], Loss: 5.2516\n",
      "Epoch [1/10], Step [59700/68337], Loss: 5.2571\n",
      "Epoch [1/10], Step [59775/68337], Loss: 5.5549\n",
      "Epoch [1/10], Step [59850/68337], Loss: 5.3553\n",
      "Epoch [1/10], Step [59925/68337], Loss: 5.4588\n",
      "Epoch [1/10], Step [60000/68337], Loss: 5.3346\n",
      "Validation perplexity: 169.3441639997456\n",
      "Epoch [1/10], Step [60075/68337], Loss: 5.5984\n",
      "Epoch [1/10], Step [60150/68337], Loss: 5.1968\n",
      "Epoch [1/10], Step [60225/68337], Loss: 5.1959\n",
      "Epoch [1/10], Step [60300/68337], Loss: 5.3590\n",
      "Epoch [1/10], Step [60375/68337], Loss: 5.3757\n",
      "Epoch [1/10], Step [60450/68337], Loss: 5.4619\n",
      "Epoch [1/10], Step [60525/68337], Loss: 5.2893\n",
      "Epoch [1/10], Step [60600/68337], Loss: 5.1801\n",
      "Epoch [1/10], Step [60675/68337], Loss: 5.2877\n",
      "Epoch [1/10], Step [60750/68337], Loss: 5.0508\n",
      "Epoch [1/10], Step [60825/68337], Loss: 5.2352\n",
      "Epoch [1/10], Step [60900/68337], Loss: 5.2764\n",
      "Epoch [1/10], Step [60975/68337], Loss: 5.2332\n",
      "Epoch [1/10], Step [61050/68337], Loss: 5.4328\n",
      "Epoch [1/10], Step [61125/68337], Loss: 5.1933\n",
      "Epoch [1/10], Step [61200/68337], Loss: 5.3782\n",
      "Epoch [1/10], Step [61275/68337], Loss: 5.3638\n",
      "Epoch [1/10], Step [61350/68337], Loss: 5.1618\n",
      "Epoch [1/10], Step [61425/68337], Loss: 5.1782\n",
      "Epoch [1/10], Step [61500/68337], Loss: 5.3086\n",
      "Epoch [1/10], Step [61575/68337], Loss: 5.4217\n",
      "Epoch [1/10], Step [61650/68337], Loss: 5.2694\n",
      "Epoch [1/10], Step [61725/68337], Loss: 5.3318\n",
      "Epoch [1/10], Step [61800/68337], Loss: 5.1795\n",
      "Epoch [1/10], Step [61875/68337], Loss: 5.5434\n",
      "Epoch [1/10], Step [61950/68337], Loss: 5.1360\n",
      "Epoch [1/10], Step [62025/68337], Loss: 5.5367\n",
      "Epoch [1/10], Step [62100/68337], Loss: 5.3591\n",
      "Epoch [1/10], Step [62175/68337], Loss: 5.4127\n",
      "Epoch [1/10], Step [62250/68337], Loss: 5.2385\n",
      "Epoch [1/10], Step [62325/68337], Loss: 5.0544\n",
      "Epoch [1/10], Step [62400/68337], Loss: 5.0696\n",
      "Epoch [1/10], Step [62475/68337], Loss: 5.6392\n",
      "Epoch [1/10], Step [62550/68337], Loss: 5.2967\n",
      "Epoch [1/10], Step [62625/68337], Loss: 5.2742\n",
      "Epoch [1/10], Step [62700/68337], Loss: 5.5282\n",
      "Epoch [1/10], Step [62775/68337], Loss: 5.4911\n",
      "Epoch [1/10], Step [62850/68337], Loss: 5.1902\n",
      "Epoch [1/10], Step [62925/68337], Loss: 5.3712\n",
      "Epoch [1/10], Step [63000/68337], Loss: 5.3218\n",
      "Epoch [1/10], Step [63075/68337], Loss: 5.3613\n",
      "Epoch [1/10], Step [63150/68337], Loss: 5.4000\n",
      "Epoch [1/10], Step [63225/68337], Loss: 5.1496\n",
      "Epoch [1/10], Step [63300/68337], Loss: 5.5178\n",
      "Epoch [1/10], Step [63375/68337], Loss: 5.3857\n",
      "Epoch [1/10], Step [63450/68337], Loss: 5.2862\n",
      "Epoch [1/10], Step [63525/68337], Loss: 5.3485\n",
      "Epoch [1/10], Step [63600/68337], Loss: 5.6273\n",
      "Epoch [1/10], Step [63675/68337], Loss: 5.4199\n",
      "Epoch [1/10], Step [63750/68337], Loss: 5.3052\n",
      "Epoch [1/10], Step [63825/68337], Loss: 5.4819\n",
      "Epoch [1/10], Step [63900/68337], Loss: 5.4544\n",
      "Epoch [1/10], Step [63975/68337], Loss: 5.3460\n",
      "Epoch [1/10], Step [64050/68337], Loss: 5.1631\n",
      "Epoch [1/10], Step [64125/68337], Loss: 5.2712\n",
      "Epoch [1/10], Step [64200/68337], Loss: 5.4288\n",
      "Epoch [1/10], Step [64275/68337], Loss: 5.3635\n",
      "Epoch [1/10], Step [64350/68337], Loss: 5.3034\n",
      "Epoch [1/10], Step [64425/68337], Loss: 5.2801\n",
      "Epoch [1/10], Step [64500/68337], Loss: 5.2843\n",
      "Epoch [1/10], Step [64575/68337], Loss: 5.4378\n",
      "Epoch [1/10], Step [64650/68337], Loss: 5.5341\n",
      "Epoch [1/10], Step [64725/68337], Loss: 5.3504\n",
      "Epoch [1/10], Step [64800/68337], Loss: 5.2723\n",
      "Epoch [1/10], Step [64875/68337], Loss: 5.4695\n",
      "Epoch [1/10], Step [64950/68337], Loss: 5.2081\n",
      "Epoch [1/10], Step [65025/68337], Loss: 5.4338\n",
      "Epoch [1/10], Step [65100/68337], Loss: 5.2213\n",
      "Epoch [1/10], Step [65175/68337], Loss: 5.4740\n",
      "Epoch [1/10], Step [65250/68337], Loss: 5.4549\n",
      "Epoch [1/10], Step [65325/68337], Loss: 5.2431\n",
      "Epoch [1/10], Step [65400/68337], Loss: 5.2385\n",
      "Epoch [1/10], Step [65475/68337], Loss: 5.5269\n",
      "Epoch [1/10], Step [65550/68337], Loss: 5.3313\n",
      "Epoch [1/10], Step [65625/68337], Loss: 5.3250\n",
      "Epoch [1/10], Step [65700/68337], Loss: 5.2614\n",
      "Epoch [1/10], Step [65775/68337], Loss: 5.3823\n",
      "Epoch [1/10], Step [65850/68337], Loss: 5.3049\n",
      "Epoch [1/10], Step [65925/68337], Loss: 5.4297\n",
      "Epoch [1/10], Step [66000/68337], Loss: 5.1264\n",
      "Epoch [1/10], Step [66075/68337], Loss: 5.3562\n",
      "Epoch [1/10], Step [66150/68337], Loss: 5.1645\n",
      "Epoch [1/10], Step [66225/68337], Loss: 5.3562\n",
      "Epoch [1/10], Step [66300/68337], Loss: 5.2700\n",
      "Epoch [1/10], Step [66375/68337], Loss: 5.4343\n",
      "Epoch [1/10], Step [66450/68337], Loss: 5.3229\n",
      "Epoch [1/10], Step [66525/68337], Loss: 5.2271\n",
      "Epoch [1/10], Step [66600/68337], Loss: 5.1892\n",
      "Epoch [1/10], Step [66675/68337], Loss: 5.2123\n",
      "Epoch [1/10], Step [66750/68337], Loss: 5.2291\n",
      "Epoch [1/10], Step [66825/68337], Loss: 5.2204\n",
      "Epoch [1/10], Step [66900/68337], Loss: 5.4127\n",
      "Epoch [1/10], Step [66975/68337], Loss: 5.2394\n",
      "Epoch [1/10], Step [67050/68337], Loss: 5.4101\n",
      "Epoch [1/10], Step [67125/68337], Loss: 5.4946\n",
      "Epoch [1/10], Step [67200/68337], Loss: 5.3203\n",
      "Epoch [1/10], Step [67275/68337], Loss: 5.2831\n",
      "Epoch [1/10], Step [67350/68337], Loss: 5.2131\n",
      "Epoch [1/10], Step [67425/68337], Loss: 5.3120\n",
      "Epoch [1/10], Step [67500/68337], Loss: 5.3187\n",
      "Epoch [1/10], Step [67575/68337], Loss: 5.3182\n",
      "Epoch [1/10], Step [67650/68337], Loss: 5.3353\n",
      "Epoch [1/10], Step [67725/68337], Loss: 5.3357\n",
      "Epoch [1/10], Step [67800/68337], Loss: 5.1078\n",
      "Epoch [1/10], Step [67875/68337], Loss: 5.3519\n",
      "Epoch [1/10], Step [67950/68337], Loss: 5.2875\n",
      "Epoch [1/10], Step [68025/68337], Loss: 5.4573\n",
      "Epoch [1/10], Step [68100/68337], Loss: 5.4083\n",
      "Epoch [1/10], Step [68175/68337], Loss: 5.4054\n",
      "Epoch [1/10], Step [68250/68337], Loss: 5.4548\n",
      "Epoch [1/10], Step [68325/68337], Loss: 5.3516\n",
      "Epoch [1/10] Average Loss: 5.5917, Perplexity: 268.18\n",
      "Epoch [2/10], Step [0/68337], Loss: 5.1993\n",
      "Validation perplexity: 164.00341419226288\n",
      "Epoch [2/10], Step [75/68337], Loss: 5.4528\n",
      "Epoch [2/10], Step [150/68337], Loss: 5.3800\n",
      "Epoch [2/10], Step [225/68337], Loss: 5.1911\n",
      "Epoch [2/10], Step [300/68337], Loss: 5.3393\n",
      "Epoch [2/10], Step [375/68337], Loss: 5.3314\n",
      "Epoch [2/10], Step [450/68337], Loss: 5.2176\n",
      "Epoch [2/10], Step [525/68337], Loss: 5.4348\n",
      "Epoch [2/10], Step [600/68337], Loss: 5.2097\n",
      "Epoch [2/10], Step [675/68337], Loss: 5.1286\n",
      "Epoch [2/10], Step [750/68337], Loss: 5.4064\n",
      "Epoch [2/10], Step [825/68337], Loss: 5.1648\n",
      "Epoch [2/10], Step [900/68337], Loss: 5.1579\n",
      "Epoch [2/10], Step [975/68337], Loss: 5.3449\n",
      "Epoch [2/10], Step [1050/68337], Loss: 5.2685\n",
      "Epoch [2/10], Step [1125/68337], Loss: 5.1821\n",
      "Epoch [2/10], Step [1200/68337], Loss: 5.1244\n",
      "Epoch [2/10], Step [1275/68337], Loss: 5.2675\n",
      "Epoch [2/10], Step [1350/68337], Loss: 5.1205\n",
      "Epoch [2/10], Step [1425/68337], Loss: 5.1844\n",
      "Epoch [2/10], Step [1500/68337], Loss: 5.4133\n",
      "Epoch [2/10], Step [1575/68337], Loss: 5.3000\n",
      "Epoch [2/10], Step [1650/68337], Loss: 5.1254\n",
      "Epoch [2/10], Step [1725/68337], Loss: 5.4133\n",
      "Epoch [2/10], Step [1800/68337], Loss: 5.3963\n",
      "Epoch [2/10], Step [1875/68337], Loss: 5.4002\n",
      "Epoch [2/10], Step [1950/68337], Loss: 5.1178\n",
      "Epoch [2/10], Step [2025/68337], Loss: 5.2486\n",
      "Epoch [2/10], Step [2100/68337], Loss: 5.3539\n",
      "Epoch [2/10], Step [2175/68337], Loss: 5.2587\n",
      "Epoch [2/10], Step [2250/68337], Loss: 5.3722\n",
      "Epoch [2/10], Step [2325/68337], Loss: 5.1211\n",
      "Epoch [2/10], Step [2400/68337], Loss: 5.1538\n",
      "Epoch [2/10], Step [2475/68337], Loss: 5.3507\n",
      "Epoch [2/10], Step [2550/68337], Loss: 5.1559\n",
      "Epoch [2/10], Step [2625/68337], Loss: 5.2208\n",
      "Epoch [2/10], Step [2700/68337], Loss: 5.3245\n",
      "Epoch [2/10], Step [2775/68337], Loss: 5.2859\n",
      "Epoch [2/10], Step [2850/68337], Loss: 5.5107\n",
      "Epoch [2/10], Step [2925/68337], Loss: 5.1706\n",
      "Epoch [2/10], Step [3000/68337], Loss: 5.2367\n",
      "Epoch [2/10], Step [3075/68337], Loss: 5.3483\n",
      "Epoch [2/10], Step [3150/68337], Loss: 5.2758\n",
      "Epoch [2/10], Step [3225/68337], Loss: 5.3174\n",
      "Epoch [2/10], Step [3300/68337], Loss: 5.6836\n",
      "Epoch [2/10], Step [3375/68337], Loss: 5.2681\n",
      "Epoch [2/10], Step [3450/68337], Loss: 5.3158\n",
      "Epoch [2/10], Step [3525/68337], Loss: 5.3204\n",
      "Epoch [2/10], Step [3600/68337], Loss: 5.3046\n",
      "Epoch [2/10], Step [3675/68337], Loss: 5.3550\n",
      "Epoch [2/10], Step [3750/68337], Loss: 5.2038\n",
      "Epoch [2/10], Step [3825/68337], Loss: 5.2477\n",
      "Epoch [2/10], Step [3900/68337], Loss: 5.2293\n",
      "Epoch [2/10], Step [3975/68337], Loss: 5.0275\n",
      "Epoch [2/10], Step [4050/68337], Loss: 5.4419\n",
      "Epoch [2/10], Step [4125/68337], Loss: 5.2217\n",
      "Epoch [2/10], Step [4200/68337], Loss: 5.2615\n",
      "Epoch [2/10], Step [4275/68337], Loss: 5.4114\n",
      "Epoch [2/10], Step [4350/68337], Loss: 5.3776\n",
      "Epoch [2/10], Step [4425/68337], Loss: 5.2339\n",
      "Epoch [2/10], Step [4500/68337], Loss: 5.4972\n",
      "Epoch [2/10], Step [4575/68337], Loss: 5.2184\n",
      "Epoch [2/10], Step [4650/68337], Loss: 5.1774\n",
      "Epoch [2/10], Step [4725/68337], Loss: 5.2215\n",
      "Epoch [2/10], Step [4800/68337], Loss: 5.5573\n",
      "Epoch [2/10], Step [4875/68337], Loss: 5.4229\n",
      "Epoch [2/10], Step [4950/68337], Loss: 5.3735\n",
      "Epoch [2/10], Step [5025/68337], Loss: 5.4665\n",
      "Epoch [2/10], Step [5100/68337], Loss: 5.2636\n",
      "Epoch [2/10], Step [5175/68337], Loss: 5.1913\n",
      "Epoch [2/10], Step [5250/68337], Loss: 5.2535\n",
      "Epoch [2/10], Step [5325/68337], Loss: 5.1649\n",
      "Epoch [2/10], Step [5400/68337], Loss: 5.2352\n",
      "Epoch [2/10], Step [5475/68337], Loss: 5.5076\n",
      "Epoch [2/10], Step [5550/68337], Loss: 5.3597\n",
      "Epoch [2/10], Step [5625/68337], Loss: 5.3895\n",
      "Epoch [2/10], Step [5700/68337], Loss: 5.2174\n",
      "Epoch [2/10], Step [5775/68337], Loss: 5.3003\n",
      "Epoch [2/10], Step [5850/68337], Loss: 5.4577\n",
      "Epoch [2/10], Step [5925/68337], Loss: 5.0122\n",
      "Epoch [2/10], Step [6000/68337], Loss: 5.2009\n",
      "Epoch [2/10], Step [6075/68337], Loss: 5.2317\n",
      "Epoch [2/10], Step [6150/68337], Loss: 5.3739\n",
      "Epoch [2/10], Step [6225/68337], Loss: 5.4684\n",
      "Epoch [2/10], Step [6300/68337], Loss: 5.3616\n",
      "Epoch [2/10], Step [6375/68337], Loss: 5.1686\n",
      "Epoch [2/10], Step [6450/68337], Loss: 5.2562\n",
      "Epoch [2/10], Step [6525/68337], Loss: 5.1755\n",
      "Epoch [2/10], Step [6600/68337], Loss: 5.4379\n",
      "Epoch [2/10], Step [6675/68337], Loss: 5.5147\n",
      "Epoch [2/10], Step [6750/68337], Loss: 5.3179\n",
      "Epoch [2/10], Step [6825/68337], Loss: 5.4164\n",
      "Epoch [2/10], Step [6900/68337], Loss: 5.4061\n",
      "Epoch [2/10], Step [6975/68337], Loss: 5.1640\n",
      "Epoch [2/10], Step [7050/68337], Loss: 5.1032\n",
      "Epoch [2/10], Step [7125/68337], Loss: 5.2450\n",
      "Epoch [2/10], Step [7200/68337], Loss: 5.4484\n",
      "Epoch [2/10], Step [7275/68337], Loss: 5.0300\n",
      "Epoch [2/10], Step [7350/68337], Loss: 5.3623\n",
      "Epoch [2/10], Step [7425/68337], Loss: 5.1966\n",
      "Epoch [2/10], Step [7500/68337], Loss: 5.3329\n",
      "Epoch [2/10], Step [7575/68337], Loss: 5.1411\n",
      "Epoch [2/10], Step [7650/68337], Loss: 5.2656\n",
      "Epoch [2/10], Step [7725/68337], Loss: 5.3886\n",
      "Epoch [2/10], Step [7800/68337], Loss: 5.3104\n",
      "Epoch [2/10], Step [7875/68337], Loss: 5.1423\n",
      "Epoch [2/10], Step [7950/68337], Loss: 5.1817\n",
      "Epoch [2/10], Step [8025/68337], Loss: 5.4566\n",
      "Epoch [2/10], Step [8100/68337], Loss: 5.3654\n",
      "Epoch [2/10], Step [8175/68337], Loss: 5.2184\n",
      "Epoch [2/10], Step [8250/68337], Loss: 5.3406\n",
      "Epoch [2/10], Step [8325/68337], Loss: 5.2680\n",
      "Epoch [2/10], Step [8400/68337], Loss: 5.1551\n",
      "Epoch [2/10], Step [8475/68337], Loss: 5.3195\n",
      "Epoch [2/10], Step [8550/68337], Loss: 5.4478\n",
      "Epoch [2/10], Step [8625/68337], Loss: 5.4201\n",
      "Epoch [2/10], Step [8700/68337], Loss: 5.2437\n",
      "Epoch [2/10], Step [8775/68337], Loss: 5.5009\n",
      "Epoch [2/10], Step [8850/68337], Loss: 5.2205\n",
      "Epoch [2/10], Step [8925/68337], Loss: 5.2030\n",
      "Epoch [2/10], Step [9000/68337], Loss: 5.2990\n",
      "Epoch [2/10], Step [9075/68337], Loss: 5.3078\n",
      "Epoch [2/10], Step [9150/68337], Loss: 5.2636\n",
      "Epoch [2/10], Step [9225/68337], Loss: 5.3313\n",
      "Epoch [2/10], Step [9300/68337], Loss: 5.2882\n",
      "Epoch [2/10], Step [9375/68337], Loss: 5.1472\n",
      "Epoch [2/10], Step [9450/68337], Loss: 5.3647\n",
      "Epoch [2/10], Step [9525/68337], Loss: 5.3040\n",
      "Epoch [2/10], Step [9600/68337], Loss: 5.2632\n",
      "Epoch [2/10], Step [9675/68337], Loss: 5.2357\n",
      "Epoch [2/10], Step [9750/68337], Loss: 5.4880\n",
      "Epoch [2/10], Step [9825/68337], Loss: 5.1804\n",
      "Epoch [2/10], Step [9900/68337], Loss: 5.4288\n",
      "Epoch [2/10], Step [9975/68337], Loss: 5.2882\n",
      "Validation perplexity: 159.690503847275\n",
      "Epoch [2/10], Step [10050/68337], Loss: 5.1750\n",
      "Epoch [2/10], Step [10125/68337], Loss: 5.4269\n",
      "Epoch [2/10], Step [10200/68337], Loss: 5.3416\n",
      "Epoch [2/10], Step [10275/68337], Loss: 5.1424\n",
      "Epoch [2/10], Step [10350/68337], Loss: 5.2014\n",
      "Epoch [2/10], Step [10425/68337], Loss: 5.4369\n",
      "Epoch [2/10], Step [10500/68337], Loss: 5.4545\n",
      "Epoch [2/10], Step [10575/68337], Loss: 5.1909\n",
      "Epoch [2/10], Step [10650/68337], Loss: 5.3214\n",
      "Epoch [2/10], Step [10725/68337], Loss: 5.4251\n",
      "Epoch [2/10], Step [10800/68337], Loss: 5.2797\n",
      "Epoch [2/10], Step [10875/68337], Loss: 5.2056\n",
      "Epoch [2/10], Step [10950/68337], Loss: 5.2126\n",
      "Epoch [2/10], Step [11025/68337], Loss: 5.2363\n",
      "Epoch [2/10], Step [11100/68337], Loss: 5.2117\n",
      "Epoch [2/10], Step [11175/68337], Loss: 5.1155\n",
      "Epoch [2/10], Step [11250/68337], Loss: 5.2231\n",
      "Epoch [2/10], Step [11325/68337], Loss: 5.3411\n",
      "Epoch [2/10], Step [11400/68337], Loss: 5.2909\n",
      "Epoch [2/10], Step [11475/68337], Loss: 5.1880\n",
      "Epoch [2/10], Step [11550/68337], Loss: 5.4178\n",
      "Epoch [2/10], Step [11625/68337], Loss: 5.3825\n",
      "Epoch [2/10], Step [11700/68337], Loss: 5.3492\n",
      "Epoch [2/10], Step [11775/68337], Loss: 5.3718\n",
      "Epoch [2/10], Step [11850/68337], Loss: 5.3284\n",
      "Epoch [2/10], Step [11925/68337], Loss: 5.0727\n",
      "Epoch [2/10], Step [12000/68337], Loss: 5.2148\n",
      "Epoch [2/10], Step [12075/68337], Loss: 5.4061\n",
      "Epoch [2/10], Step [12150/68337], Loss: 5.0999\n",
      "Epoch [2/10], Step [12225/68337], Loss: 5.2376\n",
      "Epoch [2/10], Step [12300/68337], Loss: 5.4676\n",
      "Epoch [2/10], Step [12375/68337], Loss: 5.2564\n",
      "Epoch [2/10], Step [12450/68337], Loss: 5.4703\n",
      "Epoch [2/10], Step [12525/68337], Loss: 5.2560\n",
      "Epoch [2/10], Step [12600/68337], Loss: 5.3588\n",
      "Epoch [2/10], Step [12675/68337], Loss: 5.2507\n",
      "Epoch [2/10], Step [12750/68337], Loss: 5.6114\n",
      "Epoch [2/10], Step [12825/68337], Loss: 5.1921\n",
      "Epoch [2/10], Step [12900/68337], Loss: 5.2728\n",
      "Epoch [2/10], Step [12975/68337], Loss: 5.3571\n",
      "Epoch [2/10], Step [13050/68337], Loss: 5.2283\n",
      "Epoch [2/10], Step [13125/68337], Loss: 5.2461\n",
      "Epoch [2/10], Step [13200/68337], Loss: 5.1309\n",
      "Epoch [2/10], Step [13275/68337], Loss: 5.4494\n",
      "Epoch [2/10], Step [13350/68337], Loss: 5.2789\n",
      "Epoch [2/10], Step [13425/68337], Loss: 5.3595\n",
      "Epoch [2/10], Step [13500/68337], Loss: 5.3253\n",
      "Epoch [2/10], Step [13575/68337], Loss: 5.2348\n",
      "Epoch [2/10], Step [13650/68337], Loss: 5.2375\n",
      "Epoch [2/10], Step [13725/68337], Loss: 5.2881\n",
      "Epoch [2/10], Step [13800/68337], Loss: 5.3753\n",
      "Epoch [2/10], Step [13875/68337], Loss: 5.3705\n",
      "Epoch [2/10], Step [13950/68337], Loss: 5.2466\n",
      "Epoch [2/10], Step [14025/68337], Loss: 5.1308\n",
      "Epoch [2/10], Step [14100/68337], Loss: 5.3811\n",
      "Epoch [2/10], Step [14175/68337], Loss: 5.2905\n",
      "Epoch [2/10], Step [14250/68337], Loss: 5.1176\n",
      "Epoch [2/10], Step [14325/68337], Loss: 5.4506\n",
      "Epoch [2/10], Step [14400/68337], Loss: 5.3538\n",
      "Epoch [2/10], Step [14475/68337], Loss: 5.2401\n",
      "Epoch [2/10], Step [14550/68337], Loss: 5.3754\n",
      "Epoch [2/10], Step [14625/68337], Loss: 5.2727\n",
      "Epoch [2/10], Step [14700/68337], Loss: 5.4396\n",
      "Epoch [2/10], Step [14775/68337], Loss: 5.3563\n",
      "Epoch [2/10], Step [14850/68337], Loss: 5.3053\n",
      "Epoch [2/10], Step [14925/68337], Loss: 5.3376\n",
      "Epoch [2/10], Step [15000/68337], Loss: 5.3080\n",
      "Epoch [2/10], Step [15075/68337], Loss: 5.0820\n",
      "Epoch [2/10], Step [15150/68337], Loss: 5.2269\n",
      "Epoch [2/10], Step [15225/68337], Loss: 5.1437\n",
      "Epoch [2/10], Step [15300/68337], Loss: 5.4116\n",
      "Epoch [2/10], Step [15375/68337], Loss: 5.2628\n",
      "Epoch [2/10], Step [15450/68337], Loss: 5.1885\n",
      "Epoch [2/10], Step [15525/68337], Loss: 5.0328\n",
      "Epoch [2/10], Step [15600/68337], Loss: 5.5772\n",
      "Epoch [2/10], Step [15675/68337], Loss: 5.2054\n",
      "Epoch [2/10], Step [15750/68337], Loss: 5.3628\n",
      "Epoch [2/10], Step [15825/68337], Loss: 5.2164\n",
      "Epoch [2/10], Step [15900/68337], Loss: 5.0475\n",
      "Epoch [2/10], Step [15975/68337], Loss: 5.4016\n",
      "Epoch [2/10], Step [16050/68337], Loss: 5.1877\n",
      "Epoch [2/10], Step [16125/68337], Loss: 5.1787\n",
      "Epoch [2/10], Step [16200/68337], Loss: 5.4187\n",
      "Epoch [2/10], Step [16275/68337], Loss: 5.3286\n",
      "Epoch [2/10], Step [16350/68337], Loss: 5.4708\n",
      "Epoch [2/10], Step [16425/68337], Loss: 5.3356\n",
      "Epoch [2/10], Step [16500/68337], Loss: 5.3691\n",
      "Epoch [2/10], Step [16575/68337], Loss: 5.4286\n",
      "Epoch [2/10], Step [16650/68337], Loss: 5.0606\n",
      "Epoch [2/10], Step [16725/68337], Loss: 5.2352\n",
      "Epoch [2/10], Step [16800/68337], Loss: 5.4488\n",
      "Epoch [2/10], Step [16875/68337], Loss: 5.3565\n",
      "Epoch [2/10], Step [16950/68337], Loss: 5.1440\n",
      "Epoch [2/10], Step [17025/68337], Loss: 5.2410\n",
      "Epoch [2/10], Step [17100/68337], Loss: 5.2324\n",
      "Epoch [2/10], Step [17175/68337], Loss: 5.2372\n",
      "Epoch [2/10], Step [17250/68337], Loss: 5.3379\n",
      "Epoch [2/10], Step [17325/68337], Loss: 5.1521\n",
      "Epoch [2/10], Step [17400/68337], Loss: 5.1832\n",
      "Epoch [2/10], Step [17475/68337], Loss: 5.3262\n",
      "Epoch [2/10], Step [17550/68337], Loss: 5.4185\n",
      "Epoch [2/10], Step [17625/68337], Loss: 5.2555\n",
      "Epoch [2/10], Step [17700/68337], Loss: 5.2841\n",
      "Epoch [2/10], Step [17775/68337], Loss: 5.2114\n",
      "Epoch [2/10], Step [17850/68337], Loss: 5.5975\n",
      "Epoch [2/10], Step [17925/68337], Loss: 5.3606\n",
      "Epoch [2/10], Step [18000/68337], Loss: 5.0506\n",
      "Epoch [2/10], Step [18075/68337], Loss: 5.1528\n",
      "Epoch [2/10], Step [18150/68337], Loss: 5.2100\n",
      "Epoch [2/10], Step [18225/68337], Loss: 5.1847\n",
      "Epoch [2/10], Step [18300/68337], Loss: 5.1980\n",
      "Epoch [2/10], Step [18375/68337], Loss: 5.1778\n",
      "Epoch [2/10], Step [18450/68337], Loss: 5.3264\n",
      "Epoch [2/10], Step [18525/68337], Loss: 5.3270\n",
      "Epoch [2/10], Step [18600/68337], Loss: 5.1195\n",
      "Epoch [2/10], Step [18675/68337], Loss: 5.1247\n",
      "Epoch [2/10], Step [18750/68337], Loss: 5.2965\n",
      "Epoch [2/10], Step [18825/68337], Loss: 5.5232\n",
      "Epoch [2/10], Step [18900/68337], Loss: 5.3951\n",
      "Epoch [2/10], Step [18975/68337], Loss: 5.1457\n",
      "Epoch [2/10], Step [19050/68337], Loss: 5.4950\n",
      "Epoch [2/10], Step [19125/68337], Loss: 5.2932\n",
      "Epoch [2/10], Step [19200/68337], Loss: 5.5570\n",
      "Epoch [2/10], Step [19275/68337], Loss: 5.1467\n",
      "Epoch [2/10], Step [19350/68337], Loss: 5.4002\n",
      "Epoch [2/10], Step [19425/68337], Loss: 5.3116\n",
      "Epoch [2/10], Step [19500/68337], Loss: 5.0279\n",
      "Epoch [2/10], Step [19575/68337], Loss: 5.2517\n",
      "Epoch [2/10], Step [19650/68337], Loss: 5.2721\n",
      "Epoch [2/10], Step [19725/68337], Loss: 5.2827\n",
      "Epoch [2/10], Step [19800/68337], Loss: 5.1129\n",
      "Epoch [2/10], Step [19875/68337], Loss: 5.3013\n",
      "Epoch [2/10], Step [19950/68337], Loss: 5.0907\n",
      "Validation perplexity: 156.7121462252644\n",
      "Epoch [2/10], Step [20025/68337], Loss: 5.3563\n",
      "Epoch [2/10], Step [20100/68337], Loss: 5.2373\n",
      "Epoch [2/10], Step [20175/68337], Loss: 5.1429\n",
      "Epoch [2/10], Step [20250/68337], Loss: 5.2181\n",
      "Epoch [2/10], Step [20325/68337], Loss: 5.0604\n",
      "Epoch [2/10], Step [20400/68337], Loss: 5.1686\n",
      "Epoch [2/10], Step [20475/68337], Loss: 5.3868\n",
      "Epoch [2/10], Step [20550/68337], Loss: 5.2548\n",
      "Epoch [2/10], Step [20625/68337], Loss: 5.3533\n",
      "Epoch [2/10], Step [20700/68337], Loss: 5.2626\n",
      "Epoch [2/10], Step [20775/68337], Loss: 5.2740\n",
      "Epoch [2/10], Step [20850/68337], Loss: 5.3556\n",
      "Epoch [2/10], Step [20925/68337], Loss: 5.1566\n",
      "Epoch [2/10], Step [21000/68337], Loss: 5.4063\n",
      "Epoch [2/10], Step [21075/68337], Loss: 5.2282\n",
      "Epoch [2/10], Step [21150/68337], Loss: 5.2900\n",
      "Epoch [2/10], Step [21225/68337], Loss: 5.2516\n",
      "Epoch [2/10], Step [21300/68337], Loss: 5.0796\n",
      "Epoch [2/10], Step [21375/68337], Loss: 5.2618\n",
      "Epoch [2/10], Step [21450/68337], Loss: 5.2685\n",
      "Epoch [2/10], Step [21525/68337], Loss: 5.0333\n",
      "Epoch [2/10], Step [21600/68337], Loss: 5.3993\n",
      "Epoch [2/10], Step [21675/68337], Loss: 5.3318\n",
      "Epoch [2/10], Step [21750/68337], Loss: 5.1549\n",
      "Epoch [2/10], Step [21825/68337], Loss: 5.3483\n",
      "Epoch [2/10], Step [21900/68337], Loss: 4.9567\n",
      "Epoch [2/10], Step [21975/68337], Loss: 5.3895\n",
      "Epoch [2/10], Step [22050/68337], Loss: 5.2555\n",
      "Epoch [2/10], Step [22125/68337], Loss: 5.3722\n",
      "Epoch [2/10], Step [22200/68337], Loss: 5.4385\n",
      "Epoch [2/10], Step [22275/68337], Loss: 5.2130\n",
      "Epoch [2/10], Step [22350/68337], Loss: 5.3434\n",
      "Epoch [2/10], Step [22425/68337], Loss: 5.2417\n",
      "Epoch [2/10], Step [22500/68337], Loss: 5.2257\n",
      "Epoch [2/10], Step [22575/68337], Loss: 5.3086\n",
      "Epoch [2/10], Step [22650/68337], Loss: 5.1420\n",
      "Epoch [2/10], Step [22725/68337], Loss: 5.0586\n",
      "Epoch [2/10], Step [22800/68337], Loss: 5.2963\n",
      "Epoch [2/10], Step [22875/68337], Loss: 5.3528\n",
      "Epoch [2/10], Step [22950/68337], Loss: 5.3747\n",
      "Epoch [2/10], Step [23025/68337], Loss: 5.3729\n",
      "Epoch [2/10], Step [23100/68337], Loss: 5.4407\n",
      "Epoch [2/10], Step [23175/68337], Loss: 5.2387\n",
      "Epoch [2/10], Step [23250/68337], Loss: 5.4203\n",
      "Epoch [2/10], Step [23325/68337], Loss: 5.2836\n",
      "Epoch [2/10], Step [23400/68337], Loss: 5.1490\n",
      "Epoch [2/10], Step [23475/68337], Loss: 5.2533\n",
      "Epoch [2/10], Step [23550/68337], Loss: 5.3645\n",
      "Epoch [2/10], Step [23625/68337], Loss: 5.2944\n",
      "Epoch [2/10], Step [23700/68337], Loss: 5.3226\n",
      "Epoch [2/10], Step [23775/68337], Loss: 5.2129\n",
      "Epoch [2/10], Step [23850/68337], Loss: 5.2522\n",
      "Epoch [2/10], Step [23925/68337], Loss: 5.2700\n",
      "Epoch [2/10], Step [24000/68337], Loss: 5.3976\n",
      "Epoch [2/10], Step [24075/68337], Loss: 5.2248\n",
      "Epoch [2/10], Step [24150/68337], Loss: 4.9177\n",
      "Epoch [2/10], Step [24225/68337], Loss: 5.3063\n",
      "Epoch [2/10], Step [24300/68337], Loss: 5.3833\n",
      "Epoch [2/10], Step [24375/68337], Loss: 5.1603\n",
      "Epoch [2/10], Step [24450/68337], Loss: 5.3408\n",
      "Epoch [2/10], Step [24525/68337], Loss: 5.2623\n",
      "Epoch [2/10], Step [24600/68337], Loss: 5.4134\n",
      "Epoch [2/10], Step [24675/68337], Loss: 5.2822\n",
      "Epoch [2/10], Step [24750/68337], Loss: 5.3519\n",
      "Epoch [2/10], Step [24825/68337], Loss: 5.3785\n",
      "Epoch [2/10], Step [24900/68337], Loss: 5.3544\n",
      "Epoch [2/10], Step [24975/68337], Loss: 5.4129\n",
      "Epoch [2/10], Step [25050/68337], Loss: 5.0780\n",
      "Epoch [2/10], Step [25125/68337], Loss: 5.2299\n",
      "Epoch [2/10], Step [25200/68337], Loss: 5.3179\n",
      "Epoch [2/10], Step [25275/68337], Loss: 5.2021\n",
      "Epoch [2/10], Step [25350/68337], Loss: 5.2008\n",
      "Epoch [2/10], Step [25425/68337], Loss: 5.3908\n",
      "Epoch [2/10], Step [25500/68337], Loss: 5.3617\n",
      "Epoch [2/10], Step [25575/68337], Loss: 5.2461\n",
      "Epoch [2/10], Step [25650/68337], Loss: 5.3083\n",
      "Epoch [2/10], Step [25725/68337], Loss: 5.2884\n",
      "Epoch [2/10], Step [25800/68337], Loss: 5.2983\n",
      "Epoch [2/10], Step [25875/68337], Loss: 5.0788\n",
      "Epoch [2/10], Step [25950/68337], Loss: 5.3448\n",
      "Epoch [2/10], Step [26025/68337], Loss: 5.1964\n",
      "Epoch [2/10], Step [26100/68337], Loss: 5.1503\n",
      "Epoch [2/10], Step [26175/68337], Loss: 5.2513\n",
      "Epoch [2/10], Step [26250/68337], Loss: 5.3664\n",
      "Epoch [2/10], Step [26325/68337], Loss: 5.2665\n",
      "Epoch [2/10], Step [26400/68337], Loss: 5.1931\n",
      "Epoch [2/10], Step [26475/68337], Loss: 5.4122\n",
      "Epoch [2/10], Step [26550/68337], Loss: 5.0462\n",
      "Epoch [2/10], Step [26625/68337], Loss: 5.2552\n",
      "Epoch [2/10], Step [26700/68337], Loss: 5.2483\n",
      "Epoch [2/10], Step [26775/68337], Loss: 5.3033\n",
      "Epoch [2/10], Step [26850/68337], Loss: 5.3788\n",
      "Epoch [2/10], Step [26925/68337], Loss: 5.2648\n",
      "Epoch [2/10], Step [27000/68337], Loss: 5.2605\n",
      "Epoch [2/10], Step [27075/68337], Loss: 5.1894\n",
      "Epoch [2/10], Step [27150/68337], Loss: 5.1808\n",
      "Epoch [2/10], Step [27225/68337], Loss: 5.2241\n",
      "Epoch [2/10], Step [27300/68337], Loss: 5.3457\n",
      "Epoch [2/10], Step [27375/68337], Loss: 5.3722\n",
      "Epoch [2/10], Step [27450/68337], Loss: 5.3515\n",
      "Epoch [2/10], Step [27525/68337], Loss: 5.1755\n",
      "Epoch [2/10], Step [27600/68337], Loss: 5.4936\n",
      "Epoch [2/10], Step [27675/68337], Loss: 5.1709\n",
      "Epoch [2/10], Step [27750/68337], Loss: 5.1986\n",
      "Epoch [2/10], Step [27825/68337], Loss: 5.4048\n",
      "Epoch [2/10], Step [27900/68337], Loss: 5.3080\n",
      "Epoch [2/10], Step [27975/68337], Loss: 5.2766\n",
      "Epoch [2/10], Step [28050/68337], Loss: 5.3062\n",
      "Epoch [2/10], Step [28125/68337], Loss: 5.3988\n",
      "Epoch [2/10], Step [28200/68337], Loss: 5.0419\n",
      "Epoch [2/10], Step [28275/68337], Loss: 5.3938\n",
      "Epoch [2/10], Step [28350/68337], Loss: 5.3889\n",
      "Epoch [2/10], Step [28425/68337], Loss: 5.1898\n",
      "Epoch [2/10], Step [28500/68337], Loss: 5.4379\n",
      "Epoch [2/10], Step [28575/68337], Loss: 5.0519\n",
      "Epoch [2/10], Step [28650/68337], Loss: 5.1308\n",
      "Epoch [2/10], Step [28725/68337], Loss: 5.1921\n",
      "Epoch [2/10], Step [28800/68337], Loss: 5.0060\n",
      "Epoch [2/10], Step [28875/68337], Loss: 5.1890\n",
      "Epoch [2/10], Step [28950/68337], Loss: 5.3658\n",
      "Epoch [2/10], Step [29025/68337], Loss: 5.1176\n",
      "Epoch [2/10], Step [29100/68337], Loss: 5.3787\n",
      "Epoch [2/10], Step [29175/68337], Loss: 5.1135\n",
      "Epoch [2/10], Step [29250/68337], Loss: 5.3827\n",
      "Epoch [2/10], Step [29325/68337], Loss: 5.2318\n",
      "Epoch [2/10], Step [29400/68337], Loss: 5.2496\n",
      "Epoch [2/10], Step [29475/68337], Loss: 5.1510\n",
      "Epoch [2/10], Step [29550/68337], Loss: 5.3150\n",
      "Epoch [2/10], Step [29625/68337], Loss: 5.3747\n",
      "Epoch [2/10], Step [29700/68337], Loss: 5.2693\n",
      "Epoch [2/10], Step [29775/68337], Loss: 5.4750\n",
      "Epoch [2/10], Step [29850/68337], Loss: 5.2012\n",
      "Epoch [2/10], Step [29925/68337], Loss: 5.2957\n",
      "Epoch [2/10], Step [30000/68337], Loss: 5.4852\n",
      "Validation perplexity: 152.7891930160101\n",
      "Epoch [2/10], Step [30075/68337], Loss: 5.1513\n",
      "Epoch [2/10], Step [30150/68337], Loss: 4.9271\n",
      "Epoch [2/10], Step [30225/68337], Loss: 5.4053\n",
      "Epoch [2/10], Step [30300/68337], Loss: 5.1607\n",
      "Epoch [2/10], Step [30375/68337], Loss: 5.3743\n",
      "Epoch [2/10], Step [30450/68337], Loss: 5.3062\n",
      "Epoch [2/10], Step [30525/68337], Loss: 5.2395\n",
      "Epoch [2/10], Step [30600/68337], Loss: 5.3897\n",
      "Epoch [2/10], Step [30675/68337], Loss: 5.2064\n",
      "Epoch [2/10], Step [30750/68337], Loss: 5.2794\n",
      "Epoch [2/10], Step [30825/68337], Loss: 5.2171\n",
      "Epoch [2/10], Step [30900/68337], Loss: 5.3214\n",
      "Epoch [2/10], Step [30975/68337], Loss: 5.1486\n",
      "Epoch [2/10], Step [31050/68337], Loss: 5.4427\n",
      "Epoch [2/10], Step [31125/68337], Loss: 5.1734\n",
      "Epoch [2/10], Step [31200/68337], Loss: 5.5405\n",
      "Epoch [2/10], Step [31275/68337], Loss: 5.4058\n",
      "Epoch [2/10], Step [31350/68337], Loss: 5.4594\n",
      "Epoch [2/10], Step [31425/68337], Loss: 5.0596\n",
      "Epoch [2/10], Step [31500/68337], Loss: 5.4679\n",
      "Epoch [2/10], Step [31575/68337], Loss: 5.1763\n",
      "Epoch [2/10], Step [31650/68337], Loss: 5.3471\n",
      "Epoch [2/10], Step [31725/68337], Loss: 5.1470\n",
      "Epoch [2/10], Step [31800/68337], Loss: 5.3468\n",
      "Epoch [2/10], Step [31875/68337], Loss: 5.1905\n",
      "Epoch [2/10], Step [31950/68337], Loss: 5.3097\n",
      "Epoch [2/10], Step [32025/68337], Loss: 5.1894\n",
      "Epoch [2/10], Step [32100/68337], Loss: 5.1369\n",
      "Epoch [2/10], Step [32175/68337], Loss: 5.1459\n",
      "Epoch [2/10], Step [32250/68337], Loss: 5.1757\n",
      "Epoch [2/10], Step [32325/68337], Loss: 5.3706\n",
      "Epoch [2/10], Step [32400/68337], Loss: 5.1759\n",
      "Epoch [2/10], Step [32475/68337], Loss: 5.0570\n",
      "Epoch [2/10], Step [32550/68337], Loss: 5.1943\n",
      "Epoch [2/10], Step [32625/68337], Loss: 5.3229\n",
      "Epoch [2/10], Step [32700/68337], Loss: 5.4192\n",
      "Epoch [2/10], Step [32775/68337], Loss: 5.3144\n",
      "Epoch [2/10], Step [32850/68337], Loss: 5.3002\n",
      "Epoch [2/10], Step [32925/68337], Loss: 5.2489\n",
      "Epoch [2/10], Step [33000/68337], Loss: 5.2478\n",
      "Epoch [2/10], Step [33075/68337], Loss: 5.2174\n",
      "Epoch [2/10], Step [33150/68337], Loss: 5.4310\n",
      "Epoch [2/10], Step [33225/68337], Loss: 5.3861\n",
      "Epoch [2/10], Step [33300/68337], Loss: 5.1989\n",
      "Epoch [2/10], Step [33375/68337], Loss: 5.2853\n",
      "Epoch [2/10], Step [33450/68337], Loss: 5.1661\n",
      "Epoch [2/10], Step [33525/68337], Loss: 5.1520\n",
      "Epoch [2/10], Step [33600/68337], Loss: 5.5601\n",
      "Epoch [2/10], Step [33675/68337], Loss: 5.1119\n",
      "Epoch [2/10], Step [33750/68337], Loss: 5.4065\n",
      "Epoch [2/10], Step [33825/68337], Loss: 5.4354\n",
      "Epoch [2/10], Step [33900/68337], Loss: 5.0530\n",
      "Epoch [2/10], Step [33975/68337], Loss: 5.1120\n",
      "Epoch [2/10], Step [34050/68337], Loss: 5.2435\n",
      "Epoch [2/10], Step [34125/68337], Loss: 5.1686\n",
      "Epoch [2/10], Step [34200/68337], Loss: 5.1063\n",
      "Epoch [2/10], Step [34275/68337], Loss: 5.2123\n",
      "Epoch [2/10], Step [34350/68337], Loss: 5.3133\n",
      "Epoch [2/10], Step [34425/68337], Loss: 5.2440\n",
      "Epoch [2/10], Step [34500/68337], Loss: 5.1451\n",
      "Epoch [2/10], Step [34575/68337], Loss: 5.3386\n",
      "Epoch [2/10], Step [34650/68337], Loss: 5.4043\n",
      "Epoch [2/10], Step [34725/68337], Loss: 5.3629\n",
      "Epoch [2/10], Step [34800/68337], Loss: 5.5412\n",
      "Epoch [2/10], Step [34875/68337], Loss: 5.0631\n",
      "Epoch [2/10], Step [34950/68337], Loss: 5.2583\n",
      "Epoch [2/10], Step [35025/68337], Loss: 5.3441\n",
      "Epoch [2/10], Step [35100/68337], Loss: 5.2575\n",
      "Epoch [2/10], Step [35175/68337], Loss: 5.2530\n",
      "Epoch [2/10], Step [35250/68337], Loss: 5.2374\n",
      "Epoch [2/10], Step [35325/68337], Loss: 5.4387\n",
      "Epoch [2/10], Step [35400/68337], Loss: 5.0912\n",
      "Epoch [2/10], Step [35475/68337], Loss: 5.2162\n",
      "Epoch [2/10], Step [35550/68337], Loss: 5.3462\n",
      "Epoch [2/10], Step [35625/68337], Loss: 5.3118\n",
      "Epoch [2/10], Step [35700/68337], Loss: 5.2006\n",
      "Epoch [2/10], Step [35775/68337], Loss: 5.1967\n",
      "Epoch [2/10], Step [35850/68337], Loss: 5.4402\n",
      "Epoch [2/10], Step [35925/68337], Loss: 5.1882\n",
      "Epoch [2/10], Step [36000/68337], Loss: 5.2040\n",
      "Epoch [2/10], Step [36075/68337], Loss: 5.3648\n",
      "Epoch [2/10], Step [36150/68337], Loss: 5.1381\n",
      "Epoch [2/10], Step [36225/68337], Loss: 5.2715\n",
      "Epoch [2/10], Step [36300/68337], Loss: 5.3435\n",
      "Epoch [2/10], Step [36375/68337], Loss: 5.2329\n",
      "Epoch [2/10], Step [36450/68337], Loss: 5.2416\n",
      "Epoch [2/10], Step [36525/68337], Loss: 5.2224\n",
      "Epoch [2/10], Step [36600/68337], Loss: 5.3564\n",
      "Epoch [2/10], Step [36675/68337], Loss: 4.9230\n",
      "Epoch [2/10], Step [36750/68337], Loss: 5.0772\n",
      "Epoch [2/10], Step [36825/68337], Loss: 5.4345\n",
      "Epoch [2/10], Step [36900/68337], Loss: 5.1533\n",
      "Epoch [2/10], Step [36975/68337], Loss: 5.2352\n",
      "Epoch [2/10], Step [37050/68337], Loss: 5.2821\n",
      "Epoch [2/10], Step [37125/68337], Loss: 5.3263\n",
      "Epoch [2/10], Step [37200/68337], Loss: 5.2872\n",
      "Epoch [2/10], Step [37275/68337], Loss: 5.1032\n",
      "Epoch [2/10], Step [37350/68337], Loss: 5.1608\n",
      "Epoch [2/10], Step [37425/68337], Loss: 5.3737\n",
      "Epoch [2/10], Step [37500/68337], Loss: 5.2099\n",
      "Epoch [2/10], Step [37575/68337], Loss: 5.2575\n",
      "Epoch [2/10], Step [37650/68337], Loss: 5.2826\n",
      "Epoch [2/10], Step [37725/68337], Loss: 5.2814\n",
      "Epoch [2/10], Step [37800/68337], Loss: 5.1231\n",
      "Epoch [2/10], Step [37875/68337], Loss: 5.2449\n",
      "Epoch [2/10], Step [37950/68337], Loss: 5.1954\n",
      "Epoch [2/10], Step [38025/68337], Loss: 5.1963\n",
      "Epoch [2/10], Step [38100/68337], Loss: 5.1967\n",
      "Epoch [2/10], Step [38175/68337], Loss: 5.3208\n",
      "Epoch [2/10], Step [38250/68337], Loss: 5.0579\n",
      "Epoch [2/10], Step [38325/68337], Loss: 5.1858\n",
      "Epoch [2/10], Step [38400/68337], Loss: 5.2478\n",
      "Epoch [2/10], Step [38475/68337], Loss: 5.4345\n",
      "Epoch [2/10], Step [38550/68337], Loss: 5.3613\n",
      "Epoch [2/10], Step [38625/68337], Loss: 5.1692\n",
      "Epoch [2/10], Step [38700/68337], Loss: 5.2038\n",
      "Epoch [2/10], Step [38775/68337], Loss: 5.4133\n",
      "Epoch [2/10], Step [38850/68337], Loss: 5.1994\n",
      "Epoch [2/10], Step [38925/68337], Loss: 5.0674\n",
      "Epoch [2/10], Step [39000/68337], Loss: 5.2367\n",
      "Epoch [2/10], Step [39075/68337], Loss: 5.1694\n",
      "Epoch [2/10], Step [39150/68337], Loss: 5.3326\n",
      "Epoch [2/10], Step [39225/68337], Loss: 5.2664\n",
      "Epoch [2/10], Step [39300/68337], Loss: 5.2859\n",
      "Epoch [2/10], Step [39375/68337], Loss: 5.2007\n",
      "Epoch [2/10], Step [39450/68337], Loss: 5.3958\n",
      "Epoch [2/10], Step [39525/68337], Loss: 5.3072\n",
      "Epoch [2/10], Step [39600/68337], Loss: 5.1282\n",
      "Epoch [2/10], Step [39675/68337], Loss: 5.2712\n",
      "Epoch [2/10], Step [39750/68337], Loss: 5.3181\n",
      "Epoch [2/10], Step [39825/68337], Loss: 5.0605\n",
      "Epoch [2/10], Step [39900/68337], Loss: 5.2916\n",
      "Epoch [2/10], Step [39975/68337], Loss: 5.4606\n",
      "Validation perplexity: 150.430440441919\n",
      "Epoch [2/10], Step [40050/68337], Loss: 5.3339\n",
      "Epoch [2/10], Step [40125/68337], Loss: 5.2246\n",
      "Epoch [2/10], Step [40200/68337], Loss: 5.1589\n",
      "Epoch [2/10], Step [40275/68337], Loss: 5.2527\n",
      "Epoch [2/10], Step [40350/68337], Loss: 5.2978\n",
      "Epoch [2/10], Step [40425/68337], Loss: 5.4216\n",
      "Epoch [2/10], Step [40500/68337], Loss: 5.2217\n",
      "Epoch [2/10], Step [40575/68337], Loss: 5.4987\n",
      "Epoch [2/10], Step [40650/68337], Loss: 5.2471\n",
      "Epoch [2/10], Step [40725/68337], Loss: 5.3708\n",
      "Epoch [2/10], Step [40800/68337], Loss: 5.1267\n",
      "Epoch [2/10], Step [40875/68337], Loss: 5.3056\n",
      "Epoch [2/10], Step [40950/68337], Loss: 5.1915\n",
      "Epoch [2/10], Step [41025/68337], Loss: 5.1806\n",
      "Epoch [2/10], Step [41100/68337], Loss: 5.2415\n",
      "Epoch [2/10], Step [41175/68337], Loss: 5.2670\n",
      "Epoch [2/10], Step [41250/68337], Loss: 5.1830\n",
      "Epoch [2/10], Step [41325/68337], Loss: 5.1460\n",
      "Epoch [2/10], Step [41400/68337], Loss: 5.1050\n",
      "Epoch [2/10], Step [41475/68337], Loss: 5.0884\n",
      "Epoch [2/10], Step [41550/68337], Loss: 5.1786\n",
      "Epoch [2/10], Step [41625/68337], Loss: 5.1444\n",
      "Epoch [2/10], Step [41700/68337], Loss: 5.2581\n",
      "Epoch [2/10], Step [41775/68337], Loss: 5.2683\n",
      "Epoch [2/10], Step [41850/68337], Loss: 5.3099\n",
      "Epoch [2/10], Step [41925/68337], Loss: 5.2633\n",
      "Epoch [2/10], Step [42000/68337], Loss: 5.3286\n",
      "Epoch [2/10], Step [42075/68337], Loss: 5.3062\n",
      "Epoch [2/10], Step [42150/68337], Loss: 5.1014\n",
      "Epoch [2/10], Step [42225/68337], Loss: 5.2301\n",
      "Epoch [2/10], Step [42300/68337], Loss: 5.2487\n",
      "Epoch [2/10], Step [42375/68337], Loss: 5.4433\n",
      "Epoch [2/10], Step [42450/68337], Loss: 5.3379\n",
      "Epoch [2/10], Step [42525/68337], Loss: 5.2788\n",
      "Epoch [2/10], Step [42600/68337], Loss: 5.1143\n",
      "Epoch [2/10], Step [42675/68337], Loss: 5.0372\n",
      "Epoch [2/10], Step [42750/68337], Loss: 5.2223\n",
      "Epoch [2/10], Step [42825/68337], Loss: 5.0897\n",
      "Epoch [2/10], Step [42900/68337], Loss: 5.3240\n",
      "Epoch [2/10], Step [42975/68337], Loss: 5.1277\n",
      "Epoch [2/10], Step [43050/68337], Loss: 5.1450\n",
      "Epoch [2/10], Step [43125/68337], Loss: 5.3490\n",
      "Epoch [2/10], Step [43200/68337], Loss: 5.2181\n",
      "Epoch [2/10], Step [43275/68337], Loss: 4.9535\n",
      "Epoch [2/10], Step [43350/68337], Loss: 5.1744\n",
      "Epoch [2/10], Step [43425/68337], Loss: 5.3612\n",
      "Epoch [2/10], Step [43500/68337], Loss: 5.1148\n",
      "Epoch [2/10], Step [43575/68337], Loss: 5.3210\n",
      "Epoch [2/10], Step [43650/68337], Loss: 5.2201\n",
      "Epoch [2/10], Step [43725/68337], Loss: 5.4843\n",
      "Epoch [2/10], Step [43800/68337], Loss: 5.2374\n",
      "Epoch [2/10], Step [43875/68337], Loss: 5.3607\n",
      "Epoch [2/10], Step [43950/68337], Loss: 5.2368\n",
      "Epoch [2/10], Step [44025/68337], Loss: 5.0210\n",
      "Epoch [2/10], Step [44100/68337], Loss: 5.2541\n",
      "Epoch [2/10], Step [44175/68337], Loss: 5.3156\n",
      "Epoch [2/10], Step [44250/68337], Loss: 5.1100\n",
      "Epoch [2/10], Step [44325/68337], Loss: 5.3893\n",
      "Epoch [2/10], Step [44400/68337], Loss: 5.3516\n",
      "Epoch [2/10], Step [44475/68337], Loss: 5.0481\n",
      "Epoch [2/10], Step [44550/68337], Loss: 5.1611\n",
      "Epoch [2/10], Step [44625/68337], Loss: 5.0451\n",
      "Epoch [2/10], Step [44700/68337], Loss: 5.2267\n",
      "Epoch [2/10], Step [44775/68337], Loss: 5.3740\n",
      "Epoch [2/10], Step [44850/68337], Loss: 5.1717\n",
      "Epoch [2/10], Step [44925/68337], Loss: 5.2325\n",
      "Epoch [2/10], Step [45000/68337], Loss: 5.2077\n",
      "Epoch [2/10], Step [45075/68337], Loss: 5.1716\n",
      "Epoch [2/10], Step [45150/68337], Loss: 5.2098\n",
      "Epoch [2/10], Step [45225/68337], Loss: 5.3080\n",
      "Epoch [2/10], Step [45300/68337], Loss: 5.2620\n",
      "Epoch [2/10], Step [45375/68337], Loss: 5.3306\n",
      "Epoch [2/10], Step [45450/68337], Loss: 5.3443\n",
      "Epoch [2/10], Step [45525/68337], Loss: 5.2224\n",
      "Epoch [2/10], Step [45600/68337], Loss: 5.3701\n",
      "Epoch [2/10], Step [45675/68337], Loss: 5.0890\n",
      "Epoch [2/10], Step [45750/68337], Loss: 5.2760\n",
      "Epoch [2/10], Step [45825/68337], Loss: 5.2667\n",
      "Epoch [2/10], Step [45900/68337], Loss: 5.0743\n",
      "Epoch [2/10], Step [45975/68337], Loss: 5.2438\n",
      "Epoch [2/10], Step [46050/68337], Loss: 5.1867\n",
      "Epoch [2/10], Step [46125/68337], Loss: 5.3583\n",
      "Epoch [2/10], Step [46200/68337], Loss: 5.2442\n",
      "Epoch [2/10], Step [46275/68337], Loss: 5.3497\n",
      "Epoch [2/10], Step [46350/68337], Loss: 5.0383\n",
      "Epoch [2/10], Step [46425/68337], Loss: 5.3241\n",
      "Epoch [2/10], Step [46500/68337], Loss: 5.3974\n",
      "Epoch [2/10], Step [46575/68337], Loss: 5.3374\n",
      "Epoch [2/10], Step [46650/68337], Loss: 5.4874\n",
      "Epoch [2/10], Step [46725/68337], Loss: 5.3658\n",
      "Epoch [2/10], Step [46800/68337], Loss: 5.1376\n",
      "Epoch [2/10], Step [46875/68337], Loss: 5.0607\n",
      "Epoch [2/10], Step [46950/68337], Loss: 5.1302\n",
      "Epoch [2/10], Step [47025/68337], Loss: 5.2808\n",
      "Epoch [2/10], Step [47100/68337], Loss: 5.3006\n",
      "Epoch [2/10], Step [47175/68337], Loss: 5.1378\n",
      "Epoch [2/10], Step [47250/68337], Loss: 5.2315\n",
      "Epoch [2/10], Step [47325/68337], Loss: 5.2384\n",
      "Epoch [2/10], Step [47400/68337], Loss: 5.1936\n",
      "Epoch [2/10], Step [47475/68337], Loss: 5.2330\n",
      "Epoch [2/10], Step [47550/68337], Loss: 5.0027\n",
      "Epoch [2/10], Step [47625/68337], Loss: 5.0834\n",
      "Epoch [2/10], Step [47700/68337], Loss: 5.1768\n",
      "Epoch [2/10], Step [47775/68337], Loss: 5.2716\n",
      "Epoch [2/10], Step [47850/68337], Loss: 4.9443\n",
      "Epoch [2/10], Step [47925/68337], Loss: 5.5316\n",
      "Epoch [2/10], Step [48000/68337], Loss: 5.2471\n",
      "Epoch [2/10], Step [48075/68337], Loss: 5.0928\n",
      "Epoch [2/10], Step [48150/68337], Loss: 5.1899\n",
      "Epoch [2/10], Step [48225/68337], Loss: 5.0760\n",
      "Epoch [2/10], Step [48300/68337], Loss: 5.0812\n",
      "Epoch [2/10], Step [48375/68337], Loss: 5.1508\n",
      "Epoch [2/10], Step [48450/68337], Loss: 5.0765\n",
      "Epoch [2/10], Step [48525/68337], Loss: 5.4375\n",
      "Epoch [2/10], Step [48600/68337], Loss: 5.1393\n",
      "Epoch [2/10], Step [48675/68337], Loss: 5.3046\n",
      "Epoch [2/10], Step [48750/68337], Loss: 5.0850\n",
      "Epoch [2/10], Step [48825/68337], Loss: 5.1932\n",
      "Epoch [2/10], Step [48900/68337], Loss: 5.4617\n",
      "Epoch [2/10], Step [48975/68337], Loss: 5.3565\n",
      "Epoch [2/10], Step [49050/68337], Loss: 4.8080\n",
      "Epoch [2/10], Step [49125/68337], Loss: 5.1698\n",
      "Epoch [2/10], Step [49200/68337], Loss: 5.2184\n",
      "Epoch [2/10], Step [49275/68337], Loss: 5.1833\n",
      "Epoch [2/10], Step [49350/68337], Loss: 5.4009\n",
      "Epoch [2/10], Step [49425/68337], Loss: 5.2871\n",
      "Epoch [2/10], Step [49500/68337], Loss: 4.9517\n",
      "Epoch [2/10], Step [49575/68337], Loss: 5.2771\n",
      "Epoch [2/10], Step [49650/68337], Loss: 5.1287\n",
      "Epoch [2/10], Step [49725/68337], Loss: 5.1932\n",
      "Epoch [2/10], Step [49800/68337], Loss: 5.1136\n",
      "Epoch [2/10], Step [49875/68337], Loss: 5.3934\n",
      "Epoch [2/10], Step [49950/68337], Loss: 5.2373\n",
      "Validation perplexity: 147.44272842455806\n",
      "Epoch [2/10], Step [50025/68337], Loss: 5.1725\n",
      "Epoch [2/10], Step [50100/68337], Loss: 5.4748\n",
      "Epoch [2/10], Step [50175/68337], Loss: 5.2646\n",
      "Epoch [2/10], Step [50250/68337], Loss: 5.1806\n",
      "Epoch [2/10], Step [50325/68337], Loss: 5.0344\n",
      "Epoch [2/10], Step [50400/68337], Loss: 5.3983\n",
      "Epoch [2/10], Step [50475/68337], Loss: 5.1211\n",
      "Epoch [2/10], Step [50550/68337], Loss: 5.2252\n",
      "Epoch [2/10], Step [50625/68337], Loss: 5.3092\n",
      "Epoch [2/10], Step [50700/68337], Loss: 5.3175\n",
      "Epoch [2/10], Step [50775/68337], Loss: 5.0854\n",
      "Epoch [2/10], Step [50850/68337], Loss: 5.4120\n",
      "Epoch [2/10], Step [50925/68337], Loss: 5.4217\n",
      "Epoch [2/10], Step [51000/68337], Loss: 5.3539\n",
      "Epoch [2/10], Step [51075/68337], Loss: 5.1957\n",
      "Epoch [2/10], Step [51150/68337], Loss: 5.0579\n",
      "Epoch [2/10], Step [51225/68337], Loss: 5.2973\n",
      "Epoch [2/10], Step [51300/68337], Loss: 5.2741\n",
      "Epoch [2/10], Step [51375/68337], Loss: 5.1577\n",
      "Epoch [2/10], Step [51450/68337], Loss: 5.3010\n",
      "Epoch [2/10], Step [51525/68337], Loss: 5.1718\n",
      "Epoch [2/10], Step [51600/68337], Loss: 5.3122\n",
      "Epoch [2/10], Step [51675/68337], Loss: 5.2493\n",
      "Epoch [2/10], Step [51750/68337], Loss: 5.1329\n",
      "Epoch [2/10], Step [51825/68337], Loss: 5.3238\n",
      "Epoch [2/10], Step [51900/68337], Loss: 5.2433\n",
      "Epoch [2/10], Step [51975/68337], Loss: 5.1480\n",
      "Epoch [2/10], Step [52050/68337], Loss: 5.2076\n",
      "Epoch [2/10], Step [52125/68337], Loss: 5.1944\n",
      "Epoch [2/10], Step [52200/68337], Loss: 5.3623\n",
      "Epoch [2/10], Step [52275/68337], Loss: 5.0740\n",
      "Epoch [2/10], Step [52350/68337], Loss: 5.1872\n",
      "Epoch [2/10], Step [52425/68337], Loss: 5.2520\n",
      "Epoch [2/10], Step [52500/68337], Loss: 5.2680\n",
      "Epoch [2/10], Step [52575/68337], Loss: 5.3216\n",
      "Epoch [2/10], Step [52650/68337], Loss: 5.0849\n",
      "Epoch [2/10], Step [52725/68337], Loss: 5.3091\n",
      "Epoch [2/10], Step [52800/68337], Loss: 5.4306\n",
      "Epoch [2/10], Step [52875/68337], Loss: 5.3389\n",
      "Epoch [2/10], Step [52950/68337], Loss: 5.3081\n",
      "Epoch [2/10], Step [53025/68337], Loss: 5.3086\n",
      "Epoch [2/10], Step [53100/68337], Loss: 5.2990\n",
      "Epoch [2/10], Step [53175/68337], Loss: 5.1838\n",
      "Epoch [2/10], Step [53250/68337], Loss: 5.1177\n",
      "Epoch [2/10], Step [53325/68337], Loss: 5.3877\n",
      "Epoch [2/10], Step [53400/68337], Loss: 5.3598\n",
      "Epoch [2/10], Step [53475/68337], Loss: 5.2992\n",
      "Epoch [2/10], Step [53550/68337], Loss: 5.1537\n",
      "Epoch [2/10], Step [53625/68337], Loss: 5.1015\n",
      "Epoch [2/10], Step [53700/68337], Loss: 5.3713\n",
      "Epoch [2/10], Step [53775/68337], Loss: 5.1582\n",
      "Epoch [2/10], Step [53850/68337], Loss: 5.3391\n",
      "Epoch [2/10], Step [53925/68337], Loss: 5.1115\n",
      "Epoch [2/10], Step [54000/68337], Loss: 5.2286\n",
      "Epoch [2/10], Step [54075/68337], Loss: 5.3112\n",
      "Epoch [2/10], Step [54150/68337], Loss: 5.0215\n",
      "Epoch [2/10], Step [54225/68337], Loss: 4.9925\n",
      "Epoch [2/10], Step [54300/68337], Loss: 5.2807\n",
      "Epoch [2/10], Step [54375/68337], Loss: 5.3017\n",
      "Epoch [2/10], Step [54450/68337], Loss: 5.3042\n",
      "Epoch [2/10], Step [54525/68337], Loss: 5.3097\n",
      "Epoch [2/10], Step [54600/68337], Loss: 5.2736\n",
      "Epoch [2/10], Step [54675/68337], Loss: 5.2706\n",
      "Epoch [2/10], Step [54750/68337], Loss: 5.2020\n",
      "Epoch [2/10], Step [54825/68337], Loss: 5.3783\n",
      "Epoch [2/10], Step [54900/68337], Loss: 5.4212\n",
      "Epoch [2/10], Step [54975/68337], Loss: 5.2176\n",
      "Epoch [2/10], Step [55050/68337], Loss: 5.3308\n",
      "Epoch [2/10], Step [55125/68337], Loss: 5.0382\n",
      "Epoch [2/10], Step [55200/68337], Loss: 5.3571\n",
      "Epoch [2/10], Step [55275/68337], Loss: 5.0809\n",
      "Epoch [2/10], Step [55350/68337], Loss: 5.3914\n",
      "Epoch [2/10], Step [55425/68337], Loss: 5.2740\n",
      "Epoch [2/10], Step [55500/68337], Loss: 5.1212\n",
      "Epoch [2/10], Step [55575/68337], Loss: 5.1902\n",
      "Epoch [2/10], Step [55650/68337], Loss: 5.3729\n",
      "Epoch [2/10], Step [55725/68337], Loss: 5.1534\n",
      "Epoch [2/10], Step [55800/68337], Loss: 5.1397\n",
      "Epoch [2/10], Step [55875/68337], Loss: 5.2671\n",
      "Epoch [2/10], Step [55950/68337], Loss: 5.1685\n",
      "Epoch [2/10], Step [56025/68337], Loss: 5.2595\n",
      "Epoch [2/10], Step [56100/68337], Loss: 5.1856\n",
      "Epoch [2/10], Step [56175/68337], Loss: 5.0926\n",
      "Epoch [2/10], Step [56250/68337], Loss: 5.0351\n",
      "Epoch [2/10], Step [56325/68337], Loss: 5.1564\n",
      "Epoch [2/10], Step [56400/68337], Loss: 5.2369\n",
      "Epoch [2/10], Step [56475/68337], Loss: 5.1459\n",
      "Epoch [2/10], Step [56550/68337], Loss: 5.3692\n",
      "Epoch [2/10], Step [56625/68337], Loss: 5.0495\n",
      "Epoch [2/10], Step [56700/68337], Loss: 5.4100\n",
      "Epoch [2/10], Step [56775/68337], Loss: 5.0958\n",
      "Epoch [2/10], Step [56850/68337], Loss: 5.1165\n",
      "Epoch [2/10], Step [56925/68337], Loss: 4.9287\n",
      "Epoch [2/10], Step [57000/68337], Loss: 5.0840\n",
      "Epoch [2/10], Step [57075/68337], Loss: 5.0707\n",
      "Epoch [2/10], Step [57150/68337], Loss: 5.1560\n",
      "Epoch [2/10], Step [57225/68337], Loss: 5.2492\n",
      "Epoch [2/10], Step [57300/68337], Loss: 5.1328\n",
      "Epoch [2/10], Step [57375/68337], Loss: 5.2412\n",
      "Epoch [2/10], Step [57450/68337], Loss: 5.2088\n",
      "Epoch [2/10], Step [57525/68337], Loss: 5.1639\n",
      "Epoch [2/10], Step [57600/68337], Loss: 5.4932\n",
      "Epoch [2/10], Step [57675/68337], Loss: 5.1429\n",
      "Epoch [2/10], Step [57750/68337], Loss: 4.8979\n",
      "Epoch [2/10], Step [57825/68337], Loss: 5.3866\n",
      "Epoch [2/10], Step [57900/68337], Loss: 5.1222\n",
      "Epoch [2/10], Step [57975/68337], Loss: 5.0966\n",
      "Epoch [2/10], Step [58050/68337], Loss: 5.1486\n",
      "Epoch [2/10], Step [58125/68337], Loss: 5.1466\n",
      "Epoch [2/10], Step [58200/68337], Loss: 5.1170\n",
      "Epoch [2/10], Step [58275/68337], Loss: 5.1271\n",
      "Epoch [2/10], Step [58350/68337], Loss: 5.1001\n",
      "Epoch [2/10], Step [58425/68337], Loss: 5.3005\n",
      "Epoch [2/10], Step [58500/68337], Loss: 5.3184\n",
      "Epoch [2/10], Step [58575/68337], Loss: 5.0202\n",
      "Epoch [2/10], Step [58650/68337], Loss: 5.3834\n",
      "Epoch [2/10], Step [58725/68337], Loss: 5.2023\n",
      "Epoch [2/10], Step [58800/68337], Loss: 5.2189\n",
      "Epoch [2/10], Step [58875/68337], Loss: 5.1359\n",
      "Epoch [2/10], Step [58950/68337], Loss: 5.5779\n",
      "Epoch [2/10], Step [59025/68337], Loss: 5.3813\n",
      "Epoch [2/10], Step [59100/68337], Loss: 5.1878\n",
      "Epoch [2/10], Step [59175/68337], Loss: 5.3033\n",
      "Epoch [2/10], Step [59250/68337], Loss: 5.1554\n",
      "Epoch [2/10], Step [59325/68337], Loss: 5.1187\n",
      "Epoch [2/10], Step [59400/68337], Loss: 5.2252\n",
      "Epoch [2/10], Step [59475/68337], Loss: 5.1270\n",
      "Epoch [2/10], Step [59550/68337], Loss: 5.2974\n",
      "Epoch [2/10], Step [59625/68337], Loss: 5.1715\n",
      "Epoch [2/10], Step [59700/68337], Loss: 5.3552\n",
      "Epoch [2/10], Step [59775/68337], Loss: 5.2876\n",
      "Epoch [2/10], Step [59850/68337], Loss: 5.1887\n",
      "Epoch [2/10], Step [59925/68337], Loss: 5.1776\n",
      "Epoch [2/10], Step [60000/68337], Loss: 5.1998\n",
      "Validation perplexity: 145.20034672342317\n",
      "Epoch [2/10], Step [60075/68337], Loss: 5.2235\n",
      "Epoch [2/10], Step [60150/68337], Loss: 5.1326\n",
      "Epoch [2/10], Step [60225/68337], Loss: 5.3098\n",
      "Epoch [2/10], Step [60300/68337], Loss: 5.1950\n",
      "Epoch [2/10], Step [60375/68337], Loss: 5.2709\n",
      "Epoch [2/10], Step [60450/68337], Loss: 5.1219\n",
      "Epoch [2/10], Step [60525/68337], Loss: 4.7112\n",
      "Epoch [2/10], Step [60600/68337], Loss: 5.1544\n",
      "Epoch [2/10], Step [60675/68337], Loss: 5.1184\n",
      "Epoch [2/10], Step [60750/68337], Loss: 5.2482\n",
      "Epoch [2/10], Step [60825/68337], Loss: 5.2644\n",
      "Epoch [2/10], Step [60900/68337], Loss: 5.2161\n",
      "Epoch [2/10], Step [60975/68337], Loss: 5.1890\n",
      "Epoch [2/10], Step [61050/68337], Loss: 5.2987\n",
      "Epoch [2/10], Step [61125/68337], Loss: 5.3189\n",
      "Epoch [2/10], Step [61200/68337], Loss: 5.2070\n",
      "Epoch [2/10], Step [61275/68337], Loss: 5.1015\n",
      "Epoch [2/10], Step [61350/68337], Loss: 5.3192\n",
      "Epoch [2/10], Step [61425/68337], Loss: 5.3333\n",
      "Epoch [2/10], Step [61500/68337], Loss: 5.0646\n",
      "Epoch [2/10], Step [61575/68337], Loss: 5.3356\n",
      "Epoch [2/10], Step [61650/68337], Loss: 5.3097\n",
      "Epoch [2/10], Step [61725/68337], Loss: 5.2120\n",
      "Epoch [2/10], Step [61800/68337], Loss: 5.2313\n",
      "Epoch [2/10], Step [61875/68337], Loss: 5.2391\n",
      "Epoch [2/10], Step [61950/68337], Loss: 5.3714\n",
      "Epoch [2/10], Step [62025/68337], Loss: 5.3058\n",
      "Epoch [2/10], Step [62100/68337], Loss: 5.2016\n",
      "Epoch [2/10], Step [62175/68337], Loss: 5.2030\n",
      "Epoch [2/10], Step [62250/68337], Loss: 5.2840\n",
      "Epoch [2/10], Step [62325/68337], Loss: 5.0366\n",
      "Epoch [2/10], Step [62400/68337], Loss: 5.0264\n",
      "Epoch [2/10], Step [62475/68337], Loss: 5.4817\n",
      "Epoch [2/10], Step [62550/68337], Loss: 5.3833\n",
      "Epoch [2/10], Step [62625/68337], Loss: 5.2954\n",
      "Epoch [2/10], Step [62700/68337], Loss: 5.3774\n",
      "Epoch [2/10], Step [62775/68337], Loss: 5.1299\n",
      "Epoch [2/10], Step [62850/68337], Loss: 5.2750\n",
      "Epoch [2/10], Step [62925/68337], Loss: 5.1302\n",
      "Epoch [2/10], Step [63000/68337], Loss: 5.1006\n",
      "Epoch [2/10], Step [63075/68337], Loss: 5.2870\n",
      "Epoch [2/10], Step [63150/68337], Loss: 5.0907\n",
      "Epoch [2/10], Step [63225/68337], Loss: 5.3024\n",
      "Epoch [2/10], Step [63300/68337], Loss: 5.1787\n",
      "Epoch [2/10], Step [63375/68337], Loss: 5.3113\n",
      "Epoch [2/10], Step [63450/68337], Loss: 5.2240\n",
      "Epoch [2/10], Step [63525/68337], Loss: 4.9487\n",
      "Epoch [2/10], Step [63600/68337], Loss: 5.4224\n",
      "Epoch [2/10], Step [63675/68337], Loss: 5.2340\n",
      "Epoch [2/10], Step [63750/68337], Loss: 5.0025\n",
      "Epoch [2/10], Step [63825/68337], Loss: 5.3256\n",
      "Epoch [2/10], Step [63900/68337], Loss: 5.1217\n",
      "Epoch [2/10], Step [63975/68337], Loss: 5.2065\n",
      "Epoch [2/10], Step [64050/68337], Loss: 5.2562\n",
      "Epoch [2/10], Step [64125/68337], Loss: 5.2688\n",
      "Epoch [2/10], Step [64200/68337], Loss: 5.1721\n",
      "Epoch [2/10], Step [64275/68337], Loss: 5.1481\n",
      "Epoch [2/10], Step [64350/68337], Loss: 5.2146\n",
      "Epoch [2/10], Step [64425/68337], Loss: 5.3371\n",
      "Epoch [2/10], Step [64500/68337], Loss: 5.2132\n",
      "Epoch [2/10], Step [64575/68337], Loss: 5.2908\n",
      "Epoch [2/10], Step [64650/68337], Loss: 5.4854\n",
      "Epoch [2/10], Step [64725/68337], Loss: 5.2009\n",
      "Epoch [2/10], Step [64800/68337], Loss: 5.0223\n",
      "Epoch [2/10], Step [64875/68337], Loss: 5.0001\n",
      "Epoch [2/10], Step [64950/68337], Loss: 5.0477\n",
      "Epoch [2/10], Step [65025/68337], Loss: 5.2695\n",
      "Epoch [2/10], Step [65100/68337], Loss: 5.0861\n",
      "Epoch [2/10], Step [65175/68337], Loss: 5.3354\n",
      "Epoch [2/10], Step [65250/68337], Loss: 5.1856\n",
      "Epoch [2/10], Step [65325/68337], Loss: 5.2585\n",
      "Epoch [2/10], Step [65400/68337], Loss: 5.1386\n",
      "Epoch [2/10], Step [65475/68337], Loss: 5.3541\n",
      "Epoch [2/10], Step [65550/68337], Loss: 5.1589\n",
      "Epoch [2/10], Step [65625/68337], Loss: 5.0064\n",
      "Epoch [2/10], Step [65700/68337], Loss: 4.9422\n",
      "Epoch [2/10], Step [65775/68337], Loss: 5.0140\n",
      "Epoch [2/10], Step [65850/68337], Loss: 5.3339\n",
      "Epoch [2/10], Step [65925/68337], Loss: 5.4103\n",
      "Epoch [2/10], Step [66000/68337], Loss: 5.2998\n",
      "Epoch [2/10], Step [66075/68337], Loss: 5.2246\n",
      "Epoch [2/10], Step [66150/68337], Loss: 4.9030\n",
      "Epoch [2/10], Step [66225/68337], Loss: 5.2504\n",
      "Epoch [2/10], Step [66300/68337], Loss: 5.4026\n",
      "Epoch [2/10], Step [66375/68337], Loss: 5.2522\n",
      "Epoch [2/10], Step [66450/68337], Loss: 4.9785\n",
      "Epoch [2/10], Step [66525/68337], Loss: 5.2976\n",
      "Epoch [2/10], Step [66600/68337], Loss: 5.0457\n",
      "Epoch [2/10], Step [66675/68337], Loss: 5.2497\n",
      "Epoch [2/10], Step [66750/68337], Loss: 5.3026\n",
      "Epoch [2/10], Step [66825/68337], Loss: 5.4097\n",
      "Epoch [2/10], Step [66900/68337], Loss: 5.1390\n",
      "Epoch [2/10], Step [66975/68337], Loss: 5.0755\n",
      "Epoch [2/10], Step [67050/68337], Loss: 5.0071\n",
      "Epoch [2/10], Step [67125/68337], Loss: 5.0244\n",
      "Epoch [2/10], Step [67200/68337], Loss: 5.0910\n",
      "Epoch [2/10], Step [67275/68337], Loss: 5.0295\n",
      "Epoch [2/10], Step [67350/68337], Loss: 5.1452\n",
      "Epoch [2/10], Step [67425/68337], Loss: 5.2155\n",
      "Epoch [2/10], Step [67500/68337], Loss: 5.2280\n",
      "Epoch [2/10], Step [67575/68337], Loss: 5.2847\n",
      "Epoch [2/10], Step [67650/68337], Loss: 5.0590\n",
      "Epoch [2/10], Step [67725/68337], Loss: 5.1560\n",
      "Epoch [2/10], Step [67800/68337], Loss: 4.9081\n",
      "Epoch [2/10], Step [67875/68337], Loss: 5.2986\n",
      "Epoch [2/10], Step [67950/68337], Loss: 5.0639\n",
      "Epoch [2/10], Step [68025/68337], Loss: 5.1236\n",
      "Epoch [2/10], Step [68100/68337], Loss: 5.3376\n",
      "Epoch [2/10], Step [68175/68337], Loss: 5.3523\n",
      "Epoch [2/10], Step [68250/68337], Loss: 5.2804\n",
      "Epoch [2/10], Step [68325/68337], Loss: 5.1274\n",
      "Epoch [2/10] Average Loss: 5.2521, Perplexity: 190.97\n",
      "Epoch [3/10], Step [0/68337], Loss: 5.2624\n",
      "Validation perplexity: 144.5430286686174\n",
      "Epoch [3/10], Step [75/68337], Loss: 5.0288\n",
      "Epoch [3/10], Step [150/68337], Loss: 5.2598\n",
      "Epoch [3/10], Step [225/68337], Loss: 5.2774\n",
      "Epoch [3/10], Step [300/68337], Loss: 5.3030\n",
      "Epoch [3/10], Step [375/68337], Loss: 5.1156\n",
      "Epoch [3/10], Step [450/68337], Loss: 5.0263\n",
      "Epoch [3/10], Step [525/68337], Loss: 5.2921\n",
      "Epoch [3/10], Step [600/68337], Loss: 5.0889\n",
      "Epoch [3/10], Step [675/68337], Loss: 5.1949\n",
      "Epoch [3/10], Step [750/68337], Loss: 5.2597\n",
      "Epoch [3/10], Step [825/68337], Loss: 5.4278\n",
      "Epoch [3/10], Step [900/68337], Loss: 5.1512\n",
      "Epoch [3/10], Step [975/68337], Loss: 4.9075\n",
      "Epoch [3/10], Step [1050/68337], Loss: 5.1237\n",
      "Epoch [3/10], Step [1125/68337], Loss: 5.1772\n",
      "Epoch [3/10], Step [1200/68337], Loss: 5.1599\n",
      "Epoch [3/10], Step [1275/68337], Loss: 5.2942\n",
      "Epoch [3/10], Step [1350/68337], Loss: 5.0939\n",
      "Epoch [3/10], Step [1425/68337], Loss: 5.3062\n",
      "Epoch [3/10], Step [1500/68337], Loss: 5.3951\n",
      "Epoch [3/10], Step [1575/68337], Loss: 5.4113\n",
      "Epoch [3/10], Step [1650/68337], Loss: 5.0530\n",
      "Epoch [3/10], Step [1725/68337], Loss: 5.4358\n",
      "Epoch [3/10], Step [1800/68337], Loss: 5.3204\n",
      "Epoch [3/10], Step [1875/68337], Loss: 5.1098\n",
      "Epoch [3/10], Step [1950/68337], Loss: 5.1092\n",
      "Epoch [3/10], Step [2025/68337], Loss: 5.1274\n",
      "Epoch [3/10], Step [2100/68337], Loss: 5.2402\n",
      "Epoch [3/10], Step [2175/68337], Loss: 5.3073\n",
      "Epoch [3/10], Step [2250/68337], Loss: 5.2947\n",
      "Epoch [3/10], Step [2325/68337], Loss: 5.1296\n",
      "Epoch [3/10], Step [2400/68337], Loss: 5.1701\n",
      "Epoch [3/10], Step [2475/68337], Loss: 5.4641\n",
      "Epoch [3/10], Step [2550/68337], Loss: 5.1516\n",
      "Epoch [3/10], Step [2625/68337], Loss: 5.0585\n",
      "Epoch [3/10], Step [2700/68337], Loss: 5.5204\n",
      "Epoch [3/10], Step [2775/68337], Loss: 5.3279\n",
      "Epoch [3/10], Step [2850/68337], Loss: 5.2309\n",
      "Epoch [3/10], Step [2925/68337], Loss: 5.2065\n",
      "Epoch [3/10], Step [3000/68337], Loss: 5.1649\n",
      "Epoch [3/10], Step [3075/68337], Loss: 5.2807\n",
      "Epoch [3/10], Step [3150/68337], Loss: 4.9548\n",
      "Epoch [3/10], Step [3225/68337], Loss: 5.0350\n",
      "Epoch [3/10], Step [3300/68337], Loss: 4.9964\n",
      "Epoch [3/10], Step [3375/68337], Loss: 5.2256\n",
      "Epoch [3/10], Step [3450/68337], Loss: 5.2736\n",
      "Epoch [3/10], Step [3525/68337], Loss: 5.1301\n",
      "Epoch [3/10], Step [3600/68337], Loss: 5.1890\n",
      "Epoch [3/10], Step [3675/68337], Loss: 5.3961\n",
      "Epoch [3/10], Step [3750/68337], Loss: 4.9394\n",
      "Epoch [3/10], Step [3825/68337], Loss: 5.3582\n",
      "Epoch [3/10], Step [3900/68337], Loss: 5.1826\n",
      "Epoch [3/10], Step [3975/68337], Loss: 5.1014\n",
      "Epoch [3/10], Step [4050/68337], Loss: 5.0090\n",
      "Epoch [3/10], Step [4125/68337], Loss: 5.3021\n",
      "Epoch [3/10], Step [4200/68337], Loss: 5.2738\n",
      "Epoch [3/10], Step [4275/68337], Loss: 5.2554\n",
      "Epoch [3/10], Step [4350/68337], Loss: 5.1955\n",
      "Epoch [3/10], Step [4425/68337], Loss: 5.1980\n",
      "Epoch [3/10], Step [4500/68337], Loss: 5.5014\n",
      "Epoch [3/10], Step [4575/68337], Loss: 5.2478\n",
      "Epoch [3/10], Step [4650/68337], Loss: 5.2357\n",
      "Epoch [3/10], Step [4725/68337], Loss: 5.1542\n",
      "Epoch [3/10], Step [4800/68337], Loss: 5.1155\n",
      "Epoch [3/10], Step [4875/68337], Loss: 5.0515\n",
      "Epoch [3/10], Step [4950/68337], Loss: 5.5244\n",
      "Epoch [3/10], Step [5025/68337], Loss: 5.1564\n",
      "Epoch [3/10], Step [5100/68337], Loss: 5.3001\n",
      "Epoch [3/10], Step [5175/68337], Loss: 5.2219\n",
      "Epoch [3/10], Step [5250/68337], Loss: 5.0904\n",
      "Epoch [3/10], Step [5325/68337], Loss: 5.1949\n",
      "Epoch [3/10], Step [5400/68337], Loss: 5.1624\n",
      "Epoch [3/10], Step [5475/68337], Loss: 4.9470\n",
      "Epoch [3/10], Step [5550/68337], Loss: 5.3410\n",
      "Epoch [3/10], Step [5625/68337], Loss: 5.0746\n",
      "Epoch [3/10], Step [5700/68337], Loss: 5.0738\n",
      "Epoch [3/10], Step [5775/68337], Loss: 5.1741\n",
      "Epoch [3/10], Step [5850/68337], Loss: 5.2948\n",
      "Epoch [3/10], Step [5925/68337], Loss: 5.2358\n",
      "Epoch [3/10], Step [6000/68337], Loss: 5.3043\n",
      "Epoch [3/10], Step [6075/68337], Loss: 5.2882\n",
      "Epoch [3/10], Step [6150/68337], Loss: 5.2203\n",
      "Epoch [3/10], Step [6225/68337], Loss: 5.2613\n",
      "Epoch [3/10], Step [6300/68337], Loss: 5.1840\n",
      "Epoch [3/10], Step [6375/68337], Loss: 5.3625\n",
      "Epoch [3/10], Step [6450/68337], Loss: 5.0967\n",
      "Epoch [3/10], Step [6525/68337], Loss: 5.2308\n",
      "Epoch [3/10], Step [6600/68337], Loss: 5.3342\n",
      "Epoch [3/10], Step [6675/68337], Loss: 5.2455\n",
      "Epoch [3/10], Step [6750/68337], Loss: 5.1737\n",
      "Epoch [3/10], Step [6825/68337], Loss: 5.1870\n",
      "Epoch [3/10], Step [6900/68337], Loss: 5.1490\n",
      "Epoch [3/10], Step [6975/68337], Loss: 5.0483\n",
      "Epoch [3/10], Step [7050/68337], Loss: 5.0453\n",
      "Epoch [3/10], Step [7125/68337], Loss: 5.1981\n",
      "Epoch [3/10], Step [7200/68337], Loss: 5.1722\n",
      "Epoch [3/10], Step [7275/68337], Loss: 5.1343\n",
      "Epoch [3/10], Step [7350/68337], Loss: 5.0494\n",
      "Epoch [3/10], Step [7425/68337], Loss: 4.9578\n",
      "Epoch [3/10], Step [7500/68337], Loss: 5.1901\n",
      "Epoch [3/10], Step [7575/68337], Loss: 5.2600\n",
      "Epoch [3/10], Step [7650/68337], Loss: 4.9824\n",
      "Epoch [3/10], Step [7725/68337], Loss: 5.1219\n",
      "Epoch [3/10], Step [7800/68337], Loss: 5.2000\n",
      "Epoch [3/10], Step [7875/68337], Loss: 5.2811\n",
      "Epoch [3/10], Step [7950/68337], Loss: 5.1330\n",
      "Epoch [3/10], Step [8025/68337], Loss: 4.9941\n",
      "Epoch [3/10], Step [8100/68337], Loss: 5.3674\n",
      "Epoch [3/10], Step [8175/68337], Loss: 5.2439\n",
      "Epoch [3/10], Step [8250/68337], Loss: 5.3621\n",
      "Epoch [3/10], Step [8325/68337], Loss: 5.1716\n",
      "Epoch [3/10], Step [8400/68337], Loss: 5.3173\n",
      "Epoch [3/10], Step [8475/68337], Loss: 5.4045\n",
      "Epoch [3/10], Step [8550/68337], Loss: 5.3144\n",
      "Epoch [3/10], Step [8625/68337], Loss: 5.2441\n",
      "Epoch [3/10], Step [8700/68337], Loss: 5.2451\n",
      "Epoch [3/10], Step [8775/68337], Loss: 5.3317\n",
      "Epoch [3/10], Step [8850/68337], Loss: 5.2571\n",
      "Epoch [3/10], Step [8925/68337], Loss: 5.2261\n",
      "Epoch [3/10], Step [9000/68337], Loss: 5.2702\n",
      "Epoch [3/10], Step [9075/68337], Loss: 5.3729\n",
      "Epoch [3/10], Step [9150/68337], Loss: 5.0347\n",
      "Epoch [3/10], Step [9225/68337], Loss: 5.2161\n",
      "Epoch [3/10], Step [9300/68337], Loss: 5.1163\n",
      "Epoch [3/10], Step [9375/68337], Loss: 5.1368\n",
      "Epoch [3/10], Step [9450/68337], Loss: 5.0198\n",
      "Epoch [3/10], Step [9525/68337], Loss: 5.2286\n",
      "Epoch [3/10], Step [9600/68337], Loss: 5.3779\n",
      "Epoch [3/10], Step [9675/68337], Loss: 5.2523\n",
      "Epoch [3/10], Step [9750/68337], Loss: 5.2136\n",
      "Epoch [3/10], Step [9825/68337], Loss: 5.1962\n",
      "Epoch [3/10], Step [9900/68337], Loss: 5.2298\n",
      "Epoch [3/10], Step [9975/68337], Loss: 5.2479\n",
      "Validation perplexity: 142.49398898950886\n",
      "Epoch [3/10], Step [10050/68337], Loss: 5.0732\n",
      "Epoch [3/10], Step [10125/68337], Loss: 5.1916\n",
      "Epoch [3/10], Step [10200/68337], Loss: 5.1573\n",
      "Epoch [3/10], Step [10275/68337], Loss: 5.0560\n",
      "Epoch [3/10], Step [10350/68337], Loss: 5.2138\n",
      "Epoch [3/10], Step [10425/68337], Loss: 5.1274\n",
      "Epoch [3/10], Step [10500/68337], Loss: 5.1449\n",
      "Epoch [3/10], Step [10575/68337], Loss: 5.2314\n",
      "Epoch [3/10], Step [10650/68337], Loss: 5.2288\n",
      "Epoch [3/10], Step [10725/68337], Loss: 5.2599\n",
      "Epoch [3/10], Step [10800/68337], Loss: 5.3161\n",
      "Epoch [3/10], Step [10875/68337], Loss: 5.2328\n",
      "Epoch [3/10], Step [10950/68337], Loss: 5.1001\n",
      "Epoch [3/10], Step [11025/68337], Loss: 5.2532\n",
      "Epoch [3/10], Step [11100/68337], Loss: 5.3329\n",
      "Epoch [3/10], Step [11175/68337], Loss: 4.9411\n",
      "Epoch [3/10], Step [11250/68337], Loss: 5.4313\n",
      "Epoch [3/10], Step [11325/68337], Loss: 4.9220\n",
      "Epoch [3/10], Step [11400/68337], Loss: 5.2386\n",
      "Epoch [3/10], Step [11475/68337], Loss: 5.0135\n",
      "Epoch [3/10], Step [11550/68337], Loss: 4.9226\n",
      "Epoch [3/10], Step [11625/68337], Loss: 5.2930\n",
      "Epoch [3/10], Step [11700/68337], Loss: 5.1230\n",
      "Epoch [3/10], Step [11775/68337], Loss: 5.1807\n",
      "Epoch [3/10], Step [11850/68337], Loss: 5.2823\n",
      "Epoch [3/10], Step [11925/68337], Loss: 5.2568\n",
      "Epoch [3/10], Step [12000/68337], Loss: 5.0680\n",
      "Epoch [3/10], Step [12075/68337], Loss: 5.1448\n",
      "Epoch [3/10], Step [12150/68337], Loss: 5.3814\n",
      "Epoch [3/10], Step [12225/68337], Loss: 5.2637\n",
      "Epoch [3/10], Step [12300/68337], Loss: 4.9927\n",
      "Epoch [3/10], Step [12375/68337], Loss: 5.3010\n",
      "Epoch [3/10], Step [12450/68337], Loss: 5.0772\n",
      "Epoch [3/10], Step [12525/68337], Loss: 5.0931\n",
      "Epoch [3/10], Step [12600/68337], Loss: 5.2687\n",
      "Epoch [3/10], Step [12675/68337], Loss: 5.0854\n",
      "Epoch [3/10], Step [12750/68337], Loss: 5.1989\n",
      "Epoch [3/10], Step [12825/68337], Loss: 5.0719\n",
      "Epoch [3/10], Step [12900/68337], Loss: 5.3477\n",
      "Epoch [3/10], Step [12975/68337], Loss: 5.2473\n",
      "Epoch [3/10], Step [13050/68337], Loss: 5.1236\n",
      "Epoch [3/10], Step [13125/68337], Loss: 5.1248\n",
      "Epoch [3/10], Step [13200/68337], Loss: 5.1154\n",
      "Epoch [3/10], Step [13275/68337], Loss: 5.1359\n",
      "Epoch [3/10], Step [13350/68337], Loss: 5.1070\n",
      "Epoch [3/10], Step [13425/68337], Loss: 5.1130\n",
      "Epoch [3/10], Step [13500/68337], Loss: 5.1233\n",
      "Epoch [3/10], Step [13575/68337], Loss: 5.4359\n",
      "Epoch [3/10], Step [13650/68337], Loss: 5.0958\n",
      "Epoch [3/10], Step [13725/68337], Loss: 5.1342\n",
      "Epoch [3/10], Step [13800/68337], Loss: 5.2667\n",
      "Epoch [3/10], Step [13875/68337], Loss: 5.0175\n",
      "Epoch [3/10], Step [13950/68337], Loss: 5.3066\n",
      "Epoch [3/10], Step [14025/68337], Loss: 5.1684\n",
      "Epoch [3/10], Step [14100/68337], Loss: 5.2790\n",
      "Epoch [3/10], Step [14175/68337], Loss: 5.2305\n",
      "Epoch [3/10], Step [14250/68337], Loss: 5.2153\n",
      "Epoch [3/10], Step [14325/68337], Loss: 5.0726\n",
      "Epoch [3/10], Step [14400/68337], Loss: 5.2277\n",
      "Epoch [3/10], Step [14475/68337], Loss: 5.3221\n",
      "Epoch [3/10], Step [14550/68337], Loss: 5.0657\n",
      "Epoch [3/10], Step [14625/68337], Loss: 5.3084\n",
      "Epoch [3/10], Step [14700/68337], Loss: 5.1286\n",
      "Epoch [3/10], Step [14775/68337], Loss: 5.2143\n",
      "Epoch [3/10], Step [14850/68337], Loss: 5.3641\n",
      "Epoch [3/10], Step [14925/68337], Loss: 5.2536\n",
      "Epoch [3/10], Step [15000/68337], Loss: 5.0678\n",
      "Epoch [3/10], Step [15075/68337], Loss: 5.3087\n",
      "Epoch [3/10], Step [15150/68337], Loss: 5.1046\n",
      "Epoch [3/10], Step [15225/68337], Loss: 5.2224\n",
      "Epoch [3/10], Step [15300/68337], Loss: 5.1941\n",
      "Epoch [3/10], Step [15375/68337], Loss: 5.4140\n",
      "Epoch [3/10], Step [15450/68337], Loss: 5.2829\n",
      "Epoch [3/10], Step [15525/68337], Loss: 5.1904\n",
      "Epoch [3/10], Step [15600/68337], Loss: 5.1955\n",
      "Epoch [3/10], Step [15675/68337], Loss: 5.2003\n",
      "Epoch [3/10], Step [15750/68337], Loss: 5.0886\n",
      "Epoch [3/10], Step [15825/68337], Loss: 5.3887\n",
      "Epoch [3/10], Step [15900/68337], Loss: 5.0119\n",
      "Epoch [3/10], Step [15975/68337], Loss: 5.2884\n",
      "Epoch [3/10], Step [16050/68337], Loss: 5.1354\n",
      "Epoch [3/10], Step [16125/68337], Loss: 5.2911\n",
      "Epoch [3/10], Step [16200/68337], Loss: 5.1949\n",
      "Epoch [3/10], Step [16275/68337], Loss: 5.1049\n",
      "Epoch [3/10], Step [16350/68337], Loss: 5.1374\n",
      "Epoch [3/10], Step [16425/68337], Loss: 4.9720\n",
      "Epoch [3/10], Step [16500/68337], Loss: 5.1894\n",
      "Epoch [3/10], Step [16575/68337], Loss: 5.2555\n",
      "Epoch [3/10], Step [16650/68337], Loss: 5.4028\n",
      "Epoch [3/10], Step [16725/68337], Loss: 5.2764\n",
      "Epoch [3/10], Step [16800/68337], Loss: 5.2411\n",
      "Epoch [3/10], Step [16875/68337], Loss: 5.2713\n",
      "Epoch [3/10], Step [16950/68337], Loss: 5.1781\n",
      "Epoch [3/10], Step [17025/68337], Loss: 5.0671\n",
      "Epoch [3/10], Step [17100/68337], Loss: 5.0865\n",
      "Epoch [3/10], Step [17175/68337], Loss: 5.1260\n",
      "Epoch [3/10], Step [17250/68337], Loss: 5.3121\n",
      "Epoch [3/10], Step [17325/68337], Loss: 5.2107\n",
      "Epoch [3/10], Step [17400/68337], Loss: 5.0516\n",
      "Epoch [3/10], Step [17475/68337], Loss: 5.3723\n",
      "Epoch [3/10], Step [17550/68337], Loss: 5.1633\n",
      "Epoch [3/10], Step [17625/68337], Loss: 5.1095\n",
      "Epoch [3/10], Step [17700/68337], Loss: 5.2057\n",
      "Epoch [3/10], Step [17775/68337], Loss: 5.1874\n",
      "Epoch [3/10], Step [17850/68337], Loss: 5.1100\n",
      "Epoch [3/10], Step [17925/68337], Loss: 5.2015\n",
      "Epoch [3/10], Step [18000/68337], Loss: 5.2391\n",
      "Epoch [3/10], Step [18075/68337], Loss: 5.2084\n",
      "Epoch [3/10], Step [18150/68337], Loss: 5.1305\n",
      "Epoch [3/10], Step [18225/68337], Loss: 5.0198\n",
      "Epoch [3/10], Step [18300/68337], Loss: 5.2838\n",
      "Epoch [3/10], Step [18375/68337], Loss: 5.1194\n",
      "Epoch [3/10], Step [18450/68337], Loss: 5.1344\n",
      "Epoch [3/10], Step [18525/68337], Loss: 5.2648\n",
      "Epoch [3/10], Step [18600/68337], Loss: 5.0784\n",
      "Epoch [3/10], Step [18675/68337], Loss: 5.2957\n",
      "Epoch [3/10], Step [18750/68337], Loss: 5.1801\n",
      "Epoch [3/10], Step [18825/68337], Loss: 5.0451\n",
      "Epoch [3/10], Step [18900/68337], Loss: 5.0935\n",
      "Epoch [3/10], Step [18975/68337], Loss: 5.1642\n",
      "Epoch [3/10], Step [19050/68337], Loss: 5.4243\n",
      "Epoch [3/10], Step [19125/68337], Loss: 5.2565\n",
      "Epoch [3/10], Step [19200/68337], Loss: 5.0538\n",
      "Epoch [3/10], Step [19275/68337], Loss: 5.2979\n",
      "Epoch [3/10], Step [19350/68337], Loss: 5.3092\n",
      "Epoch [3/10], Step [19425/68337], Loss: 5.1421\n",
      "Epoch [3/10], Step [19500/68337], Loss: 4.9705\n",
      "Epoch [3/10], Step [19575/68337], Loss: 5.0972\n",
      "Epoch [3/10], Step [19650/68337], Loss: 5.2523\n",
      "Epoch [3/10], Step [19725/68337], Loss: 5.3623\n",
      "Epoch [3/10], Step [19800/68337], Loss: 5.1548\n",
      "Epoch [3/10], Step [19875/68337], Loss: 5.2986\n",
      "Epoch [3/10], Step [19950/68337], Loss: 5.1611\n",
      "Validation perplexity: 141.07332578964073\n",
      "Epoch [3/10], Step [20025/68337], Loss: 5.2052\n",
      "Epoch [3/10], Step [20100/68337], Loss: 4.9379\n",
      "Epoch [3/10], Step [20175/68337], Loss: 5.1735\n",
      "Epoch [3/10], Step [20250/68337], Loss: 5.1817\n",
      "Epoch [3/10], Step [20325/68337], Loss: 5.0702\n",
      "Epoch [3/10], Step [20400/68337], Loss: 5.2460\n",
      "Epoch [3/10], Step [20475/68337], Loss: 5.0877\n",
      "Epoch [3/10], Step [20550/68337], Loss: 5.1010\n",
      "Epoch [3/10], Step [20625/68337], Loss: 5.3554\n",
      "Epoch [3/10], Step [20700/68337], Loss: 5.2477\n",
      "Epoch [3/10], Step [20775/68337], Loss: 5.1688\n",
      "Epoch [3/10], Step [20850/68337], Loss: 5.1493\n",
      "Epoch [3/10], Step [20925/68337], Loss: 5.3196\n",
      "Epoch [3/10], Step [21000/68337], Loss: 5.0269\n",
      "Epoch [3/10], Step [21075/68337], Loss: 5.1467\n",
      "Epoch [3/10], Step [21150/68337], Loss: 5.1080\n",
      "Epoch [3/10], Step [21225/68337], Loss: 5.4074\n",
      "Epoch [3/10], Step [21300/68337], Loss: 5.0827\n",
      "Epoch [3/10], Step [21375/68337], Loss: 5.3071\n",
      "Epoch [3/10], Step [21450/68337], Loss: 5.0737\n",
      "Epoch [3/10], Step [21525/68337], Loss: 5.3468\n",
      "Epoch [3/10], Step [21600/68337], Loss: 5.0049\n",
      "Epoch [3/10], Step [21675/68337], Loss: 5.3351\n",
      "Epoch [3/10], Step [21750/68337], Loss: 5.2891\n",
      "Epoch [3/10], Step [21825/68337], Loss: 5.2531\n",
      "Epoch [3/10], Step [21900/68337], Loss: 5.2694\n",
      "Epoch [3/10], Step [21975/68337], Loss: 5.4593\n",
      "Epoch [3/10], Step [22050/68337], Loss: 4.9910\n",
      "Epoch [3/10], Step [22125/68337], Loss: 4.9913\n",
      "Epoch [3/10], Step [22200/68337], Loss: 5.0886\n",
      "Epoch [3/10], Step [22275/68337], Loss: 5.1207\n",
      "Epoch [3/10], Step [22350/68337], Loss: 5.2074\n",
      "Epoch [3/10], Step [22425/68337], Loss: 5.1343\n",
      "Epoch [3/10], Step [22500/68337], Loss: 5.1799\n",
      "Epoch [3/10], Step [22575/68337], Loss: 5.0546\n",
      "Epoch [3/10], Step [22650/68337], Loss: 5.3857\n",
      "Epoch [3/10], Step [22725/68337], Loss: 5.3182\n",
      "Epoch [3/10], Step [22800/68337], Loss: 5.0853\n",
      "Epoch [3/10], Step [22875/68337], Loss: 5.2174\n",
      "Epoch [3/10], Step [22950/68337], Loss: 5.2208\n",
      "Epoch [3/10], Step [23025/68337], Loss: 5.0739\n",
      "Epoch [3/10], Step [23100/68337], Loss: 4.9887\n",
      "Epoch [3/10], Step [23175/68337], Loss: 5.1083\n",
      "Epoch [3/10], Step [23250/68337], Loss: 5.0580\n",
      "Epoch [3/10], Step [23325/68337], Loss: 5.1426\n",
      "Epoch [3/10], Step [23400/68337], Loss: 5.2643\n",
      "Epoch [3/10], Step [23475/68337], Loss: 5.2077\n",
      "Epoch [3/10], Step [23550/68337], Loss: 5.1762\n",
      "Epoch [3/10], Step [23625/68337], Loss: 5.1445\n",
      "Epoch [3/10], Step [23700/68337], Loss: 4.9536\n",
      "Epoch [3/10], Step [23775/68337], Loss: 4.9789\n",
      "Epoch [3/10], Step [23850/68337], Loss: 5.3600\n",
      "Epoch [3/10], Step [23925/68337], Loss: 5.1682\n",
      "Epoch [3/10], Step [24000/68337], Loss: 5.3015\n",
      "Epoch [3/10], Step [24075/68337], Loss: 5.1187\n",
      "Epoch [3/10], Step [24150/68337], Loss: 5.0983\n",
      "Epoch [3/10], Step [24225/68337], Loss: 5.1434\n",
      "Epoch [3/10], Step [24300/68337], Loss: 5.1819\n",
      "Epoch [3/10], Step [24375/68337], Loss: 4.9209\n",
      "Epoch [3/10], Step [24450/68337], Loss: 5.4153\n",
      "Epoch [3/10], Step [24525/68337], Loss: 5.0948\n",
      "Epoch [3/10], Step [24600/68337], Loss: 5.0827\n",
      "Epoch [3/10], Step [24675/68337], Loss: 5.0881\n",
      "Epoch [3/10], Step [24750/68337], Loss: 5.1106\n",
      "Epoch [3/10], Step [24825/68337], Loss: 5.2249\n",
      "Epoch [3/10], Step [24900/68337], Loss: 5.2028\n",
      "Epoch [3/10], Step [24975/68337], Loss: 5.1062\n",
      "Epoch [3/10], Step [25050/68337], Loss: 5.0902\n",
      "Epoch [3/10], Step [25125/68337], Loss: 5.3590\n",
      "Epoch [3/10], Step [25200/68337], Loss: 5.2002\n",
      "Epoch [3/10], Step [25275/68337], Loss: 5.2657\n",
      "Epoch [3/10], Step [25350/68337], Loss: 5.3434\n",
      "Epoch [3/10], Step [25425/68337], Loss: 5.2721\n",
      "Epoch [3/10], Step [25500/68337], Loss: 5.2606\n",
      "Epoch [3/10], Step [25575/68337], Loss: 5.3099\n",
      "Epoch [3/10], Step [25650/68337], Loss: 5.0104\n",
      "Epoch [3/10], Step [25725/68337], Loss: 5.1732\n",
      "Epoch [3/10], Step [25800/68337], Loss: 5.3366\n",
      "Epoch [3/10], Step [25875/68337], Loss: 4.9943\n",
      "Epoch [3/10], Step [25950/68337], Loss: 5.0086\n",
      "Epoch [3/10], Step [26025/68337], Loss: 5.3415\n",
      "Epoch [3/10], Step [26100/68337], Loss: 5.1899\n",
      "Epoch [3/10], Step [26175/68337], Loss: 5.1697\n",
      "Epoch [3/10], Step [26250/68337], Loss: 5.1358\n",
      "Epoch [3/10], Step [26325/68337], Loss: 5.0819\n",
      "Epoch [3/10], Step [26400/68337], Loss: 5.2022\n",
      "Epoch [3/10], Step [26475/68337], Loss: 5.1586\n",
      "Epoch [3/10], Step [26550/68337], Loss: 5.1821\n",
      "Epoch [3/10], Step [26625/68337], Loss: 5.1581\n",
      "Epoch [3/10], Step [26700/68337], Loss: 5.0875\n",
      "Epoch [3/10], Step [26775/68337], Loss: 5.2451\n",
      "Epoch [3/10], Step [26850/68337], Loss: 5.1433\n",
      "Epoch [3/10], Step [26925/68337], Loss: 5.1265\n",
      "Epoch [3/10], Step [27000/68337], Loss: 5.0325\n",
      "Epoch [3/10], Step [27075/68337], Loss: 5.1928\n",
      "Epoch [3/10], Step [27150/68337], Loss: 5.1742\n",
      "Epoch [3/10], Step [27225/68337], Loss: 5.2504\n",
      "Epoch [3/10], Step [27300/68337], Loss: 5.2733\n",
      "Epoch [3/10], Step [27375/68337], Loss: 4.9384\n",
      "Epoch [3/10], Step [27450/68337], Loss: 5.2718\n",
      "Epoch [3/10], Step [27525/68337], Loss: 5.1676\n",
      "Epoch [3/10], Step [27600/68337], Loss: 5.3018\n",
      "Epoch [3/10], Step [27675/68337], Loss: 5.1798\n",
      "Epoch [3/10], Step [27750/68337], Loss: 5.3043\n",
      "Epoch [3/10], Step [27825/68337], Loss: 4.9908\n",
      "Epoch [3/10], Step [27900/68337], Loss: 5.2635\n",
      "Epoch [3/10], Step [27975/68337], Loss: 5.0257\n",
      "Epoch [3/10], Step [28050/68337], Loss: 5.2846\n",
      "Epoch [3/10], Step [28125/68337], Loss: 5.1360\n",
      "Epoch [3/10], Step [28200/68337], Loss: 5.4623\n",
      "Epoch [3/10], Step [28275/68337], Loss: 5.0593\n",
      "Epoch [3/10], Step [28350/68337], Loss: 5.1135\n",
      "Epoch [3/10], Step [28425/68337], Loss: 5.3079\n",
      "Epoch [3/10], Step [28500/68337], Loss: 5.2119\n",
      "Epoch [3/10], Step [28575/68337], Loss: 5.2353\n",
      "Epoch [3/10], Step [28650/68337], Loss: 5.2144\n",
      "Epoch [3/10], Step [28725/68337], Loss: 5.2961\n",
      "Epoch [3/10], Step [28800/68337], Loss: 5.2035\n",
      "Epoch [3/10], Step [28875/68337], Loss: 5.0374\n",
      "Epoch [3/10], Step [28950/68337], Loss: 5.2037\n",
      "Epoch [3/10], Step [29025/68337], Loss: 5.3101\n",
      "Epoch [3/10], Step [29100/68337], Loss: 5.4704\n",
      "Epoch [3/10], Step [29175/68337], Loss: 5.2437\n",
      "Epoch [3/10], Step [29250/68337], Loss: 5.3408\n",
      "Epoch [3/10], Step [29325/68337], Loss: 5.1357\n",
      "Epoch [3/10], Step [29400/68337], Loss: 5.1602\n",
      "Epoch [3/10], Step [29475/68337], Loss: 5.3511\n",
      "Epoch [3/10], Step [29550/68337], Loss: 5.1570\n",
      "Epoch [3/10], Step [29625/68337], Loss: 5.4541\n",
      "Epoch [3/10], Step [29700/68337], Loss: 5.1033\n",
      "Epoch [3/10], Step [29775/68337], Loss: 5.2563\n",
      "Epoch [3/10], Step [29850/68337], Loss: 5.3241\n",
      "Epoch [3/10], Step [29925/68337], Loss: 5.1530\n",
      "Epoch [3/10], Step [30000/68337], Loss: 5.2575\n",
      "Validation perplexity: 139.81880557694686\n",
      "Epoch [3/10], Step [30075/68337], Loss: 5.2852\n",
      "Epoch [3/10], Step [30150/68337], Loss: 5.2700\n",
      "Epoch [3/10], Step [30225/68337], Loss: 5.2213\n",
      "Epoch [3/10], Step [30300/68337], Loss: 5.0853\n",
      "Epoch [3/10], Step [30375/68337], Loss: 5.2410\n",
      "Epoch [3/10], Step [30450/68337], Loss: 5.2737\n",
      "Epoch [3/10], Step [30525/68337], Loss: 5.1433\n",
      "Epoch [3/10], Step [30600/68337], Loss: 5.1363\n",
      "Epoch [3/10], Step [30675/68337], Loss: 5.1859\n",
      "Epoch [3/10], Step [30750/68337], Loss: 5.1708\n",
      "Epoch [3/10], Step [30825/68337], Loss: 5.1861\n",
      "Epoch [3/10], Step [30900/68337], Loss: 5.2987\n",
      "Epoch [3/10], Step [30975/68337], Loss: 5.1385\n",
      "Epoch [3/10], Step [31050/68337], Loss: 5.4139\n",
      "Epoch [3/10], Step [31125/68337], Loss: 5.2373\n",
      "Epoch [3/10], Step [31200/68337], Loss: 5.1571\n",
      "Epoch [3/10], Step [31275/68337], Loss: 5.0739\n",
      "Epoch [3/10], Step [31350/68337], Loss: 5.0398\n",
      "Epoch [3/10], Step [31425/68337], Loss: 5.0201\n",
      "Epoch [3/10], Step [31500/68337], Loss: 5.1439\n",
      "Epoch [3/10], Step [31575/68337], Loss: 5.2570\n",
      "Epoch [3/10], Step [31650/68337], Loss: 5.2525\n",
      "Epoch [3/10], Step [31725/68337], Loss: 5.2794\n",
      "Epoch [3/10], Step [31800/68337], Loss: 5.0154\n",
      "Epoch [3/10], Step [31875/68337], Loss: 5.1587\n",
      "Epoch [3/10], Step [31950/68337], Loss: 5.2875\n",
      "Epoch [3/10], Step [32025/68337], Loss: 5.1716\n",
      "Epoch [3/10], Step [32100/68337], Loss: 5.1064\n",
      "Epoch [3/10], Step [32175/68337], Loss: 5.0145\n",
      "Epoch [3/10], Step [32250/68337], Loss: 5.1175\n",
      "Epoch [3/10], Step [32325/68337], Loss: 5.1564\n",
      "Epoch [3/10], Step [32400/68337], Loss: 5.1692\n",
      "Epoch [3/10], Step [32475/68337], Loss: 4.9413\n",
      "Epoch [3/10], Step [32550/68337], Loss: 5.1143\n",
      "Epoch [3/10], Step [32625/68337], Loss: 5.2050\n",
      "Epoch [3/10], Step [32700/68337], Loss: 5.1751\n",
      "Epoch [3/10], Step [32775/68337], Loss: 5.1680\n",
      "Epoch [3/10], Step [32850/68337], Loss: 5.2064\n",
      "Epoch [3/10], Step [32925/68337], Loss: 5.1675\n",
      "Epoch [3/10], Step [33000/68337], Loss: 5.1868\n",
      "Epoch [3/10], Step [33075/68337], Loss: 5.2239\n",
      "Epoch [3/10], Step [33150/68337], Loss: 5.3862\n",
      "Epoch [3/10], Step [33225/68337], Loss: 4.9928\n",
      "Epoch [3/10], Step [33300/68337], Loss: 5.2026\n",
      "Epoch [3/10], Step [33375/68337], Loss: 5.1377\n",
      "Epoch [3/10], Step [33450/68337], Loss: 4.9524\n",
      "Epoch [3/10], Step [33525/68337], Loss: 5.1153\n",
      "Epoch [3/10], Step [33600/68337], Loss: 5.1820\n",
      "Epoch [3/10], Step [33675/68337], Loss: 5.2465\n",
      "Epoch [3/10], Step [33750/68337], Loss: 5.2506\n",
      "Epoch [3/10], Step [33825/68337], Loss: 5.1880\n",
      "Epoch [3/10], Step [33900/68337], Loss: 5.3962\n",
      "Epoch [3/10], Step [33975/68337], Loss: 5.1166\n",
      "Epoch [3/10], Step [34050/68337], Loss: 5.2154\n",
      "Epoch [3/10], Step [34125/68337], Loss: 5.1217\n",
      "Epoch [3/10], Step [34200/68337], Loss: 5.0929\n",
      "Epoch [3/10], Step [34275/68337], Loss: 5.2716\n",
      "Epoch [3/10], Step [34350/68337], Loss: 5.1795\n",
      "Epoch [3/10], Step [34425/68337], Loss: 5.2572\n",
      "Epoch [3/10], Step [34500/68337], Loss: 5.3234\n",
      "Epoch [3/10], Step [34575/68337], Loss: 5.1588\n",
      "Epoch [3/10], Step [34650/68337], Loss: 5.2121\n",
      "Epoch [3/10], Step [34725/68337], Loss: 5.2595\n",
      "Epoch [3/10], Step [34800/68337], Loss: 5.2732\n",
      "Epoch [3/10], Step [34875/68337], Loss: 5.0959\n",
      "Epoch [3/10], Step [34950/68337], Loss: 5.4681\n",
      "Epoch [3/10], Step [35025/68337], Loss: 4.9951\n",
      "Epoch [3/10], Step [35100/68337], Loss: 5.2205\n",
      "Epoch [3/10], Step [35175/68337], Loss: 5.1015\n",
      "Epoch [3/10], Step [35250/68337], Loss: 5.2712\n",
      "Epoch [3/10], Step [35325/68337], Loss: 5.3294\n",
      "Epoch [3/10], Step [35400/68337], Loss: 5.0752\n",
      "Epoch [3/10], Step [35475/68337], Loss: 5.1248\n",
      "Epoch [3/10], Step [35550/68337], Loss: 4.9998\n",
      "Epoch [3/10], Step [35625/68337], Loss: 5.2605\n",
      "Epoch [3/10], Step [35700/68337], Loss: 5.2939\n",
      "Epoch [3/10], Step [35775/68337], Loss: 5.0994\n",
      "Epoch [3/10], Step [35850/68337], Loss: 5.5362\n",
      "Epoch [3/10], Step [35925/68337], Loss: 5.2899\n",
      "Epoch [3/10], Step [36000/68337], Loss: 5.1690\n",
      "Epoch [3/10], Step [36075/68337], Loss: 5.0995\n",
      "Epoch [3/10], Step [36150/68337], Loss: 5.0450\n",
      "Epoch [3/10], Step [36225/68337], Loss: 5.2240\n",
      "Epoch [3/10], Step [36300/68337], Loss: 5.0513\n",
      "Epoch [3/10], Step [36375/68337], Loss: 5.3036\n",
      "Epoch [3/10], Step [36450/68337], Loss: 5.2060\n",
      "Epoch [3/10], Step [36525/68337], Loss: 4.9368\n",
      "Epoch [3/10], Step [36600/68337], Loss: 4.9457\n",
      "Epoch [3/10], Step [36675/68337], Loss: 5.2113\n",
      "Epoch [3/10], Step [36750/68337], Loss: 5.1533\n",
      "Epoch [3/10], Step [36825/68337], Loss: 5.2894\n",
      "Epoch [3/10], Step [36900/68337], Loss: 5.2933\n",
      "Epoch [3/10], Step [36975/68337], Loss: 5.1900\n",
      "Epoch [3/10], Step [37050/68337], Loss: 5.2750\n",
      "Epoch [3/10], Step [37125/68337], Loss: 4.9521\n",
      "Epoch [3/10], Step [37200/68337], Loss: 5.2929\n",
      "Epoch [3/10], Step [37275/68337], Loss: 5.2986\n",
      "Epoch [3/10], Step [37350/68337], Loss: 5.2091\n",
      "Epoch [3/10], Step [37425/68337], Loss: 5.1428\n",
      "Epoch [3/10], Step [37500/68337], Loss: 5.1360\n",
      "Epoch [3/10], Step [37575/68337], Loss: 5.3177\n",
      "Epoch [3/10], Step [37650/68337], Loss: 5.0955\n",
      "Epoch [3/10], Step [37725/68337], Loss: 4.9542\n",
      "Epoch [3/10], Step [37800/68337], Loss: 5.0467\n",
      "Epoch [3/10], Step [37875/68337], Loss: 5.0804\n",
      "Epoch [3/10], Step [37950/68337], Loss: 5.2387\n",
      "Epoch [3/10], Step [38025/68337], Loss: 5.3535\n",
      "Epoch [3/10], Step [38100/68337], Loss: 5.2036\n",
      "Epoch [3/10], Step [38175/68337], Loss: 5.0993\n",
      "Epoch [3/10], Step [38250/68337], Loss: 5.2631\n",
      "Epoch [3/10], Step [38325/68337], Loss: 5.2252\n",
      "Epoch [3/10], Step [38400/68337], Loss: 5.1493\n",
      "Epoch [3/10], Step [38475/68337], Loss: 5.0130\n",
      "Epoch [3/10], Step [38550/68337], Loss: 5.1690\n",
      "Epoch [3/10], Step [38625/68337], Loss: 4.9499\n",
      "Epoch [3/10], Step [38700/68337], Loss: 5.0664\n",
      "Epoch [3/10], Step [38775/68337], Loss: 5.0855\n",
      "Epoch [3/10], Step [38850/68337], Loss: 5.1367\n",
      "Epoch [3/10], Step [38925/68337], Loss: 5.2819\n",
      "Epoch [3/10], Step [39000/68337], Loss: 5.1664\n",
      "Epoch [3/10], Step [39075/68337], Loss: 5.2691\n",
      "Epoch [3/10], Step [39150/68337], Loss: 4.9519\n",
      "Epoch [3/10], Step [39225/68337], Loss: 5.3456\n",
      "Epoch [3/10], Step [39300/68337], Loss: 5.1127\n",
      "Epoch [3/10], Step [39375/68337], Loss: 5.2103\n",
      "Epoch [3/10], Step [39450/68337], Loss: 5.1790\n",
      "Epoch [3/10], Step [39525/68337], Loss: 5.2963\n",
      "Epoch [3/10], Step [39600/68337], Loss: 5.2417\n",
      "Epoch [3/10], Step [39675/68337], Loss: 5.2089\n",
      "Epoch [3/10], Step [39750/68337], Loss: 4.9773\n",
      "Epoch [3/10], Step [39825/68337], Loss: 4.9710\n",
      "Epoch [3/10], Step [39900/68337], Loss: 5.3397\n",
      "Epoch [3/10], Step [39975/68337], Loss: 5.1756\n",
      "Validation perplexity: 139.345875413945\n",
      "Epoch [3/10], Step [40050/68337], Loss: 4.9899\n",
      "Epoch [3/10], Step [40125/68337], Loss: 5.0998\n",
      "Epoch [3/10], Step [40200/68337], Loss: 5.2533\n",
      "Epoch [3/10], Step [40275/68337], Loss: 5.2520\n",
      "Epoch [3/10], Step [40350/68337], Loss: 5.1505\n",
      "Epoch [3/10], Step [40425/68337], Loss: 5.1259\n",
      "Epoch [3/10], Step [40500/68337], Loss: 4.9845\n",
      "Epoch [3/10], Step [40575/68337], Loss: 5.1610\n",
      "Epoch [3/10], Step [40650/68337], Loss: 5.2379\n",
      "Epoch [3/10], Step [40725/68337], Loss: 5.2453\n",
      "Epoch [3/10], Step [40800/68337], Loss: 5.1540\n",
      "Epoch [3/10], Step [40875/68337], Loss: 5.0849\n",
      "Epoch [3/10], Step [40950/68337], Loss: 5.1886\n",
      "Epoch [3/10], Step [41025/68337], Loss: 5.1040\n",
      "Epoch [3/10], Step [41100/68337], Loss: 5.2386\n",
      "Epoch [3/10], Step [41175/68337], Loss: 5.0576\n",
      "Epoch [3/10], Step [41250/68337], Loss: 5.3524\n",
      "Epoch [3/10], Step [41325/68337], Loss: 5.1081\n",
      "Epoch [3/10], Step [41400/68337], Loss: 5.0557\n",
      "Epoch [3/10], Step [41475/68337], Loss: 5.3201\n",
      "Epoch [3/10], Step [41550/68337], Loss: 5.0648\n",
      "Epoch [3/10], Step [41625/68337], Loss: 5.2696\n",
      "Epoch [3/10], Step [41700/68337], Loss: 5.4050\n",
      "Epoch [3/10], Step [41775/68337], Loss: 5.1112\n",
      "Epoch [3/10], Step [41850/68337], Loss: 5.0136\n",
      "Epoch [3/10], Step [41925/68337], Loss: 4.9997\n",
      "Epoch [3/10], Step [42000/68337], Loss: 5.2475\n",
      "Epoch [3/10], Step [42075/68337], Loss: 5.0928\n",
      "Epoch [3/10], Step [42150/68337], Loss: 5.1017\n",
      "Epoch [3/10], Step [42225/68337], Loss: 5.0693\n",
      "Epoch [3/10], Step [42300/68337], Loss: 5.2393\n",
      "Epoch [3/10], Step [42375/68337], Loss: 5.1319\n",
      "Epoch [3/10], Step [42450/68337], Loss: 5.1312\n",
      "Epoch [3/10], Step [42525/68337], Loss: 5.0022\n",
      "Epoch [3/10], Step [42600/68337], Loss: 5.1338\n",
      "Epoch [3/10], Step [42675/68337], Loss: 5.2074\n",
      "Epoch [3/10], Step [42750/68337], Loss: 5.0426\n",
      "Epoch [3/10], Step [42825/68337], Loss: 5.3603\n",
      "Epoch [3/10], Step [42900/68337], Loss: 5.2225\n",
      "Epoch [3/10], Step [42975/68337], Loss: 5.1433\n",
      "Epoch [3/10], Step [43050/68337], Loss: 5.2921\n",
      "Epoch [3/10], Step [43125/68337], Loss: 5.1398\n",
      "Epoch [3/10], Step [43200/68337], Loss: 5.0846\n",
      "Epoch [3/10], Step [43275/68337], Loss: 5.1107\n",
      "Epoch [3/10], Step [43350/68337], Loss: 5.1247\n",
      "Epoch [3/10], Step [43425/68337], Loss: 5.0235\n",
      "Epoch [3/10], Step [43500/68337], Loss: 5.3409\n",
      "Epoch [3/10], Step [43575/68337], Loss: 5.1591\n",
      "Epoch [3/10], Step [43650/68337], Loss: 5.1648\n",
      "Epoch [3/10], Step [43725/68337], Loss: 5.3132\n",
      "Epoch [3/10], Step [43800/68337], Loss: 5.2069\n",
      "Epoch [3/10], Step [43875/68337], Loss: 5.3273\n",
      "Epoch [3/10], Step [43950/68337], Loss: 5.0916\n",
      "Epoch [3/10], Step [44025/68337], Loss: 5.1236\n",
      "Epoch [3/10], Step [44100/68337], Loss: 5.1213\n",
      "Epoch [3/10], Step [44175/68337], Loss: 5.3972\n",
      "Epoch [3/10], Step [44250/68337], Loss: 5.1439\n",
      "Epoch [3/10], Step [44325/68337], Loss: 5.0418\n",
      "Epoch [3/10], Step [44400/68337], Loss: 5.2366\n",
      "Epoch [3/10], Step [44475/68337], Loss: 5.2768\n",
      "Epoch [3/10], Step [44550/68337], Loss: 5.1310\n",
      "Epoch [3/10], Step [44625/68337], Loss: 5.2486\n",
      "Epoch [3/10], Step [44700/68337], Loss: 5.0691\n",
      "Epoch [3/10], Step [44775/68337], Loss: 5.1445\n",
      "Epoch [3/10], Step [44850/68337], Loss: 5.2305\n",
      "Epoch [3/10], Step [44925/68337], Loss: 5.2120\n",
      "Epoch [3/10], Step [45000/68337], Loss: 5.3887\n",
      "Epoch [3/10], Step [45075/68337], Loss: 5.3042\n",
      "Epoch [3/10], Step [45150/68337], Loss: 5.3087\n",
      "Epoch [3/10], Step [45225/68337], Loss: 5.2028\n",
      "Epoch [3/10], Step [45300/68337], Loss: 5.2898\n",
      "Epoch [3/10], Step [45375/68337], Loss: 5.0198\n",
      "Epoch [3/10], Step [45450/68337], Loss: 5.0442\n",
      "Epoch [3/10], Step [45525/68337], Loss: 5.1720\n",
      "Epoch [3/10], Step [45600/68337], Loss: 5.0217\n",
      "Epoch [3/10], Step [45675/68337], Loss: 5.2081\n",
      "Epoch [3/10], Step [45750/68337], Loss: 5.4109\n",
      "Epoch [3/10], Step [45825/68337], Loss: 5.2296\n",
      "Epoch [3/10], Step [45900/68337], Loss: 5.0781\n",
      "Epoch [3/10], Step [45975/68337], Loss: 5.1849\n",
      "Epoch [3/10], Step [46050/68337], Loss: 5.1103\n",
      "Epoch [3/10], Step [46125/68337], Loss: 5.1447\n",
      "Epoch [3/10], Step [46200/68337], Loss: 5.2404\n",
      "Epoch [3/10], Step [46275/68337], Loss: 5.0683\n",
      "Epoch [3/10], Step [46350/68337], Loss: 5.1017\n",
      "Epoch [3/10], Step [46425/68337], Loss: 5.2273\n",
      "Epoch [3/10], Step [46500/68337], Loss: 5.4471\n",
      "Epoch [3/10], Step [46575/68337], Loss: 5.1892\n",
      "Epoch [3/10], Step [46650/68337], Loss: 5.2651\n",
      "Epoch [3/10], Step [46725/68337], Loss: 5.0605\n",
      "Epoch [3/10], Step [46800/68337], Loss: 5.2431\n",
      "Epoch [3/10], Step [46875/68337], Loss: 5.3266\n",
      "Epoch [3/10], Step [46950/68337], Loss: 5.1632\n",
      "Epoch [3/10], Step [47025/68337], Loss: 5.2422\n",
      "Epoch [3/10], Step [47100/68337], Loss: 5.3807\n",
      "Epoch [3/10], Step [47175/68337], Loss: 4.9777\n",
      "Epoch [3/10], Step [47250/68337], Loss: 5.0928\n",
      "Epoch [3/10], Step [47325/68337], Loss: 5.1639\n",
      "Epoch [3/10], Step [47400/68337], Loss: 5.3233\n",
      "Epoch [3/10], Step [47475/68337], Loss: 5.2212\n",
      "Epoch [3/10], Step [47550/68337], Loss: 5.3258\n",
      "Epoch [3/10], Step [47625/68337], Loss: 5.1118\n",
      "Epoch [3/10], Step [47700/68337], Loss: 5.1339\n",
      "Epoch [3/10], Step [47775/68337], Loss: 5.1827\n",
      "Epoch [3/10], Step [47850/68337], Loss: 5.0145\n",
      "Epoch [3/10], Step [47925/68337], Loss: 5.2804\n",
      "Epoch [3/10], Step [48000/68337], Loss: 5.2700\n",
      "Epoch [3/10], Step [48075/68337], Loss: 5.2504\n",
      "Epoch [3/10], Step [48150/68337], Loss: 5.1158\n",
      "Epoch [3/10], Step [48225/68337], Loss: 5.2736\n",
      "Epoch [3/10], Step [48300/68337], Loss: 5.2419\n",
      "Epoch [3/10], Step [48375/68337], Loss: 5.2038\n",
      "Epoch [3/10], Step [48450/68337], Loss: 5.0243\n",
      "Epoch [3/10], Step [48525/68337], Loss: 5.1570\n",
      "Epoch [3/10], Step [48600/68337], Loss: 5.2602\n",
      "Epoch [3/10], Step [48675/68337], Loss: 5.1987\n",
      "Epoch [3/10], Step [48750/68337], Loss: 5.1060\n",
      "Epoch [3/10], Step [48825/68337], Loss: 4.9282\n",
      "Epoch [3/10], Step [48900/68337], Loss: 5.1168\n",
      "Epoch [3/10], Step [48975/68337], Loss: 5.0228\n",
      "Epoch [3/10], Step [49050/68337], Loss: 5.1985\n",
      "Epoch [3/10], Step [49125/68337], Loss: 5.2182\n",
      "Epoch [3/10], Step [49200/68337], Loss: 5.1539\n",
      "Epoch [3/10], Step [49275/68337], Loss: 5.2426\n",
      "Epoch [3/10], Step [49350/68337], Loss: 5.2849\n",
      "Epoch [3/10], Step [49425/68337], Loss: 5.1224\n",
      "Epoch [3/10], Step [49500/68337], Loss: 5.1640\n",
      "Epoch [3/10], Step [49575/68337], Loss: 5.1716\n",
      "Epoch [3/10], Step [49650/68337], Loss: 5.2133\n",
      "Epoch [3/10], Step [49725/68337], Loss: 5.1310\n",
      "Epoch [3/10], Step [49800/68337], Loss: 5.0388\n",
      "Epoch [3/10], Step [49875/68337], Loss: 5.2027\n",
      "Epoch [3/10], Step [49950/68337], Loss: 5.0836\n",
      "Validation perplexity: 137.3692775100622\n",
      "Epoch [3/10], Step [50025/68337], Loss: 5.0747\n",
      "Epoch [3/10], Step [50100/68337], Loss: 5.2516\n",
      "Epoch [3/10], Step [50175/68337], Loss: 5.2689\n",
      "Epoch [3/10], Step [50250/68337], Loss: 5.2923\n",
      "Epoch [3/10], Step [50325/68337], Loss: 5.2242\n",
      "Epoch [3/10], Step [50400/68337], Loss: 5.1501\n",
      "Epoch [3/10], Step [50475/68337], Loss: 5.2053\n",
      "Epoch [3/10], Step [50550/68337], Loss: 5.2442\n",
      "Epoch [3/10], Step [50625/68337], Loss: 5.2136\n",
      "Epoch [3/10], Step [50700/68337], Loss: 5.3082\n",
      "Epoch [3/10], Step [50775/68337], Loss: 5.1672\n",
      "Epoch [3/10], Step [50850/68337], Loss: 5.1551\n",
      "Epoch [3/10], Step [50925/68337], Loss: 5.4364\n",
      "Epoch [3/10], Step [51000/68337], Loss: 5.1973\n",
      "Epoch [3/10], Step [51075/68337], Loss: 5.2085\n",
      "Epoch [3/10], Step [51150/68337], Loss: 5.2711\n",
      "Epoch [3/10], Step [51225/68337], Loss: 5.2511\n",
      "Epoch [3/10], Step [51300/68337], Loss: 5.1678\n",
      "Epoch [3/10], Step [51375/68337], Loss: 5.2414\n",
      "Epoch [3/10], Step [51450/68337], Loss: 5.3759\n",
      "Epoch [3/10], Step [51525/68337], Loss: 5.3490\n",
      "Epoch [3/10], Step [51600/68337], Loss: 4.9264\n",
      "Epoch [3/10], Step [51675/68337], Loss: 5.1383\n",
      "Epoch [3/10], Step [51750/68337], Loss: 5.0509\n",
      "Epoch [3/10], Step [51825/68337], Loss: 5.1805\n",
      "Epoch [3/10], Step [51900/68337], Loss: 5.1827\n",
      "Epoch [3/10], Step [51975/68337], Loss: 5.2633\n",
      "Epoch [3/10], Step [52050/68337], Loss: 5.1739\n",
      "Epoch [3/10], Step [52125/68337], Loss: 5.2046\n",
      "Epoch [3/10], Step [52200/68337], Loss: 5.2929\n",
      "Epoch [3/10], Step [52275/68337], Loss: 5.2146\n",
      "Epoch [3/10], Step [52350/68337], Loss: 5.2154\n",
      "Epoch [3/10], Step [52425/68337], Loss: 5.2065\n",
      "Epoch [3/10], Step [52500/68337], Loss: 5.1982\n",
      "Epoch [3/10], Step [52575/68337], Loss: 5.1053\n",
      "Epoch [3/10], Step [52650/68337], Loss: 5.1645\n",
      "Epoch [3/10], Step [52725/68337], Loss: 5.3517\n",
      "Epoch [3/10], Step [52800/68337], Loss: 5.1138\n",
      "Epoch [3/10], Step [52875/68337], Loss: 5.1586\n",
      "Epoch [3/10], Step [52950/68337], Loss: 5.2075\n",
      "Epoch [3/10], Step [53025/68337], Loss: 5.1405\n",
      "Epoch [3/10], Step [53100/68337], Loss: 5.0358\n",
      "Epoch [3/10], Step [53175/68337], Loss: 5.2149\n",
      "Epoch [3/10], Step [53250/68337], Loss: 5.2061\n",
      "Epoch [3/10], Step [53325/68337], Loss: 5.0099\n",
      "Epoch [3/10], Step [53400/68337], Loss: 5.1807\n",
      "Epoch [3/10], Step [53475/68337], Loss: 5.2372\n",
      "Epoch [3/10], Step [53550/68337], Loss: 4.9453\n",
      "Epoch [3/10], Step [53625/68337], Loss: 5.0674\n",
      "Epoch [3/10], Step [53700/68337], Loss: 5.3524\n",
      "Epoch [3/10], Step [53775/68337], Loss: 5.0491\n",
      "Epoch [3/10], Step [53850/68337], Loss: 5.0133\n",
      "Epoch [3/10], Step [53925/68337], Loss: 5.3714\n",
      "Epoch [3/10], Step [54000/68337], Loss: 5.5348\n",
      "Epoch [3/10], Step [54075/68337], Loss: 5.1169\n",
      "Epoch [3/10], Step [54150/68337], Loss: 5.1009\n",
      "Epoch [3/10], Step [54225/68337], Loss: 5.0975\n",
      "Epoch [3/10], Step [54300/68337], Loss: 5.1190\n",
      "Epoch [3/10], Step [54375/68337], Loss: 5.3824\n",
      "Epoch [3/10], Step [54450/68337], Loss: 5.0962\n",
      "Epoch [3/10], Step [54525/68337], Loss: 5.0360\n",
      "Epoch [3/10], Step [54600/68337], Loss: 5.0504\n",
      "Epoch [3/10], Step [54675/68337], Loss: 5.1063\n",
      "Epoch [3/10], Step [54750/68337], Loss: 5.2350\n",
      "Epoch [3/10], Step [54825/68337], Loss: 5.0614\n",
      "Epoch [3/10], Step [54900/68337], Loss: 5.0449\n",
      "Epoch [3/10], Step [54975/68337], Loss: 5.0926\n",
      "Epoch [3/10], Step [55050/68337], Loss: 5.2511\n",
      "Epoch [3/10], Step [55125/68337], Loss: 5.0402\n",
      "Epoch [3/10], Step [55200/68337], Loss: 5.3046\n",
      "Epoch [3/10], Step [55275/68337], Loss: 5.1464\n",
      "Epoch [3/10], Step [55350/68337], Loss: 5.2408\n",
      "Epoch [3/10], Step [55425/68337], Loss: 5.1761\n",
      "Epoch [3/10], Step [55500/68337], Loss: 5.1070\n",
      "Epoch [3/10], Step [55575/68337], Loss: 5.1629\n",
      "Epoch [3/10], Step [55650/68337], Loss: 5.4362\n",
      "Epoch [3/10], Step [55725/68337], Loss: 5.1171\n",
      "Epoch [3/10], Step [55800/68337], Loss: 5.1304\n",
      "Epoch [3/10], Step [55875/68337], Loss: 5.0897\n",
      "Epoch [3/10], Step [55950/68337], Loss: 5.1701\n",
      "Epoch [3/10], Step [56025/68337], Loss: 5.0155\n",
      "Epoch [3/10], Step [56100/68337], Loss: 4.9474\n",
      "Epoch [3/10], Step [56175/68337], Loss: 5.0940\n",
      "Epoch [3/10], Step [56250/68337], Loss: 5.1414\n",
      "Epoch [3/10], Step [56325/68337], Loss: 5.3156\n",
      "Epoch [3/10], Step [56400/68337], Loss: 5.1651\n",
      "Epoch [3/10], Step [56475/68337], Loss: 5.1868\n",
      "Epoch [3/10], Step [56550/68337], Loss: 5.1814\n",
      "Epoch [3/10], Step [56625/68337], Loss: 4.9963\n",
      "Epoch [3/10], Step [56700/68337], Loss: 5.2186\n",
      "Epoch [3/10], Step [56775/68337], Loss: 5.1112\n",
      "Epoch [3/10], Step [56850/68337], Loss: 5.0036\n",
      "Epoch [3/10], Step [56925/68337], Loss: 5.2816\n",
      "Epoch [3/10], Step [57000/68337], Loss: 5.2846\n",
      "Epoch [3/10], Step [57075/68337], Loss: 5.0581\n",
      "Epoch [3/10], Step [57150/68337], Loss: 5.2814\n",
      "Epoch [3/10], Step [57225/68337], Loss: 5.0640\n",
      "Epoch [3/10], Step [57300/68337], Loss: 5.1324\n",
      "Epoch [3/10], Step [57375/68337], Loss: 5.1617\n",
      "Epoch [3/10], Step [57450/68337], Loss: 5.0263\n",
      "Epoch [3/10], Step [57525/68337], Loss: 5.2497\n",
      "Epoch [3/10], Step [57600/68337], Loss: 5.1309\n",
      "Epoch [3/10], Step [57675/68337], Loss: 5.2094\n",
      "Epoch [3/10], Step [57750/68337], Loss: 5.1123\n",
      "Epoch [3/10], Step [57825/68337], Loss: 5.3113\n",
      "Epoch [3/10], Step [57900/68337], Loss: 5.1023\n",
      "Epoch [3/10], Step [57975/68337], Loss: 5.2176\n",
      "Epoch [3/10], Step [58050/68337], Loss: 5.2142\n",
      "Epoch [3/10], Step [58125/68337], Loss: 5.1091\n",
      "Epoch [3/10], Step [58200/68337], Loss: 5.1656\n",
      "Epoch [3/10], Step [58275/68337], Loss: 5.1235\n",
      "Epoch [3/10], Step [58350/68337], Loss: 5.0965\n",
      "Epoch [3/10], Step [58425/68337], Loss: 4.8886\n",
      "Epoch [3/10], Step [58500/68337], Loss: 5.2549\n",
      "Epoch [3/10], Step [58575/68337], Loss: 5.0829\n",
      "Epoch [3/10], Step [58650/68337], Loss: 5.0728\n",
      "Epoch [3/10], Step [58725/68337], Loss: 5.1251\n",
      "Epoch [3/10], Step [58800/68337], Loss: 5.1204\n",
      "Epoch [3/10], Step [58875/68337], Loss: 5.1581\n",
      "Epoch [3/10], Step [58950/68337], Loss: 5.0618\n",
      "Epoch [3/10], Step [59025/68337], Loss: 5.0969\n",
      "Epoch [3/10], Step [59100/68337], Loss: 5.2089\n",
      "Epoch [3/10], Step [59175/68337], Loss: 4.9407\n",
      "Epoch [3/10], Step [59250/68337], Loss: 4.9807\n",
      "Epoch [3/10], Step [59325/68337], Loss: 5.3002\n",
      "Epoch [3/10], Step [59400/68337], Loss: 5.4122\n",
      "Epoch [3/10], Step [59475/68337], Loss: 4.9726\n",
      "Epoch [3/10], Step [59550/68337], Loss: 5.1702\n",
      "Epoch [3/10], Step [59625/68337], Loss: 4.9120\n",
      "Epoch [3/10], Step [59700/68337], Loss: 5.1842\n",
      "Epoch [3/10], Step [59775/68337], Loss: 5.4872\n",
      "Epoch [3/10], Step [59850/68337], Loss: 4.9936\n",
      "Epoch [3/10], Step [59925/68337], Loss: 5.0406\n",
      "Epoch [3/10], Step [60000/68337], Loss: 5.3589\n",
      "Validation perplexity: 136.3371876867463\n",
      "Epoch [3/10], Step [60075/68337], Loss: 5.0263\n",
      "Epoch [3/10], Step [60150/68337], Loss: 5.0921\n",
      "Epoch [3/10], Step [60225/68337], Loss: 5.1494\n",
      "Epoch [3/10], Step [60300/68337], Loss: 5.1182\n",
      "Epoch [3/10], Step [60375/68337], Loss: 4.8589\n",
      "Epoch [3/10], Step [60450/68337], Loss: 5.1379\n",
      "Epoch [3/10], Step [60525/68337], Loss: 5.1389\n",
      "Epoch [3/10], Step [60600/68337], Loss: 5.0539\n",
      "Epoch [3/10], Step [60675/68337], Loss: 5.3822\n",
      "Epoch [3/10], Step [60750/68337], Loss: 5.0814\n",
      "Epoch [3/10], Step [60825/68337], Loss: 5.2807\n",
      "Epoch [3/10], Step [60900/68337], Loss: 5.1998\n",
      "Epoch [3/10], Step [60975/68337], Loss: 5.3822\n",
      "Epoch [3/10], Step [61050/68337], Loss: 5.3878\n",
      "Epoch [3/10], Step [61125/68337], Loss: 5.1457\n",
      "Epoch [3/10], Step [61200/68337], Loss: 5.1396\n",
      "Epoch [3/10], Step [61275/68337], Loss: 5.1422\n",
      "Epoch [3/10], Step [61350/68337], Loss: 5.3157\n",
      "Epoch [3/10], Step [61425/68337], Loss: 5.2726\n",
      "Epoch [3/10], Step [61500/68337], Loss: 4.9880\n",
      "Epoch [3/10], Step [61575/68337], Loss: 5.1457\n",
      "Epoch [3/10], Step [61650/68337], Loss: 5.1127\n",
      "Epoch [3/10], Step [61725/68337], Loss: 5.2011\n",
      "Epoch [3/10], Step [61800/68337], Loss: 5.2138\n",
      "Epoch [3/10], Step [61875/68337], Loss: 5.3220\n",
      "Epoch [3/10], Step [61950/68337], Loss: 5.1084\n",
      "Epoch [3/10], Step [62025/68337], Loss: 5.3575\n",
      "Epoch [3/10], Step [62100/68337], Loss: 5.0229\n",
      "Epoch [3/10], Step [62175/68337], Loss: 5.1387\n",
      "Epoch [3/10], Step [62250/68337], Loss: 5.1246\n",
      "Epoch [3/10], Step [62325/68337], Loss: 5.1737\n",
      "Epoch [3/10], Step [62400/68337], Loss: 5.3737\n",
      "Epoch [3/10], Step [62475/68337], Loss: 5.0220\n",
      "Epoch [3/10], Step [62550/68337], Loss: 5.0735\n",
      "Epoch [3/10], Step [62625/68337], Loss: 5.1019\n",
      "Epoch [3/10], Step [62700/68337], Loss: 5.1686\n",
      "Epoch [3/10], Step [62775/68337], Loss: 5.1235\n",
      "Epoch [3/10], Step [62850/68337], Loss: 5.1945\n",
      "Epoch [3/10], Step [62925/68337], Loss: 5.2818\n",
      "Epoch [3/10], Step [63000/68337], Loss: 5.3273\n",
      "Epoch [3/10], Step [63075/68337], Loss: 5.2262\n",
      "Epoch [3/10], Step [63150/68337], Loss: 4.9270\n",
      "Epoch [3/10], Step [63225/68337], Loss: 5.2482\n",
      "Epoch [3/10], Step [63300/68337], Loss: 5.2571\n",
      "Epoch [3/10], Step [63375/68337], Loss: 5.1110\n",
      "Epoch [3/10], Step [63450/68337], Loss: 5.0000\n",
      "Epoch [3/10], Step [63525/68337], Loss: 5.1155\n",
      "Epoch [3/10], Step [63600/68337], Loss: 5.2357\n",
      "Epoch [3/10], Step [63675/68337], Loss: 4.9462\n",
      "Epoch [3/10], Step [63750/68337], Loss: 5.3550\n",
      "Epoch [3/10], Step [63825/68337], Loss: 5.0918\n",
      "Epoch [3/10], Step [63900/68337], Loss: 5.2449\n",
      "Epoch [3/10], Step [63975/68337], Loss: 5.0508\n",
      "Epoch [3/10], Step [64050/68337], Loss: 5.1527\n",
      "Epoch [3/10], Step [64125/68337], Loss: 5.3572\n",
      "Epoch [3/10], Step [64200/68337], Loss: 5.0894\n",
      "Epoch [3/10], Step [64275/68337], Loss: 5.1995\n",
      "Epoch [3/10], Step [64350/68337], Loss: 5.0498\n",
      "Epoch [3/10], Step [64425/68337], Loss: 5.2444\n",
      "Epoch [3/10], Step [64500/68337], Loss: 5.1053\n",
      "Epoch [3/10], Step [64575/68337], Loss: 5.3348\n",
      "Epoch [3/10], Step [64650/68337], Loss: 5.1760\n",
      "Epoch [3/10], Step [64725/68337], Loss: 5.2475\n",
      "Epoch [3/10], Step [64800/68337], Loss: 5.3561\n",
      "Epoch [3/10], Step [64875/68337], Loss: 5.0932\n",
      "Epoch [3/10], Step [64950/68337], Loss: 5.2996\n",
      "Epoch [3/10], Step [65025/68337], Loss: 5.2318\n",
      "Epoch [3/10], Step [65100/68337], Loss: 5.0584\n",
      "Epoch [3/10], Step [65175/68337], Loss: 5.1067\n",
      "Epoch [3/10], Step [65250/68337], Loss: 5.1068\n",
      "Epoch [3/10], Step [65325/68337], Loss: 5.2482\n",
      "Epoch [3/10], Step [65400/68337], Loss: 5.2548\n",
      "Epoch [3/10], Step [65475/68337], Loss: 5.4136\n",
      "Epoch [3/10], Step [65550/68337], Loss: 5.2430\n",
      "Epoch [3/10], Step [65625/68337], Loss: 5.1257\n",
      "Epoch [3/10], Step [65700/68337], Loss: 5.2465\n",
      "Epoch [3/10], Step [65775/68337], Loss: 5.2650\n",
      "Epoch [3/10], Step [65850/68337], Loss: 5.2834\n",
      "Epoch [3/10], Step [65925/68337], Loss: 5.3374\n",
      "Epoch [3/10], Step [66000/68337], Loss: 5.3100\n",
      "Epoch [3/10], Step [66075/68337], Loss: 4.9689\n",
      "Epoch [3/10], Step [66150/68337], Loss: 5.2159\n",
      "Epoch [3/10], Step [66225/68337], Loss: 5.0954\n",
      "Epoch [3/10], Step [66300/68337], Loss: 5.1885\n",
      "Epoch [3/10], Step [66375/68337], Loss: 5.2033\n",
      "Epoch [3/10], Step [66450/68337], Loss: 5.0958\n",
      "Epoch [3/10], Step [66525/68337], Loss: 5.1238\n",
      "Epoch [3/10], Step [66600/68337], Loss: 5.2788\n",
      "Epoch [3/10], Step [66675/68337], Loss: 5.1862\n",
      "Epoch [3/10], Step [66750/68337], Loss: 5.3014\n",
      "Epoch [3/10], Step [66825/68337], Loss: 5.1005\n",
      "Epoch [3/10], Step [66900/68337], Loss: 5.0764\n",
      "Epoch [3/10], Step [66975/68337], Loss: 5.0740\n",
      "Epoch [3/10], Step [67050/68337], Loss: 5.1771\n",
      "Epoch [3/10], Step [67125/68337], Loss: 5.2145\n",
      "Epoch [3/10], Step [67200/68337], Loss: 5.0618\n",
      "Epoch [3/10], Step [67275/68337], Loss: 5.2425\n",
      "Epoch [3/10], Step [67350/68337], Loss: 5.3641\n",
      "Epoch [3/10], Step [67425/68337], Loss: 5.2059\n",
      "Epoch [3/10], Step [67500/68337], Loss: 5.2357\n",
      "Epoch [3/10], Step [67575/68337], Loss: 5.4246\n",
      "Epoch [3/10], Step [67650/68337], Loss: 5.2242\n",
      "Epoch [3/10], Step [67725/68337], Loss: 5.2732\n",
      "Epoch [3/10], Step [67800/68337], Loss: 5.4009\n",
      "Epoch [3/10], Step [67875/68337], Loss: 5.2482\n",
      "Epoch [3/10], Step [67950/68337], Loss: 5.2119\n",
      "Epoch [3/10], Step [68025/68337], Loss: 5.0419\n",
      "Epoch [3/10], Step [68100/68337], Loss: 5.0390\n",
      "Epoch [3/10], Step [68175/68337], Loss: 5.2013\n",
      "Epoch [3/10], Step [68250/68337], Loss: 5.2448\n",
      "Epoch [3/10], Step [68325/68337], Loss: 5.3082\n",
      "Epoch [3/10] Average Loss: 5.1810, Perplexity: 177.86\n",
      "Epoch [4/10], Step [0/68337], Loss: 5.2361\n",
      "Validation perplexity: 135.60925322824048\n",
      "Epoch [4/10], Step [75/68337], Loss: 5.0639\n",
      "Epoch [4/10], Step [150/68337], Loss: 5.2272\n",
      "Epoch [4/10], Step [225/68337], Loss: 5.0664\n",
      "Epoch [4/10], Step [300/68337], Loss: 5.1141\n",
      "Epoch [4/10], Step [375/68337], Loss: 5.1502\n",
      "Epoch [4/10], Step [450/68337], Loss: 5.1472\n",
      "Epoch [4/10], Step [525/68337], Loss: 5.1210\n",
      "Epoch [4/10], Step [600/68337], Loss: 5.1964\n",
      "Epoch [4/10], Step [675/68337], Loss: 5.0629\n",
      "Epoch [4/10], Step [750/68337], Loss: 5.0678\n",
      "Epoch [4/10], Step [825/68337], Loss: 5.1677\n",
      "Epoch [4/10], Step [900/68337], Loss: 5.2024\n",
      "Epoch [4/10], Step [975/68337], Loss: 5.0660\n",
      "Epoch [4/10], Step [1050/68337], Loss: 5.1892\n",
      "Epoch [4/10], Step [1125/68337], Loss: 5.0004\n",
      "Epoch [4/10], Step [1200/68337], Loss: 5.2692\n",
      "Epoch [4/10], Step [1275/68337], Loss: 5.1287\n",
      "Epoch [4/10], Step [1350/68337], Loss: 5.1211\n",
      "Epoch [4/10], Step [1425/68337], Loss: 5.0647\n",
      "Epoch [4/10], Step [1500/68337], Loss: 5.1921\n",
      "Epoch [4/10], Step [1575/68337], Loss: 5.1950\n",
      "Epoch [4/10], Step [1650/68337], Loss: 5.2008\n",
      "Epoch [4/10], Step [1725/68337], Loss: 4.8667\n",
      "Epoch [4/10], Step [1800/68337], Loss: 5.2120\n",
      "Epoch [4/10], Step [1875/68337], Loss: 5.2314\n",
      "Epoch [4/10], Step [1950/68337], Loss: 5.1324\n",
      "Epoch [4/10], Step [2025/68337], Loss: 5.2275\n",
      "Epoch [4/10], Step [2100/68337], Loss: 5.1689\n",
      "Epoch [4/10], Step [2175/68337], Loss: 5.1813\n",
      "Epoch [4/10], Step [2250/68337], Loss: 5.0915\n",
      "Epoch [4/10], Step [2325/68337], Loss: 5.1554\n",
      "Epoch [4/10], Step [2400/68337], Loss: 5.4336\n",
      "Epoch [4/10], Step [2475/68337], Loss: 5.2609\n",
      "Epoch [4/10], Step [2550/68337], Loss: 5.1764\n",
      "Epoch [4/10], Step [2625/68337], Loss: 5.2816\n",
      "Epoch [4/10], Step [2700/68337], Loss: 5.3674\n",
      "Epoch [4/10], Step [2775/68337], Loss: 5.2278\n",
      "Epoch [4/10], Step [2850/68337], Loss: 5.1739\n",
      "Epoch [4/10], Step [2925/68337], Loss: 5.1661\n",
      "Epoch [4/10], Step [3000/68337], Loss: 4.9577\n",
      "Epoch [4/10], Step [3075/68337], Loss: 5.2600\n",
      "Epoch [4/10], Step [3150/68337], Loss: 5.1535\n",
      "Epoch [4/10], Step [3225/68337], Loss: 4.9179\n",
      "Epoch [4/10], Step [3300/68337], Loss: 5.2187\n",
      "Epoch [4/10], Step [3375/68337], Loss: 5.1778\n",
      "Epoch [4/10], Step [3450/68337], Loss: 5.1034\n",
      "Epoch [4/10], Step [3525/68337], Loss: 5.1812\n",
      "Epoch [4/10], Step [3600/68337], Loss: 5.0773\n",
      "Epoch [4/10], Step [3675/68337], Loss: 5.1565\n",
      "Epoch [4/10], Step [3750/68337], Loss: 5.1226\n",
      "Epoch [4/10], Step [3825/68337], Loss: 5.0923\n",
      "Epoch [4/10], Step [3900/68337], Loss: 5.1246\n",
      "Epoch [4/10], Step [3975/68337], Loss: 5.3939\n",
      "Epoch [4/10], Step [4050/68337], Loss: 5.1993\n",
      "Epoch [4/10], Step [4125/68337], Loss: 5.1865\n",
      "Epoch [4/10], Step [4200/68337], Loss: 5.1052\n",
      "Epoch [4/10], Step [4275/68337], Loss: 5.2386\n",
      "Epoch [4/10], Step [4350/68337], Loss: 5.2153\n",
      "Epoch [4/10], Step [4425/68337], Loss: 5.0066\n",
      "Epoch [4/10], Step [4500/68337], Loss: 5.1143\n",
      "Epoch [4/10], Step [4575/68337], Loss: 5.1273\n",
      "Epoch [4/10], Step [4650/68337], Loss: 5.1486\n",
      "Epoch [4/10], Step [4725/68337], Loss: 5.0443\n",
      "Epoch [4/10], Step [4800/68337], Loss: 5.0642\n",
      "Epoch [4/10], Step [4875/68337], Loss: 5.1771\n",
      "Epoch [4/10], Step [4950/68337], Loss: 5.0589\n",
      "Epoch [4/10], Step [5025/68337], Loss: 5.1897\n",
      "Epoch [4/10], Step [5100/68337], Loss: 5.0415\n",
      "Epoch [4/10], Step [5175/68337], Loss: 5.0479\n",
      "Epoch [4/10], Step [5250/68337], Loss: 5.2434\n",
      "Epoch [4/10], Step [5325/68337], Loss: 5.1322\n",
      "Epoch [4/10], Step [5400/68337], Loss: 5.1890\n",
      "Epoch [4/10], Step [5475/68337], Loss: 5.0522\n",
      "Epoch [4/10], Step [5550/68337], Loss: 4.9790\n",
      "Epoch [4/10], Step [5625/68337], Loss: 5.1508\n",
      "Epoch [4/10], Step [5700/68337], Loss: 5.1328\n",
      "Epoch [4/10], Step [5775/68337], Loss: 5.2386\n",
      "Epoch [4/10], Step [5850/68337], Loss: 5.3400\n",
      "Epoch [4/10], Step [5925/68337], Loss: 5.0939\n",
      "Epoch [4/10], Step [6000/68337], Loss: 5.2211\n",
      "Epoch [4/10], Step [6075/68337], Loss: 5.0907\n",
      "Epoch [4/10], Step [6150/68337], Loss: 5.0715\n",
      "Epoch [4/10], Step [6225/68337], Loss: 4.9592\n",
      "Epoch [4/10], Step [6300/68337], Loss: 5.1168\n",
      "Epoch [4/10], Step [6375/68337], Loss: 5.0876\n",
      "Epoch [4/10], Step [6450/68337], Loss: 5.3250\n",
      "Epoch [4/10], Step [6525/68337], Loss: 5.1996\n",
      "Epoch [4/10], Step [6600/68337], Loss: 5.3074\n",
      "Epoch [4/10], Step [6675/68337], Loss: 5.1484\n",
      "Epoch [4/10], Step [6750/68337], Loss: 5.3237\n",
      "Epoch [4/10], Step [6825/68337], Loss: 5.0482\n",
      "Epoch [4/10], Step [6900/68337], Loss: 5.1999\n",
      "Epoch [4/10], Step [6975/68337], Loss: 5.2849\n",
      "Epoch [4/10], Step [7050/68337], Loss: 5.0974\n",
      "Epoch [4/10], Step [7125/68337], Loss: 5.2277\n",
      "Epoch [4/10], Step [7200/68337], Loss: 5.3345\n",
      "Epoch [4/10], Step [7275/68337], Loss: 5.1738\n",
      "Epoch [4/10], Step [7350/68337], Loss: 4.9800\n",
      "Epoch [4/10], Step [7425/68337], Loss: 5.1430\n",
      "Epoch [4/10], Step [7500/68337], Loss: 5.3138\n",
      "Epoch [4/10], Step [7575/68337], Loss: 5.0609\n",
      "Epoch [4/10], Step [7650/68337], Loss: 5.1297\n",
      "Epoch [4/10], Step [7725/68337], Loss: 5.0431\n",
      "Epoch [4/10], Step [7800/68337], Loss: 5.3191\n",
      "Epoch [4/10], Step [7875/68337], Loss: 5.3120\n",
      "Epoch [4/10], Step [7950/68337], Loss: 5.1970\n",
      "Epoch [4/10], Step [8025/68337], Loss: 5.0785\n",
      "Epoch [4/10], Step [8100/68337], Loss: 5.0610\n",
      "Epoch [4/10], Step [8175/68337], Loss: 5.1491\n",
      "Epoch [4/10], Step [8250/68337], Loss: 5.2100\n",
      "Epoch [4/10], Step [8325/68337], Loss: 5.2391\n",
      "Epoch [4/10], Step [8400/68337], Loss: 5.2265\n",
      "Epoch [4/10], Step [8475/68337], Loss: 5.2537\n",
      "Epoch [4/10], Step [8550/68337], Loss: 5.1964\n",
      "Epoch [4/10], Step [8625/68337], Loss: 5.2494\n",
      "Epoch [4/10], Step [8700/68337], Loss: 5.0731\n",
      "Epoch [4/10], Step [8775/68337], Loss: 5.1964\n",
      "Epoch [4/10], Step [8850/68337], Loss: 5.0192\n",
      "Epoch [4/10], Step [8925/68337], Loss: 5.0571\n",
      "Epoch [4/10], Step [9000/68337], Loss: 5.2072\n",
      "Epoch [4/10], Step [9075/68337], Loss: 5.1784\n",
      "Epoch [4/10], Step [9150/68337], Loss: 5.2693\n",
      "Epoch [4/10], Step [9225/68337], Loss: 5.2623\n",
      "Epoch [4/10], Step [9300/68337], Loss: 5.1475\n",
      "Epoch [4/10], Step [9375/68337], Loss: 5.1360\n",
      "Epoch [4/10], Step [9450/68337], Loss: 5.2255\n",
      "Epoch [4/10], Step [9525/68337], Loss: 5.1911\n",
      "Epoch [4/10], Step [9600/68337], Loss: 5.0684\n",
      "Epoch [4/10], Step [9675/68337], Loss: 5.1629\n",
      "Epoch [4/10], Step [9750/68337], Loss: 5.1502\n",
      "Epoch [4/10], Step [9825/68337], Loss: 5.0081\n",
      "Epoch [4/10], Step [9900/68337], Loss: 5.1493\n",
      "Epoch [4/10], Step [9975/68337], Loss: 5.2110\n",
      "Validation perplexity: 134.9723375894618\n",
      "Epoch [4/10], Step [10050/68337], Loss: 5.2607\n",
      "Epoch [4/10], Step [10125/68337], Loss: 5.1845\n",
      "Epoch [4/10], Step [10200/68337], Loss: 5.0483\n",
      "Epoch [4/10], Step [10275/68337], Loss: 5.3263\n",
      "Epoch [4/10], Step [10350/68337], Loss: 5.0172\n",
      "Epoch [4/10], Step [10425/68337], Loss: 5.1980\n",
      "Epoch [4/10], Step [10500/68337], Loss: 5.1592\n",
      "Epoch [4/10], Step [10575/68337], Loss: 5.0392\n",
      "Epoch [4/10], Step [10650/68337], Loss: 4.8509\n",
      "Epoch [4/10], Step [10725/68337], Loss: 5.4605\n",
      "Epoch [4/10], Step [10800/68337], Loss: 5.3420\n",
      "Epoch [4/10], Step [10875/68337], Loss: 5.0887\n",
      "Epoch [4/10], Step [10950/68337], Loss: 5.0756\n",
      "Epoch [4/10], Step [11025/68337], Loss: 5.2766\n",
      "Epoch [4/10], Step [11100/68337], Loss: 5.2890\n",
      "Epoch [4/10], Step [11175/68337], Loss: 5.3467\n",
      "Epoch [4/10], Step [11250/68337], Loss: 5.0885\n",
      "Epoch [4/10], Step [11325/68337], Loss: 5.2324\n",
      "Epoch [4/10], Step [11400/68337], Loss: 5.1788\n",
      "Epoch [4/10], Step [11475/68337], Loss: 5.2713\n",
      "Epoch [4/10], Step [11550/68337], Loss: 5.0195\n",
      "Epoch [4/10], Step [11625/68337], Loss: 5.0968\n",
      "Epoch [4/10], Step [11700/68337], Loss: 5.1969\n",
      "Epoch [4/10], Step [11775/68337], Loss: 5.3320\n",
      "Epoch [4/10], Step [11850/68337], Loss: 5.2993\n",
      "Epoch [4/10], Step [11925/68337], Loss: 5.0632\n",
      "Epoch [4/10], Step [12000/68337], Loss: 5.2325\n",
      "Epoch [4/10], Step [12075/68337], Loss: 4.9388\n",
      "Epoch [4/10], Step [12150/68337], Loss: 4.9326\n",
      "Epoch [4/10], Step [12225/68337], Loss: 5.0310\n",
      "Epoch [4/10], Step [12300/68337], Loss: 5.2995\n",
      "Epoch [4/10], Step [12375/68337], Loss: 4.9680\n",
      "Epoch [4/10], Step [12450/68337], Loss: 5.2217\n",
      "Epoch [4/10], Step [12525/68337], Loss: 5.0509\n",
      "Epoch [4/10], Step [12600/68337], Loss: 5.0585\n",
      "Epoch [4/10], Step [12675/68337], Loss: 4.8786\n",
      "Epoch [4/10], Step [12750/68337], Loss: 5.0425\n",
      "Epoch [4/10], Step [12825/68337], Loss: 5.2272\n",
      "Epoch [4/10], Step [12900/68337], Loss: 5.2566\n",
      "Epoch [4/10], Step [12975/68337], Loss: 5.2190\n",
      "Epoch [4/10], Step [13050/68337], Loss: 5.1494\n",
      "Epoch [4/10], Step [13125/68337], Loss: 5.2755\n",
      "Epoch [4/10], Step [13200/68337], Loss: 5.2173\n",
      "Epoch [4/10], Step [13275/68337], Loss: 5.0604\n",
      "Epoch [4/10], Step [13350/68337], Loss: 5.1156\n",
      "Epoch [4/10], Step [13425/68337], Loss: 5.1807\n",
      "Epoch [4/10], Step [13500/68337], Loss: 5.0692\n",
      "Epoch [4/10], Step [13575/68337], Loss: 5.2489\n",
      "Epoch [4/10], Step [13650/68337], Loss: 5.0524\n",
      "Epoch [4/10], Step [13725/68337], Loss: 5.1175\n",
      "Epoch [4/10], Step [13800/68337], Loss: 5.1465\n",
      "Epoch [4/10], Step [13875/68337], Loss: 5.1256\n",
      "Epoch [4/10], Step [13950/68337], Loss: 5.3221\n",
      "Epoch [4/10], Step [14025/68337], Loss: 5.0965\n",
      "Epoch [4/10], Step [14100/68337], Loss: 5.1298\n",
      "Epoch [4/10], Step [14175/68337], Loss: 4.9938\n",
      "Epoch [4/10], Step [14250/68337], Loss: 5.1035\n",
      "Epoch [4/10], Step [14325/68337], Loss: 5.0385\n",
      "Epoch [4/10], Step [14400/68337], Loss: 5.1832\n",
      "Epoch [4/10], Step [14475/68337], Loss: 5.0183\n",
      "Epoch [4/10], Step [14550/68337], Loss: 5.2269\n",
      "Epoch [4/10], Step [14625/68337], Loss: 5.1340\n",
      "Epoch [4/10], Step [14700/68337], Loss: 5.2489\n",
      "Epoch [4/10], Step [14775/68337], Loss: 5.2463\n",
      "Epoch [4/10], Step [14850/68337], Loss: 5.2415\n",
      "Epoch [4/10], Step [14925/68337], Loss: 5.0179\n",
      "Epoch [4/10], Step [15000/68337], Loss: 5.1507\n",
      "Epoch [4/10], Step [15075/68337], Loss: 5.1675\n",
      "Epoch [4/10], Step [15150/68337], Loss: 5.3368\n",
      "Epoch [4/10], Step [15225/68337], Loss: 5.0958\n",
      "Epoch [4/10], Step [15300/68337], Loss: 5.1340\n",
      "Epoch [4/10], Step [15375/68337], Loss: 4.9460\n",
      "Epoch [4/10], Step [15450/68337], Loss: 4.9396\n",
      "Epoch [4/10], Step [15525/68337], Loss: 5.1109\n",
      "Epoch [4/10], Step [15600/68337], Loss: 5.1932\n",
      "Epoch [4/10], Step [15675/68337], Loss: 5.0948\n",
      "Epoch [4/10], Step [15750/68337], Loss: 5.1074\n",
      "Epoch [4/10], Step [15825/68337], Loss: 5.1009\n",
      "Epoch [4/10], Step [15900/68337], Loss: 4.9578\n",
      "Epoch [4/10], Step [15975/68337], Loss: 5.2714\n",
      "Epoch [4/10], Step [16050/68337], Loss: 5.2320\n",
      "Epoch [4/10], Step [16125/68337], Loss: 5.0724\n",
      "Epoch [4/10], Step [16200/68337], Loss: 4.9835\n",
      "Epoch [4/10], Step [16275/68337], Loss: 5.3026\n",
      "Epoch [4/10], Step [16350/68337], Loss: 5.0560\n",
      "Epoch [4/10], Step [16425/68337], Loss: 5.1053\n",
      "Epoch [4/10], Step [16500/68337], Loss: 5.4337\n",
      "Epoch [4/10], Step [16575/68337], Loss: 5.0581\n",
      "Epoch [4/10], Step [16650/68337], Loss: 5.1929\n",
      "Epoch [4/10], Step [16725/68337], Loss: 5.3649\n",
      "Epoch [4/10], Step [16800/68337], Loss: 5.3221\n",
      "Epoch [4/10], Step [16875/68337], Loss: 5.2275\n",
      "Epoch [4/10], Step [16950/68337], Loss: 5.2798\n",
      "Epoch [4/10], Step [17025/68337], Loss: 5.3623\n",
      "Epoch [4/10], Step [17100/68337], Loss: 5.1393\n",
      "Epoch [4/10], Step [17175/68337], Loss: 4.9606\n",
      "Epoch [4/10], Step [17250/68337], Loss: 5.2809\n",
      "Epoch [4/10], Step [17325/68337], Loss: 4.9198\n",
      "Epoch [4/10], Step [17400/68337], Loss: 5.1767\n",
      "Epoch [4/10], Step [17475/68337], Loss: 5.1875\n",
      "Epoch [4/10], Step [17550/68337], Loss: 5.3078\n",
      "Epoch [4/10], Step [17625/68337], Loss: 5.0169\n",
      "Epoch [4/10], Step [17700/68337], Loss: 5.2190\n",
      "Epoch [4/10], Step [17775/68337], Loss: 5.2210\n",
      "Epoch [4/10], Step [17850/68337], Loss: 5.0848\n",
      "Epoch [4/10], Step [17925/68337], Loss: 5.0630\n",
      "Epoch [4/10], Step [18000/68337], Loss: 4.9178\n",
      "Epoch [4/10], Step [18075/68337], Loss: 5.0934\n",
      "Epoch [4/10], Step [18150/68337], Loss: 5.1244\n",
      "Epoch [4/10], Step [18225/68337], Loss: 5.0462\n",
      "Epoch [4/10], Step [18300/68337], Loss: 5.1244\n",
      "Epoch [4/10], Step [18375/68337], Loss: 5.2274\n",
      "Epoch [4/10], Step [18450/68337], Loss: 5.3054\n",
      "Epoch [4/10], Step [18525/68337], Loss: 5.2929\n",
      "Epoch [4/10], Step [18600/68337], Loss: 5.0386\n",
      "Epoch [4/10], Step [18675/68337], Loss: 5.1847\n",
      "Epoch [4/10], Step [18750/68337], Loss: 5.0660\n",
      "Epoch [4/10], Step [18825/68337], Loss: 5.1374\n",
      "Epoch [4/10], Step [18900/68337], Loss: 5.2086\n",
      "Epoch [4/10], Step [18975/68337], Loss: 5.0938\n",
      "Epoch [4/10], Step [19050/68337], Loss: 4.9417\n",
      "Epoch [4/10], Step [19125/68337], Loss: 5.0919\n",
      "Epoch [4/10], Step [19200/68337], Loss: 5.0727\n",
      "Epoch [4/10], Step [19275/68337], Loss: 5.3131\n",
      "Epoch [4/10], Step [19350/68337], Loss: 5.0493\n",
      "Epoch [4/10], Step [19425/68337], Loss: 5.2021\n",
      "Epoch [4/10], Step [19500/68337], Loss: 5.1540\n",
      "Epoch [4/10], Step [19575/68337], Loss: 4.9348\n",
      "Epoch [4/10], Step [19650/68337], Loss: 5.2832\n",
      "Epoch [4/10], Step [19725/68337], Loss: 5.2110\n",
      "Epoch [4/10], Step [19800/68337], Loss: 5.3727\n",
      "Epoch [4/10], Step [19875/68337], Loss: 5.1009\n",
      "Epoch [4/10], Step [19950/68337], Loss: 5.2693\n",
      "Validation perplexity: 134.4583968694607\n",
      "Epoch [4/10], Step [20025/68337], Loss: 5.3515\n",
      "Epoch [4/10], Step [20100/68337], Loss: 5.0837\n",
      "Epoch [4/10], Step [20175/68337], Loss: 5.2346\n",
      "Epoch [4/10], Step [20250/68337], Loss: 5.0875\n",
      "Epoch [4/10], Step [20325/68337], Loss: 5.1763\n",
      "Epoch [4/10], Step [20400/68337], Loss: 5.2226\n",
      "Epoch [4/10], Step [20475/68337], Loss: 5.2395\n",
      "Epoch [4/10], Step [20550/68337], Loss: 4.9401\n",
      "Epoch [4/10], Step [20625/68337], Loss: 5.1090\n",
      "Epoch [4/10], Step [20700/68337], Loss: 5.2833\n",
      "Epoch [4/10], Step [20775/68337], Loss: 5.0469\n",
      "Epoch [4/10], Step [20850/68337], Loss: 5.0260\n",
      "Epoch [4/10], Step [20925/68337], Loss: 5.0029\n",
      "Epoch [4/10], Step [21000/68337], Loss: 5.1616\n",
      "Epoch [4/10], Step [21075/68337], Loss: 5.1376\n",
      "Epoch [4/10], Step [21150/68337], Loss: 5.1913\n",
      "Epoch [4/10], Step [21225/68337], Loss: 5.0255\n",
      "Epoch [4/10], Step [21300/68337], Loss: 5.1580\n",
      "Epoch [4/10], Step [21375/68337], Loss: 5.0144\n",
      "Epoch [4/10], Step [21450/68337], Loss: 5.0962\n",
      "Epoch [4/10], Step [21525/68337], Loss: 5.1667\n",
      "Epoch [4/10], Step [21600/68337], Loss: 5.3203\n",
      "Epoch [4/10], Step [21675/68337], Loss: 5.1242\n",
      "Epoch [4/10], Step [21750/68337], Loss: 5.1252\n",
      "Epoch [4/10], Step [21825/68337], Loss: 5.3007\n",
      "Epoch [4/10], Step [21900/68337], Loss: 5.4097\n",
      "Epoch [4/10], Step [21975/68337], Loss: 5.2313\n",
      "Epoch [4/10], Step [22050/68337], Loss: 5.0346\n",
      "Epoch [4/10], Step [22125/68337], Loss: 5.1050\n",
      "Epoch [4/10], Step [22200/68337], Loss: 5.3175\n",
      "Epoch [4/10], Step [22275/68337], Loss: 5.3186\n",
      "Epoch [4/10], Step [22350/68337], Loss: 5.0691\n",
      "Epoch [4/10], Step [22425/68337], Loss: 5.1517\n",
      "Epoch [4/10], Step [22500/68337], Loss: 5.2612\n",
      "Epoch [4/10], Step [22575/68337], Loss: 5.3428\n",
      "Epoch [4/10], Step [22650/68337], Loss: 5.0356\n",
      "Epoch [4/10], Step [22725/68337], Loss: 5.0984\n",
      "Epoch [4/10], Step [22800/68337], Loss: 5.4040\n",
      "Epoch [4/10], Step [22875/68337], Loss: 5.0507\n",
      "Epoch [4/10], Step [22950/68337], Loss: 4.8986\n",
      "Epoch [4/10], Step [23025/68337], Loss: 5.1541\n",
      "Epoch [4/10], Step [23100/68337], Loss: 4.9867\n",
      "Epoch [4/10], Step [23175/68337], Loss: 5.0489\n",
      "Epoch [4/10], Step [23250/68337], Loss: 5.2935\n",
      "Epoch [4/10], Step [23325/68337], Loss: 5.0490\n",
      "Epoch [4/10], Step [23400/68337], Loss: 5.3422\n",
      "Epoch [4/10], Step [23475/68337], Loss: 5.0629\n",
      "Epoch [4/10], Step [23550/68337], Loss: 5.1619\n",
      "Epoch [4/10], Step [23625/68337], Loss: 5.1249\n",
      "Epoch [4/10], Step [23700/68337], Loss: 5.2397\n",
      "Epoch [4/10], Step [23775/68337], Loss: 5.0407\n",
      "Epoch [4/10], Step [23850/68337], Loss: 5.1020\n",
      "Epoch [4/10], Step [23925/68337], Loss: 5.0818\n",
      "Epoch [4/10], Step [24000/68337], Loss: 5.0527\n",
      "Epoch [4/10], Step [24075/68337], Loss: 5.1127\n",
      "Epoch [4/10], Step [24150/68337], Loss: 5.1967\n",
      "Epoch [4/10], Step [24225/68337], Loss: 4.9603\n",
      "Epoch [4/10], Step [24300/68337], Loss: 5.3292\n",
      "Epoch [4/10], Step [24375/68337], Loss: 5.1578\n",
      "Epoch [4/10], Step [24450/68337], Loss: 4.8711\n",
      "Epoch [4/10], Step [24525/68337], Loss: 5.2817\n",
      "Epoch [4/10], Step [24600/68337], Loss: 4.9760\n",
      "Epoch [4/10], Step [24675/68337], Loss: 5.0180\n",
      "Epoch [4/10], Step [24750/68337], Loss: 5.0284\n",
      "Epoch [4/10], Step [24825/68337], Loss: 5.0796\n",
      "Epoch [4/10], Step [24900/68337], Loss: 5.0317\n",
      "Epoch [4/10], Step [24975/68337], Loss: 5.1240\n",
      "Epoch [4/10], Step [25050/68337], Loss: 5.1768\n",
      "Epoch [4/10], Step [25125/68337], Loss: 5.2691\n",
      "Epoch [4/10], Step [25200/68337], Loss: 5.2035\n",
      "Epoch [4/10], Step [25275/68337], Loss: 5.1109\n",
      "Epoch [4/10], Step [25350/68337], Loss: 5.2157\n",
      "Epoch [4/10], Step [25425/68337], Loss: 5.2761\n",
      "Epoch [4/10], Step [25500/68337], Loss: 5.3110\n",
      "Epoch [4/10], Step [25575/68337], Loss: 5.1406\n",
      "Epoch [4/10], Step [25650/68337], Loss: 4.9261\n",
      "Epoch [4/10], Step [25725/68337], Loss: 4.9477\n",
      "Epoch [4/10], Step [25800/68337], Loss: 5.2593\n",
      "Epoch [4/10], Step [25875/68337], Loss: 5.2402\n",
      "Epoch [4/10], Step [25950/68337], Loss: 5.2177\n",
      "Epoch [4/10], Step [26025/68337], Loss: 5.2008\n",
      "Epoch [4/10], Step [26100/68337], Loss: 5.0856\n",
      "Epoch [4/10], Step [26175/68337], Loss: 5.0561\n",
      "Epoch [4/10], Step [26250/68337], Loss: 5.1679\n",
      "Epoch [4/10], Step [26325/68337], Loss: 5.0527\n",
      "Epoch [4/10], Step [26400/68337], Loss: 5.0383\n",
      "Epoch [4/10], Step [26475/68337], Loss: 5.1113\n",
      "Epoch [4/10], Step [26550/68337], Loss: 4.9558\n",
      "Epoch [4/10], Step [26625/68337], Loss: 5.0678\n",
      "Epoch [4/10], Step [26700/68337], Loss: 5.1070\n",
      "Epoch [4/10], Step [26775/68337], Loss: 5.2777\n",
      "Epoch [4/10], Step [26850/68337], Loss: 5.2338\n",
      "Epoch [4/10], Step [26925/68337], Loss: 5.2383\n",
      "Epoch [4/10], Step [27000/68337], Loss: 4.9432\n",
      "Epoch [4/10], Step [27075/68337], Loss: 5.0993\n",
      "Epoch [4/10], Step [27150/68337], Loss: 5.1249\n",
      "Epoch [4/10], Step [27225/68337], Loss: 5.1091\n",
      "Epoch [4/10], Step [27300/68337], Loss: 5.2512\n",
      "Epoch [4/10], Step [27375/68337], Loss: 4.7920\n",
      "Epoch [4/10], Step [27450/68337], Loss: 5.0862\n",
      "Epoch [4/10], Step [27525/68337], Loss: 5.1761\n",
      "Epoch [4/10], Step [27600/68337], Loss: 5.2535\n",
      "Epoch [4/10], Step [27675/68337], Loss: 4.9531\n",
      "Epoch [4/10], Step [27750/68337], Loss: 5.0408\n",
      "Epoch [4/10], Step [27825/68337], Loss: 5.1433\n",
      "Epoch [4/10], Step [27900/68337], Loss: 5.2356\n",
      "Epoch [4/10], Step [27975/68337], Loss: 5.2068\n",
      "Epoch [4/10], Step [28050/68337], Loss: 5.1059\n",
      "Epoch [4/10], Step [28125/68337], Loss: 5.4845\n",
      "Epoch [4/10], Step [28200/68337], Loss: 4.8484\n",
      "Epoch [4/10], Step [28275/68337], Loss: 5.1649\n",
      "Epoch [4/10], Step [28350/68337], Loss: 5.1215\n",
      "Epoch [4/10], Step [28425/68337], Loss: 5.0438\n",
      "Epoch [4/10], Step [28500/68337], Loss: 4.9815\n",
      "Epoch [4/10], Step [28575/68337], Loss: 4.9868\n",
      "Epoch [4/10], Step [28650/68337], Loss: 5.2260\n",
      "Epoch [4/10], Step [28725/68337], Loss: 5.0584\n",
      "Epoch [4/10], Step [28800/68337], Loss: 5.1198\n",
      "Epoch [4/10], Step [28875/68337], Loss: 5.0053\n",
      "Epoch [4/10], Step [28950/68337], Loss: 5.2221\n",
      "Epoch [4/10], Step [29025/68337], Loss: 5.1218\n",
      "Epoch [4/10], Step [29100/68337], Loss: 5.1644\n",
      "Epoch [4/10], Step [29175/68337], Loss: 5.3386\n",
      "Epoch [4/10], Step [29250/68337], Loss: 5.2401\n",
      "Epoch [4/10], Step [29325/68337], Loss: 5.1953\n",
      "Epoch [4/10], Step [29400/68337], Loss: 5.1098\n",
      "Epoch [4/10], Step [29475/68337], Loss: 5.1770\n",
      "Epoch [4/10], Step [29550/68337], Loss: 5.2152\n",
      "Epoch [4/10], Step [29625/68337], Loss: 5.3409\n",
      "Epoch [4/10], Step [29700/68337], Loss: 5.2097\n",
      "Epoch [4/10], Step [29775/68337], Loss: 5.1489\n",
      "Epoch [4/10], Step [29850/68337], Loss: 5.0900\n",
      "Epoch [4/10], Step [29925/68337], Loss: 5.1209\n",
      "Epoch [4/10], Step [30000/68337], Loss: 5.3743\n",
      "Validation perplexity: 133.73167049625226\n",
      "Epoch [4/10], Step [30075/68337], Loss: 5.0372\n",
      "Epoch [4/10], Step [30150/68337], Loss: 4.9107\n",
      "Epoch [4/10], Step [30225/68337], Loss: 5.2082\n",
      "Epoch [4/10], Step [30300/68337], Loss: 5.0390\n",
      "Epoch [4/10], Step [30375/68337], Loss: 5.1773\n",
      "Epoch [4/10], Step [30450/68337], Loss: 5.1927\n",
      "Epoch [4/10], Step [30525/68337], Loss: 5.1141\n",
      "Epoch [4/10], Step [30600/68337], Loss: 5.2308\n",
      "Epoch [4/10], Step [30675/68337], Loss: 5.1703\n",
      "Epoch [4/10], Step [30750/68337], Loss: 5.2811\n",
      "Epoch [4/10], Step [30825/68337], Loss: 5.0585\n",
      "Epoch [4/10], Step [30900/68337], Loss: 5.2185\n",
      "Epoch [4/10], Step [30975/68337], Loss: 5.0840\n",
      "Epoch [4/10], Step [31050/68337], Loss: 4.9872\n",
      "Epoch [4/10], Step [31125/68337], Loss: 5.0564\n",
      "Epoch [4/10], Step [31200/68337], Loss: 4.9544\n",
      "Epoch [4/10], Step [31275/68337], Loss: 5.0843\n",
      "Epoch [4/10], Step [31350/68337], Loss: 4.9520\n",
      "Epoch [4/10], Step [31425/68337], Loss: 5.2870\n",
      "Epoch [4/10], Step [31500/68337], Loss: 5.0959\n",
      "Epoch [4/10], Step [31575/68337], Loss: 5.0791\n",
      "Epoch [4/10], Step [31650/68337], Loss: 5.3762\n",
      "Epoch [4/10], Step [31725/68337], Loss: 4.9618\n",
      "Epoch [4/10], Step [31800/68337], Loss: 4.9958\n",
      "Epoch [4/10], Step [31875/68337], Loss: 5.2192\n",
      "Epoch [4/10], Step [31950/68337], Loss: 5.2843\n",
      "Epoch [4/10], Step [32025/68337], Loss: 5.3899\n",
      "Epoch [4/10], Step [32100/68337], Loss: 5.2702\n",
      "Epoch [4/10], Step [32175/68337], Loss: 5.0629\n",
      "Epoch [4/10], Step [32250/68337], Loss: 5.0155\n",
      "Epoch [4/10], Step [32325/68337], Loss: 5.0155\n",
      "Epoch [4/10], Step [32400/68337], Loss: 4.8609\n",
      "Epoch [4/10], Step [32475/68337], Loss: 5.1658\n",
      "Epoch [4/10], Step [32550/68337], Loss: 5.1515\n",
      "Epoch [4/10], Step [32625/68337], Loss: 5.2712\n",
      "Epoch [4/10], Step [32700/68337], Loss: 5.1048\n",
      "Epoch [4/10], Step [32775/68337], Loss: 5.1617\n",
      "Epoch [4/10], Step [32850/68337], Loss: 5.1793\n",
      "Epoch [4/10], Step [32925/68337], Loss: 5.1289\n",
      "Epoch [4/10], Step [33000/68337], Loss: 5.0656\n",
      "Epoch [4/10], Step [33075/68337], Loss: 5.0011\n",
      "Epoch [4/10], Step [33150/68337], Loss: 4.9720\n",
      "Epoch [4/10], Step [33225/68337], Loss: 5.1785\n",
      "Epoch [4/10], Step [33300/68337], Loss: 5.0998\n",
      "Epoch [4/10], Step [33375/68337], Loss: 5.2119\n",
      "Epoch [4/10], Step [33450/68337], Loss: 5.2085\n",
      "Epoch [4/10], Step [33525/68337], Loss: 5.0495\n",
      "Epoch [4/10], Step [33600/68337], Loss: 5.1436\n",
      "Epoch [4/10], Step [33675/68337], Loss: 5.0543\n",
      "Epoch [4/10], Step [33750/68337], Loss: 5.0956\n",
      "Epoch [4/10], Step [33825/68337], Loss: 5.3783\n",
      "Epoch [4/10], Step [33900/68337], Loss: 5.0061\n",
      "Epoch [4/10], Step [33975/68337], Loss: 5.0920\n",
      "Epoch [4/10], Step [34050/68337], Loss: 5.2903\n",
      "Epoch [4/10], Step [34125/68337], Loss: 5.0867\n",
      "Epoch [4/10], Step [34200/68337], Loss: 5.1844\n",
      "Epoch [4/10], Step [34275/68337], Loss: 5.2615\n",
      "Epoch [4/10], Step [34350/68337], Loss: 5.3219\n",
      "Epoch [4/10], Step [34425/68337], Loss: 5.3018\n",
      "Epoch [4/10], Step [34500/68337], Loss: 5.1410\n",
      "Epoch [4/10], Step [34575/68337], Loss: 5.0773\n",
      "Epoch [4/10], Step [34650/68337], Loss: 4.9284\n",
      "Epoch [4/10], Step [34725/68337], Loss: 5.1570\n",
      "Epoch [4/10], Step [34800/68337], Loss: 5.0848\n",
      "Epoch [4/10], Step [34875/68337], Loss: 5.0281\n",
      "Epoch [4/10], Step [34950/68337], Loss: 5.0096\n",
      "Epoch [4/10], Step [35025/68337], Loss: 5.1087\n",
      "Epoch [4/10], Step [35100/68337], Loss: 5.0529\n",
      "Epoch [4/10], Step [35175/68337], Loss: 5.1110\n",
      "Epoch [4/10], Step [35250/68337], Loss: 5.0850\n",
      "Epoch [4/10], Step [35325/68337], Loss: 4.9856\n",
      "Epoch [4/10], Step [35400/68337], Loss: 5.0160\n",
      "Epoch [4/10], Step [35475/68337], Loss: 5.1094\n",
      "Epoch [4/10], Step [35550/68337], Loss: 5.0727\n",
      "Epoch [4/10], Step [35625/68337], Loss: 5.1133\n",
      "Epoch [4/10], Step [35700/68337], Loss: 5.1597\n",
      "Epoch [4/10], Step [35775/68337], Loss: 5.2098\n",
      "Epoch [4/10], Step [35850/68337], Loss: 5.2012\n",
      "Epoch [4/10], Step [35925/68337], Loss: 5.2530\n",
      "Epoch [4/10], Step [36000/68337], Loss: 5.2372\n",
      "Epoch [4/10], Step [36075/68337], Loss: 5.2418\n",
      "Epoch [4/10], Step [36150/68337], Loss: 5.2167\n",
      "Epoch [4/10], Step [36225/68337], Loss: 5.2618\n",
      "Epoch [4/10], Step [36300/68337], Loss: 4.8834\n",
      "Epoch [4/10], Step [36375/68337], Loss: 5.2492\n",
      "Epoch [4/10], Step [36450/68337], Loss: 5.2487\n",
      "Epoch [4/10], Step [36525/68337], Loss: 5.1636\n",
      "Epoch [4/10], Step [36600/68337], Loss: 5.2002\n",
      "Epoch [4/10], Step [36675/68337], Loss: 4.9972\n",
      "Epoch [4/10], Step [36750/68337], Loss: 5.2179\n",
      "Epoch [4/10], Step [36825/68337], Loss: 5.0701\n",
      "Epoch [4/10], Step [36900/68337], Loss: 5.2131\n",
      "Epoch [4/10], Step [36975/68337], Loss: 5.2002\n",
      "Epoch [4/10], Step [37050/68337], Loss: 5.3441\n",
      "Epoch [4/10], Step [37125/68337], Loss: 5.2835\n",
      "Epoch [4/10], Step [37200/68337], Loss: 5.0179\n",
      "Epoch [4/10], Step [37275/68337], Loss: 5.1765\n",
      "Epoch [4/10], Step [37350/68337], Loss: 5.0065\n",
      "Epoch [4/10], Step [37425/68337], Loss: 5.4428\n",
      "Epoch [4/10], Step [37500/68337], Loss: 5.0255\n",
      "Epoch [4/10], Step [37575/68337], Loss: 5.0426\n",
      "Epoch [4/10], Step [37650/68337], Loss: 5.1877\n",
      "Epoch [4/10], Step [37725/68337], Loss: 5.1376\n",
      "Epoch [4/10], Step [37800/68337], Loss: 4.9731\n",
      "Epoch [4/10], Step [37875/68337], Loss: 5.0588\n",
      "Epoch [4/10], Step [37950/68337], Loss: 5.2491\n",
      "Epoch [4/10], Step [38025/68337], Loss: 5.2453\n",
      "Epoch [4/10], Step [38100/68337], Loss: 5.3038\n",
      "Epoch [4/10], Step [38175/68337], Loss: 5.1478\n",
      "Epoch [4/10], Step [38250/68337], Loss: 5.0172\n",
      "Epoch [4/10], Step [38325/68337], Loss: 5.1632\n",
      "Epoch [4/10], Step [38400/68337], Loss: 5.1661\n",
      "Epoch [4/10], Step [38475/68337], Loss: 5.3618\n",
      "Epoch [4/10], Step [38550/68337], Loss: 5.2906\n",
      "Epoch [4/10], Step [38625/68337], Loss: 5.0988\n",
      "Epoch [4/10], Step [38700/68337], Loss: 5.1921\n",
      "Epoch [4/10], Step [38775/68337], Loss: 5.1052\n",
      "Epoch [4/10], Step [38850/68337], Loss: 4.9242\n",
      "Epoch [4/10], Step [38925/68337], Loss: 5.1492\n",
      "Epoch [4/10], Step [39000/68337], Loss: 5.2475\n",
      "Epoch [4/10], Step [39075/68337], Loss: 5.2372\n",
      "Epoch [4/10], Step [39150/68337], Loss: 5.1666\n",
      "Epoch [4/10], Step [39225/68337], Loss: 5.2087\n",
      "Epoch [4/10], Step [39300/68337], Loss: 4.9370\n",
      "Epoch [4/10], Step [39375/68337], Loss: 5.1774\n",
      "Epoch [4/10], Step [39450/68337], Loss: 5.1899\n",
      "Epoch [4/10], Step [39525/68337], Loss: 5.1097\n",
      "Epoch [4/10], Step [39600/68337], Loss: 5.2259\n",
      "Epoch [4/10], Step [39675/68337], Loss: 5.4999\n",
      "Epoch [4/10], Step [39750/68337], Loss: 5.1970\n",
      "Epoch [4/10], Step [39825/68337], Loss: 5.1990\n",
      "Epoch [4/10], Step [39900/68337], Loss: 5.1489\n",
      "Epoch [4/10], Step [39975/68337], Loss: 5.0518\n",
      "Validation perplexity: 133.08810424523404\n",
      "Epoch [4/10], Step [40050/68337], Loss: 5.1049\n",
      "Epoch [4/10], Step [40125/68337], Loss: 5.0388\n",
      "Epoch [4/10], Step [40200/68337], Loss: 5.0811\n",
      "Epoch [4/10], Step [40275/68337], Loss: 5.2143\n",
      "Epoch [4/10], Step [40350/68337], Loss: 5.1721\n",
      "Epoch [4/10], Step [40425/68337], Loss: 5.1779\n",
      "Epoch [4/10], Step [40500/68337], Loss: 5.2574\n",
      "Epoch [4/10], Step [40575/68337], Loss: 5.0950\n",
      "Epoch [4/10], Step [40650/68337], Loss: 5.1486\n",
      "Epoch [4/10], Step [40725/68337], Loss: 5.1925\n",
      "Epoch [4/10], Step [40800/68337], Loss: 5.0828\n",
      "Epoch [4/10], Step [40875/68337], Loss: 5.2097\n",
      "Epoch [4/10], Step [40950/68337], Loss: 5.1424\n",
      "Epoch [4/10], Step [41025/68337], Loss: 5.1350\n",
      "Epoch [4/10], Step [41100/68337], Loss: 5.1396\n",
      "Epoch [4/10], Step [41175/68337], Loss: 5.0579\n",
      "Epoch [4/10], Step [41250/68337], Loss: 5.0652\n",
      "Epoch [4/10], Step [41325/68337], Loss: 5.3352\n",
      "Epoch [4/10], Step [41400/68337], Loss: 5.3420\n",
      "Epoch [4/10], Step [41475/68337], Loss: 5.0408\n",
      "Epoch [4/10], Step [41550/68337], Loss: 5.3916\n",
      "Epoch [4/10], Step [41625/68337], Loss: 5.1425\n",
      "Epoch [4/10], Step [41700/68337], Loss: 5.1172\n",
      "Epoch [4/10], Step [41775/68337], Loss: 5.3541\n",
      "Epoch [4/10], Step [41850/68337], Loss: 5.1805\n",
      "Epoch [4/10], Step [41925/68337], Loss: 5.0121\n",
      "Epoch [4/10], Step [42000/68337], Loss: 5.2327\n",
      "Epoch [4/10], Step [42075/68337], Loss: 5.2119\n",
      "Epoch [4/10], Step [42150/68337], Loss: 5.0054\n",
      "Epoch [4/10], Step [42225/68337], Loss: 5.2518\n",
      "Epoch [4/10], Step [42300/68337], Loss: 5.2326\n",
      "Epoch [4/10], Step [42375/68337], Loss: 5.1803\n",
      "Epoch [4/10], Step [42450/68337], Loss: 5.1022\n",
      "Epoch [4/10], Step [42525/68337], Loss: 5.2408\n",
      "Epoch [4/10], Step [42600/68337], Loss: 4.8631\n",
      "Epoch [4/10], Step [42675/68337], Loss: 5.2366\n",
      "Epoch [4/10], Step [42750/68337], Loss: 5.1718\n",
      "Epoch [4/10], Step [42825/68337], Loss: 5.3762\n",
      "Epoch [4/10], Step [42900/68337], Loss: 4.9475\n",
      "Epoch [4/10], Step [42975/68337], Loss: 5.3641\n",
      "Epoch [4/10], Step [43050/68337], Loss: 5.3425\n",
      "Epoch [4/10], Step [43125/68337], Loss: 5.1116\n",
      "Epoch [4/10], Step [43200/68337], Loss: 5.0023\n",
      "Epoch [4/10], Step [43275/68337], Loss: 5.0058\n",
      "Epoch [4/10], Step [43350/68337], Loss: 5.1128\n",
      "Epoch [4/10], Step [43425/68337], Loss: 5.1537\n",
      "Epoch [4/10], Step [43500/68337], Loss: 5.2358\n",
      "Epoch [4/10], Step [43575/68337], Loss: 5.1714\n",
      "Epoch [4/10], Step [43650/68337], Loss: 5.0171\n",
      "Epoch [4/10], Step [43725/68337], Loss: 5.0439\n",
      "Epoch [4/10], Step [43800/68337], Loss: 4.9436\n",
      "Epoch [4/10], Step [43875/68337], Loss: 5.0866\n",
      "Epoch [4/10], Step [43950/68337], Loss: 5.0782\n",
      "Epoch [4/10], Step [44025/68337], Loss: 5.2141\n",
      "Epoch [4/10], Step [44100/68337], Loss: 4.9843\n",
      "Epoch [4/10], Step [44175/68337], Loss: 5.0173\n",
      "Epoch [4/10], Step [44250/68337], Loss: 5.2423\n",
      "Epoch [4/10], Step [44325/68337], Loss: 5.1400\n",
      "Epoch [4/10], Step [44400/68337], Loss: 5.1049\n",
      "Epoch [4/10], Step [44475/68337], Loss: 5.1652\n",
      "Epoch [4/10], Step [44550/68337], Loss: 5.1151\n",
      "Epoch [4/10], Step [44625/68337], Loss: 5.0841\n",
      "Epoch [4/10], Step [44700/68337], Loss: 5.0812\n",
      "Epoch [4/10], Step [44775/68337], Loss: 5.2932\n",
      "Epoch [4/10], Step [44850/68337], Loss: 5.2571\n",
      "Epoch [4/10], Step [44925/68337], Loss: 4.9395\n",
      "Epoch [4/10], Step [45000/68337], Loss: 5.2488\n",
      "Epoch [4/10], Step [45075/68337], Loss: 5.1305\n",
      "Epoch [4/10], Step [45150/68337], Loss: 5.1199\n",
      "Epoch [4/10], Step [45225/68337], Loss: 5.1443\n",
      "Epoch [4/10], Step [45300/68337], Loss: 5.0819\n",
      "Epoch [4/10], Step [45375/68337], Loss: 5.1249\n",
      "Epoch [4/10], Step [45450/68337], Loss: 5.3421\n",
      "Epoch [4/10], Step [45525/68337], Loss: 5.0084\n",
      "Epoch [4/10], Step [45600/68337], Loss: 5.1792\n",
      "Epoch [4/10], Step [45675/68337], Loss: 5.1893\n",
      "Epoch [4/10], Step [45750/68337], Loss: 5.3555\n",
      "Epoch [4/10], Step [45825/68337], Loss: 5.1117\n",
      "Epoch [4/10], Step [45900/68337], Loss: 5.1858\n",
      "Epoch [4/10], Step [45975/68337], Loss: 4.9905\n",
      "Epoch [4/10], Step [46050/68337], Loss: 5.0871\n",
      "Epoch [4/10], Step [46125/68337], Loss: 5.0763\n",
      "Epoch [4/10], Step [46200/68337], Loss: 4.8959\n",
      "Epoch [4/10], Step [46275/68337], Loss: 4.8739\n",
      "Epoch [4/10], Step [46350/68337], Loss: 5.2326\n",
      "Epoch [4/10], Step [46425/68337], Loss: 5.1294\n",
      "Epoch [4/10], Step [46500/68337], Loss: 5.2546\n",
      "Epoch [4/10], Step [46575/68337], Loss: 5.0985\n",
      "Epoch [4/10], Step [46650/68337], Loss: 5.2042\n",
      "Epoch [4/10], Step [46725/68337], Loss: 5.2344\n",
      "Epoch [4/10], Step [46800/68337], Loss: 5.2604\n",
      "Epoch [4/10], Step [46875/68337], Loss: 4.9626\n",
      "Epoch [4/10], Step [46950/68337], Loss: 5.0066\n",
      "Epoch [4/10], Step [47025/68337], Loss: 5.0850\n",
      "Epoch [4/10], Step [47100/68337], Loss: 5.1984\n",
      "Epoch [4/10], Step [47175/68337], Loss: 5.3681\n",
      "Epoch [4/10], Step [47250/68337], Loss: 5.2037\n",
      "Epoch [4/10], Step [47325/68337], Loss: 5.2982\n",
      "Epoch [4/10], Step [47400/68337], Loss: 5.1901\n",
      "Epoch [4/10], Step [47475/68337], Loss: 5.3264\n",
      "Epoch [4/10], Step [47550/68337], Loss: 5.1851\n",
      "Epoch [4/10], Step [47625/68337], Loss: 5.1634\n",
      "Epoch [4/10], Step [47700/68337], Loss: 4.9624\n",
      "Epoch [4/10], Step [47775/68337], Loss: 4.9477\n",
      "Epoch [4/10], Step [47850/68337], Loss: 5.0931\n",
      "Epoch [4/10], Step [47925/68337], Loss: 5.0609\n",
      "Epoch [4/10], Step [48000/68337], Loss: 5.0166\n",
      "Epoch [4/10], Step [48075/68337], Loss: 5.1821\n",
      "Epoch [4/10], Step [48150/68337], Loss: 5.2781\n",
      "Epoch [4/10], Step [48225/68337], Loss: 5.2717\n",
      "Epoch [4/10], Step [48300/68337], Loss: 5.2228\n",
      "Epoch [4/10], Step [48375/68337], Loss: 5.2021\n",
      "Epoch [4/10], Step [48450/68337], Loss: 5.0625\n",
      "Epoch [4/10], Step [48525/68337], Loss: 4.9407\n",
      "Epoch [4/10], Step [48600/68337], Loss: 5.2165\n",
      "Epoch [4/10], Step [48675/68337], Loss: 5.0375\n",
      "Epoch [4/10], Step [48750/68337], Loss: 5.0410\n",
      "Epoch [4/10], Step [48825/68337], Loss: 4.9651\n",
      "Epoch [4/10], Step [48900/68337], Loss: 4.9446\n",
      "Epoch [4/10], Step [48975/68337], Loss: 4.9732\n",
      "Epoch [4/10], Step [49050/68337], Loss: 5.3257\n",
      "Epoch [4/10], Step [49125/68337], Loss: 4.9198\n",
      "Epoch [4/10], Step [49200/68337], Loss: 5.0103\n",
      "Epoch [4/10], Step [49275/68337], Loss: 4.9828\n",
      "Epoch [4/10], Step [49350/68337], Loss: 5.1906\n",
      "Epoch [4/10], Step [49425/68337], Loss: 5.1463\n",
      "Epoch [4/10], Step [49500/68337], Loss: 5.2085\n",
      "Epoch [4/10], Step [49575/68337], Loss: 5.2007\n",
      "Epoch [4/10], Step [49650/68337], Loss: 5.1476\n",
      "Epoch [4/10], Step [49725/68337], Loss: 5.1283\n",
      "Epoch [4/10], Step [49800/68337], Loss: 5.0475\n",
      "Epoch [4/10], Step [49875/68337], Loss: 5.1283\n",
      "Epoch [4/10], Step [49950/68337], Loss: 5.2479\n",
      "Validation perplexity: 131.85507329443288\n",
      "Epoch [4/10], Step [50025/68337], Loss: 5.2358\n",
      "Epoch [4/10], Step [50100/68337], Loss: 5.0333\n",
      "Epoch [4/10], Step [50175/68337], Loss: 5.0288\n",
      "Epoch [4/10], Step [50250/68337], Loss: 5.1018\n",
      "Epoch [4/10], Step [50325/68337], Loss: 5.0504\n",
      "Epoch [4/10], Step [50400/68337], Loss: 5.0655\n",
      "Epoch [4/10], Step [50475/68337], Loss: 5.1980\n",
      "Epoch [4/10], Step [50550/68337], Loss: 5.2099\n",
      "Epoch [4/10], Step [50625/68337], Loss: 5.0711\n",
      "Epoch [4/10], Step [50700/68337], Loss: 5.0731\n",
      "Epoch [4/10], Step [50775/68337], Loss: 5.2396\n",
      "Epoch [4/10], Step [50850/68337], Loss: 5.1502\n",
      "Epoch [4/10], Step [50925/68337], Loss: 5.0781\n",
      "Epoch [4/10], Step [51000/68337], Loss: 5.1002\n",
      "Epoch [4/10], Step [51075/68337], Loss: 5.3890\n",
      "Epoch [4/10], Step [51150/68337], Loss: 5.0620\n",
      "Epoch [4/10], Step [51225/68337], Loss: 5.2760\n",
      "Epoch [4/10], Step [51300/68337], Loss: 5.0043\n",
      "Epoch [4/10], Step [51375/68337], Loss: 5.3203\n",
      "Epoch [4/10], Step [51450/68337], Loss: 5.2824\n",
      "Epoch [4/10], Step [51525/68337], Loss: 5.4479\n",
      "Epoch [4/10], Step [51600/68337], Loss: 5.3485\n",
      "Epoch [4/10], Step [51675/68337], Loss: 5.0296\n",
      "Epoch [4/10], Step [51750/68337], Loss: 5.0404\n",
      "Epoch [4/10], Step [51825/68337], Loss: 5.3442\n",
      "Epoch [4/10], Step [51900/68337], Loss: 5.2941\n",
      "Epoch [4/10], Step [51975/68337], Loss: 5.0319\n",
      "Epoch [4/10], Step [52050/68337], Loss: 5.2586\n",
      "Epoch [4/10], Step [52125/68337], Loss: 5.0304\n",
      "Epoch [4/10], Step [52200/68337], Loss: 5.1471\n",
      "Epoch [4/10], Step [52275/68337], Loss: 5.2758\n",
      "Epoch [4/10], Step [52350/68337], Loss: 5.1911\n",
      "Epoch [4/10], Step [52425/68337], Loss: 5.2272\n",
      "Epoch [4/10], Step [52500/68337], Loss: 5.0687\n",
      "Epoch [4/10], Step [52575/68337], Loss: 5.1062\n",
      "Epoch [4/10], Step [52650/68337], Loss: 4.9514\n",
      "Epoch [4/10], Step [52725/68337], Loss: 5.1384\n",
      "Epoch [4/10], Step [52800/68337], Loss: 5.2159\n",
      "Epoch [4/10], Step [52875/68337], Loss: 5.0072\n",
      "Epoch [4/10], Step [52950/68337], Loss: 4.7242\n",
      "Epoch [4/10], Step [53025/68337], Loss: 5.3011\n",
      "Epoch [4/10], Step [53100/68337], Loss: 5.1290\n",
      "Epoch [4/10], Step [53175/68337], Loss: 5.1438\n",
      "Epoch [4/10], Step [53250/68337], Loss: 5.0315\n",
      "Epoch [4/10], Step [53325/68337], Loss: 5.2382\n",
      "Epoch [4/10], Step [53400/68337], Loss: 5.1277\n",
      "Epoch [4/10], Step [53475/68337], Loss: 5.1884\n",
      "Epoch [4/10], Step [53550/68337], Loss: 5.1936\n",
      "Epoch [4/10], Step [53625/68337], Loss: 5.1602\n",
      "Epoch [4/10], Step [53700/68337], Loss: 5.1141\n",
      "Epoch [4/10], Step [53775/68337], Loss: 5.2117\n",
      "Epoch [4/10], Step [53850/68337], Loss: 4.9864\n",
      "Epoch [4/10], Step [53925/68337], Loss: 5.0266\n",
      "Epoch [4/10], Step [54000/68337], Loss: 5.0576\n",
      "Epoch [4/10], Step [54075/68337], Loss: 5.1273\n",
      "Epoch [4/10], Step [54150/68337], Loss: 5.3208\n",
      "Epoch [4/10], Step [54225/68337], Loss: 5.1319\n",
      "Epoch [4/10], Step [54300/68337], Loss: 5.1821\n",
      "Epoch [4/10], Step [54375/68337], Loss: 5.0271\n",
      "Epoch [4/10], Step [54450/68337], Loss: 5.0924\n",
      "Epoch [4/10], Step [54525/68337], Loss: 5.1963\n",
      "Epoch [4/10], Step [54600/68337], Loss: 5.2971\n",
      "Epoch [4/10], Step [54675/68337], Loss: 5.2314\n",
      "Epoch [4/10], Step [54750/68337], Loss: 5.2188\n",
      "Epoch [4/10], Step [54825/68337], Loss: 5.0545\n",
      "Epoch [4/10], Step [54900/68337], Loss: 5.0286\n",
      "Epoch [4/10], Step [54975/68337], Loss: 4.9704\n",
      "Epoch [4/10], Step [55050/68337], Loss: 5.0044\n",
      "Epoch [4/10], Step [55125/68337], Loss: 5.0959\n",
      "Epoch [4/10], Step [55200/68337], Loss: 5.3513\n",
      "Epoch [4/10], Step [55275/68337], Loss: 5.2256\n",
      "Epoch [4/10], Step [55350/68337], Loss: 5.2186\n",
      "Epoch [4/10], Step [55425/68337], Loss: 5.1224\n",
      "Epoch [4/10], Step [55500/68337], Loss: 5.2901\n",
      "Epoch [4/10], Step [55575/68337], Loss: 5.2414\n",
      "Epoch [4/10], Step [55650/68337], Loss: 5.0427\n",
      "Epoch [4/10], Step [55725/68337], Loss: 5.1987\n",
      "Epoch [4/10], Step [55800/68337], Loss: 5.2982\n",
      "Epoch [4/10], Step [55875/68337], Loss: 5.1769\n",
      "Epoch [4/10], Step [55950/68337], Loss: 5.2317\n",
      "Epoch [4/10], Step [56025/68337], Loss: 5.2412\n",
      "Epoch [4/10], Step [56100/68337], Loss: 5.1423\n",
      "Epoch [4/10], Step [56175/68337], Loss: 5.4216\n",
      "Epoch [4/10], Step [56250/68337], Loss: 5.2162\n",
      "Epoch [4/10], Step [56325/68337], Loss: 5.0699\n",
      "Epoch [4/10], Step [56400/68337], Loss: 5.0260\n",
      "Epoch [4/10], Step [56475/68337], Loss: 5.0703\n",
      "Epoch [4/10], Step [56550/68337], Loss: 5.3606\n",
      "Epoch [4/10], Step [56625/68337], Loss: 4.9655\n",
      "Epoch [4/10], Step [56700/68337], Loss: 5.2779\n",
      "Epoch [4/10], Step [56775/68337], Loss: 4.9946\n",
      "Epoch [4/10], Step [56850/68337], Loss: 5.1047\n",
      "Epoch [4/10], Step [56925/68337], Loss: 5.1717\n",
      "Epoch [4/10], Step [57000/68337], Loss: 5.2063\n",
      "Epoch [4/10], Step [57075/68337], Loss: 5.2326\n",
      "Epoch [4/10], Step [57150/68337], Loss: 4.9698\n",
      "Epoch [4/10], Step [57225/68337], Loss: 5.1623\n",
      "Epoch [4/10], Step [57300/68337], Loss: 5.2210\n",
      "Epoch [4/10], Step [57375/68337], Loss: 5.0710\n",
      "Epoch [4/10], Step [57450/68337], Loss: 5.2398\n",
      "Epoch [4/10], Step [57525/68337], Loss: 5.0583\n",
      "Epoch [4/10], Step [57600/68337], Loss: 4.9753\n",
      "Epoch [4/10], Step [57675/68337], Loss: 4.9984\n",
      "Epoch [4/10], Step [57750/68337], Loss: 4.9912\n",
      "Epoch [4/10], Step [57825/68337], Loss: 5.0423\n",
      "Epoch [4/10], Step [57900/68337], Loss: 5.1860\n",
      "Epoch [4/10], Step [57975/68337], Loss: 5.2787\n",
      "Epoch [4/10], Step [58050/68337], Loss: 5.3910\n",
      "Epoch [4/10], Step [58125/68337], Loss: 5.2189\n",
      "Epoch [4/10], Step [58200/68337], Loss: 5.4251\n",
      "Epoch [4/10], Step [58275/68337], Loss: 5.2922\n",
      "Epoch [4/10], Step [58350/68337], Loss: 5.3411\n",
      "Epoch [4/10], Step [58425/68337], Loss: 5.0857\n",
      "Epoch [4/10], Step [58500/68337], Loss: 5.1366\n",
      "Epoch [4/10], Step [58575/68337], Loss: 5.1065\n",
      "Epoch [4/10], Step [58650/68337], Loss: 5.2608\n",
      "Epoch [4/10], Step [58725/68337], Loss: 5.0908\n",
      "Epoch [4/10], Step [58800/68337], Loss: 5.3234\n",
      "Epoch [4/10], Step [58875/68337], Loss: 5.0666\n",
      "Epoch [4/10], Step [58950/68337], Loss: 5.1010\n",
      "Epoch [4/10], Step [59025/68337], Loss: 5.1158\n",
      "Epoch [4/10], Step [59100/68337], Loss: 5.1455\n",
      "Epoch [4/10], Step [59175/68337], Loss: 5.0833\n",
      "Epoch [4/10], Step [59250/68337], Loss: 5.0594\n",
      "Epoch [4/10], Step [59325/68337], Loss: 5.2118\n",
      "Epoch [4/10], Step [59400/68337], Loss: 5.2268\n",
      "Epoch [4/10], Step [59475/68337], Loss: 5.3185\n",
      "Epoch [4/10], Step [59550/68337], Loss: 4.9182\n",
      "Epoch [4/10], Step [59625/68337], Loss: 5.3605\n",
      "Epoch [4/10], Step [59700/68337], Loss: 5.3142\n",
      "Epoch [4/10], Step [59775/68337], Loss: 5.1548\n",
      "Epoch [4/10], Step [59850/68337], Loss: 5.1186\n",
      "Epoch [4/10], Step [59925/68337], Loss: 5.4366\n",
      "Epoch [4/10], Step [60000/68337], Loss: 5.2618\n",
      "Validation perplexity: 132.15354202916825\n",
      "Epoch [4/10], Step [60075/68337], Loss: 4.9263\n",
      "Epoch [4/10], Step [60150/68337], Loss: 5.1390\n",
      "Epoch [4/10], Step [60225/68337], Loss: 4.9274\n",
      "Epoch [4/10], Step [60300/68337], Loss: 5.0668\n",
      "Epoch [4/10], Step [60375/68337], Loss: 5.1411\n",
      "Epoch [4/10], Step [60450/68337], Loss: 5.1482\n",
      "Epoch [4/10], Step [60525/68337], Loss: 5.1629\n",
      "Epoch [4/10], Step [60600/68337], Loss: 5.1363\n",
      "Epoch [4/10], Step [60675/68337], Loss: 5.2440\n",
      "Epoch [4/10], Step [60750/68337], Loss: 5.2069\n",
      "Epoch [4/10], Step [60825/68337], Loss: 5.1416\n",
      "Epoch [4/10], Step [60900/68337], Loss: 5.1522\n",
      "Epoch [4/10], Step [60975/68337], Loss: 4.9704\n",
      "Epoch [4/10], Step [61050/68337], Loss: 5.2321\n",
      "Epoch [4/10], Step [61125/68337], Loss: 5.1540\n",
      "Epoch [4/10], Step [61200/68337], Loss: 5.1977\n",
      "Epoch [4/10], Step [61275/68337], Loss: 5.3046\n",
      "Epoch [4/10], Step [61350/68337], Loss: 5.1860\n",
      "Epoch [4/10], Step [61425/68337], Loss: 5.2117\n",
      "Epoch [4/10], Step [61500/68337], Loss: 5.1624\n",
      "Epoch [4/10], Step [61575/68337], Loss: 5.2314\n",
      "Epoch [4/10], Step [61650/68337], Loss: 5.4050\n",
      "Epoch [4/10], Step [61725/68337], Loss: 5.2851\n",
      "Epoch [4/10], Step [61800/68337], Loss: 5.3454\n",
      "Epoch [4/10], Step [61875/68337], Loss: 5.1797\n",
      "Epoch [4/10], Step [61950/68337], Loss: 5.0006\n",
      "Epoch [4/10], Step [62025/68337], Loss: 5.1293\n",
      "Epoch [4/10], Step [62100/68337], Loss: 4.9059\n",
      "Epoch [4/10], Step [62175/68337], Loss: 5.3621\n",
      "Epoch [4/10], Step [62250/68337], Loss: 5.2333\n",
      "Epoch [4/10], Step [62325/68337], Loss: 5.1820\n",
      "Epoch [4/10], Step [62400/68337], Loss: 5.2328\n",
      "Epoch [4/10], Step [62475/68337], Loss: 5.2259\n",
      "Epoch [4/10], Step [62550/68337], Loss: 5.3328\n",
      "Epoch [4/10], Step [62625/68337], Loss: 5.3777\n",
      "Epoch [4/10], Step [62700/68337], Loss: 5.3738\n",
      "Epoch [4/10], Step [62775/68337], Loss: 5.0637\n",
      "Epoch [4/10], Step [62850/68337], Loss: 4.9705\n",
      "Epoch [4/10], Step [62925/68337], Loss: 5.3612\n",
      "Epoch [4/10], Step [63000/68337], Loss: 5.4174\n",
      "Epoch [4/10], Step [63075/68337], Loss: 5.2699\n",
      "Epoch [4/10], Step [63150/68337], Loss: 5.0401\n",
      "Epoch [4/10], Step [63225/68337], Loss: 5.3455\n",
      "Epoch [4/10], Step [63300/68337], Loss: 5.1640\n",
      "Epoch [4/10], Step [63375/68337], Loss: 4.9993\n",
      "Epoch [4/10], Step [63450/68337], Loss: 4.9681\n",
      "Epoch [4/10], Step [63525/68337], Loss: 5.3330\n",
      "Epoch [4/10], Step [63600/68337], Loss: 5.2963\n",
      "Epoch [4/10], Step [63675/68337], Loss: 5.0981\n",
      "Epoch [4/10], Step [63750/68337], Loss: 5.1337\n",
      "Epoch [4/10], Step [63825/68337], Loss: 5.1616\n",
      "Epoch [4/10], Step [63900/68337], Loss: 5.0084\n",
      "Epoch [4/10], Step [63975/68337], Loss: 5.1375\n",
      "Epoch [4/10], Step [64050/68337], Loss: 5.1679\n",
      "Epoch [4/10], Step [64125/68337], Loss: 5.0787\n",
      "Epoch [4/10], Step [64200/68337], Loss: 5.1429\n",
      "Epoch [4/10], Step [64275/68337], Loss: 5.1639\n",
      "Epoch [4/10], Step [64350/68337], Loss: 5.2212\n",
      "Epoch [4/10], Step [64425/68337], Loss: 5.2285\n",
      "Epoch [4/10], Step [64500/68337], Loss: 5.1490\n",
      "Epoch [4/10], Step [64575/68337], Loss: 5.1888\n",
      "Epoch [4/10], Step [64650/68337], Loss: 5.1799\n",
      "Epoch [4/10], Step [64725/68337], Loss: 5.1642\n",
      "Epoch [4/10], Step [64800/68337], Loss: 5.1284\n",
      "Epoch [4/10], Step [64875/68337], Loss: 5.0736\n",
      "Epoch [4/10], Step [64950/68337], Loss: 5.1066\n",
      "Epoch [4/10], Step [65025/68337], Loss: 5.0621\n",
      "Epoch [4/10], Step [65100/68337], Loss: 5.1121\n",
      "Epoch [4/10], Step [65175/68337], Loss: 5.1438\n",
      "Epoch [4/10], Step [65250/68337], Loss: 5.1008\n",
      "Epoch [4/10], Step [65325/68337], Loss: 5.1796\n",
      "Epoch [4/10], Step [65400/68337], Loss: 5.1832\n",
      "Epoch [4/10], Step [65475/68337], Loss: 5.0881\n",
      "Epoch [4/10], Step [65550/68337], Loss: 5.0816\n",
      "Epoch [4/10], Step [65625/68337], Loss: 5.0007\n",
      "Epoch [4/10], Step [65700/68337], Loss: 5.2238\n",
      "Epoch [4/10], Step [65775/68337], Loss: 4.9742\n",
      "Epoch [4/10], Step [65850/68337], Loss: 5.3251\n",
      "Epoch [4/10], Step [65925/68337], Loss: 5.2277\n",
      "Epoch [4/10], Step [66000/68337], Loss: 5.2134\n",
      "Epoch [4/10], Step [66075/68337], Loss: 5.1217\n",
      "Epoch [4/10], Step [66150/68337], Loss: 5.1888\n",
      "Epoch [4/10], Step [66225/68337], Loss: 5.0863\n",
      "Epoch [4/10], Step [66300/68337], Loss: 4.9815\n",
      "Epoch [4/10], Step [66375/68337], Loss: 5.2347\n",
      "Epoch [4/10], Step [66450/68337], Loss: 5.0959\n",
      "Epoch [4/10], Step [66525/68337], Loss: 5.0529\n",
      "Epoch [4/10], Step [66600/68337], Loss: 5.1975\n",
      "Epoch [4/10], Step [66675/68337], Loss: 5.2025\n",
      "Epoch [4/10], Step [66750/68337], Loss: 5.1533\n",
      "Epoch [4/10], Step [66825/68337], Loss: 5.2154\n",
      "Epoch [4/10], Step [66900/68337], Loss: 5.1629\n",
      "Epoch [4/10], Step [66975/68337], Loss: 4.9706\n",
      "Epoch [4/10], Step [67050/68337], Loss: 5.2456\n",
      "Epoch [4/10], Step [67125/68337], Loss: 5.2283\n",
      "Epoch [4/10], Step [67200/68337], Loss: 5.1300\n",
      "Epoch [4/10], Step [67275/68337], Loss: 5.1705\n",
      "Epoch [4/10], Step [67350/68337], Loss: 4.9443\n",
      "Epoch [4/10], Step [67425/68337], Loss: 5.2534\n",
      "Epoch [4/10], Step [67500/68337], Loss: 5.3649\n",
      "Epoch [4/10], Step [67575/68337], Loss: 4.9985\n",
      "Epoch [4/10], Step [67650/68337], Loss: 5.1762\n",
      "Epoch [4/10], Step [67725/68337], Loss: 5.1803\n",
      "Epoch [4/10], Step [67800/68337], Loss: 5.3694\n",
      "Epoch [4/10], Step [67875/68337], Loss: 5.1773\n",
      "Epoch [4/10], Step [67950/68337], Loss: 5.1415\n",
      "Epoch [4/10], Step [68025/68337], Loss: 5.3456\n",
      "Epoch [4/10], Step [68100/68337], Loss: 5.2545\n",
      "Epoch [4/10], Step [68175/68337], Loss: 5.2123\n",
      "Epoch [4/10], Step [68250/68337], Loss: 5.1526\n",
      "Epoch [4/10], Step [68325/68337], Loss: 5.2731\n",
      "Epoch [4/10] Average Loss: 5.1454, Perplexity: 171.63\n",
      "Epoch [5/10], Step [0/68337], Loss: 5.2450\n",
      "Validation perplexity: 131.33088539159982\n",
      "Epoch [5/10], Step [75/68337], Loss: 5.0717\n",
      "Epoch [5/10], Step [150/68337], Loss: 5.0830\n",
      "Epoch [5/10], Step [225/68337], Loss: 4.9737\n",
      "Epoch [5/10], Step [300/68337], Loss: 5.2578\n",
      "Epoch [5/10], Step [375/68337], Loss: 5.2215\n",
      "Epoch [5/10], Step [450/68337], Loss: 5.1035\n",
      "Epoch [5/10], Step [525/68337], Loss: 5.0227\n",
      "Epoch [5/10], Step [600/68337], Loss: 5.1426\n",
      "Epoch [5/10], Step [675/68337], Loss: 5.0985\n",
      "Epoch [5/10], Step [750/68337], Loss: 5.0022\n",
      "Epoch [5/10], Step [825/68337], Loss: 5.0875\n",
      "Epoch [5/10], Step [900/68337], Loss: 5.0454\n",
      "Epoch [5/10], Step [975/68337], Loss: 5.2651\n",
      "Epoch [5/10], Step [1050/68337], Loss: 4.8833\n",
      "Epoch [5/10], Step [1125/68337], Loss: 5.1420\n",
      "Epoch [5/10], Step [1200/68337], Loss: 5.1183\n",
      "Epoch [5/10], Step [1275/68337], Loss: 5.1650\n",
      "Epoch [5/10], Step [1350/68337], Loss: 5.1566\n",
      "Epoch [5/10], Step [1425/68337], Loss: 5.1476\n",
      "Epoch [5/10], Step [1500/68337], Loss: 5.1025\n",
      "Epoch [5/10], Step [1575/68337], Loss: 5.1268\n",
      "Epoch [5/10], Step [1650/68337], Loss: 5.1642\n",
      "Epoch [5/10], Step [1725/68337], Loss: 5.0267\n",
      "Epoch [5/10], Step [1800/68337], Loss: 5.1799\n",
      "Epoch [5/10], Step [1875/68337], Loss: 4.9461\n",
      "Epoch [5/10], Step [1950/68337], Loss: 5.1396\n",
      "Epoch [5/10], Step [2025/68337], Loss: 5.0547\n",
      "Epoch [5/10], Step [2100/68337], Loss: 4.9774\n",
      "Epoch [5/10], Step [2175/68337], Loss: 5.0283\n",
      "Epoch [5/10], Step [2250/68337], Loss: 5.1800\n",
      "Epoch [5/10], Step [2325/68337], Loss: 5.0443\n",
      "Epoch [5/10], Step [2400/68337], Loss: 5.0523\n",
      "Epoch [5/10], Step [2475/68337], Loss: 5.1291\n",
      "Epoch [5/10], Step [2550/68337], Loss: 5.2124\n",
      "Epoch [5/10], Step [2625/68337], Loss: 5.2355\n",
      "Epoch [5/10], Step [2700/68337], Loss: 5.1497\n",
      "Epoch [5/10], Step [2775/68337], Loss: 4.9733\n",
      "Epoch [5/10], Step [2850/68337], Loss: 5.0822\n",
      "Epoch [5/10], Step [2925/68337], Loss: 5.1625\n",
      "Epoch [5/10], Step [3000/68337], Loss: 5.1838\n",
      "Epoch [5/10], Step [3075/68337], Loss: 5.1406\n",
      "Epoch [5/10], Step [3150/68337], Loss: 4.9976\n",
      "Epoch [5/10], Step [3225/68337], Loss: 4.9537\n",
      "Epoch [5/10], Step [3300/68337], Loss: 5.1918\n",
      "Epoch [5/10], Step [3375/68337], Loss: 4.9617\n",
      "Epoch [5/10], Step [3450/68337], Loss: 5.1143\n",
      "Epoch [5/10], Step [3525/68337], Loss: 5.1315\n",
      "Epoch [5/10], Step [3600/68337], Loss: 5.1182\n",
      "Epoch [5/10], Step [3675/68337], Loss: 4.9539\n",
      "Epoch [5/10], Step [3750/68337], Loss: 5.1252\n",
      "Epoch [5/10], Step [3825/68337], Loss: 5.0245\n",
      "Epoch [5/10], Step [3900/68337], Loss: 5.0842\n",
      "Epoch [5/10], Step [3975/68337], Loss: 5.2824\n",
      "Epoch [5/10], Step [4050/68337], Loss: 5.1108\n",
      "Epoch [5/10], Step [4125/68337], Loss: 4.9048\n",
      "Epoch [5/10], Step [4200/68337], Loss: 5.1391\n",
      "Epoch [5/10], Step [4275/68337], Loss: 5.2070\n",
      "Epoch [5/10], Step [4350/68337], Loss: 4.9735\n",
      "Epoch [5/10], Step [4425/68337], Loss: 4.8080\n",
      "Epoch [5/10], Step [4500/68337], Loss: 5.0195\n",
      "Epoch [5/10], Step [4575/68337], Loss: 5.2491\n",
      "Epoch [5/10], Step [4650/68337], Loss: 5.3331\n",
      "Epoch [5/10], Step [4725/68337], Loss: 5.1636\n",
      "Epoch [5/10], Step [4800/68337], Loss: 4.9610\n",
      "Epoch [5/10], Step [4875/68337], Loss: 5.0637\n",
      "Epoch [5/10], Step [4950/68337], Loss: 5.2947\n",
      "Epoch [5/10], Step [5025/68337], Loss: 5.1255\n",
      "Epoch [5/10], Step [5100/68337], Loss: 5.2818\n",
      "Epoch [5/10], Step [5175/68337], Loss: 5.0864\n",
      "Epoch [5/10], Step [5250/68337], Loss: 5.0448\n",
      "Epoch [5/10], Step [5325/68337], Loss: 5.2317\n",
      "Epoch [5/10], Step [5400/68337], Loss: 5.1455\n",
      "Epoch [5/10], Step [5475/68337], Loss: 5.0461\n",
      "Epoch [5/10], Step [5550/68337], Loss: 5.1611\n",
      "Epoch [5/10], Step [5625/68337], Loss: 5.1316\n",
      "Epoch [5/10], Step [5700/68337], Loss: 5.0002\n",
      "Epoch [5/10], Step [5775/68337], Loss: 5.2417\n",
      "Epoch [5/10], Step [5850/68337], Loss: 5.3180\n",
      "Epoch [5/10], Step [5925/68337], Loss: 5.1257\n",
      "Epoch [5/10], Step [6000/68337], Loss: 5.2328\n",
      "Epoch [5/10], Step [6075/68337], Loss: 4.7983\n",
      "Epoch [5/10], Step [6150/68337], Loss: 5.2628\n",
      "Epoch [5/10], Step [6225/68337], Loss: 5.1876\n",
      "Epoch [5/10], Step [6300/68337], Loss: 5.0431\n",
      "Epoch [5/10], Step [6375/68337], Loss: 5.2108\n",
      "Epoch [5/10], Step [6450/68337], Loss: 5.0529\n",
      "Epoch [5/10], Step [6525/68337], Loss: 4.9822\n",
      "Epoch [5/10], Step [6600/68337], Loss: 5.1258\n",
      "Epoch [5/10], Step [6675/68337], Loss: 5.1005\n",
      "Epoch [5/10], Step [6750/68337], Loss: 5.0273\n",
      "Epoch [5/10], Step [6825/68337], Loss: 5.4168\n",
      "Epoch [5/10], Step [6900/68337], Loss: 4.9205\n",
      "Epoch [5/10], Step [6975/68337], Loss: 5.0316\n",
      "Epoch [5/10], Step [7050/68337], Loss: 5.0169\n",
      "Epoch [5/10], Step [7125/68337], Loss: 5.0882\n",
      "Epoch [5/10], Step [7200/68337], Loss: 5.2168\n",
      "Epoch [5/10], Step [7275/68337], Loss: 5.0145\n",
      "Epoch [5/10], Step [7350/68337], Loss: 5.1899\n",
      "Epoch [5/10], Step [7425/68337], Loss: 4.9859\n",
      "Epoch [5/10], Step [7500/68337], Loss: 5.2907\n",
      "Epoch [5/10], Step [7575/68337], Loss: 5.2497\n",
      "Epoch [5/10], Step [7650/68337], Loss: 5.1530\n",
      "Epoch [5/10], Step [7725/68337], Loss: 5.1125\n",
      "Epoch [5/10], Step [7800/68337], Loss: 5.0332\n",
      "Epoch [5/10], Step [7875/68337], Loss: 5.0324\n",
      "Epoch [5/10], Step [7950/68337], Loss: 5.1462\n",
      "Epoch [5/10], Step [8025/68337], Loss: 5.0386\n",
      "Epoch [5/10], Step [8100/68337], Loss: 5.0088\n",
      "Epoch [5/10], Step [8175/68337], Loss: 4.9980\n",
      "Epoch [5/10], Step [8250/68337], Loss: 5.0692\n",
      "Epoch [5/10], Step [8325/68337], Loss: 5.1564\n",
      "Epoch [5/10], Step [8400/68337], Loss: 5.2068\n",
      "Epoch [5/10], Step [8475/68337], Loss: 4.9991\n",
      "Epoch [5/10], Step [8550/68337], Loss: 5.1128\n",
      "Epoch [5/10], Step [8625/68337], Loss: 5.1670\n",
      "Epoch [5/10], Step [8700/68337], Loss: 5.2667\n",
      "Epoch [5/10], Step [8775/68337], Loss: 5.0341\n",
      "Epoch [5/10], Step [8850/68337], Loss: 5.1731\n",
      "Epoch [5/10], Step [8925/68337], Loss: 5.0529\n",
      "Epoch [5/10], Step [9000/68337], Loss: 4.9301\n",
      "Epoch [5/10], Step [9075/68337], Loss: 4.9149\n",
      "Epoch [5/10], Step [9150/68337], Loss: 5.2115\n",
      "Epoch [5/10], Step [9225/68337], Loss: 5.2853\n",
      "Epoch [5/10], Step [9300/68337], Loss: 5.2266\n",
      "Epoch [5/10], Step [9375/68337], Loss: 5.1571\n",
      "Epoch [5/10], Step [9450/68337], Loss: 5.0919\n",
      "Epoch [5/10], Step [9525/68337], Loss: 5.0683\n",
      "Epoch [5/10], Step [9600/68337], Loss: 5.1916\n",
      "Epoch [5/10], Step [9675/68337], Loss: 5.1434\n",
      "Epoch [5/10], Step [9750/68337], Loss: 5.0857\n",
      "Epoch [5/10], Step [9825/68337], Loss: 5.1510\n",
      "Epoch [5/10], Step [9900/68337], Loss: 5.2640\n",
      "Epoch [5/10], Step [9975/68337], Loss: 5.0127\n",
      "Validation perplexity: 130.3583399730733\n",
      "Epoch [5/10], Step [10050/68337], Loss: 4.8916\n",
      "Epoch [5/10], Step [10125/68337], Loss: 5.0180\n",
      "Epoch [5/10], Step [10200/68337], Loss: 5.1028\n",
      "Epoch [5/10], Step [10275/68337], Loss: 5.1559\n",
      "Epoch [5/10], Step [10350/68337], Loss: 5.2332\n",
      "Epoch [5/10], Step [10425/68337], Loss: 5.0329\n",
      "Epoch [5/10], Step [10500/68337], Loss: 5.0975\n",
      "Epoch [5/10], Step [10575/68337], Loss: 4.9987\n",
      "Epoch [5/10], Step [10650/68337], Loss: 5.1636\n",
      "Epoch [5/10], Step [10725/68337], Loss: 5.0697\n",
      "Epoch [5/10], Step [10800/68337], Loss: 5.0878\n",
      "Epoch [5/10], Step [10875/68337], Loss: 5.3032\n",
      "Epoch [5/10], Step [10950/68337], Loss: 5.2303\n",
      "Epoch [5/10], Step [11025/68337], Loss: 5.1767\n",
      "Epoch [5/10], Step [11100/68337], Loss: 5.0901\n",
      "Epoch [5/10], Step [11175/68337], Loss: 4.9958\n",
      "Epoch [5/10], Step [11250/68337], Loss: 5.2050\n",
      "Epoch [5/10], Step [11325/68337], Loss: 5.0720\n",
      "Epoch [5/10], Step [11400/68337], Loss: 4.9282\n",
      "Epoch [5/10], Step [11475/68337], Loss: 5.0366\n",
      "Epoch [5/10], Step [11550/68337], Loss: 5.1537\n",
      "Epoch [5/10], Step [11625/68337], Loss: 5.0688\n",
      "Epoch [5/10], Step [11700/68337], Loss: 5.0743\n",
      "Epoch [5/10], Step [11775/68337], Loss: 5.0674\n",
      "Epoch [5/10], Step [11850/68337], Loss: 5.0825\n",
      "Epoch [5/10], Step [11925/68337], Loss: 5.0403\n",
      "Epoch [5/10], Step [12000/68337], Loss: 5.2277\n",
      "Epoch [5/10], Step [12075/68337], Loss: 5.1396\n",
      "Epoch [5/10], Step [12150/68337], Loss: 5.1539\n",
      "Epoch [5/10], Step [12225/68337], Loss: 5.2225\n",
      "Epoch [5/10], Step [12300/68337], Loss: 5.0860\n",
      "Epoch [5/10], Step [12375/68337], Loss: 5.1779\n",
      "Epoch [5/10], Step [12450/68337], Loss: 5.0864\n",
      "Epoch [5/10], Step [12525/68337], Loss: 5.0944\n",
      "Epoch [5/10], Step [12600/68337], Loss: 5.1396\n",
      "Epoch [5/10], Step [12675/68337], Loss: 5.0299\n",
      "Epoch [5/10], Step [12750/68337], Loss: 5.4002\n",
      "Epoch [5/10], Step [12825/68337], Loss: 5.2143\n",
      "Epoch [5/10], Step [12900/68337], Loss: 4.9659\n",
      "Epoch [5/10], Step [12975/68337], Loss: 4.9440\n",
      "Epoch [5/10], Step [13050/68337], Loss: 5.0531\n",
      "Epoch [5/10], Step [13125/68337], Loss: 5.1506\n",
      "Epoch [5/10], Step [13200/68337], Loss: 4.9389\n",
      "Epoch [5/10], Step [13275/68337], Loss: 5.2973\n",
      "Epoch [5/10], Step [13350/68337], Loss: 5.0850\n",
      "Epoch [5/10], Step [13425/68337], Loss: 5.0795\n",
      "Epoch [5/10], Step [13500/68337], Loss: 5.0202\n",
      "Epoch [5/10], Step [13575/68337], Loss: 5.0683\n",
      "Epoch [5/10], Step [13650/68337], Loss: 5.1354\n",
      "Epoch [5/10], Step [13725/68337], Loss: 5.0194\n",
      "Epoch [5/10], Step [13800/68337], Loss: 5.0832\n",
      "Epoch [5/10], Step [13875/68337], Loss: 5.1453\n",
      "Epoch [5/10], Step [13950/68337], Loss: 5.0316\n",
      "Epoch [5/10], Step [14025/68337], Loss: 5.2013\n",
      "Epoch [5/10], Step [14100/68337], Loss: 5.3546\n",
      "Epoch [5/10], Step [14175/68337], Loss: 5.0790\n",
      "Epoch [5/10], Step [14250/68337], Loss: 5.2333\n",
      "Epoch [5/10], Step [14325/68337], Loss: 4.9569\n",
      "Epoch [5/10], Step [14400/68337], Loss: 5.2020\n",
      "Epoch [5/10], Step [14475/68337], Loss: 5.4380\n",
      "Epoch [5/10], Step [14550/68337], Loss: 4.9631\n",
      "Epoch [5/10], Step [14625/68337], Loss: 5.2238\n",
      "Epoch [5/10], Step [14700/68337], Loss: 5.2813\n",
      "Epoch [5/10], Step [14775/68337], Loss: 4.9952\n",
      "Epoch [5/10], Step [14850/68337], Loss: 5.0970\n",
      "Epoch [5/10], Step [14925/68337], Loss: 5.1081\n",
      "Epoch [5/10], Step [15000/68337], Loss: 5.1465\n",
      "Epoch [5/10], Step [15075/68337], Loss: 4.9863\n",
      "Epoch [5/10], Step [15150/68337], Loss: 4.9562\n",
      "Epoch [5/10], Step [15225/68337], Loss: 5.1983\n",
      "Epoch [5/10], Step [15300/68337], Loss: 5.0513\n",
      "Epoch [5/10], Step [15375/68337], Loss: 5.1665\n",
      "Epoch [5/10], Step [15450/68337], Loss: 5.0745\n",
      "Epoch [5/10], Step [15525/68337], Loss: 5.0177\n",
      "Epoch [5/10], Step [15600/68337], Loss: 5.1203\n",
      "Epoch [5/10], Step [15675/68337], Loss: 5.2401\n",
      "Epoch [5/10], Step [15750/68337], Loss: 5.1978\n",
      "Epoch [5/10], Step [15825/68337], Loss: 5.1035\n",
      "Epoch [5/10], Step [15900/68337], Loss: 5.0958\n",
      "Epoch [5/10], Step [15975/68337], Loss: 5.0985\n",
      "Epoch [5/10], Step [16050/68337], Loss: 5.0432\n",
      "Epoch [5/10], Step [16125/68337], Loss: 5.2534\n",
      "Epoch [5/10], Step [16200/68337], Loss: 5.0565\n",
      "Epoch [5/10], Step [16275/68337], Loss: 5.0180\n",
      "Epoch [5/10], Step [16350/68337], Loss: 5.1879\n",
      "Epoch [5/10], Step [16425/68337], Loss: 5.0689\n",
      "Epoch [5/10], Step [16500/68337], Loss: 5.2755\n",
      "Epoch [5/10], Step [16575/68337], Loss: 5.2975\n",
      "Epoch [5/10], Step [16650/68337], Loss: 4.8771\n",
      "Epoch [5/10], Step [16725/68337], Loss: 5.2981\n",
      "Epoch [5/10], Step [16800/68337], Loss: 5.0357\n",
      "Epoch [5/10], Step [16875/68337], Loss: 5.1426\n",
      "Epoch [5/10], Step [16950/68337], Loss: 5.1595\n",
      "Epoch [5/10], Step [17025/68337], Loss: 5.4112\n",
      "Epoch [5/10], Step [17100/68337], Loss: 4.9146\n",
      "Epoch [5/10], Step [17175/68337], Loss: 5.0565\n",
      "Epoch [5/10], Step [17250/68337], Loss: 5.2236\n",
      "Epoch [5/10], Step [17325/68337], Loss: 5.0036\n",
      "Epoch [5/10], Step [17400/68337], Loss: 5.1567\n",
      "Epoch [5/10], Step [17475/68337], Loss: 5.0713\n",
      "Epoch [5/10], Step [17550/68337], Loss: 5.1247\n",
      "Epoch [5/10], Step [17625/68337], Loss: 5.2549\n",
      "Epoch [5/10], Step [17700/68337], Loss: 5.1578\n",
      "Epoch [5/10], Step [17775/68337], Loss: 5.0749\n",
      "Epoch [5/10], Step [17850/68337], Loss: 5.2248\n",
      "Epoch [5/10], Step [17925/68337], Loss: 5.1396\n",
      "Epoch [5/10], Step [18000/68337], Loss: 5.1349\n",
      "Epoch [5/10], Step [18075/68337], Loss: 5.3609\n",
      "Epoch [5/10], Step [18150/68337], Loss: 5.2285\n",
      "Epoch [5/10], Step [18225/68337], Loss: 5.1978\n",
      "Epoch [5/10], Step [18300/68337], Loss: 5.0764\n",
      "Epoch [5/10], Step [18375/68337], Loss: 5.0818\n",
      "Epoch [5/10], Step [18450/68337], Loss: 5.2596\n",
      "Epoch [5/10], Step [18525/68337], Loss: 5.0409\n",
      "Epoch [5/10], Step [18600/68337], Loss: 5.0530\n",
      "Epoch [5/10], Step [18675/68337], Loss: 5.0203\n",
      "Epoch [5/10], Step [18750/68337], Loss: 4.9732\n",
      "Epoch [5/10], Step [18825/68337], Loss: 5.1000\n",
      "Epoch [5/10], Step [18900/68337], Loss: 5.1409\n",
      "Epoch [5/10], Step [18975/68337], Loss: 5.1647\n",
      "Epoch [5/10], Step [19050/68337], Loss: 5.1901\n",
      "Epoch [5/10], Step [19125/68337], Loss: 5.2537\n",
      "Epoch [5/10], Step [19200/68337], Loss: 5.0012\n",
      "Epoch [5/10], Step [19275/68337], Loss: 5.2653\n",
      "Epoch [5/10], Step [19350/68337], Loss: 4.9328\n",
      "Epoch [5/10], Step [19425/68337], Loss: 5.0938\n",
      "Epoch [5/10], Step [19500/68337], Loss: 5.1651\n",
      "Epoch [5/10], Step [19575/68337], Loss: 5.2256\n",
      "Epoch [5/10], Step [19650/68337], Loss: 5.0703\n",
      "Epoch [5/10], Step [19725/68337], Loss: 5.1825\n",
      "Epoch [5/10], Step [19800/68337], Loss: 5.0813\n",
      "Epoch [5/10], Step [19875/68337], Loss: 4.9756\n",
      "Epoch [5/10], Step [19950/68337], Loss: 4.9588\n",
      "Validation perplexity: 130.2564851553962\n",
      "Epoch [5/10], Step [20025/68337], Loss: 5.0439\n",
      "Epoch [5/10], Step [20100/68337], Loss: 5.0874\n",
      "Epoch [5/10], Step [20175/68337], Loss: 5.0818\n",
      "Epoch [5/10], Step [20250/68337], Loss: 5.2640\n",
      "Epoch [5/10], Step [20325/68337], Loss: 5.0703\n",
      "Epoch [5/10], Step [20400/68337], Loss: 5.1099\n",
      "Epoch [5/10], Step [20475/68337], Loss: 4.8309\n",
      "Epoch [5/10], Step [20550/68337], Loss: 5.0898\n",
      "Epoch [5/10], Step [20625/68337], Loss: 5.0787\n",
      "Epoch [5/10], Step [20700/68337], Loss: 5.1122\n",
      "Epoch [5/10], Step [20775/68337], Loss: 5.1410\n",
      "Epoch [5/10], Step [20850/68337], Loss: 4.9610\n",
      "Epoch [5/10], Step [20925/68337], Loss: 5.1850\n",
      "Epoch [5/10], Step [21000/68337], Loss: 4.8671\n",
      "Epoch [5/10], Step [21075/68337], Loss: 4.8791\n",
      "Epoch [5/10], Step [21150/68337], Loss: 5.1071\n",
      "Epoch [5/10], Step [21225/68337], Loss: 5.1462\n",
      "Epoch [5/10], Step [21300/68337], Loss: 5.2505\n",
      "Epoch [5/10], Step [21375/68337], Loss: 5.2336\n",
      "Epoch [5/10], Step [21450/68337], Loss: 5.1020\n",
      "Epoch [5/10], Step [21525/68337], Loss: 5.2921\n",
      "Epoch [5/10], Step [21600/68337], Loss: 5.0022\n",
      "Epoch [5/10], Step [21675/68337], Loss: 5.1987\n",
      "Epoch [5/10], Step [21750/68337], Loss: 5.1463\n",
      "Epoch [5/10], Step [21825/68337], Loss: 4.9559\n",
      "Epoch [5/10], Step [21900/68337], Loss: 5.0373\n",
      "Epoch [5/10], Step [21975/68337], Loss: 5.1366\n",
      "Epoch [5/10], Step [22050/68337], Loss: 5.1754\n",
      "Epoch [5/10], Step [22125/68337], Loss: 5.1555\n",
      "Epoch [5/10], Step [22200/68337], Loss: 5.0705\n",
      "Epoch [5/10], Step [22275/68337], Loss: 5.1654\n",
      "Epoch [5/10], Step [22350/68337], Loss: 5.2596\n",
      "Epoch [5/10], Step [22425/68337], Loss: 5.0495\n",
      "Epoch [5/10], Step [22500/68337], Loss: 5.3171\n",
      "Epoch [5/10], Step [22575/68337], Loss: 4.9774\n",
      "Epoch [5/10], Step [22650/68337], Loss: 5.1274\n",
      "Epoch [5/10], Step [22725/68337], Loss: 5.1313\n",
      "Epoch [5/10], Step [22800/68337], Loss: 5.2625\n",
      "Epoch [5/10], Step [22875/68337], Loss: 4.8306\n",
      "Epoch [5/10], Step [22950/68337], Loss: 5.3593\n",
      "Epoch [5/10], Step [23025/68337], Loss: 5.0124\n",
      "Epoch [5/10], Step [23100/68337], Loss: 5.1753\n",
      "Epoch [5/10], Step [23175/68337], Loss: 5.2015\n",
      "Epoch [5/10], Step [23250/68337], Loss: 5.0741\n",
      "Epoch [5/10], Step [23325/68337], Loss: 5.2194\n",
      "Epoch [5/10], Step [23400/68337], Loss: 5.1212\n",
      "Epoch [5/10], Step [23475/68337], Loss: 5.1540\n",
      "Epoch [5/10], Step [23550/68337], Loss: 5.1768\n",
      "Epoch [5/10], Step [23625/68337], Loss: 5.2677\n",
      "Epoch [5/10], Step [23700/68337], Loss: 5.2839\n",
      "Epoch [5/10], Step [23775/68337], Loss: 5.2719\n",
      "Epoch [5/10], Step [23850/68337], Loss: 5.1479\n",
      "Epoch [5/10], Step [23925/68337], Loss: 5.1856\n",
      "Epoch [5/10], Step [24000/68337], Loss: 5.1513\n",
      "Epoch [5/10], Step [24075/68337], Loss: 5.1866\n",
      "Epoch [5/10], Step [24150/68337], Loss: 4.9646\n",
      "Epoch [5/10], Step [24225/68337], Loss: 5.2477\n",
      "Epoch [5/10], Step [24300/68337], Loss: 4.9841\n",
      "Epoch [5/10], Step [24375/68337], Loss: 5.2397\n",
      "Epoch [5/10], Step [24450/68337], Loss: 5.0845\n",
      "Epoch [5/10], Step [24525/68337], Loss: 4.9134\n",
      "Epoch [5/10], Step [24600/68337], Loss: 4.9462\n",
      "Epoch [5/10], Step [24675/68337], Loss: 5.0526\n",
      "Epoch [5/10], Step [24750/68337], Loss: 5.1641\n",
      "Epoch [5/10], Step [24825/68337], Loss: 5.0326\n",
      "Epoch [5/10], Step [24900/68337], Loss: 5.2514\n",
      "Epoch [5/10], Step [24975/68337], Loss: 5.1259\n",
      "Epoch [5/10], Step [25050/68337], Loss: 5.0592\n",
      "Epoch [5/10], Step [25125/68337], Loss: 5.1479\n",
      "Epoch [5/10], Step [25200/68337], Loss: 5.0967\n",
      "Epoch [5/10], Step [25275/68337], Loss: 5.2376\n",
      "Epoch [5/10], Step [25350/68337], Loss: 5.1368\n",
      "Epoch [5/10], Step [25425/68337], Loss: 5.2069\n",
      "Epoch [5/10], Step [25500/68337], Loss: 5.0049\n",
      "Epoch [5/10], Step [25575/68337], Loss: 5.0168\n",
      "Epoch [5/10], Step [25650/68337], Loss: 4.9274\n",
      "Epoch [5/10], Step [25725/68337], Loss: 4.9584\n",
      "Epoch [5/10], Step [25800/68337], Loss: 4.9756\n",
      "Epoch [5/10], Step [25875/68337], Loss: 5.2036\n",
      "Epoch [5/10], Step [25950/68337], Loss: 5.2115\n",
      "Epoch [5/10], Step [26025/68337], Loss: 5.1450\n",
      "Epoch [5/10], Step [26100/68337], Loss: 5.2598\n",
      "Epoch [5/10], Step [26175/68337], Loss: 5.3313\n",
      "Epoch [5/10], Step [26250/68337], Loss: 5.1894\n",
      "Epoch [5/10], Step [26325/68337], Loss: 5.1145\n",
      "Epoch [5/10], Step [26400/68337], Loss: 5.0741\n",
      "Epoch [5/10], Step [26475/68337], Loss: 4.9852\n",
      "Epoch [5/10], Step [26550/68337], Loss: 5.1035\n",
      "Epoch [5/10], Step [26625/68337], Loss: 5.2236\n",
      "Epoch [5/10], Step [26700/68337], Loss: 5.1195\n",
      "Epoch [5/10], Step [26775/68337], Loss: 5.1918\n",
      "Epoch [5/10], Step [26850/68337], Loss: 5.1025\n",
      "Epoch [5/10], Step [26925/68337], Loss: 5.0218\n",
      "Epoch [5/10], Step [27000/68337], Loss: 4.9416\n",
      "Epoch [5/10], Step [27075/68337], Loss: 5.0818\n",
      "Epoch [5/10], Step [27150/68337], Loss: 4.8860\n",
      "Epoch [5/10], Step [27225/68337], Loss: 5.0815\n",
      "Epoch [5/10], Step [27300/68337], Loss: 5.0059\n",
      "Epoch [5/10], Step [27375/68337], Loss: 5.0962\n",
      "Epoch [5/10], Step [27450/68337], Loss: 5.1405\n",
      "Epoch [5/10], Step [27525/68337], Loss: 5.1092\n",
      "Epoch [5/10], Step [27600/68337], Loss: 4.9883\n",
      "Epoch [5/10], Step [27675/68337], Loss: 5.1825\n",
      "Epoch [5/10], Step [27750/68337], Loss: 5.2546\n",
      "Epoch [5/10], Step [27825/68337], Loss: 5.1830\n",
      "Epoch [5/10], Step [27900/68337], Loss: 5.1827\n",
      "Epoch [5/10], Step [27975/68337], Loss: 5.3726\n",
      "Epoch [5/10], Step [28050/68337], Loss: 5.2449\n",
      "Epoch [5/10], Step [28125/68337], Loss: 5.2757\n",
      "Epoch [5/10], Step [28200/68337], Loss: 5.2079\n",
      "Epoch [5/10], Step [28275/68337], Loss: 4.9315\n",
      "Epoch [5/10], Step [28350/68337], Loss: 5.1038\n",
      "Epoch [5/10], Step [28425/68337], Loss: 5.0856\n",
      "Epoch [5/10], Step [28500/68337], Loss: 5.3308\n",
      "Epoch [5/10], Step [28575/68337], Loss: 5.1743\n",
      "Epoch [5/10], Step [28650/68337], Loss: 5.1244\n",
      "Epoch [5/10], Step [28725/68337], Loss: 5.1749\n",
      "Epoch [5/10], Step [28800/68337], Loss: 5.1031\n",
      "Epoch [5/10], Step [28875/68337], Loss: 5.1340\n",
      "Epoch [5/10], Step [28950/68337], Loss: 5.1324\n",
      "Epoch [5/10], Step [29025/68337], Loss: 5.2539\n",
      "Epoch [5/10], Step [29100/68337], Loss: 5.1287\n",
      "Epoch [5/10], Step [29175/68337], Loss: 5.0606\n",
      "Epoch [5/10], Step [29250/68337], Loss: 5.2299\n",
      "Epoch [5/10], Step [29325/68337], Loss: 5.2366\n",
      "Epoch [5/10], Step [29400/68337], Loss: 5.2405\n",
      "Epoch [5/10], Step [29475/68337], Loss: 5.0375\n",
      "Epoch [5/10], Step [29550/68337], Loss: 5.3697\n",
      "Epoch [5/10], Step [29625/68337], Loss: 4.9357\n",
      "Epoch [5/10], Step [29700/68337], Loss: 5.1120\n",
      "Epoch [5/10], Step [29775/68337], Loss: 4.9265\n",
      "Epoch [5/10], Step [29850/68337], Loss: 5.3315\n",
      "Epoch [5/10], Step [29925/68337], Loss: 5.1649\n",
      "Epoch [5/10], Step [30000/68337], Loss: 5.1284\n",
      "Validation perplexity: 130.06399024036637\n",
      "Epoch [5/10], Step [30075/68337], Loss: 5.1077\n",
      "Epoch [5/10], Step [30150/68337], Loss: 5.3079\n",
      "Epoch [5/10], Step [30225/68337], Loss: 5.3297\n",
      "Epoch [5/10], Step [30300/68337], Loss: 5.1172\n",
      "Epoch [5/10], Step [30375/68337], Loss: 5.1581\n",
      "Epoch [5/10], Step [30450/68337], Loss: 4.8360\n",
      "Epoch [5/10], Step [30525/68337], Loss: 5.3255\n",
      "Epoch [5/10], Step [30600/68337], Loss: 5.2342\n",
      "Epoch [5/10], Step [30675/68337], Loss: 5.2366\n",
      "Epoch [5/10], Step [30750/68337], Loss: 5.0210\n",
      "Epoch [5/10], Step [30825/68337], Loss: 5.1510\n",
      "Epoch [5/10], Step [30900/68337], Loss: 5.3348\n",
      "Epoch [5/10], Step [30975/68337], Loss: 5.2670\n",
      "Epoch [5/10], Step [31050/68337], Loss: 5.1304\n",
      "Epoch [5/10], Step [31125/68337], Loss: 5.3652\n",
      "Epoch [5/10], Step [31200/68337], Loss: 5.2885\n",
      "Epoch [5/10], Step [31275/68337], Loss: 4.8068\n",
      "Epoch [5/10], Step [31350/68337], Loss: 4.9247\n",
      "Epoch [5/10], Step [31425/68337], Loss: 5.2174\n",
      "Epoch [5/10], Step [31500/68337], Loss: 5.1495\n",
      "Epoch [5/10], Step [31575/68337], Loss: 5.1900\n",
      "Epoch [5/10], Step [31650/68337], Loss: 5.1367\n",
      "Epoch [5/10], Step [31725/68337], Loss: 5.1005\n",
      "Epoch [5/10], Step [31800/68337], Loss: 5.4183\n",
      "Epoch [5/10], Step [31875/68337], Loss: 4.9585\n",
      "Epoch [5/10], Step [31950/68337], Loss: 4.9667\n",
      "Epoch [5/10], Step [32025/68337], Loss: 5.1279\n",
      "Epoch [5/10], Step [32100/68337], Loss: 4.9195\n",
      "Epoch [5/10], Step [32175/68337], Loss: 5.0197\n",
      "Epoch [5/10], Step [32250/68337], Loss: 5.0878\n",
      "Epoch [5/10], Step [32325/68337], Loss: 4.9588\n",
      "Epoch [5/10], Step [32400/68337], Loss: 5.1023\n",
      "Epoch [5/10], Step [32475/68337], Loss: 5.1869\n",
      "Epoch [5/10], Step [32550/68337], Loss: 5.2230\n",
      "Epoch [5/10], Step [32625/68337], Loss: 5.0551\n",
      "Epoch [5/10], Step [32700/68337], Loss: 5.3454\n",
      "Epoch [5/10], Step [32775/68337], Loss: 5.2683\n",
      "Epoch [5/10], Step [32850/68337], Loss: 4.9622\n",
      "Epoch [5/10], Step [32925/68337], Loss: 5.1143\n",
      "Epoch [5/10], Step [33000/68337], Loss: 4.9994\n",
      "Epoch [5/10], Step [33075/68337], Loss: 5.0261\n",
      "Epoch [5/10], Step [33150/68337], Loss: 5.0751\n",
      "Epoch [5/10], Step [33225/68337], Loss: 5.1136\n",
      "Epoch [5/10], Step [33300/68337], Loss: 5.2262\n",
      "Epoch [5/10], Step [33375/68337], Loss: 5.2251\n",
      "Epoch [5/10], Step [33450/68337], Loss: 4.8829\n",
      "Epoch [5/10], Step [33525/68337], Loss: 4.9528\n",
      "Epoch [5/10], Step [33600/68337], Loss: 5.0003\n",
      "Epoch [5/10], Step [33675/68337], Loss: 5.1172\n",
      "Epoch [5/10], Step [33750/68337], Loss: 5.0952\n",
      "Epoch [5/10], Step [33825/68337], Loss: 5.0158\n",
      "Epoch [5/10], Step [33900/68337], Loss: 4.9507\n",
      "Epoch [5/10], Step [33975/68337], Loss: 5.2564\n",
      "Epoch [5/10], Step [34050/68337], Loss: 5.1830\n",
      "Epoch [5/10], Step [34125/68337], Loss: 5.0107\n",
      "Epoch [5/10], Step [34200/68337], Loss: 5.1954\n",
      "Epoch [5/10], Step [34275/68337], Loss: 5.1385\n",
      "Epoch [5/10], Step [34350/68337], Loss: 5.0968\n",
      "Epoch [5/10], Step [34425/68337], Loss: 5.1307\n",
      "Epoch [5/10], Step [34500/68337], Loss: 5.0484\n",
      "Epoch [5/10], Step [34575/68337], Loss: 4.8356\n",
      "Epoch [5/10], Step [34650/68337], Loss: 5.0332\n",
      "Epoch [5/10], Step [34725/68337], Loss: 4.9163\n",
      "Epoch [5/10], Step [34800/68337], Loss: 5.2968\n",
      "Epoch [5/10], Step [34875/68337], Loss: 5.0150\n",
      "Epoch [5/10], Step [34950/68337], Loss: 5.0537\n",
      "Epoch [5/10], Step [35025/68337], Loss: 5.1553\n",
      "Epoch [5/10], Step [35100/68337], Loss: 5.1312\n",
      "Epoch [5/10], Step [35175/68337], Loss: 5.1236\n",
      "Epoch [5/10], Step [35250/68337], Loss: 5.1556\n",
      "Epoch [5/10], Step [35325/68337], Loss: 5.2202\n",
      "Epoch [5/10], Step [35400/68337], Loss: 5.1319\n",
      "Epoch [5/10], Step [35475/68337], Loss: 5.2120\n",
      "Epoch [5/10], Step [35550/68337], Loss: 5.1840\n",
      "Epoch [5/10], Step [35625/68337], Loss: 5.3515\n",
      "Epoch [5/10], Step [35700/68337], Loss: 5.1764\n",
      "Epoch [5/10], Step [35775/68337], Loss: 5.1601\n",
      "Epoch [5/10], Step [35850/68337], Loss: 5.0981\n",
      "Epoch [5/10], Step [35925/68337], Loss: 4.9391\n",
      "Epoch [5/10], Step [36000/68337], Loss: 5.0427\n",
      "Epoch [5/10], Step [36075/68337], Loss: 5.1112\n",
      "Epoch [5/10], Step [36150/68337], Loss: 5.2712\n",
      "Epoch [5/10], Step [36225/68337], Loss: 5.3304\n",
      "Epoch [5/10], Step [36300/68337], Loss: 5.0848\n",
      "Epoch [5/10], Step [36375/68337], Loss: 4.9898\n",
      "Epoch [5/10], Step [36450/68337], Loss: 5.2258\n",
      "Epoch [5/10], Step [36525/68337], Loss: 5.0761\n",
      "Epoch [5/10], Step [36600/68337], Loss: 5.0088\n",
      "Epoch [5/10], Step [36675/68337], Loss: 5.1857\n",
      "Epoch [5/10], Step [36750/68337], Loss: 5.1854\n",
      "Epoch [5/10], Step [36825/68337], Loss: 5.1025\n",
      "Epoch [5/10], Step [36900/68337], Loss: 4.9456\n",
      "Epoch [5/10], Step [36975/68337], Loss: 5.0530\n",
      "Epoch [5/10], Step [37050/68337], Loss: 5.0046\n",
      "Epoch [5/10], Step [37125/68337], Loss: 5.3203\n",
      "Epoch [5/10], Step [37200/68337], Loss: 5.0038\n",
      "Epoch [5/10], Step [37275/68337], Loss: 5.0332\n",
      "Epoch [5/10], Step [37350/68337], Loss: 5.1433\n",
      "Epoch [5/10], Step [37425/68337], Loss: 5.0366\n",
      "Epoch [5/10], Step [37500/68337], Loss: 4.9840\n",
      "Epoch [5/10], Step [37575/68337], Loss: 4.9253\n",
      "Epoch [5/10], Step [37650/68337], Loss: 5.5176\n",
      "Epoch [5/10], Step [37725/68337], Loss: 5.1382\n",
      "Epoch [5/10], Step [37800/68337], Loss: 4.9122\n",
      "Epoch [5/10], Step [37875/68337], Loss: 5.2756\n",
      "Epoch [5/10], Step [37950/68337], Loss: 5.1142\n",
      "Epoch [5/10], Step [38025/68337], Loss: 5.1918\n",
      "Epoch [5/10], Step [38100/68337], Loss: 5.1688\n",
      "Epoch [5/10], Step [38175/68337], Loss: 4.9046\n",
      "Epoch [5/10], Step [38250/68337], Loss: 5.1551\n",
      "Epoch [5/10], Step [38325/68337], Loss: 5.1615\n",
      "Epoch [5/10], Step [38400/68337], Loss: 4.9276\n",
      "Epoch [5/10], Step [38475/68337], Loss: 5.1306\n",
      "Epoch [5/10], Step [38550/68337], Loss: 5.0992\n",
      "Epoch [5/10], Step [38625/68337], Loss: 5.1306\n",
      "Epoch [5/10], Step [38700/68337], Loss: 5.0233\n",
      "Epoch [5/10], Step [38775/68337], Loss: 5.0431\n",
      "Epoch [5/10], Step [38850/68337], Loss: 5.0397\n",
      "Epoch [5/10], Step [38925/68337], Loss: 5.2900\n",
      "Epoch [5/10], Step [39000/68337], Loss: 5.0882\n",
      "Epoch [5/10], Step [39075/68337], Loss: 5.0882\n",
      "Epoch [5/10], Step [39150/68337], Loss: 4.9217\n",
      "Epoch [5/10], Step [39225/68337], Loss: 5.1354\n",
      "Epoch [5/10], Step [39300/68337], Loss: 5.0737\n",
      "Epoch [5/10], Step [39375/68337], Loss: 5.1652\n",
      "Epoch [5/10], Step [39450/68337], Loss: 4.9557\n",
      "Epoch [5/10], Step [39525/68337], Loss: 4.9487\n",
      "Epoch [5/10], Step [39600/68337], Loss: 4.9849\n",
      "Epoch [5/10], Step [39675/68337], Loss: 5.2951\n",
      "Epoch [5/10], Step [39750/68337], Loss: 5.1440\n",
      "Epoch [5/10], Step [39825/68337], Loss: 5.4338\n",
      "Epoch [5/10], Step [39900/68337], Loss: 5.1982\n",
      "Epoch [5/10], Step [39975/68337], Loss: 5.0237\n",
      "Validation perplexity: 129.23696865947826\n",
      "Epoch [5/10], Step [40050/68337], Loss: 5.1617\n",
      "Epoch [5/10], Step [40125/68337], Loss: 5.0351\n",
      "Epoch [5/10], Step [40200/68337], Loss: 4.9659\n",
      "Epoch [5/10], Step [40275/68337], Loss: 5.1047\n",
      "Epoch [5/10], Step [40350/68337], Loss: 5.1116\n",
      "Epoch [5/10], Step [40425/68337], Loss: 5.2451\n",
      "Epoch [5/10], Step [40500/68337], Loss: 5.2347\n",
      "Epoch [5/10], Step [40575/68337], Loss: 5.1815\n",
      "Epoch [5/10], Step [40650/68337], Loss: 5.2743\n",
      "Epoch [5/10], Step [40725/68337], Loss: 5.0951\n",
      "Epoch [5/10], Step [40800/68337], Loss: 4.9058\n",
      "Epoch [5/10], Step [40875/68337], Loss: 5.2798\n",
      "Epoch [5/10], Step [40950/68337], Loss: 5.0034\n",
      "Epoch [5/10], Step [41025/68337], Loss: 5.2965\n",
      "Epoch [5/10], Step [41100/68337], Loss: 5.1091\n",
      "Epoch [5/10], Step [41175/68337], Loss: 5.2901\n",
      "Epoch [5/10], Step [41250/68337], Loss: 4.9561\n",
      "Epoch [5/10], Step [41325/68337], Loss: 5.1222\n",
      "Epoch [5/10], Step [41400/68337], Loss: 5.1901\n",
      "Epoch [5/10], Step [41475/68337], Loss: 5.3318\n",
      "Epoch [5/10], Step [41550/68337], Loss: 5.0375\n",
      "Epoch [5/10], Step [41625/68337], Loss: 5.1909\n",
      "Epoch [5/10], Step [41700/68337], Loss: 5.1941\n",
      "Epoch [5/10], Step [41775/68337], Loss: 5.2163\n",
      "Epoch [5/10], Step [41850/68337], Loss: 5.0759\n",
      "Epoch [5/10], Step [41925/68337], Loss: 4.9949\n",
      "Epoch [5/10], Step [42000/68337], Loss: 5.2194\n",
      "Epoch [5/10], Step [42075/68337], Loss: 5.1875\n",
      "Epoch [5/10], Step [42150/68337], Loss: 5.1913\n",
      "Epoch [5/10], Step [42225/68337], Loss: 4.9631\n",
      "Epoch [5/10], Step [42300/68337], Loss: 5.1518\n",
      "Epoch [5/10], Step [42375/68337], Loss: 5.0430\n",
      "Epoch [5/10], Step [42450/68337], Loss: 5.0659\n",
      "Epoch [5/10], Step [42525/68337], Loss: 5.0609\n",
      "Epoch [5/10], Step [42600/68337], Loss: 5.1765\n",
      "Epoch [5/10], Step [42675/68337], Loss: 5.3243\n",
      "Epoch [5/10], Step [42750/68337], Loss: 5.2282\n",
      "Epoch [5/10], Step [42825/68337], Loss: 5.0727\n",
      "Epoch [5/10], Step [42900/68337], Loss: 5.1676\n",
      "Epoch [5/10], Step [42975/68337], Loss: 5.1317\n",
      "Epoch [5/10], Step [43050/68337], Loss: 5.1997\n",
      "Epoch [5/10], Step [43125/68337], Loss: 4.9868\n",
      "Epoch [5/10], Step [43200/68337], Loss: 4.9822\n",
      "Epoch [5/10], Step [43275/68337], Loss: 5.0764\n",
      "Epoch [5/10], Step [43350/68337], Loss: 5.1166\n",
      "Epoch [5/10], Step [43425/68337], Loss: 5.1863\n",
      "Epoch [5/10], Step [43500/68337], Loss: 5.1571\n",
      "Epoch [5/10], Step [43575/68337], Loss: 4.9123\n",
      "Epoch [5/10], Step [43650/68337], Loss: 5.3080\n",
      "Epoch [5/10], Step [43725/68337], Loss: 5.1645\n",
      "Epoch [5/10], Step [43800/68337], Loss: 4.8731\n",
      "Epoch [5/10], Step [43875/68337], Loss: 5.0383\n",
      "Epoch [5/10], Step [43950/68337], Loss: 5.0884\n",
      "Epoch [5/10], Step [44025/68337], Loss: 4.7648\n",
      "Epoch [5/10], Step [44100/68337], Loss: 4.9199\n",
      "Epoch [5/10], Step [44175/68337], Loss: 5.1426\n",
      "Epoch [5/10], Step [44250/68337], Loss: 5.2719\n",
      "Epoch [5/10], Step [44325/68337], Loss: 5.2188\n",
      "Epoch [5/10], Step [44400/68337], Loss: 5.0634\n",
      "Epoch [5/10], Step [44475/68337], Loss: 5.1724\n",
      "Epoch [5/10], Step [44550/68337], Loss: 5.2925\n",
      "Epoch [5/10], Step [44625/68337], Loss: 5.0134\n",
      "Epoch [5/10], Step [44700/68337], Loss: 5.1375\n",
      "Epoch [5/10], Step [44775/68337], Loss: 5.0953\n",
      "Epoch [5/10], Step [44850/68337], Loss: 5.2337\n",
      "Epoch [5/10], Step [44925/68337], Loss: 5.1863\n",
      "Epoch [5/10], Step [45000/68337], Loss: 5.2270\n",
      "Epoch [5/10], Step [45075/68337], Loss: 5.1757\n",
      "Epoch [5/10], Step [45150/68337], Loss: 5.1455\n",
      "Epoch [5/10], Step [45225/68337], Loss: 5.0994\n",
      "Epoch [5/10], Step [45300/68337], Loss: 5.1744\n",
      "Epoch [5/10], Step [45375/68337], Loss: 4.9275\n",
      "Epoch [5/10], Step [45450/68337], Loss: 4.9971\n",
      "Epoch [5/10], Step [45525/68337], Loss: 5.0911\n",
      "Epoch [5/10], Step [45600/68337], Loss: 5.2007\n",
      "Epoch [5/10], Step [45675/68337], Loss: 5.1957\n",
      "Epoch [5/10], Step [45750/68337], Loss: 5.1181\n",
      "Epoch [5/10], Step [45825/68337], Loss: 5.3559\n",
      "Epoch [5/10], Step [45900/68337], Loss: 5.1116\n",
      "Epoch [5/10], Step [45975/68337], Loss: 5.1451\n",
      "Epoch [5/10], Step [46050/68337], Loss: 5.0774\n",
      "Epoch [5/10], Step [46125/68337], Loss: 5.0521\n",
      "Epoch [5/10], Step [46200/68337], Loss: 5.1419\n",
      "Epoch [5/10], Step [46275/68337], Loss: 5.0176\n",
      "Epoch [5/10], Step [46350/68337], Loss: 5.1630\n",
      "Epoch [5/10], Step [46425/68337], Loss: 5.2739\n",
      "Epoch [5/10], Step [46500/68337], Loss: 5.0691\n",
      "Epoch [5/10], Step [46575/68337], Loss: 5.1007\n",
      "Epoch [5/10], Step [46650/68337], Loss: 5.3818\n",
      "Epoch [5/10], Step [46725/68337], Loss: 5.1765\n",
      "Epoch [5/10], Step [46800/68337], Loss: 5.0742\n",
      "Epoch [5/10], Step [46875/68337], Loss: 5.2118\n",
      "Epoch [5/10], Step [46950/68337], Loss: 5.0609\n",
      "Epoch [5/10], Step [47025/68337], Loss: 5.0939\n",
      "Epoch [5/10], Step [47100/68337], Loss: 5.0437\n",
      "Epoch [5/10], Step [47175/68337], Loss: 5.0437\n",
      "Epoch [5/10], Step [47250/68337], Loss: 5.0717\n",
      "Epoch [5/10], Step [47325/68337], Loss: 5.0352\n",
      "Epoch [5/10], Step [47400/68337], Loss: 5.2268\n",
      "Epoch [5/10], Step [47475/68337], Loss: 5.2159\n",
      "Epoch [5/10], Step [47550/68337], Loss: 5.2628\n",
      "Epoch [5/10], Step [47625/68337], Loss: 5.0750\n",
      "Epoch [5/10], Step [47700/68337], Loss: 5.0456\n",
      "Epoch [5/10], Step [47775/68337], Loss: 5.2112\n",
      "Epoch [5/10], Step [47850/68337], Loss: 5.1811\n",
      "Epoch [5/10], Step [47925/68337], Loss: 5.0477\n",
      "Epoch [5/10], Step [48000/68337], Loss: 4.9927\n",
      "Epoch [5/10], Step [48075/68337], Loss: 5.2132\n",
      "Epoch [5/10], Step [48150/68337], Loss: 4.9720\n",
      "Epoch [5/10], Step [48225/68337], Loss: 5.2716\n",
      "Epoch [5/10], Step [48300/68337], Loss: 5.1264\n",
      "Epoch [5/10], Step [48375/68337], Loss: 5.0882\n",
      "Epoch [5/10], Step [48450/68337], Loss: 5.1037\n",
      "Epoch [5/10], Step [48525/68337], Loss: 4.9545\n",
      "Epoch [5/10], Step [48600/68337], Loss: 5.3444\n",
      "Epoch [5/10], Step [48675/68337], Loss: 5.3020\n",
      "Epoch [5/10], Step [48750/68337], Loss: 5.1515\n",
      "Epoch [5/10], Step [48825/68337], Loss: 4.9222\n",
      "Epoch [5/10], Step [48900/68337], Loss: 5.1834\n",
      "Epoch [5/10], Step [48975/68337], Loss: 4.9222\n",
      "Epoch [5/10], Step [49050/68337], Loss: 5.0753\n",
      "Epoch [5/10], Step [49125/68337], Loss: 5.0693\n",
      "Epoch [5/10], Step [49200/68337], Loss: 4.7934\n",
      "Epoch [5/10], Step [49275/68337], Loss: 5.1159\n",
      "Epoch [5/10], Step [49350/68337], Loss: 5.2948\n",
      "Epoch [5/10], Step [49425/68337], Loss: 5.2280\n",
      "Epoch [5/10], Step [49500/68337], Loss: 5.0829\n",
      "Epoch [5/10], Step [49575/68337], Loss: 5.1345\n",
      "Epoch [5/10], Step [49650/68337], Loss: 5.1076\n",
      "Epoch [5/10], Step [49725/68337], Loss: 5.0029\n",
      "Epoch [5/10], Step [49800/68337], Loss: 4.8379\n",
      "Epoch [5/10], Step [49875/68337], Loss: 5.1645\n",
      "Epoch [5/10], Step [49950/68337], Loss: 5.0900\n",
      "Validation perplexity: 128.74147009138917\n",
      "Epoch [5/10], Step [50025/68337], Loss: 4.9616\n",
      "Epoch [5/10], Step [50100/68337], Loss: 5.0291\n",
      "Epoch [5/10], Step [50175/68337], Loss: 5.1109\n",
      "Epoch [5/10], Step [50250/68337], Loss: 5.2331\n",
      "Epoch [5/10], Step [50325/68337], Loss: 5.1020\n",
      "Epoch [5/10], Step [50400/68337], Loss: 5.0978\n",
      "Epoch [5/10], Step [50475/68337], Loss: 4.9499\n",
      "Epoch [5/10], Step [50550/68337], Loss: 5.2482\n",
      "Epoch [5/10], Step [50625/68337], Loss: 5.2067\n",
      "Epoch [5/10], Step [50700/68337], Loss: 5.1150\n",
      "Epoch [5/10], Step [50775/68337], Loss: 5.3850\n",
      "Epoch [5/10], Step [50850/68337], Loss: 5.0495\n",
      "Epoch [5/10], Step [50925/68337], Loss: 5.0738\n",
      "Epoch [5/10], Step [51000/68337], Loss: 5.1250\n",
      "Epoch [5/10], Step [51075/68337], Loss: 4.9624\n",
      "Epoch [5/10], Step [51150/68337], Loss: 5.1814\n",
      "Epoch [5/10], Step [51225/68337], Loss: 5.1047\n",
      "Epoch [5/10], Step [51300/68337], Loss: 5.0915\n",
      "Epoch [5/10], Step [51375/68337], Loss: 5.1186\n",
      "Epoch [5/10], Step [51450/68337], Loss: 5.0334\n",
      "Epoch [5/10], Step [51525/68337], Loss: 5.1914\n",
      "Epoch [5/10], Step [51600/68337], Loss: 4.9925\n",
      "Epoch [5/10], Step [51675/68337], Loss: 5.2500\n",
      "Epoch [5/10], Step [51750/68337], Loss: 5.1845\n",
      "Epoch [5/10], Step [51825/68337], Loss: 5.1469\n",
      "Epoch [5/10], Step [51900/68337], Loss: 5.2518\n",
      "Epoch [5/10], Step [51975/68337], Loss: 5.2338\n",
      "Epoch [5/10], Step [52050/68337], Loss: 5.1151\n",
      "Epoch [5/10], Step [52125/68337], Loss: 5.1417\n",
      "Epoch [5/10], Step [52200/68337], Loss: 4.9526\n",
      "Epoch [5/10], Step [52275/68337], Loss: 5.1321\n",
      "Epoch [5/10], Step [52350/68337], Loss: 5.0800\n",
      "Epoch [5/10], Step [52425/68337], Loss: 5.1816\n",
      "Epoch [5/10], Step [52500/68337], Loss: 5.2634\n",
      "Epoch [5/10], Step [52575/68337], Loss: 4.9590\n",
      "Epoch [5/10], Step [52650/68337], Loss: 5.1387\n",
      "Epoch [5/10], Step [52725/68337], Loss: 5.1589\n",
      "Epoch [5/10], Step [52800/68337], Loss: 5.1807\n",
      "Epoch [5/10], Step [52875/68337], Loss: 5.0025\n",
      "Epoch [5/10], Step [52950/68337], Loss: 5.2237\n",
      "Epoch [5/10], Step [53025/68337], Loss: 5.0388\n",
      "Epoch [5/10], Step [53100/68337], Loss: 5.2004\n",
      "Epoch [5/10], Step [53175/68337], Loss: 5.0953\n",
      "Epoch [5/10], Step [53250/68337], Loss: 5.0640\n",
      "Epoch [5/10], Step [53325/68337], Loss: 5.0493\n",
      "Epoch [5/10], Step [53400/68337], Loss: 5.0933\n",
      "Epoch [5/10], Step [53475/68337], Loss: 5.0136\n",
      "Epoch [5/10], Step [53550/68337], Loss: 5.3041\n",
      "Epoch [5/10], Step [53625/68337], Loss: 4.6885\n",
      "Epoch [5/10], Step [53700/68337], Loss: 5.1063\n",
      "Epoch [5/10], Step [53775/68337], Loss: 5.1125\n",
      "Epoch [5/10], Step [53850/68337], Loss: 4.9328\n",
      "Epoch [5/10], Step [53925/68337], Loss: 5.1103\n",
      "Epoch [5/10], Step [54000/68337], Loss: 5.0984\n",
      "Epoch [5/10], Step [54075/68337], Loss: 5.1106\n",
      "Epoch [5/10], Step [54150/68337], Loss: 5.1661\n",
      "Epoch [5/10], Step [54225/68337], Loss: 5.1435\n",
      "Epoch [5/10], Step [54300/68337], Loss: 5.0722\n",
      "Epoch [5/10], Step [54375/68337], Loss: 5.1837\n",
      "Epoch [5/10], Step [54450/68337], Loss: 4.9179\n",
      "Epoch [5/10], Step [54525/68337], Loss: 5.1418\n",
      "Epoch [5/10], Step [54600/68337], Loss: 5.1058\n",
      "Epoch [5/10], Step [54675/68337], Loss: 5.0311\n",
      "Epoch [5/10], Step [54750/68337], Loss: 5.2447\n",
      "Epoch [5/10], Step [54825/68337], Loss: 5.0698\n",
      "Epoch [5/10], Step [54900/68337], Loss: 5.0738\n",
      "Epoch [5/10], Step [54975/68337], Loss: 4.8950\n",
      "Epoch [5/10], Step [55050/68337], Loss: 5.2033\n",
      "Epoch [5/10], Step [55125/68337], Loss: 5.2498\n",
      "Epoch [5/10], Step [55200/68337], Loss: 4.9570\n",
      "Epoch [5/10], Step [55275/68337], Loss: 5.2731\n",
      "Epoch [5/10], Step [55350/68337], Loss: 5.0785\n",
      "Epoch [5/10], Step [55425/68337], Loss: 4.9792\n",
      "Epoch [5/10], Step [55500/68337], Loss: 5.1670\n",
      "Epoch [5/10], Step [55575/68337], Loss: 5.1545\n",
      "Epoch [5/10], Step [55650/68337], Loss: 5.0967\n",
      "Epoch [5/10], Step [55725/68337], Loss: 5.1060\n",
      "Epoch [5/10], Step [55800/68337], Loss: 4.9031\n",
      "Epoch [5/10], Step [55875/68337], Loss: 5.2679\n",
      "Epoch [5/10], Step [55950/68337], Loss: 5.1804\n",
      "Epoch [5/10], Step [56025/68337], Loss: 4.9308\n",
      "Epoch [5/10], Step [56100/68337], Loss: 5.0632\n",
      "Epoch [5/10], Step [56175/68337], Loss: 5.1972\n",
      "Epoch [5/10], Step [56250/68337], Loss: 5.3508\n",
      "Epoch [5/10], Step [56325/68337], Loss: 5.1434\n",
      "Epoch [5/10], Step [56400/68337], Loss: 5.0812\n",
      "Epoch [5/10], Step [56475/68337], Loss: 5.1477\n",
      "Epoch [5/10], Step [56550/68337], Loss: 5.0353\n",
      "Epoch [5/10], Step [56625/68337], Loss: 5.0524\n",
      "Epoch [5/10], Step [56700/68337], Loss: 5.0819\n",
      "Epoch [5/10], Step [56775/68337], Loss: 5.0101\n",
      "Epoch [5/10], Step [56850/68337], Loss: 5.2199\n",
      "Epoch [5/10], Step [56925/68337], Loss: 5.0869\n",
      "Epoch [5/10], Step [57000/68337], Loss: 5.2306\n",
      "Epoch [5/10], Step [57075/68337], Loss: 5.2100\n",
      "Epoch [5/10], Step [57150/68337], Loss: 4.9912\n",
      "Epoch [5/10], Step [57225/68337], Loss: 5.0243\n",
      "Epoch [5/10], Step [57300/68337], Loss: 5.1859\n",
      "Epoch [5/10], Step [57375/68337], Loss: 5.1594\n",
      "Epoch [5/10], Step [57450/68337], Loss: 5.0516\n",
      "Epoch [5/10], Step [57525/68337], Loss: 4.9311\n",
      "Epoch [5/10], Step [57600/68337], Loss: 5.1796\n",
      "Epoch [5/10], Step [57675/68337], Loss: 5.0843\n",
      "Epoch [5/10], Step [57750/68337], Loss: 4.9141\n",
      "Epoch [5/10], Step [57825/68337], Loss: 5.1779\n",
      "Epoch [5/10], Step [57900/68337], Loss: 4.9833\n",
      "Epoch [5/10], Step [57975/68337], Loss: 5.3810\n",
      "Epoch [5/10], Step [58050/68337], Loss: 5.2419\n",
      "Epoch [5/10], Step [58125/68337], Loss: 4.9898\n",
      "Epoch [5/10], Step [58200/68337], Loss: 5.1534\n",
      "Epoch [5/10], Step [58275/68337], Loss: 5.2671\n",
      "Epoch [5/10], Step [58350/68337], Loss: 4.9436\n",
      "Epoch [5/10], Step [58425/68337], Loss: 5.1052\n",
      "Epoch [5/10], Step [58500/68337], Loss: 5.0681\n",
      "Epoch [5/10], Step [58575/68337], Loss: 5.1645\n",
      "Epoch [5/10], Step [58650/68337], Loss: 4.9485\n",
      "Epoch [5/10], Step [58725/68337], Loss: 5.0621\n",
      "Epoch [5/10], Step [58800/68337], Loss: 4.9751\n",
      "Epoch [5/10], Step [58875/68337], Loss: 5.0302\n",
      "Epoch [5/10], Step [58950/68337], Loss: 5.0418\n",
      "Epoch [5/10], Step [59025/68337], Loss: 5.1244\n",
      "Epoch [5/10], Step [59100/68337], Loss: 5.1390\n",
      "Epoch [5/10], Step [59175/68337], Loss: 5.1357\n",
      "Epoch [5/10], Step [59250/68337], Loss: 5.0796\n",
      "Epoch [5/10], Step [59325/68337], Loss: 5.0999\n",
      "Epoch [5/10], Step [59400/68337], Loss: 5.1388\n",
      "Epoch [5/10], Step [59475/68337], Loss: 5.2727\n",
      "Epoch [5/10], Step [59550/68337], Loss: 5.0860\n",
      "Epoch [5/10], Step [59625/68337], Loss: 4.7949\n",
      "Epoch [5/10], Step [59700/68337], Loss: 5.2049\n",
      "Epoch [5/10], Step [59775/68337], Loss: 4.9461\n",
      "Epoch [5/10], Step [59850/68337], Loss: 5.2605\n",
      "Epoch [5/10], Step [59925/68337], Loss: 5.0571\n",
      "Epoch [5/10], Step [60000/68337], Loss: 4.9982\n",
      "Validation perplexity: 128.3726538108925\n",
      "Epoch [5/10], Step [60075/68337], Loss: 5.0848\n",
      "Epoch [5/10], Step [60150/68337], Loss: 5.0408\n",
      "Epoch [5/10], Step [60225/68337], Loss: 5.1541\n",
      "Epoch [5/10], Step [60300/68337], Loss: 5.3695\n",
      "Epoch [5/10], Step [60375/68337], Loss: 5.1053\n",
      "Epoch [5/10], Step [60450/68337], Loss: 5.1006\n",
      "Epoch [5/10], Step [60525/68337], Loss: 5.2590\n",
      "Epoch [5/10], Step [60600/68337], Loss: 5.2924\n",
      "Epoch [5/10], Step [60675/68337], Loss: 4.9853\n",
      "Epoch [5/10], Step [60750/68337], Loss: 5.0898\n",
      "Epoch [5/10], Step [60825/68337], Loss: 5.1768\n",
      "Epoch [5/10], Step [60900/68337], Loss: 5.1393\n",
      "Epoch [5/10], Step [60975/68337], Loss: 4.8969\n",
      "Epoch [5/10], Step [61050/68337], Loss: 5.1741\n",
      "Epoch [5/10], Step [61125/68337], Loss: 5.3406\n",
      "Epoch [5/10], Step [61200/68337], Loss: 5.1781\n",
      "Epoch [5/10], Step [61275/68337], Loss: 5.0195\n",
      "Epoch [5/10], Step [61350/68337], Loss: 5.1094\n",
      "Epoch [5/10], Step [61425/68337], Loss: 5.2253\n",
      "Epoch [5/10], Step [61500/68337], Loss: 5.2477\n",
      "Epoch [5/10], Step [61575/68337], Loss: 5.1629\n",
      "Epoch [5/10], Step [61650/68337], Loss: 4.9186\n",
      "Epoch [5/10], Step [61725/68337], Loss: 5.1714\n",
      "Epoch [5/10], Step [61800/68337], Loss: 4.9686\n",
      "Epoch [5/10], Step [61875/68337], Loss: 5.0796\n",
      "Epoch [5/10], Step [61950/68337], Loss: 5.0920\n",
      "Epoch [5/10], Step [62025/68337], Loss: 5.0432\n",
      "Epoch [5/10], Step [62100/68337], Loss: 4.8313\n",
      "Epoch [5/10], Step [62175/68337], Loss: 5.1521\n",
      "Epoch [5/10], Step [62250/68337], Loss: 5.1218\n",
      "Epoch [5/10], Step [62325/68337], Loss: 4.9059\n",
      "Epoch [5/10], Step [62400/68337], Loss: 4.9968\n",
      "Epoch [5/10], Step [62475/68337], Loss: 5.2050\n",
      "Epoch [5/10], Step [62550/68337], Loss: 5.1631\n",
      "Epoch [5/10], Step [62625/68337], Loss: 5.1619\n",
      "Epoch [5/10], Step [62700/68337], Loss: 5.2052\n",
      "Epoch [5/10], Step [62775/68337], Loss: 5.1270\n",
      "Epoch [5/10], Step [62850/68337], Loss: 4.9734\n",
      "Epoch [5/10], Step [62925/68337], Loss: 5.1102\n",
      "Epoch [5/10], Step [63000/68337], Loss: 5.2294\n",
      "Epoch [5/10], Step [63075/68337], Loss: 5.3078\n",
      "Epoch [5/10], Step [63150/68337], Loss: 5.0004\n",
      "Epoch [5/10], Step [63225/68337], Loss: 5.1309\n",
      "Epoch [5/10], Step [63300/68337], Loss: 5.2338\n",
      "Epoch [5/10], Step [63375/68337], Loss: 5.0957\n",
      "Epoch [5/10], Step [63450/68337], Loss: 5.2369\n",
      "Epoch [5/10], Step [63525/68337], Loss: 5.2832\n",
      "Epoch [5/10], Step [63600/68337], Loss: 5.1932\n",
      "Epoch [5/10], Step [63675/68337], Loss: 5.1734\n",
      "Epoch [5/10], Step [63750/68337], Loss: 5.0607\n",
      "Epoch [5/10], Step [63825/68337], Loss: 5.1256\n",
      "Epoch [5/10], Step [63900/68337], Loss: 5.1333\n",
      "Epoch [5/10], Step [63975/68337], Loss: 5.2877\n",
      "Epoch [5/10], Step [64050/68337], Loss: 5.3142\n",
      "Epoch [5/10], Step [64125/68337], Loss: 5.0518\n",
      "Epoch [5/10], Step [64200/68337], Loss: 5.0806\n",
      "Epoch [5/10], Step [64275/68337], Loss: 5.0777\n",
      "Epoch [5/10], Step [64350/68337], Loss: 5.1137\n",
      "Epoch [5/10], Step [64425/68337], Loss: 5.2161\n",
      "Epoch [5/10], Step [64500/68337], Loss: 5.0907\n",
      "Epoch [5/10], Step [64575/68337], Loss: 4.9912\n",
      "Epoch [5/10], Step [64650/68337], Loss: 5.0925\n",
      "Epoch [5/10], Step [64725/68337], Loss: 5.0073\n",
      "Epoch [5/10], Step [64800/68337], Loss: 5.2364\n",
      "Epoch [5/10], Step [64875/68337], Loss: 5.0928\n",
      "Epoch [5/10], Step [64950/68337], Loss: 5.1876\n",
      "Epoch [5/10], Step [65025/68337], Loss: 5.2530\n",
      "Epoch [5/10], Step [65100/68337], Loss: 5.1778\n",
      "Epoch [5/10], Step [65175/68337], Loss: 4.7893\n",
      "Epoch [5/10], Step [65250/68337], Loss: 5.1208\n",
      "Epoch [5/10], Step [65325/68337], Loss: 4.7976\n",
      "Epoch [5/10], Step [65400/68337], Loss: 5.1951\n",
      "Epoch [5/10], Step [65475/68337], Loss: 4.9877\n",
      "Epoch [5/10], Step [65550/68337], Loss: 5.1140\n",
      "Epoch [5/10], Step [65625/68337], Loss: 5.2240\n",
      "Epoch [5/10], Step [65700/68337], Loss: 4.9833\n",
      "Epoch [5/10], Step [65775/68337], Loss: 5.1612\n",
      "Epoch [5/10], Step [65850/68337], Loss: 5.0278\n",
      "Epoch [5/10], Step [65925/68337], Loss: 5.1135\n",
      "Epoch [5/10], Step [66000/68337], Loss: 5.1021\n",
      "Epoch [5/10], Step [66075/68337], Loss: 4.9398\n",
      "Epoch [5/10], Step [66150/68337], Loss: 5.3026\n",
      "Epoch [5/10], Step [66225/68337], Loss: 5.0064\n",
      "Epoch [5/10], Step [66300/68337], Loss: 5.1849\n",
      "Epoch [5/10], Step [66375/68337], Loss: 4.9881\n",
      "Epoch [5/10], Step [66450/68337], Loss: 5.0296\n",
      "Epoch [5/10], Step [66525/68337], Loss: 5.1724\n",
      "Epoch [5/10], Step [66600/68337], Loss: 5.0626\n",
      "Epoch [5/10], Step [66675/68337], Loss: 5.1979\n",
      "Epoch [5/10], Step [66750/68337], Loss: 5.0195\n",
      "Epoch [5/10], Step [66825/68337], Loss: 5.0191\n",
      "Epoch [5/10], Step [66900/68337], Loss: 4.9965\n",
      "Epoch [5/10], Step [66975/68337], Loss: 5.1716\n",
      "Epoch [5/10], Step [67050/68337], Loss: 5.1282\n",
      "Epoch [5/10], Step [67125/68337], Loss: 5.1185\n",
      "Epoch [5/10], Step [67200/68337], Loss: 4.9953\n",
      "Epoch [5/10], Step [67275/68337], Loss: 5.0787\n",
      "Epoch [5/10], Step [67350/68337], Loss: 5.1752\n",
      "Epoch [5/10], Step [67425/68337], Loss: 4.9475\n",
      "Epoch [5/10], Step [67500/68337], Loss: 5.0701\n",
      "Epoch [5/10], Step [67575/68337], Loss: 4.9908\n",
      "Epoch [5/10], Step [67650/68337], Loss: 5.2246\n",
      "Epoch [5/10], Step [67725/68337], Loss: 4.9459\n",
      "Epoch [5/10], Step [67800/68337], Loss: 5.0952\n",
      "Epoch [5/10], Step [67875/68337], Loss: 5.2528\n",
      "Epoch [5/10], Step [67950/68337], Loss: 5.0194\n",
      "Epoch [5/10], Step [68025/68337], Loss: 5.0472\n",
      "Epoch [5/10], Step [68100/68337], Loss: 5.1005\n",
      "Epoch [5/10], Step [68175/68337], Loss: 5.3660\n",
      "Epoch [5/10], Step [68250/68337], Loss: 5.0739\n",
      "Epoch [5/10], Step [68325/68337], Loss: 5.0942\n",
      "Epoch [5/10] Average Loss: 5.1241, Perplexity: 168.03\n",
      "Epoch [6/10], Step [0/68337], Loss: 4.9114\n",
      "Validation perplexity: 128.20623709685\n",
      "Epoch [6/10], Step [75/68337], Loss: 5.2236\n",
      "Epoch [6/10], Step [150/68337], Loss: 5.0837\n",
      "Epoch [6/10], Step [225/68337], Loss: 5.2989\n",
      "Epoch [6/10], Step [300/68337], Loss: 5.2512\n",
      "Epoch [6/10], Step [375/68337], Loss: 5.1544\n",
      "Epoch [6/10], Step [450/68337], Loss: 4.9096\n",
      "Epoch [6/10], Step [525/68337], Loss: 5.2241\n",
      "Epoch [6/10], Step [600/68337], Loss: 5.2505\n",
      "Epoch [6/10], Step [675/68337], Loss: 5.1122\n",
      "Epoch [6/10], Step [750/68337], Loss: 5.0497\n",
      "Epoch [6/10], Step [825/68337], Loss: 5.1324\n",
      "Epoch [6/10], Step [900/68337], Loss: 5.0508\n",
      "Epoch [6/10], Step [975/68337], Loss: 4.9639\n",
      "Epoch [6/10], Step [1050/68337], Loss: 5.4061\n",
      "Epoch [6/10], Step [1125/68337], Loss: 5.0164\n",
      "Epoch [6/10], Step [1200/68337], Loss: 5.3230\n",
      "Epoch [6/10], Step [1275/68337], Loss: 5.1104\n",
      "Epoch [6/10], Step [1350/68337], Loss: 5.2463\n",
      "Epoch [6/10], Step [1425/68337], Loss: 4.9117\n",
      "Epoch [6/10], Step [1500/68337], Loss: 4.9912\n",
      "Epoch [6/10], Step [1575/68337], Loss: 5.0560\n",
      "Epoch [6/10], Step [1650/68337], Loss: 5.1186\n",
      "Epoch [6/10], Step [1725/68337], Loss: 5.2354\n",
      "Epoch [6/10], Step [1800/68337], Loss: 5.0282\n",
      "Epoch [6/10], Step [1875/68337], Loss: 5.1146\n",
      "Epoch [6/10], Step [1950/68337], Loss: 5.1223\n",
      "Epoch [6/10], Step [2025/68337], Loss: 5.0557\n",
      "Epoch [6/10], Step [2100/68337], Loss: 5.0460\n",
      "Epoch [6/10], Step [2175/68337], Loss: 5.1592\n",
      "Epoch [6/10], Step [2250/68337], Loss: 5.0261\n",
      "Epoch [6/10], Step [2325/68337], Loss: 5.0795\n",
      "Epoch [6/10], Step [2400/68337], Loss: 4.9769\n",
      "Epoch [6/10], Step [2475/68337], Loss: 4.9656\n",
      "Epoch [6/10], Step [2550/68337], Loss: 4.9792\n",
      "Epoch [6/10], Step [2625/68337], Loss: 5.0866\n",
      "Epoch [6/10], Step [2700/68337], Loss: 5.2062\n",
      "Epoch [6/10], Step [2775/68337], Loss: 5.1642\n",
      "Epoch [6/10], Step [2850/68337], Loss: 5.2008\n",
      "Epoch [6/10], Step [2925/68337], Loss: 5.0127\n",
      "Epoch [6/10], Step [3000/68337], Loss: 5.0355\n",
      "Epoch [6/10], Step [3075/68337], Loss: 4.9552\n",
      "Epoch [6/10], Step [3150/68337], Loss: 4.9903\n",
      "Epoch [6/10], Step [3225/68337], Loss: 5.0581\n",
      "Epoch [6/10], Step [3300/68337], Loss: 5.0143\n",
      "Epoch [6/10], Step [3375/68337], Loss: 5.1999\n",
      "Epoch [6/10], Step [3450/68337], Loss: 5.1171\n",
      "Epoch [6/10], Step [3525/68337], Loss: 5.0969\n",
      "Epoch [6/10], Step [3600/68337], Loss: 4.7599\n",
      "Epoch [6/10], Step [3675/68337], Loss: 5.2190\n",
      "Epoch [6/10], Step [3750/68337], Loss: 5.1155\n",
      "Epoch [6/10], Step [3825/68337], Loss: 5.2152\n",
      "Epoch [6/10], Step [3900/68337], Loss: 5.0216\n",
      "Epoch [6/10], Step [3975/68337], Loss: 5.2628\n",
      "Epoch [6/10], Step [4050/68337], Loss: 5.1549\n",
      "Epoch [6/10], Step [4125/68337], Loss: 5.1130\n",
      "Epoch [6/10], Step [4200/68337], Loss: 5.2798\n",
      "Epoch [6/10], Step [4275/68337], Loss: 5.1073\n",
      "Epoch [6/10], Step [4350/68337], Loss: 4.9979\n",
      "Epoch [6/10], Step [4425/68337], Loss: 4.9096\n",
      "Epoch [6/10], Step [4500/68337], Loss: 4.9048\n",
      "Epoch [6/10], Step [4575/68337], Loss: 5.0196\n",
      "Epoch [6/10], Step [4650/68337], Loss: 5.1634\n",
      "Epoch [6/10], Step [4725/68337], Loss: 5.0370\n",
      "Epoch [6/10], Step [4800/68337], Loss: 5.1535\n",
      "Epoch [6/10], Step [4875/68337], Loss: 5.1087\n",
      "Epoch [6/10], Step [4950/68337], Loss: 4.8185\n",
      "Epoch [6/10], Step [5025/68337], Loss: 5.2367\n",
      "Epoch [6/10], Step [5100/68337], Loss: 5.1953\n",
      "Epoch [6/10], Step [5175/68337], Loss: 4.9668\n",
      "Epoch [6/10], Step [5250/68337], Loss: 5.0799\n",
      "Epoch [6/10], Step [5325/68337], Loss: 5.1128\n",
      "Epoch [6/10], Step [5400/68337], Loss: 5.2659\n",
      "Epoch [6/10], Step [5475/68337], Loss: 4.9637\n",
      "Epoch [6/10], Step [5550/68337], Loss: 5.0587\n",
      "Epoch [6/10], Step [5625/68337], Loss: 5.2103\n",
      "Epoch [6/10], Step [5700/68337], Loss: 5.3055\n",
      "Epoch [6/10], Step [5775/68337], Loss: 5.1327\n",
      "Epoch [6/10], Step [5850/68337], Loss: 5.2043\n",
      "Epoch [6/10], Step [5925/68337], Loss: 5.0930\n",
      "Epoch [6/10], Step [6000/68337], Loss: 5.1326\n",
      "Epoch [6/10], Step [6075/68337], Loss: 5.1205\n",
      "Epoch [6/10], Step [6150/68337], Loss: 4.9593\n",
      "Epoch [6/10], Step [6225/68337], Loss: 5.0119\n",
      "Epoch [6/10], Step [6300/68337], Loss: 5.0857\n",
      "Epoch [6/10], Step [6375/68337], Loss: 5.1279\n",
      "Epoch [6/10], Step [6450/68337], Loss: 5.1316\n",
      "Epoch [6/10], Step [6525/68337], Loss: 5.0785\n",
      "Epoch [6/10], Step [6600/68337], Loss: 5.0762\n",
      "Epoch [6/10], Step [6675/68337], Loss: 5.1600\n",
      "Epoch [6/10], Step [6750/68337], Loss: 5.1739\n",
      "Epoch [6/10], Step [6825/68337], Loss: 5.0573\n",
      "Epoch [6/10], Step [6900/68337], Loss: 4.8927\n",
      "Epoch [6/10], Step [6975/68337], Loss: 5.0431\n",
      "Epoch [6/10], Step [7050/68337], Loss: 5.2448\n",
      "Epoch [6/10], Step [7125/68337], Loss: 5.1322\n",
      "Epoch [6/10], Step [7200/68337], Loss: 5.0915\n",
      "Epoch [6/10], Step [7275/68337], Loss: 5.0843\n",
      "Epoch [6/10], Step [7350/68337], Loss: 5.1841\n",
      "Epoch [6/10], Step [7425/68337], Loss: 5.1582\n",
      "Epoch [6/10], Step [7500/68337], Loss: 5.2397\n",
      "Epoch [6/10], Step [7575/68337], Loss: 5.0362\n",
      "Epoch [6/10], Step [7650/68337], Loss: 4.9317\n",
      "Epoch [6/10], Step [7725/68337], Loss: 5.1129\n",
      "Epoch [6/10], Step [7800/68337], Loss: 5.0282\n",
      "Epoch [6/10], Step [7875/68337], Loss: 5.2147\n",
      "Epoch [6/10], Step [7950/68337], Loss: 5.0293\n",
      "Epoch [6/10], Step [8025/68337], Loss: 5.4157\n",
      "Epoch [6/10], Step [8100/68337], Loss: 5.0716\n",
      "Epoch [6/10], Step [8175/68337], Loss: 5.0076\n",
      "Epoch [6/10], Step [8250/68337], Loss: 5.0035\n",
      "Epoch [6/10], Step [8325/68337], Loss: 4.7904\n",
      "Epoch [6/10], Step [8400/68337], Loss: 4.8580\n",
      "Epoch [6/10], Step [8475/68337], Loss: 5.1837\n",
      "Epoch [6/10], Step [8550/68337], Loss: 5.3123\n",
      "Epoch [6/10], Step [8625/68337], Loss: 5.0197\n",
      "Epoch [6/10], Step [8700/68337], Loss: 5.0369\n",
      "Epoch [6/10], Step [8775/68337], Loss: 5.0177\n",
      "Epoch [6/10], Step [8850/68337], Loss: 5.2200\n",
      "Epoch [6/10], Step [8925/68337], Loss: 5.3553\n",
      "Epoch [6/10], Step [9000/68337], Loss: 5.1195\n",
      "Epoch [6/10], Step [9075/68337], Loss: 5.1135\n",
      "Epoch [6/10], Step [9150/68337], Loss: 5.0612\n",
      "Epoch [6/10], Step [9225/68337], Loss: 5.0522\n",
      "Epoch [6/10], Step [9300/68337], Loss: 5.1029\n",
      "Epoch [6/10], Step [9375/68337], Loss: 4.9127\n",
      "Epoch [6/10], Step [9450/68337], Loss: 5.0601\n",
      "Epoch [6/10], Step [9525/68337], Loss: 5.0610\n",
      "Epoch [6/10], Step [9600/68337], Loss: 5.1904\n",
      "Epoch [6/10], Step [9675/68337], Loss: 5.1501\n",
      "Epoch [6/10], Step [9750/68337], Loss: 5.2881\n",
      "Epoch [6/10], Step [9825/68337], Loss: 5.1063\n",
      "Epoch [6/10], Step [9900/68337], Loss: 5.1159\n",
      "Epoch [6/10], Step [9975/68337], Loss: 5.0682\n",
      "Validation perplexity: 127.54325189887653\n",
      "Epoch [6/10], Step [10050/68337], Loss: 5.4544\n",
      "Epoch [6/10], Step [10125/68337], Loss: 5.0290\n",
      "Epoch [6/10], Step [10200/68337], Loss: 5.1148\n",
      "Epoch [6/10], Step [10275/68337], Loss: 5.0230\n",
      "Epoch [6/10], Step [10350/68337], Loss: 4.9655\n",
      "Epoch [6/10], Step [10425/68337], Loss: 5.3220\n",
      "Epoch [6/10], Step [10500/68337], Loss: 5.0930\n",
      "Epoch [6/10], Step [10575/68337], Loss: 4.8387\n",
      "Epoch [6/10], Step [10650/68337], Loss: 5.3016\n",
      "Epoch [6/10], Step [10725/68337], Loss: 5.1284\n",
      "Epoch [6/10], Step [10800/68337], Loss: 5.0995\n",
      "Epoch [6/10], Step [10875/68337], Loss: 5.1683\n",
      "Epoch [6/10], Step [10950/68337], Loss: 4.8082\n",
      "Epoch [6/10], Step [11025/68337], Loss: 5.0789\n",
      "Epoch [6/10], Step [11100/68337], Loss: 5.1028\n",
      "Epoch [6/10], Step [11175/68337], Loss: 5.1469\n",
      "Epoch [6/10], Step [11250/68337], Loss: 5.0316\n",
      "Epoch [6/10], Step [11325/68337], Loss: 5.1297\n",
      "Epoch [6/10], Step [11400/68337], Loss: 5.2091\n",
      "Epoch [6/10], Step [11475/68337], Loss: 5.2612\n",
      "Epoch [6/10], Step [11550/68337], Loss: 5.0866\n",
      "Epoch [6/10], Step [11625/68337], Loss: 5.0568\n",
      "Epoch [6/10], Step [11700/68337], Loss: 5.1218\n",
      "Epoch [6/10], Step [11775/68337], Loss: 5.2101\n",
      "Epoch [6/10], Step [11850/68337], Loss: 5.0667\n",
      "Epoch [6/10], Step [11925/68337], Loss: 5.2858\n",
      "Epoch [6/10], Step [12000/68337], Loss: 5.2101\n",
      "Epoch [6/10], Step [12075/68337], Loss: 5.2333\n",
      "Epoch [6/10], Step [12150/68337], Loss: 4.9649\n",
      "Epoch [6/10], Step [12225/68337], Loss: 5.1164\n",
      "Epoch [6/10], Step [12300/68337], Loss: 5.0226\n",
      "Epoch [6/10], Step [12375/68337], Loss: 4.8810\n",
      "Epoch [6/10], Step [12450/68337], Loss: 4.9095\n",
      "Epoch [6/10], Step [12525/68337], Loss: 5.0641\n",
      "Epoch [6/10], Step [12600/68337], Loss: 5.1912\n",
      "Epoch [6/10], Step [12675/68337], Loss: 5.1483\n",
      "Epoch [6/10], Step [12750/68337], Loss: 5.1729\n",
      "Epoch [6/10], Step [12825/68337], Loss: 5.1615\n",
      "Epoch [6/10], Step [12900/68337], Loss: 5.0803\n",
      "Epoch [6/10], Step [12975/68337], Loss: 4.8129\n",
      "Epoch [6/10], Step [13050/68337], Loss: 5.1834\n",
      "Epoch [6/10], Step [13125/68337], Loss: 5.1425\n",
      "Epoch [6/10], Step [13200/68337], Loss: 5.1294\n",
      "Epoch [6/10], Step [13275/68337], Loss: 5.2144\n",
      "Epoch [6/10], Step [13350/68337], Loss: 5.2565\n",
      "Epoch [6/10], Step [13425/68337], Loss: 5.0744\n",
      "Epoch [6/10], Step [13500/68337], Loss: 5.1338\n",
      "Epoch [6/10], Step [13575/68337], Loss: 4.9628\n",
      "Epoch [6/10], Step [13650/68337], Loss: 5.1902\n",
      "Epoch [6/10], Step [13725/68337], Loss: 5.2006\n",
      "Epoch [6/10], Step [13800/68337], Loss: 5.1027\n",
      "Epoch [6/10], Step [13875/68337], Loss: 5.1713\n",
      "Epoch [6/10], Step [13950/68337], Loss: 5.3236\n",
      "Epoch [6/10], Step [14025/68337], Loss: 5.4535\n",
      "Epoch [6/10], Step [14100/68337], Loss: 5.1787\n",
      "Epoch [6/10], Step [14175/68337], Loss: 5.1172\n",
      "Epoch [6/10], Step [14250/68337], Loss: 5.1211\n",
      "Epoch [6/10], Step [14325/68337], Loss: 4.8144\n",
      "Epoch [6/10], Step [14400/68337], Loss: 5.1557\n",
      "Epoch [6/10], Step [14475/68337], Loss: 5.3846\n",
      "Epoch [6/10], Step [14550/68337], Loss: 5.0515\n",
      "Epoch [6/10], Step [14625/68337], Loss: 5.2185\n",
      "Epoch [6/10], Step [14700/68337], Loss: 5.1715\n",
      "Epoch [6/10], Step [14775/68337], Loss: 5.1403\n",
      "Epoch [6/10], Step [14850/68337], Loss: 5.2243\n",
      "Epoch [6/10], Step [14925/68337], Loss: 5.1775\n",
      "Epoch [6/10], Step [15000/68337], Loss: 5.0765\n",
      "Epoch [6/10], Step [15075/68337], Loss: 4.9755\n",
      "Epoch [6/10], Step [15150/68337], Loss: 5.0659\n",
      "Epoch [6/10], Step [15225/68337], Loss: 5.1734\n",
      "Epoch [6/10], Step [15300/68337], Loss: 5.1647\n",
      "Epoch [6/10], Step [15375/68337], Loss: 5.1558\n",
      "Epoch [6/10], Step [15450/68337], Loss: 5.0760\n",
      "Epoch [6/10], Step [15525/68337], Loss: 5.0625\n",
      "Epoch [6/10], Step [15600/68337], Loss: 5.0880\n",
      "Epoch [6/10], Step [15675/68337], Loss: 5.0766\n",
      "Epoch [6/10], Step [15750/68337], Loss: 5.0278\n",
      "Epoch [6/10], Step [15825/68337], Loss: 4.9866\n",
      "Epoch [6/10], Step [15900/68337], Loss: 5.2045\n",
      "Epoch [6/10], Step [15975/68337], Loss: 4.9663\n",
      "Epoch [6/10], Step [16050/68337], Loss: 5.2664\n",
      "Epoch [6/10], Step [16125/68337], Loss: 5.3589\n",
      "Epoch [6/10], Step [16200/68337], Loss: 5.0953\n",
      "Epoch [6/10], Step [16275/68337], Loss: 5.0337\n",
      "Epoch [6/10], Step [16350/68337], Loss: 5.0713\n",
      "Epoch [6/10], Step [16425/68337], Loss: 5.2495\n",
      "Epoch [6/10], Step [16500/68337], Loss: 5.0998\n",
      "Epoch [6/10], Step [16575/68337], Loss: 4.9459\n",
      "Epoch [6/10], Step [16650/68337], Loss: 5.0974\n",
      "Epoch [6/10], Step [16725/68337], Loss: 5.0975\n",
      "Epoch [6/10], Step [16800/68337], Loss: 5.0311\n",
      "Epoch [6/10], Step [16875/68337], Loss: 5.0009\n",
      "Epoch [6/10], Step [16950/68337], Loss: 5.3531\n",
      "Epoch [6/10], Step [17025/68337], Loss: 5.3754\n",
      "Epoch [6/10], Step [17100/68337], Loss: 5.2039\n",
      "Epoch [6/10], Step [17175/68337], Loss: 5.2018\n",
      "Epoch [6/10], Step [17250/68337], Loss: 4.9667\n",
      "Epoch [6/10], Step [17325/68337], Loss: 4.9768\n",
      "Epoch [6/10], Step [17400/68337], Loss: 4.9351\n",
      "Epoch [6/10], Step [17475/68337], Loss: 5.0917\n",
      "Epoch [6/10], Step [17550/68337], Loss: 5.0733\n",
      "Epoch [6/10], Step [17625/68337], Loss: 5.1629\n",
      "Epoch [6/10], Step [17700/68337], Loss: 5.1733\n",
      "Epoch [6/10], Step [17775/68337], Loss: 5.1190\n",
      "Epoch [6/10], Step [17850/68337], Loss: 5.1502\n",
      "Epoch [6/10], Step [17925/68337], Loss: 5.0385\n",
      "Epoch [6/10], Step [18000/68337], Loss: 5.1474\n",
      "Epoch [6/10], Step [18075/68337], Loss: 5.0663\n",
      "Epoch [6/10], Step [18150/68337], Loss: 5.1404\n",
      "Epoch [6/10], Step [18225/68337], Loss: 4.9758\n",
      "Epoch [6/10], Step [18300/68337], Loss: 4.9608\n",
      "Epoch [6/10], Step [18375/68337], Loss: 5.1023\n",
      "Epoch [6/10], Step [18450/68337], Loss: 5.2082\n",
      "Epoch [6/10], Step [18525/68337], Loss: 4.9570\n",
      "Epoch [6/10], Step [18600/68337], Loss: 4.9884\n",
      "Epoch [6/10], Step [18675/68337], Loss: 5.2616\n",
      "Epoch [6/10], Step [18750/68337], Loss: 5.0037\n",
      "Epoch [6/10], Step [18825/68337], Loss: 5.1710\n",
      "Epoch [6/10], Step [18900/68337], Loss: 4.9041\n",
      "Epoch [6/10], Step [18975/68337], Loss: 4.9727\n",
      "Epoch [6/10], Step [19050/68337], Loss: 5.1136\n",
      "Epoch [6/10], Step [19125/68337], Loss: 5.0405\n",
      "Epoch [6/10], Step [19200/68337], Loss: 5.0579\n",
      "Epoch [6/10], Step [19275/68337], Loss: 5.0603\n",
      "Epoch [6/10], Step [19350/68337], Loss: 5.2377\n",
      "Epoch [6/10], Step [19425/68337], Loss: 5.0710\n",
      "Epoch [6/10], Step [19500/68337], Loss: 5.2847\n",
      "Epoch [6/10], Step [19575/68337], Loss: 5.0789\n",
      "Epoch [6/10], Step [19650/68337], Loss: 5.3920\n",
      "Epoch [6/10], Step [19725/68337], Loss: 4.8827\n",
      "Epoch [6/10], Step [19800/68337], Loss: 4.9768\n",
      "Epoch [6/10], Step [19875/68337], Loss: 5.2395\n",
      "Epoch [6/10], Step [19950/68337], Loss: 5.0755\n",
      "Validation perplexity: 127.15298448335568\n",
      "Epoch [6/10], Step [20025/68337], Loss: 5.1967\n",
      "Epoch [6/10], Step [20100/68337], Loss: 5.3786\n",
      "Epoch [6/10], Step [20175/68337], Loss: 5.1140\n",
      "Epoch [6/10], Step [20250/68337], Loss: 4.9991\n",
      "Epoch [6/10], Step [20325/68337], Loss: 5.0987\n",
      "Epoch [6/10], Step [20400/68337], Loss: 5.1571\n",
      "Epoch [6/10], Step [20475/68337], Loss: 5.0321\n",
      "Epoch [6/10], Step [20550/68337], Loss: 5.1079\n",
      "Epoch [6/10], Step [20625/68337], Loss: 5.2360\n",
      "Epoch [6/10], Step [20700/68337], Loss: 5.2130\n",
      "Epoch [6/10], Step [20775/68337], Loss: 5.0833\n",
      "Epoch [6/10], Step [20850/68337], Loss: 4.9086\n",
      "Epoch [6/10], Step [20925/68337], Loss: 5.0423\n",
      "Epoch [6/10], Step [21000/68337], Loss: 4.7388\n",
      "Epoch [6/10], Step [21075/68337], Loss: 5.2028\n",
      "Epoch [6/10], Step [21150/68337], Loss: 5.1852\n",
      "Epoch [6/10], Step [21225/68337], Loss: 4.9729\n",
      "Epoch [6/10], Step [21300/68337], Loss: 5.3462\n",
      "Epoch [6/10], Step [21375/68337], Loss: 5.1993\n",
      "Epoch [6/10], Step [21450/68337], Loss: 5.0695\n",
      "Epoch [6/10], Step [21525/68337], Loss: 5.0071\n",
      "Epoch [6/10], Step [21600/68337], Loss: 5.1507\n",
      "Epoch [6/10], Step [21675/68337], Loss: 4.9461\n",
      "Epoch [6/10], Step [21750/68337], Loss: 5.0468\n",
      "Epoch [6/10], Step [21825/68337], Loss: 5.0915\n",
      "Epoch [6/10], Step [21900/68337], Loss: 5.4115\n",
      "Epoch [6/10], Step [21975/68337], Loss: 5.1876\n",
      "Epoch [6/10], Step [22050/68337], Loss: 5.0744\n",
      "Epoch [6/10], Step [22125/68337], Loss: 5.2439\n",
      "Epoch [6/10], Step [22200/68337], Loss: 5.1072\n",
      "Epoch [6/10], Step [22275/68337], Loss: 5.2489\n",
      "Epoch [6/10], Step [22350/68337], Loss: 5.0588\n",
      "Epoch [6/10], Step [22425/68337], Loss: 5.0099\n",
      "Epoch [6/10], Step [22500/68337], Loss: 5.1368\n",
      "Epoch [6/10], Step [22575/68337], Loss: 5.2632\n",
      "Epoch [6/10], Step [22650/68337], Loss: 5.1407\n",
      "Epoch [6/10], Step [22725/68337], Loss: 5.1645\n",
      "Epoch [6/10], Step [22800/68337], Loss: 5.0545\n",
      "Epoch [6/10], Step [22875/68337], Loss: 5.2093\n",
      "Epoch [6/10], Step [22950/68337], Loss: 5.1643\n",
      "Epoch [6/10], Step [23025/68337], Loss: 4.9989\n",
      "Epoch [6/10], Step [23100/68337], Loss: 5.2570\n",
      "Epoch [6/10], Step [23175/68337], Loss: 4.9837\n",
      "Epoch [6/10], Step [23250/68337], Loss: 5.1262\n",
      "Epoch [6/10], Step [23325/68337], Loss: 5.0623\n",
      "Epoch [6/10], Step [23400/68337], Loss: 5.1052\n",
      "Epoch [6/10], Step [23475/68337], Loss: 4.9719\n",
      "Epoch [6/10], Step [23550/68337], Loss: 5.1023\n",
      "Epoch [6/10], Step [23625/68337], Loss: 5.0190\n",
      "Epoch [6/10], Step [23700/68337], Loss: 5.0758\n",
      "Epoch [6/10], Step [23775/68337], Loss: 5.0701\n",
      "Epoch [6/10], Step [23850/68337], Loss: 5.1801\n",
      "Epoch [6/10], Step [23925/68337], Loss: 5.0492\n",
      "Epoch [6/10], Step [24000/68337], Loss: 5.1607\n",
      "Epoch [6/10], Step [24075/68337], Loss: 5.1152\n",
      "Epoch [6/10], Step [24150/68337], Loss: 5.0888\n",
      "Epoch [6/10], Step [24225/68337], Loss: 5.3513\n",
      "Epoch [6/10], Step [24300/68337], Loss: 5.1962\n",
      "Epoch [6/10], Step [24375/68337], Loss: 4.9903\n",
      "Epoch [6/10], Step [24450/68337], Loss: 5.1660\n",
      "Epoch [6/10], Step [24525/68337], Loss: 5.1378\n",
      "Epoch [6/10], Step [24600/68337], Loss: 5.0862\n",
      "Epoch [6/10], Step [24675/68337], Loss: 5.2043\n",
      "Epoch [6/10], Step [24750/68337], Loss: 5.0760\n",
      "Epoch [6/10], Step [24825/68337], Loss: 4.9725\n",
      "Epoch [6/10], Step [24900/68337], Loss: 4.9870\n",
      "Epoch [6/10], Step [24975/68337], Loss: 5.0202\n",
      "Epoch [6/10], Step [25050/68337], Loss: 5.2579\n",
      "Epoch [6/10], Step [25125/68337], Loss: 5.3139\n",
      "Epoch [6/10], Step [25200/68337], Loss: 4.9142\n",
      "Epoch [6/10], Step [25275/68337], Loss: 5.0767\n",
      "Epoch [6/10], Step [25350/68337], Loss: 4.9699\n",
      "Epoch [6/10], Step [25425/68337], Loss: 5.1249\n",
      "Epoch [6/10], Step [25500/68337], Loss: 5.0602\n",
      "Epoch [6/10], Step [25575/68337], Loss: 5.2728\n",
      "Epoch [6/10], Step [25650/68337], Loss: 5.1295\n",
      "Epoch [6/10], Step [25725/68337], Loss: 4.7680\n",
      "Epoch [6/10], Step [25800/68337], Loss: 4.9271\n",
      "Epoch [6/10], Step [25875/68337], Loss: 5.0148\n",
      "Epoch [6/10], Step [25950/68337], Loss: 5.2411\n",
      "Epoch [6/10], Step [26025/68337], Loss: 5.2983\n",
      "Epoch [6/10], Step [26100/68337], Loss: 5.1255\n",
      "Epoch [6/10], Step [26175/68337], Loss: 5.3148\n",
      "Epoch [6/10], Step [26250/68337], Loss: 5.1239\n",
      "Epoch [6/10], Step [26325/68337], Loss: 4.9846\n",
      "Epoch [6/10], Step [26400/68337], Loss: 5.1256\n",
      "Epoch [6/10], Step [26475/68337], Loss: 5.0236\n",
      "Epoch [6/10], Step [26550/68337], Loss: 5.3162\n",
      "Epoch [6/10], Step [26625/68337], Loss: 4.8462\n",
      "Epoch [6/10], Step [26700/68337], Loss: 5.0835\n",
      "Epoch [6/10], Step [26775/68337], Loss: 4.9993\n",
      "Epoch [6/10], Step [26850/68337], Loss: 5.2046\n",
      "Epoch [6/10], Step [26925/68337], Loss: 5.2381\n",
      "Epoch [6/10], Step [27000/68337], Loss: 5.2111\n",
      "Epoch [6/10], Step [27075/68337], Loss: 5.0534\n",
      "Epoch [6/10], Step [27150/68337], Loss: 4.9601\n",
      "Epoch [6/10], Step [27225/68337], Loss: 4.9773\n",
      "Epoch [6/10], Step [27300/68337], Loss: 5.0873\n",
      "Epoch [6/10], Step [27375/68337], Loss: 5.0297\n",
      "Epoch [6/10], Step [27450/68337], Loss: 4.9641\n",
      "Epoch [6/10], Step [27525/68337], Loss: 5.1427\n",
      "Epoch [6/10], Step [27600/68337], Loss: 5.3226\n",
      "Epoch [6/10], Step [27675/68337], Loss: 5.0268\n",
      "Epoch [6/10], Step [27750/68337], Loss: 5.1677\n",
      "Epoch [6/10], Step [27825/68337], Loss: 5.0757\n",
      "Epoch [6/10], Step [27900/68337], Loss: 5.3217\n",
      "Epoch [6/10], Step [27975/68337], Loss: 5.0258\n",
      "Epoch [6/10], Step [28050/68337], Loss: 5.2044\n",
      "Epoch [6/10], Step [28125/68337], Loss: 5.0761\n",
      "Epoch [6/10], Step [28200/68337], Loss: 5.0994\n",
      "Epoch [6/10], Step [28275/68337], Loss: 5.0657\n",
      "Epoch [6/10], Step [28350/68337], Loss: 5.0755\n",
      "Epoch [6/10], Step [28425/68337], Loss: 5.2814\n",
      "Epoch [6/10], Step [28500/68337], Loss: 5.2426\n",
      "Epoch [6/10], Step [28575/68337], Loss: 4.9033\n",
      "Epoch [6/10], Step [28650/68337], Loss: 5.1758\n",
      "Epoch [6/10], Step [28725/68337], Loss: 5.1856\n",
      "Epoch [6/10], Step [28800/68337], Loss: 4.9882\n",
      "Epoch [6/10], Step [28875/68337], Loss: 5.0856\n",
      "Epoch [6/10], Step [28950/68337], Loss: 5.1294\n",
      "Epoch [6/10], Step [29025/68337], Loss: 5.1624\n",
      "Epoch [6/10], Step [29100/68337], Loss: 5.1522\n",
      "Epoch [6/10], Step [29175/68337], Loss: 5.1572\n",
      "Epoch [6/10], Step [29250/68337], Loss: 5.1723\n",
      "Epoch [6/10], Step [29325/68337], Loss: 5.3260\n",
      "Epoch [6/10], Step [29400/68337], Loss: 5.1023\n",
      "Epoch [6/10], Step [29475/68337], Loss: 5.2746\n",
      "Epoch [6/10], Step [29550/68337], Loss: 4.9624\n",
      "Epoch [6/10], Step [29625/68337], Loss: 5.1908\n",
      "Epoch [6/10], Step [29700/68337], Loss: 4.8695\n",
      "Epoch [6/10], Step [29775/68337], Loss: 5.2293\n",
      "Epoch [6/10], Step [29850/68337], Loss: 5.0796\n",
      "Epoch [6/10], Step [29925/68337], Loss: 5.2518\n",
      "Epoch [6/10], Step [30000/68337], Loss: 5.2127\n",
      "Validation perplexity: 126.59261727992121\n",
      "Epoch [6/10], Step [30075/68337], Loss: 5.0657\n",
      "Epoch [6/10], Step [30150/68337], Loss: 5.1448\n",
      "Epoch [6/10], Step [30225/68337], Loss: 5.2377\n",
      "Epoch [6/10], Step [30300/68337], Loss: 5.1244\n",
      "Epoch [6/10], Step [30375/68337], Loss: 5.1279\n",
      "Epoch [6/10], Step [30450/68337], Loss: 5.1470\n",
      "Epoch [6/10], Step [30525/68337], Loss: 5.2880\n",
      "Epoch [6/10], Step [30600/68337], Loss: 5.1470\n",
      "Epoch [6/10], Step [30675/68337], Loss: 4.8945\n",
      "Epoch [6/10], Step [30750/68337], Loss: 5.0182\n",
      "Epoch [6/10], Step [30825/68337], Loss: 5.2896\n",
      "Epoch [6/10], Step [30900/68337], Loss: 5.1000\n",
      "Epoch [6/10], Step [30975/68337], Loss: 5.1866\n",
      "Epoch [6/10], Step [31050/68337], Loss: 4.9240\n",
      "Epoch [6/10], Step [31125/68337], Loss: 5.0578\n",
      "Epoch [6/10], Step [31200/68337], Loss: 5.2985\n",
      "Epoch [6/10], Step [31275/68337], Loss: 5.1949\n",
      "Epoch [6/10], Step [31350/68337], Loss: 5.0968\n",
      "Epoch [6/10], Step [31425/68337], Loss: 5.0589\n",
      "Epoch [6/10], Step [31500/68337], Loss: 5.0634\n",
      "Epoch [6/10], Step [31575/68337], Loss: 5.0744\n",
      "Epoch [6/10], Step [31650/68337], Loss: 5.1003\n",
      "Epoch [6/10], Step [31725/68337], Loss: 5.0111\n",
      "Epoch [6/10], Step [31800/68337], Loss: 5.1697\n",
      "Epoch [6/10], Step [31875/68337], Loss: 5.1403\n",
      "Epoch [6/10], Step [31950/68337], Loss: 4.8906\n",
      "Epoch [6/10], Step [32025/68337], Loss: 5.1336\n",
      "Epoch [6/10], Step [32100/68337], Loss: 4.8764\n",
      "Epoch [6/10], Step [32175/68337], Loss: 5.1126\n",
      "Epoch [6/10], Step [32250/68337], Loss: 5.0213\n",
      "Epoch [6/10], Step [32325/68337], Loss: 5.0578\n",
      "Epoch [6/10], Step [32400/68337], Loss: 5.1740\n",
      "Epoch [6/10], Step [32475/68337], Loss: 5.0411\n",
      "Epoch [6/10], Step [32550/68337], Loss: 5.0424\n",
      "Epoch [6/10], Step [32625/68337], Loss: 5.2444\n",
      "Epoch [6/10], Step [32700/68337], Loss: 5.1372\n",
      "Epoch [6/10], Step [32775/68337], Loss: 5.0796\n",
      "Epoch [6/10], Step [32850/68337], Loss: 4.9653\n",
      "Epoch [6/10], Step [32925/68337], Loss: 5.0282\n",
      "Epoch [6/10], Step [33000/68337], Loss: 4.9735\n",
      "Epoch [6/10], Step [33075/68337], Loss: 5.2964\n",
      "Epoch [6/10], Step [33150/68337], Loss: 4.8981\n",
      "Epoch [6/10], Step [33225/68337], Loss: 5.2163\n",
      "Epoch [6/10], Step [33300/68337], Loss: 5.1214\n",
      "Epoch [6/10], Step [33375/68337], Loss: 5.1466\n",
      "Epoch [6/10], Step [33450/68337], Loss: 5.0986\n",
      "Epoch [6/10], Step [33525/68337], Loss: 5.0305\n",
      "Epoch [6/10], Step [33600/68337], Loss: 5.1785\n",
      "Epoch [6/10], Step [33675/68337], Loss: 5.1196\n",
      "Epoch [6/10], Step [33750/68337], Loss: 4.9571\n",
      "Epoch [6/10], Step [33825/68337], Loss: 5.0564\n",
      "Epoch [6/10], Step [33900/68337], Loss: 5.0052\n",
      "Epoch [6/10], Step [33975/68337], Loss: 5.1368\n",
      "Epoch [6/10], Step [34050/68337], Loss: 5.0403\n",
      "Epoch [6/10], Step [34125/68337], Loss: 5.1674\n",
      "Epoch [6/10], Step [34200/68337], Loss: 5.1862\n",
      "Epoch [6/10], Step [34275/68337], Loss: 5.1008\n",
      "Epoch [6/10], Step [34350/68337], Loss: 5.1436\n",
      "Epoch [6/10], Step [34425/68337], Loss: 5.1369\n",
      "Epoch [6/10], Step [34500/68337], Loss: 5.1335\n",
      "Epoch [6/10], Step [34575/68337], Loss: 4.9205\n",
      "Epoch [6/10], Step [34650/68337], Loss: 5.0875\n",
      "Epoch [6/10], Step [34725/68337], Loss: 5.0681\n",
      "Epoch [6/10], Step [34800/68337], Loss: 4.9977\n",
      "Epoch [6/10], Step [34875/68337], Loss: 5.0856\n",
      "Epoch [6/10], Step [34950/68337], Loss: 4.8951\n",
      "Epoch [6/10], Step [35025/68337], Loss: 5.0738\n",
      "Epoch [6/10], Step [35100/68337], Loss: 5.1626\n",
      "Epoch [6/10], Step [35175/68337], Loss: 5.1279\n",
      "Epoch [6/10], Step [35250/68337], Loss: 4.9777\n",
      "Epoch [6/10], Step [35325/68337], Loss: 5.1140\n",
      "Epoch [6/10], Step [35400/68337], Loss: 5.1605\n",
      "Epoch [6/10], Step [35475/68337], Loss: 5.1622\n",
      "Epoch [6/10], Step [35550/68337], Loss: 5.1452\n",
      "Epoch [6/10], Step [35625/68337], Loss: 5.1732\n",
      "Epoch [6/10], Step [35700/68337], Loss: 5.2008\n",
      "Epoch [6/10], Step [35775/68337], Loss: 5.2007\n",
      "Epoch [6/10], Step [35850/68337], Loss: 5.0954\n",
      "Epoch [6/10], Step [35925/68337], Loss: 4.9344\n",
      "Epoch [6/10], Step [36000/68337], Loss: 4.8175\n",
      "Epoch [6/10], Step [36075/68337], Loss: 5.1845\n",
      "Epoch [6/10], Step [36150/68337], Loss: 5.1385\n",
      "Epoch [6/10], Step [36225/68337], Loss: 4.8391\n",
      "Epoch [6/10], Step [36300/68337], Loss: 5.0772\n",
      "Epoch [6/10], Step [36375/68337], Loss: 5.1165\n",
      "Epoch [6/10], Step [36450/68337], Loss: 4.9387\n",
      "Epoch [6/10], Step [36525/68337], Loss: 5.0054\n",
      "Epoch [6/10], Step [36600/68337], Loss: 5.2350\n",
      "Epoch [6/10], Step [36675/68337], Loss: 5.1373\n",
      "Epoch [6/10], Step [36750/68337], Loss: 5.0030\n",
      "Epoch [6/10], Step [36825/68337], Loss: 5.2928\n",
      "Epoch [6/10], Step [36900/68337], Loss: 5.1449\n",
      "Epoch [6/10], Step [36975/68337], Loss: 4.9021\n",
      "Epoch [6/10], Step [37050/68337], Loss: 5.0918\n",
      "Epoch [6/10], Step [37125/68337], Loss: 5.0823\n",
      "Epoch [6/10], Step [37200/68337], Loss: 4.9239\n",
      "Epoch [6/10], Step [37275/68337], Loss: 4.9998\n",
      "Epoch [6/10], Step [37350/68337], Loss: 5.0328\n",
      "Epoch [6/10], Step [37425/68337], Loss: 4.9451\n",
      "Epoch [6/10], Step [37500/68337], Loss: 5.1489\n",
      "Epoch [6/10], Step [37575/68337], Loss: 5.0961\n",
      "Epoch [6/10], Step [37650/68337], Loss: 5.0779\n",
      "Epoch [6/10], Step [37725/68337], Loss: 5.1480\n",
      "Epoch [6/10], Step [37800/68337], Loss: 5.2568\n",
      "Epoch [6/10], Step [37875/68337], Loss: 4.9576\n",
      "Epoch [6/10], Step [37950/68337], Loss: 5.1846\n",
      "Epoch [6/10], Step [38025/68337], Loss: 4.9749\n",
      "Epoch [6/10], Step [38100/68337], Loss: 5.2091\n",
      "Epoch [6/10], Step [38175/68337], Loss: 5.0828\n",
      "Epoch [6/10], Step [38250/68337], Loss: 5.0340\n",
      "Epoch [6/10], Step [38325/68337], Loss: 5.1712\n",
      "Epoch [6/10], Step [38400/68337], Loss: 5.1564\n",
      "Epoch [6/10], Step [38475/68337], Loss: 5.3121\n",
      "Epoch [6/10], Step [38550/68337], Loss: 5.0018\n",
      "Epoch [6/10], Step [38625/68337], Loss: 5.2197\n",
      "Epoch [6/10], Step [38700/68337], Loss: 5.2173\n",
      "Epoch [6/10], Step [38775/68337], Loss: 5.1854\n",
      "Epoch [6/10], Step [38850/68337], Loss: 5.0596\n",
      "Epoch [6/10], Step [38925/68337], Loss: 5.2829\n",
      "Epoch [6/10], Step [39000/68337], Loss: 4.9637\n",
      "Epoch [6/10], Step [39075/68337], Loss: 5.0741\n",
      "Epoch [6/10], Step [39150/68337], Loss: 5.2612\n",
      "Epoch [6/10], Step [39225/68337], Loss: 5.2121\n",
      "Epoch [6/10], Step [39300/68337], Loss: 5.0197\n",
      "Epoch [6/10], Step [39375/68337], Loss: 4.8566\n",
      "Epoch [6/10], Step [39450/68337], Loss: 5.0683\n",
      "Epoch [6/10], Step [39525/68337], Loss: 4.9350\n",
      "Epoch [6/10], Step [39600/68337], Loss: 5.1556\n",
      "Epoch [6/10], Step [39675/68337], Loss: 5.3857\n",
      "Epoch [6/10], Step [39750/68337], Loss: 5.1339\n",
      "Epoch [6/10], Step [39825/68337], Loss: 5.1113\n",
      "Epoch [6/10], Step [39900/68337], Loss: 5.1670\n",
      "Epoch [6/10], Step [39975/68337], Loss: 5.2003\n",
      "Validation perplexity: 127.16267145329736\n",
      "Epoch [6/10], Step [40050/68337], Loss: 5.2370\n",
      "Epoch [6/10], Step [40125/68337], Loss: 5.2382\n",
      "Epoch [6/10], Step [40200/68337], Loss: 4.9606\n",
      "Epoch [6/10], Step [40275/68337], Loss: 5.0851\n",
      "Epoch [6/10], Step [40350/68337], Loss: 5.2282\n",
      "Epoch [6/10], Step [40425/68337], Loss: 5.0298\n",
      "Epoch [6/10], Step [40500/68337], Loss: 4.9545\n",
      "Epoch [6/10], Step [40575/68337], Loss: 5.0511\n",
      "Epoch [6/10], Step [40650/68337], Loss: 5.0463\n",
      "Epoch [6/10], Step [40725/68337], Loss: 5.2191\n",
      "Epoch [6/10], Step [40800/68337], Loss: 5.2295\n",
      "Epoch [6/10], Step [40875/68337], Loss: 5.1595\n",
      "Epoch [6/10], Step [40950/68337], Loss: 5.2694\n",
      "Epoch [6/10], Step [41025/68337], Loss: 4.9672\n",
      "Epoch [6/10], Step [41100/68337], Loss: 5.2397\n",
      "Epoch [6/10], Step [41175/68337], Loss: 4.9043\n",
      "Epoch [6/10], Step [41250/68337], Loss: 5.0394\n",
      "Epoch [6/10], Step [41325/68337], Loss: 4.9052\n",
      "Epoch [6/10], Step [41400/68337], Loss: 5.1092\n",
      "Epoch [6/10], Step [41475/68337], Loss: 4.9600\n",
      "Epoch [6/10], Step [41550/68337], Loss: 5.2844\n",
      "Epoch [6/10], Step [41625/68337], Loss: 4.9930\n",
      "Epoch [6/10], Step [41700/68337], Loss: 5.1544\n",
      "Epoch [6/10], Step [41775/68337], Loss: 5.0181\n",
      "Epoch [6/10], Step [41850/68337], Loss: 5.1563\n",
      "Epoch [6/10], Step [41925/68337], Loss: 5.3352\n",
      "Epoch [6/10], Step [42000/68337], Loss: 5.0160\n",
      "Epoch [6/10], Step [42075/68337], Loss: 5.1184\n",
      "Epoch [6/10], Step [42150/68337], Loss: 5.0836\n",
      "Epoch [6/10], Step [42225/68337], Loss: 5.2039\n",
      "Epoch [6/10], Step [42300/68337], Loss: 5.0964\n",
      "Epoch [6/10], Step [42375/68337], Loss: 5.0625\n",
      "Epoch [6/10], Step [42450/68337], Loss: 5.2661\n",
      "Epoch [6/10], Step [42525/68337], Loss: 5.1312\n",
      "Epoch [6/10], Step [42600/68337], Loss: 5.0927\n",
      "Epoch [6/10], Step [42675/68337], Loss: 5.1681\n",
      "Epoch [6/10], Step [42750/68337], Loss: 5.0702\n",
      "Epoch [6/10], Step [42825/68337], Loss: 5.2486\n",
      "Epoch [6/10], Step [42900/68337], Loss: 5.1156\n",
      "Epoch [6/10], Step [42975/68337], Loss: 5.1922\n",
      "Epoch [6/10], Step [43050/68337], Loss: 5.1807\n",
      "Epoch [6/10], Step [43125/68337], Loss: 5.2261\n",
      "Epoch [6/10], Step [43200/68337], Loss: 4.9990\n",
      "Epoch [6/10], Step [43275/68337], Loss: 5.2813\n",
      "Epoch [6/10], Step [43350/68337], Loss: 5.2362\n",
      "Epoch [6/10], Step [43425/68337], Loss: 5.2089\n",
      "Epoch [6/10], Step [43500/68337], Loss: 4.9174\n",
      "Epoch [6/10], Step [43575/68337], Loss: 5.1306\n",
      "Epoch [6/10], Step [43650/68337], Loss: 5.0024\n",
      "Epoch [6/10], Step [43725/68337], Loss: 4.9635\n",
      "Epoch [6/10], Step [43800/68337], Loss: 4.9481\n",
      "Epoch [6/10], Step [43875/68337], Loss: 5.0160\n",
      "Epoch [6/10], Step [43950/68337], Loss: 5.0838\n",
      "Epoch [6/10], Step [44025/68337], Loss: 5.0796\n",
      "Epoch [6/10], Step [44100/68337], Loss: 4.9622\n",
      "Epoch [6/10], Step [44175/68337], Loss: 5.0822\n",
      "Epoch [6/10], Step [44250/68337], Loss: 5.2267\n",
      "Epoch [6/10], Step [44325/68337], Loss: 5.0058\n",
      "Epoch [6/10], Step [44400/68337], Loss: 5.2507\n",
      "Epoch [6/10], Step [44475/68337], Loss: 5.0983\n",
      "Epoch [6/10], Step [44550/68337], Loss: 5.0813\n",
      "Epoch [6/10], Step [44625/68337], Loss: 5.1080\n",
      "Epoch [6/10], Step [44700/68337], Loss: 5.0563\n",
      "Epoch [6/10], Step [44775/68337], Loss: 5.1428\n",
      "Epoch [6/10], Step [44850/68337], Loss: 5.2695\n",
      "Epoch [6/10], Step [44925/68337], Loss: 5.1627\n",
      "Epoch [6/10], Step [45000/68337], Loss: 5.2628\n",
      "Epoch [6/10], Step [45075/68337], Loss: 5.2743\n",
      "Epoch [6/10], Step [45150/68337], Loss: 5.1521\n",
      "Epoch [6/10], Step [45225/68337], Loss: 5.1261\n",
      "Epoch [6/10], Step [45300/68337], Loss: 5.1617\n",
      "Epoch [6/10], Step [45375/68337], Loss: 5.3919\n",
      "Epoch [6/10], Step [45450/68337], Loss: 5.1137\n",
      "Epoch [6/10], Step [45525/68337], Loss: 5.1933\n",
      "Epoch [6/10], Step [45600/68337], Loss: 5.2065\n",
      "Epoch [6/10], Step [45675/68337], Loss: 4.9597\n",
      "Epoch [6/10], Step [45750/68337], Loss: 5.2609\n",
      "Epoch [6/10], Step [45825/68337], Loss: 5.1962\n",
      "Epoch [6/10], Step [45900/68337], Loss: 5.0825\n",
      "Epoch [6/10], Step [45975/68337], Loss: 5.0361\n",
      "Epoch [6/10], Step [46050/68337], Loss: 5.3138\n",
      "Epoch [6/10], Step [46125/68337], Loss: 5.2488\n",
      "Epoch [6/10], Step [46200/68337], Loss: 5.2800\n",
      "Epoch [6/10], Step [46275/68337], Loss: 5.1852\n",
      "Epoch [6/10], Step [46350/68337], Loss: 5.0571\n",
      "Epoch [6/10], Step [46425/68337], Loss: 4.9297\n",
      "Epoch [6/10], Step [46500/68337], Loss: 5.1322\n",
      "Epoch [6/10], Step [46575/68337], Loss: 4.9905\n",
      "Epoch [6/10], Step [46650/68337], Loss: 5.0982\n",
      "Epoch [6/10], Step [46725/68337], Loss: 4.8974\n",
      "Epoch [6/10], Step [46800/68337], Loss: 5.0627\n",
      "Epoch [6/10], Step [46875/68337], Loss: 5.0673\n",
      "Epoch [6/10], Step [46950/68337], Loss: 5.0713\n",
      "Epoch [6/10], Step [47025/68337], Loss: 5.1612\n",
      "Epoch [6/10], Step [47100/68337], Loss: 5.1299\n",
      "Epoch [6/10], Step [47175/68337], Loss: 4.9535\n",
      "Epoch [6/10], Step [47250/68337], Loss: 4.9941\n",
      "Epoch [6/10], Step [47325/68337], Loss: 5.0226\n",
      "Epoch [6/10], Step [47400/68337], Loss: 5.1327\n",
      "Epoch [6/10], Step [47475/68337], Loss: 4.8774\n",
      "Epoch [6/10], Step [47550/68337], Loss: 4.9929\n",
      "Epoch [6/10], Step [47625/68337], Loss: 5.0738\n",
      "Epoch [6/10], Step [47700/68337], Loss: 5.2699\n",
      "Epoch [6/10], Step [47775/68337], Loss: 5.3358\n",
      "Epoch [6/10], Step [47850/68337], Loss: 5.2030\n",
      "Epoch [6/10], Step [47925/68337], Loss: 5.0957\n",
      "Epoch [6/10], Step [48000/68337], Loss: 5.1842\n",
      "Epoch [6/10], Step [48075/68337], Loss: 4.9504\n",
      "Epoch [6/10], Step [48150/68337], Loss: 4.9607\n",
      "Epoch [6/10], Step [48225/68337], Loss: 5.1884\n",
      "Epoch [6/10], Step [48300/68337], Loss: 5.0862\n",
      "Epoch [6/10], Step [48375/68337], Loss: 5.1331\n",
      "Epoch [6/10], Step [48450/68337], Loss: 5.0876\n",
      "Epoch [6/10], Step [48525/68337], Loss: 5.1514\n",
      "Epoch [6/10], Step [48600/68337], Loss: 5.1576\n",
      "Epoch [6/10], Step [48675/68337], Loss: 4.9850\n",
      "Epoch [6/10], Step [48750/68337], Loss: 5.1744\n",
      "Epoch [6/10], Step [48825/68337], Loss: 5.0131\n",
      "Epoch [6/10], Step [48900/68337], Loss: 4.9091\n",
      "Epoch [6/10], Step [48975/68337], Loss: 5.0025\n",
      "Epoch [6/10], Step [49050/68337], Loss: 5.1750\n",
      "Epoch [6/10], Step [49125/68337], Loss: 4.9830\n",
      "Epoch [6/10], Step [49200/68337], Loss: 5.1735\n",
      "Epoch [6/10], Step [49275/68337], Loss: 5.0535\n",
      "Epoch [6/10], Step [49350/68337], Loss: 5.2253\n",
      "Epoch [6/10], Step [49425/68337], Loss: 5.2065\n",
      "Epoch [6/10], Step [49500/68337], Loss: 5.0801\n",
      "Epoch [6/10], Step [49575/68337], Loss: 5.1811\n",
      "Epoch [6/10], Step [49650/68337], Loss: 5.0166\n",
      "Epoch [6/10], Step [49725/68337], Loss: 5.2653\n",
      "Epoch [6/10], Step [49800/68337], Loss: 4.9724\n",
      "Epoch [6/10], Step [49875/68337], Loss: 5.1994\n",
      "Epoch [6/10], Step [49950/68337], Loss: 5.1237\n",
      "Validation perplexity: 126.57378245738921\n",
      "Epoch [6/10], Step [50025/68337], Loss: 5.2052\n",
      "Epoch [6/10], Step [50100/68337], Loss: 5.2159\n",
      "Epoch [6/10], Step [50175/68337], Loss: 5.2437\n",
      "Epoch [6/10], Step [50250/68337], Loss: 4.8373\n",
      "Epoch [6/10], Step [50325/68337], Loss: 5.1976\n",
      "Epoch [6/10], Step [50400/68337], Loss: 5.1936\n",
      "Epoch [6/10], Step [50475/68337], Loss: 5.3374\n",
      "Epoch [6/10], Step [50550/68337], Loss: 4.9714\n",
      "Epoch [6/10], Step [50625/68337], Loss: 4.8500\n",
      "Epoch [6/10], Step [50700/68337], Loss: 5.3082\n",
      "Epoch [6/10], Step [50775/68337], Loss: 4.9506\n",
      "Epoch [6/10], Step [50850/68337], Loss: 5.1502\n",
      "Epoch [6/10], Step [50925/68337], Loss: 4.9419\n",
      "Epoch [6/10], Step [51000/68337], Loss: 5.2840\n",
      "Epoch [6/10], Step [51075/68337], Loss: 5.0002\n",
      "Epoch [6/10], Step [51150/68337], Loss: 5.1758\n",
      "Epoch [6/10], Step [51225/68337], Loss: 5.2442\n",
      "Epoch [6/10], Step [51300/68337], Loss: 5.1588\n",
      "Epoch [6/10], Step [51375/68337], Loss: 5.1954\n",
      "Epoch [6/10], Step [51450/68337], Loss: 5.2471\n",
      "Epoch [6/10], Step [51525/68337], Loss: 5.1068\n",
      "Epoch [6/10], Step [51600/68337], Loss: 4.9808\n",
      "Epoch [6/10], Step [51675/68337], Loss: 5.0101\n",
      "Epoch [6/10], Step [51750/68337], Loss: 5.1592\n",
      "Epoch [6/10], Step [51825/68337], Loss: 5.3825\n",
      "Epoch [6/10], Step [51900/68337], Loss: 5.0699\n",
      "Epoch [6/10], Step [51975/68337], Loss: 5.2465\n",
      "Epoch [6/10], Step [52050/68337], Loss: 5.2026\n",
      "Epoch [6/10], Step [52125/68337], Loss: 5.3340\n",
      "Epoch [6/10], Step [52200/68337], Loss: 5.0549\n",
      "Epoch [6/10], Step [52275/68337], Loss: 5.0417\n",
      "Epoch [6/10], Step [52350/68337], Loss: 4.8512\n",
      "Epoch [6/10], Step [52425/68337], Loss: 5.1258\n",
      "Epoch [6/10], Step [52500/68337], Loss: 4.9038\n",
      "Epoch [6/10], Step [52575/68337], Loss: 5.4069\n",
      "Epoch [6/10], Step [52650/68337], Loss: 5.1116\n",
      "Epoch [6/10], Step [52725/68337], Loss: 5.1656\n",
      "Epoch [6/10], Step [52800/68337], Loss: 5.1803\n",
      "Epoch [6/10], Step [52875/68337], Loss: 5.2648\n",
      "Epoch [6/10], Step [52950/68337], Loss: 5.1533\n",
      "Epoch [6/10], Step [53025/68337], Loss: 5.1354\n",
      "Epoch [6/10], Step [53100/68337], Loss: 4.9231\n",
      "Epoch [6/10], Step [53175/68337], Loss: 4.9924\n",
      "Epoch [6/10], Step [53250/68337], Loss: 4.9847\n",
      "Epoch [6/10], Step [53325/68337], Loss: 4.9225\n",
      "Epoch [6/10], Step [53400/68337], Loss: 5.2706\n",
      "Epoch [6/10], Step [53475/68337], Loss: 4.9642\n",
      "Epoch [6/10], Step [53550/68337], Loss: 5.1421\n",
      "Epoch [6/10], Step [53625/68337], Loss: 5.0891\n",
      "Epoch [6/10], Step [53700/68337], Loss: 5.1829\n",
      "Epoch [6/10], Step [53775/68337], Loss: 5.1539\n",
      "Epoch [6/10], Step [53850/68337], Loss: 5.2162\n",
      "Epoch [6/10], Step [53925/68337], Loss: 5.2602\n",
      "Epoch [6/10], Step [54000/68337], Loss: 5.2377\n",
      "Epoch [6/10], Step [54075/68337], Loss: 5.2825\n",
      "Epoch [6/10], Step [54150/68337], Loss: 5.0520\n",
      "Epoch [6/10], Step [54225/68337], Loss: 5.0363\n",
      "Epoch [6/10], Step [54300/68337], Loss: 5.1174\n",
      "Epoch [6/10], Step [54375/68337], Loss: 5.0788\n",
      "Epoch [6/10], Step [54450/68337], Loss: 5.1445\n",
      "Epoch [6/10], Step [54525/68337], Loss: 5.1930\n",
      "Epoch [6/10], Step [54600/68337], Loss: 5.1930\n",
      "Epoch [6/10], Step [54675/68337], Loss: 5.1167\n",
      "Epoch [6/10], Step [54750/68337], Loss: 5.0044\n",
      "Epoch [6/10], Step [54825/68337], Loss: 5.0327\n",
      "Epoch [6/10], Step [54900/68337], Loss: 5.1459\n",
      "Epoch [6/10], Step [54975/68337], Loss: 5.1474\n",
      "Epoch [6/10], Step [55050/68337], Loss: 5.2064\n",
      "Epoch [6/10], Step [55125/68337], Loss: 5.0596\n",
      "Epoch [6/10], Step [55200/68337], Loss: 5.2061\n",
      "Epoch [6/10], Step [55275/68337], Loss: 5.0221\n",
      "Epoch [6/10], Step [55350/68337], Loss: 5.0851\n",
      "Epoch [6/10], Step [55425/68337], Loss: 4.9665\n",
      "Epoch [6/10], Step [55500/68337], Loss: 4.9814\n",
      "Epoch [6/10], Step [55575/68337], Loss: 5.0693\n",
      "Epoch [6/10], Step [55650/68337], Loss: 5.0560\n",
      "Epoch [6/10], Step [55725/68337], Loss: 4.9830\n",
      "Epoch [6/10], Step [55800/68337], Loss: 5.1855\n",
      "Epoch [6/10], Step [55875/68337], Loss: 5.2011\n",
      "Epoch [6/10], Step [55950/68337], Loss: 5.2150\n",
      "Epoch [6/10], Step [56025/68337], Loss: 5.2029\n",
      "Epoch [6/10], Step [56100/68337], Loss: 5.2502\n",
      "Epoch [6/10], Step [56175/68337], Loss: 5.1391\n",
      "Epoch [6/10], Step [56250/68337], Loss: 4.9129\n",
      "Epoch [6/10], Step [56325/68337], Loss: 4.9573\n",
      "Epoch [6/10], Step [56400/68337], Loss: 5.2042\n",
      "Epoch [6/10], Step [56475/68337], Loss: 5.0087\n",
      "Epoch [6/10], Step [56550/68337], Loss: 5.1407\n",
      "Epoch [6/10], Step [56625/68337], Loss: 5.1083\n",
      "Epoch [6/10], Step [56700/68337], Loss: 5.1318\n",
      "Epoch [6/10], Step [56775/68337], Loss: 4.9726\n",
      "Epoch [6/10], Step [56850/68337], Loss: 5.2500\n",
      "Epoch [6/10], Step [56925/68337], Loss: 5.1759\n",
      "Epoch [6/10], Step [57000/68337], Loss: 5.0589\n",
      "Epoch [6/10], Step [57075/68337], Loss: 5.2567\n",
      "Epoch [6/10], Step [57150/68337], Loss: 4.9587\n",
      "Epoch [6/10], Step [57225/68337], Loss: 5.1974\n",
      "Epoch [6/10], Step [57300/68337], Loss: 5.1339\n",
      "Epoch [6/10], Step [57375/68337], Loss: 4.9988\n",
      "Epoch [6/10], Step [57450/68337], Loss: 4.9415\n",
      "Epoch [6/10], Step [57525/68337], Loss: 5.2799\n",
      "Epoch [6/10], Step [57600/68337], Loss: 5.2687\n",
      "Epoch [6/10], Step [57675/68337], Loss: 5.1946\n",
      "Epoch [6/10], Step [57750/68337], Loss: 5.0484\n",
      "Epoch [6/10], Step [57825/68337], Loss: 4.9766\n",
      "Epoch [6/10], Step [57900/68337], Loss: 5.1519\n",
      "Epoch [6/10], Step [57975/68337], Loss: 5.2502\n",
      "Epoch [6/10], Step [58050/68337], Loss: 5.1485\n",
      "Epoch [6/10], Step [58125/68337], Loss: 5.1210\n",
      "Epoch [6/10], Step [58200/68337], Loss: 5.1696\n",
      "Epoch [6/10], Step [58275/68337], Loss: 5.1147\n",
      "Epoch [6/10], Step [58350/68337], Loss: 5.1670\n",
      "Epoch [6/10], Step [58425/68337], Loss: 5.2902\n",
      "Epoch [6/10], Step [58500/68337], Loss: 5.2475\n",
      "Epoch [6/10], Step [58575/68337], Loss: 5.1691\n",
      "Epoch [6/10], Step [58650/68337], Loss: 5.0070\n",
      "Epoch [6/10], Step [58725/68337], Loss: 5.1322\n",
      "Epoch [6/10], Step [58800/68337], Loss: 5.0158\n",
      "Epoch [6/10], Step [58875/68337], Loss: 5.0212\n",
      "Epoch [6/10], Step [58950/68337], Loss: 5.1685\n",
      "Epoch [6/10], Step [59025/68337], Loss: 5.0948\n",
      "Epoch [6/10], Step [59100/68337], Loss: 5.0623\n",
      "Epoch [6/10], Step [59175/68337], Loss: 5.2981\n",
      "Epoch [6/10], Step [59250/68337], Loss: 5.0903\n",
      "Epoch [6/10], Step [59325/68337], Loss: 5.0334\n",
      "Epoch [6/10], Step [59400/68337], Loss: 5.2133\n",
      "Epoch [6/10], Step [59475/68337], Loss: 5.1017\n",
      "Epoch [6/10], Step [59550/68337], Loss: 5.0566\n",
      "Epoch [6/10], Step [59625/68337], Loss: 5.0050\n",
      "Epoch [6/10], Step [59700/68337], Loss: 5.1732\n",
      "Epoch [6/10], Step [59775/68337], Loss: 5.0737\n",
      "Epoch [6/10], Step [59850/68337], Loss: 5.2599\n",
      "Epoch [6/10], Step [59925/68337], Loss: 5.1418\n",
      "Epoch [6/10], Step [60000/68337], Loss: 5.1351\n",
      "Validation perplexity: 126.23245437380426\n",
      "Epoch [6/10], Step [60075/68337], Loss: 5.1959\n",
      "Epoch [6/10], Step [60150/68337], Loss: 5.1139\n",
      "Epoch [6/10], Step [60225/68337], Loss: 5.2644\n",
      "Epoch [6/10], Step [60300/68337], Loss: 5.0732\n",
      "Epoch [6/10], Step [60375/68337], Loss: 5.1925\n",
      "Epoch [6/10], Step [60450/68337], Loss: 5.1492\n",
      "Epoch [6/10], Step [60525/68337], Loss: 5.2539\n",
      "Epoch [6/10], Step [60600/68337], Loss: 5.1695\n",
      "Epoch [6/10], Step [60675/68337], Loss: 5.2531\n",
      "Epoch [6/10], Step [60750/68337], Loss: 5.0634\n",
      "Epoch [6/10], Step [60825/68337], Loss: 5.0917\n",
      "Epoch [6/10], Step [60900/68337], Loss: 5.2524\n",
      "Epoch [6/10], Step [60975/68337], Loss: 5.1690\n",
      "Epoch [6/10], Step [61050/68337], Loss: 5.2633\n",
      "Epoch [6/10], Step [61125/68337], Loss: 5.2103\n",
      "Epoch [6/10], Step [61200/68337], Loss: 5.0480\n",
      "Epoch [6/10], Step [61275/68337], Loss: 5.1183\n",
      "Epoch [6/10], Step [61350/68337], Loss: 5.1129\n",
      "Epoch [6/10], Step [61425/68337], Loss: 5.1243\n",
      "Epoch [6/10], Step [61500/68337], Loss: 5.1727\n",
      "Epoch [6/10], Step [61575/68337], Loss: 4.9848\n",
      "Epoch [6/10], Step [61650/68337], Loss: 4.8780\n",
      "Epoch [6/10], Step [61725/68337], Loss: 5.0720\n",
      "Epoch [6/10], Step [61800/68337], Loss: 5.1827\n",
      "Epoch [6/10], Step [61875/68337], Loss: 5.1638\n",
      "Epoch [6/10], Step [61950/68337], Loss: 4.9622\n",
      "Epoch [6/10], Step [62025/68337], Loss: 5.0684\n",
      "Epoch [6/10], Step [62100/68337], Loss: 5.1025\n",
      "Epoch [6/10], Step [62175/68337], Loss: 5.1017\n",
      "Epoch [6/10], Step [62250/68337], Loss: 4.9669\n",
      "Epoch [6/10], Step [62325/68337], Loss: 5.2599\n",
      "Epoch [6/10], Step [62400/68337], Loss: 5.1522\n",
      "Epoch [6/10], Step [62475/68337], Loss: 4.7259\n",
      "Epoch [6/10], Step [62550/68337], Loss: 5.0992\n",
      "Epoch [6/10], Step [62625/68337], Loss: 5.0270\n",
      "Epoch [6/10], Step [62700/68337], Loss: 4.9432\n",
      "Epoch [6/10], Step [62775/68337], Loss: 5.2380\n",
      "Epoch [6/10], Step [62850/68337], Loss: 5.2744\n",
      "Epoch [6/10], Step [62925/68337], Loss: 5.1377\n",
      "Epoch [6/10], Step [63000/68337], Loss: 5.3296\n",
      "Epoch [6/10], Step [63075/68337], Loss: 4.9482\n",
      "Epoch [6/10], Step [63150/68337], Loss: 5.2684\n",
      "Epoch [6/10], Step [63225/68337], Loss: 4.8685\n",
      "Epoch [6/10], Step [63300/68337], Loss: 5.1400\n",
      "Epoch [6/10], Step [63375/68337], Loss: 5.0004\n",
      "Epoch [6/10], Step [63450/68337], Loss: 4.9376\n",
      "Epoch [6/10], Step [63525/68337], Loss: 4.9989\n",
      "Epoch [6/10], Step [63600/68337], Loss: 5.0957\n",
      "Epoch [6/10], Step [63675/68337], Loss: 4.9382\n",
      "Epoch [6/10], Step [63750/68337], Loss: 5.1835\n",
      "Epoch [6/10], Step [63825/68337], Loss: 5.1511\n",
      "Epoch [6/10], Step [63900/68337], Loss: 5.2408\n",
      "Epoch [6/10], Step [63975/68337], Loss: 5.0135\n",
      "Epoch [6/10], Step [64050/68337], Loss: 5.1497\n",
      "Epoch [6/10], Step [64125/68337], Loss: 5.0388\n",
      "Epoch [6/10], Step [64200/68337], Loss: 5.1844\n",
      "Epoch [6/10], Step [64275/68337], Loss: 5.0044\n",
      "Epoch [6/10], Step [64350/68337], Loss: 5.1786\n",
      "Epoch [6/10], Step [64425/68337], Loss: 5.0877\n",
      "Epoch [6/10], Step [64500/68337], Loss: 5.2024\n",
      "Epoch [6/10], Step [64575/68337], Loss: 5.1226\n",
      "Epoch [6/10], Step [64650/68337], Loss: 4.8837\n",
      "Epoch [6/10], Step [64725/68337], Loss: 5.0028\n",
      "Epoch [6/10], Step [64800/68337], Loss: 5.0996\n",
      "Epoch [6/10], Step [64875/68337], Loss: 4.8981\n",
      "Epoch [6/10], Step [64950/68337], Loss: 5.0414\n",
      "Epoch [6/10], Step [65025/68337], Loss: 4.8498\n",
      "Epoch [6/10], Step [65100/68337], Loss: 5.2448\n",
      "Epoch [6/10], Step [65175/68337], Loss: 5.0873\n",
      "Epoch [6/10], Step [65250/68337], Loss: 5.0977\n",
      "Epoch [6/10], Step [65325/68337], Loss: 5.1069\n",
      "Epoch [6/10], Step [65400/68337], Loss: 5.2845\n",
      "Epoch [6/10], Step [65475/68337], Loss: 5.0750\n",
      "Epoch [6/10], Step [65550/68337], Loss: 5.0306\n",
      "Epoch [6/10], Step [65625/68337], Loss: 5.1660\n",
      "Epoch [6/10], Step [65700/68337], Loss: 5.1174\n",
      "Epoch [6/10], Step [65775/68337], Loss: 5.2381\n",
      "Epoch [6/10], Step [65850/68337], Loss: 5.2047\n",
      "Epoch [6/10], Step [65925/68337], Loss: 5.0808\n",
      "Epoch [6/10], Step [66000/68337], Loss: 5.0002\n",
      "Epoch [6/10], Step [66075/68337], Loss: 4.8182\n",
      "Epoch [6/10], Step [66150/68337], Loss: 5.1090\n",
      "Epoch [6/10], Step [66225/68337], Loss: 5.1225\n",
      "Epoch [6/10], Step [66300/68337], Loss: 5.3578\n",
      "Epoch [6/10], Step [66375/68337], Loss: 5.1507\n",
      "Epoch [6/10], Step [66450/68337], Loss: 5.1068\n",
      "Epoch [6/10], Step [66525/68337], Loss: 5.0977\n",
      "Epoch [6/10], Step [66600/68337], Loss: 5.1421\n",
      "Epoch [6/10], Step [66675/68337], Loss: 4.8334\n",
      "Epoch [6/10], Step [66750/68337], Loss: 5.2427\n",
      "Epoch [6/10], Step [66825/68337], Loss: 5.1087\n",
      "Epoch [6/10], Step [66900/68337], Loss: 4.9772\n",
      "Epoch [6/10], Step [66975/68337], Loss: 5.1016\n",
      "Epoch [6/10], Step [67050/68337], Loss: 4.9881\n",
      "Epoch [6/10], Step [67125/68337], Loss: 5.1071\n",
      "Epoch [6/10], Step [67200/68337], Loss: 5.0046\n",
      "Epoch [6/10], Step [67275/68337], Loss: 4.9349\n",
      "Epoch [6/10], Step [67350/68337], Loss: 4.9136\n",
      "Epoch [6/10], Step [67425/68337], Loss: 5.0493\n",
      "Epoch [6/10], Step [67500/68337], Loss: 4.9620\n",
      "Epoch [6/10], Step [67575/68337], Loss: 4.9438\n",
      "Epoch [6/10], Step [67650/68337], Loss: 5.2046\n",
      "Epoch [6/10], Step [67725/68337], Loss: 5.0367\n",
      "Epoch [6/10], Step [67800/68337], Loss: 4.9388\n",
      "Epoch [6/10], Step [67875/68337], Loss: 5.1232\n",
      "Epoch [6/10], Step [67950/68337], Loss: 5.3951\n",
      "Epoch [6/10], Step [68025/68337], Loss: 5.0459\n",
      "Epoch [6/10], Step [68100/68337], Loss: 5.4209\n",
      "Epoch [6/10], Step [68175/68337], Loss: 5.1769\n",
      "Epoch [6/10], Step [68250/68337], Loss: 5.0178\n",
      "Epoch [6/10], Step [68325/68337], Loss: 5.0210\n",
      "Epoch [6/10] Average Loss: 5.1094, Perplexity: 165.57\n",
      "Epoch [7/10], Step [0/68337], Loss: 5.0409\n",
      "Validation perplexity: 125.69282905022384\n",
      "Epoch [7/10], Step [75/68337], Loss: 5.0616\n",
      "Epoch [7/10], Step [150/68337], Loss: 5.0353\n",
      "Epoch [7/10], Step [225/68337], Loss: 5.1733\n",
      "Epoch [7/10], Step [300/68337], Loss: 4.7954\n",
      "Epoch [7/10], Step [375/68337], Loss: 4.7465\n",
      "Epoch [7/10], Step [450/68337], Loss: 5.2261\n",
      "Epoch [7/10], Step [525/68337], Loss: 5.0359\n",
      "Epoch [7/10], Step [600/68337], Loss: 5.1376\n",
      "Epoch [7/10], Step [675/68337], Loss: 5.2249\n",
      "Epoch [7/10], Step [750/68337], Loss: 5.1126\n",
      "Epoch [7/10], Step [825/68337], Loss: 4.8910\n",
      "Epoch [7/10], Step [900/68337], Loss: 5.1117\n",
      "Epoch [7/10], Step [975/68337], Loss: 5.3199\n",
      "Epoch [7/10], Step [1050/68337], Loss: 5.0367\n",
      "Epoch [7/10], Step [1125/68337], Loss: 5.0010\n",
      "Epoch [7/10], Step [1200/68337], Loss: 5.1620\n",
      "Epoch [7/10], Step [1275/68337], Loss: 5.2872\n",
      "Epoch [7/10], Step [1350/68337], Loss: 5.0321\n",
      "Epoch [7/10], Step [1425/68337], Loss: 5.0172\n",
      "Epoch [7/10], Step [1500/68337], Loss: 5.0247\n",
      "Epoch [7/10], Step [1575/68337], Loss: 5.0082\n",
      "Epoch [7/10], Step [1650/68337], Loss: 5.1800\n",
      "Epoch [7/10], Step [1725/68337], Loss: 5.0960\n",
      "Epoch [7/10], Step [1800/68337], Loss: 5.3318\n",
      "Epoch [7/10], Step [1875/68337], Loss: 4.9119\n",
      "Epoch [7/10], Step [1950/68337], Loss: 5.1548\n",
      "Epoch [7/10], Step [2025/68337], Loss: 5.1020\n",
      "Epoch [7/10], Step [2100/68337], Loss: 5.0180\n",
      "Epoch [7/10], Step [2175/68337], Loss: 4.9520\n",
      "Epoch [7/10], Step [2250/68337], Loss: 5.1535\n",
      "Epoch [7/10], Step [2325/68337], Loss: 5.2804\n",
      "Epoch [7/10], Step [2400/68337], Loss: 5.1018\n",
      "Epoch [7/10], Step [2475/68337], Loss: 5.3276\n",
      "Epoch [7/10], Step [2550/68337], Loss: 5.0582\n",
      "Epoch [7/10], Step [2625/68337], Loss: 5.0871\n",
      "Epoch [7/10], Step [2700/68337], Loss: 5.0549\n",
      "Epoch [7/10], Step [2775/68337], Loss: 5.3911\n",
      "Epoch [7/10], Step [2850/68337], Loss: 5.0273\n",
      "Epoch [7/10], Step [2925/68337], Loss: 5.0822\n",
      "Epoch [7/10], Step [3000/68337], Loss: 5.1136\n",
      "Epoch [7/10], Step [3075/68337], Loss: 5.1551\n",
      "Epoch [7/10], Step [3150/68337], Loss: 4.9559\n",
      "Epoch [7/10], Step [3225/68337], Loss: 5.0734\n",
      "Epoch [7/10], Step [3300/68337], Loss: 5.0621\n",
      "Epoch [7/10], Step [3375/68337], Loss: 5.0530\n",
      "Epoch [7/10], Step [3450/68337], Loss: 5.0430\n",
      "Epoch [7/10], Step [3525/68337], Loss: 5.1629\n",
      "Epoch [7/10], Step [3600/68337], Loss: 5.1686\n",
      "Epoch [7/10], Step [3675/68337], Loss: 5.2472\n",
      "Epoch [7/10], Step [3750/68337], Loss: 5.1689\n",
      "Epoch [7/10], Step [3825/68337], Loss: 4.9686\n",
      "Epoch [7/10], Step [3900/68337], Loss: 5.0983\n",
      "Epoch [7/10], Step [3975/68337], Loss: 5.0489\n",
      "Epoch [7/10], Step [4050/68337], Loss: 5.1411\n",
      "Epoch [7/10], Step [4125/68337], Loss: 5.1928\n",
      "Epoch [7/10], Step [4200/68337], Loss: 5.0102\n",
      "Epoch [7/10], Step [4275/68337], Loss: 5.1154\n",
      "Epoch [7/10], Step [4350/68337], Loss: 4.8520\n",
      "Epoch [7/10], Step [4425/68337], Loss: 5.0008\n",
      "Epoch [7/10], Step [4500/68337], Loss: 5.0459\n",
      "Epoch [7/10], Step [4575/68337], Loss: 4.9342\n",
      "Epoch [7/10], Step [4650/68337], Loss: 4.9269\n",
      "Epoch [7/10], Step [4725/68337], Loss: 5.1828\n",
      "Epoch [7/10], Step [4800/68337], Loss: 5.0382\n",
      "Epoch [7/10], Step [4875/68337], Loss: 5.1147\n",
      "Epoch [7/10], Step [4950/68337], Loss: 5.0961\n",
      "Epoch [7/10], Step [5025/68337], Loss: 5.0782\n",
      "Epoch [7/10], Step [5100/68337], Loss: 4.9426\n",
      "Epoch [7/10], Step [5175/68337], Loss: 5.1090\n",
      "Epoch [7/10], Step [5250/68337], Loss: 5.0334\n",
      "Epoch [7/10], Step [5325/68337], Loss: 5.0478\n",
      "Epoch [7/10], Step [5400/68337], Loss: 5.1958\n",
      "Epoch [7/10], Step [5475/68337], Loss: 5.1490\n",
      "Epoch [7/10], Step [5550/68337], Loss: 5.0279\n",
      "Epoch [7/10], Step [5625/68337], Loss: 5.3371\n",
      "Epoch [7/10], Step [5700/68337], Loss: 4.9919\n",
      "Epoch [7/10], Step [5775/68337], Loss: 5.2514\n",
      "Epoch [7/10], Step [5850/68337], Loss: 4.9548\n",
      "Epoch [7/10], Step [5925/68337], Loss: 5.1182\n",
      "Epoch [7/10], Step [6000/68337], Loss: 5.3168\n",
      "Epoch [7/10], Step [6075/68337], Loss: 5.0570\n",
      "Epoch [7/10], Step [6150/68337], Loss: 5.1504\n",
      "Epoch [7/10], Step [6225/68337], Loss: 5.1097\n",
      "Epoch [7/10], Step [6300/68337], Loss: 5.1336\n",
      "Epoch [7/10], Step [6375/68337], Loss: 5.1215\n",
      "Epoch [7/10], Step [6450/68337], Loss: 5.1463\n",
      "Epoch [7/10], Step [6525/68337], Loss: 5.1597\n",
      "Epoch [7/10], Step [6600/68337], Loss: 5.0419\n",
      "Epoch [7/10], Step [6675/68337], Loss: 5.1943\n",
      "Epoch [7/10], Step [6750/68337], Loss: 5.2351\n",
      "Epoch [7/10], Step [6825/68337], Loss: 5.0161\n",
      "Epoch [7/10], Step [6900/68337], Loss: 5.1567\n",
      "Epoch [7/10], Step [6975/68337], Loss: 5.0461\n",
      "Epoch [7/10], Step [7050/68337], Loss: 5.0940\n",
      "Epoch [7/10], Step [7125/68337], Loss: 5.0040\n",
      "Epoch [7/10], Step [7200/68337], Loss: 5.0199\n",
      "Epoch [7/10], Step [7275/68337], Loss: 4.9820\n",
      "Epoch [7/10], Step [7350/68337], Loss: 5.2773\n",
      "Epoch [7/10], Step [7425/68337], Loss: 5.1077\n",
      "Epoch [7/10], Step [7500/68337], Loss: 5.1587\n",
      "Epoch [7/10], Step [7575/68337], Loss: 4.9934\n",
      "Epoch [7/10], Step [7650/68337], Loss: 5.0241\n",
      "Epoch [7/10], Step [7725/68337], Loss: 5.0051\n",
      "Epoch [7/10], Step [7800/68337], Loss: 5.1369\n",
      "Epoch [7/10], Step [7875/68337], Loss: 5.0651\n",
      "Epoch [7/10], Step [7950/68337], Loss: 4.9758\n",
      "Epoch [7/10], Step [8025/68337], Loss: 4.9766\n",
      "Epoch [7/10], Step [8100/68337], Loss: 5.1903\n",
      "Epoch [7/10], Step [8175/68337], Loss: 5.0270\n",
      "Epoch [7/10], Step [8250/68337], Loss: 5.2475\n",
      "Epoch [7/10], Step [8325/68337], Loss: 5.2274\n",
      "Epoch [7/10], Step [8400/68337], Loss: 5.1861\n",
      "Epoch [7/10], Step [8475/68337], Loss: 4.9271\n",
      "Epoch [7/10], Step [8550/68337], Loss: 4.9004\n",
      "Epoch [7/10], Step [8625/68337], Loss: 4.9770\n",
      "Epoch [7/10], Step [8700/68337], Loss: 5.1423\n",
      "Epoch [7/10], Step [8775/68337], Loss: 5.0822\n",
      "Epoch [7/10], Step [8850/68337], Loss: 5.1416\n",
      "Epoch [7/10], Step [8925/68337], Loss: 5.1284\n",
      "Epoch [7/10], Step [9000/68337], Loss: 5.0061\n",
      "Epoch [7/10], Step [9075/68337], Loss: 5.0586\n",
      "Epoch [7/10], Step [9150/68337], Loss: 5.1126\n",
      "Epoch [7/10], Step [9225/68337], Loss: 5.2350\n",
      "Epoch [7/10], Step [9300/68337], Loss: 5.0560\n",
      "Epoch [7/10], Step [9375/68337], Loss: 5.2319\n",
      "Epoch [7/10], Step [9450/68337], Loss: 4.9353\n",
      "Epoch [7/10], Step [9525/68337], Loss: 5.0674\n",
      "Epoch [7/10], Step [9600/68337], Loss: 5.0892\n",
      "Epoch [7/10], Step [9675/68337], Loss: 5.2206\n",
      "Epoch [7/10], Step [9750/68337], Loss: 5.1612\n",
      "Epoch [7/10], Step [9825/68337], Loss: 5.2721\n",
      "Epoch [7/10], Step [9900/68337], Loss: 5.2932\n",
      "Epoch [7/10], Step [9975/68337], Loss: 5.1436\n",
      "Validation perplexity: 125.20317505468326\n",
      "Epoch [7/10], Step [10050/68337], Loss: 5.2644\n",
      "Epoch [7/10], Step [10125/68337], Loss: 5.2270\n",
      "Epoch [7/10], Step [10200/68337], Loss: 4.9074\n",
      "Epoch [7/10], Step [10275/68337], Loss: 5.0335\n",
      "Epoch [7/10], Step [10350/68337], Loss: 5.0991\n",
      "Epoch [7/10], Step [10425/68337], Loss: 5.1198\n",
      "Epoch [7/10], Step [10500/68337], Loss: 5.2670\n",
      "Epoch [7/10], Step [10575/68337], Loss: 5.1824\n",
      "Epoch [7/10], Step [10650/68337], Loss: 5.1976\n",
      "Epoch [7/10], Step [10725/68337], Loss: 5.1696\n",
      "Epoch [7/10], Step [10800/68337], Loss: 5.0572\n",
      "Epoch [7/10], Step [10875/68337], Loss: 5.0264\n",
      "Epoch [7/10], Step [10950/68337], Loss: 5.1036\n",
      "Epoch [7/10], Step [11025/68337], Loss: 4.9627\n",
      "Epoch [7/10], Step [11100/68337], Loss: 4.9907\n",
      "Epoch [7/10], Step [11175/68337], Loss: 5.0182\n",
      "Epoch [7/10], Step [11250/68337], Loss: 4.8222\n",
      "Epoch [7/10], Step [11325/68337], Loss: 5.1596\n",
      "Epoch [7/10], Step [11400/68337], Loss: 5.0198\n",
      "Epoch [7/10], Step [11475/68337], Loss: 5.0969\n",
      "Epoch [7/10], Step [11550/68337], Loss: 5.3937\n",
      "Epoch [7/10], Step [11625/68337], Loss: 4.8283\n",
      "Epoch [7/10], Step [11700/68337], Loss: 5.0988\n",
      "Epoch [7/10], Step [11775/68337], Loss: 5.2712\n",
      "Epoch [7/10], Step [11850/68337], Loss: 5.1272\n",
      "Epoch [7/10], Step [11925/68337], Loss: 5.2744\n",
      "Epoch [7/10], Step [12000/68337], Loss: 4.8599\n",
      "Epoch [7/10], Step [12075/68337], Loss: 5.1906\n",
      "Epoch [7/10], Step [12150/68337], Loss: 5.1849\n",
      "Epoch [7/10], Step [12225/68337], Loss: 4.9824\n",
      "Epoch [7/10], Step [12300/68337], Loss: 5.2135\n",
      "Epoch [7/10], Step [12375/68337], Loss: 5.2119\n",
      "Epoch [7/10], Step [12450/68337], Loss: 4.9550\n",
      "Epoch [7/10], Step [12525/68337], Loss: 4.9731\n",
      "Epoch [7/10], Step [12600/68337], Loss: 5.2580\n",
      "Epoch [7/10], Step [12675/68337], Loss: 5.1836\n",
      "Epoch [7/10], Step [12750/68337], Loss: 5.2007\n",
      "Epoch [7/10], Step [12825/68337], Loss: 5.0456\n",
      "Epoch [7/10], Step [12900/68337], Loss: 5.0868\n",
      "Epoch [7/10], Step [12975/68337], Loss: 5.1851\n",
      "Epoch [7/10], Step [13050/68337], Loss: 5.0867\n",
      "Epoch [7/10], Step [13125/68337], Loss: 5.1334\n",
      "Epoch [7/10], Step [13200/68337], Loss: 5.3764\n",
      "Epoch [7/10], Step [13275/68337], Loss: 5.0509\n",
      "Epoch [7/10], Step [13350/68337], Loss: 5.1096\n",
      "Epoch [7/10], Step [13425/68337], Loss: 5.0639\n",
      "Epoch [7/10], Step [13500/68337], Loss: 5.1581\n",
      "Epoch [7/10], Step [13575/68337], Loss: 5.1332\n",
      "Epoch [7/10], Step [13650/68337], Loss: 5.1749\n",
      "Epoch [7/10], Step [13725/68337], Loss: 5.1261\n",
      "Epoch [7/10], Step [13800/68337], Loss: 5.1578\n",
      "Epoch [7/10], Step [13875/68337], Loss: 5.2530\n",
      "Epoch [7/10], Step [13950/68337], Loss: 5.1282\n",
      "Epoch [7/10], Step [14025/68337], Loss: 5.0322\n",
      "Epoch [7/10], Step [14100/68337], Loss: 5.1282\n",
      "Epoch [7/10], Step [14175/68337], Loss: 5.1560\n",
      "Epoch [7/10], Step [14250/68337], Loss: 5.0412\n",
      "Epoch [7/10], Step [14325/68337], Loss: 5.3267\n",
      "Epoch [7/10], Step [14400/68337], Loss: 4.9195\n",
      "Epoch [7/10], Step [14475/68337], Loss: 4.8685\n",
      "Epoch [7/10], Step [14550/68337], Loss: 5.0767\n",
      "Epoch [7/10], Step [14625/68337], Loss: 5.0906\n",
      "Epoch [7/10], Step [14700/68337], Loss: 5.0862\n",
      "Epoch [7/10], Step [14775/68337], Loss: 5.3709\n",
      "Epoch [7/10], Step [14850/68337], Loss: 5.3393\n",
      "Epoch [7/10], Step [14925/68337], Loss: 5.2668\n",
      "Epoch [7/10], Step [15000/68337], Loss: 5.2766\n",
      "Epoch [7/10], Step [15075/68337], Loss: 5.0351\n",
      "Epoch [7/10], Step [15150/68337], Loss: 5.2975\n",
      "Epoch [7/10], Step [15225/68337], Loss: 5.0714\n",
      "Epoch [7/10], Step [15300/68337], Loss: 5.0068\n",
      "Epoch [7/10], Step [15375/68337], Loss: 5.0561\n",
      "Epoch [7/10], Step [15450/68337], Loss: 4.8937\n",
      "Epoch [7/10], Step [15525/68337], Loss: 5.2813\n",
      "Epoch [7/10], Step [15600/68337], Loss: 5.2258\n",
      "Epoch [7/10], Step [15675/68337], Loss: 4.9170\n",
      "Epoch [7/10], Step [15750/68337], Loss: 5.3107\n",
      "Epoch [7/10], Step [15825/68337], Loss: 5.1787\n",
      "Epoch [7/10], Step [15900/68337], Loss: 5.1388\n",
      "Epoch [7/10], Step [15975/68337], Loss: 5.1863\n",
      "Epoch [7/10], Step [16050/68337], Loss: 5.2909\n",
      "Epoch [7/10], Step [16125/68337], Loss: 4.9309\n",
      "Epoch [7/10], Step [16200/68337], Loss: 5.0833\n",
      "Epoch [7/10], Step [16275/68337], Loss: 5.0756\n",
      "Epoch [7/10], Step [16350/68337], Loss: 4.9495\n",
      "Epoch [7/10], Step [16425/68337], Loss: 4.9509\n",
      "Epoch [7/10], Step [16500/68337], Loss: 5.0249\n",
      "Epoch [7/10], Step [16575/68337], Loss: 5.1479\n",
      "Epoch [7/10], Step [16650/68337], Loss: 5.1605\n",
      "Epoch [7/10], Step [16725/68337], Loss: 5.1793\n",
      "Epoch [7/10], Step [16800/68337], Loss: 4.9569\n",
      "Epoch [7/10], Step [16875/68337], Loss: 4.8896\n",
      "Epoch [7/10], Step [16950/68337], Loss: 4.9913\n",
      "Epoch [7/10], Step [17025/68337], Loss: 5.1012\n",
      "Epoch [7/10], Step [17100/68337], Loss: 5.1626\n",
      "Epoch [7/10], Step [17175/68337], Loss: 5.1871\n",
      "Epoch [7/10], Step [17250/68337], Loss: 4.9766\n",
      "Epoch [7/10], Step [17325/68337], Loss: 5.1753\n",
      "Epoch [7/10], Step [17400/68337], Loss: 5.0046\n",
      "Epoch [7/10], Step [17475/68337], Loss: 5.2209\n",
      "Epoch [7/10], Step [17550/68337], Loss: 5.1360\n",
      "Epoch [7/10], Step [17625/68337], Loss: 5.2123\n",
      "Epoch [7/10], Step [17700/68337], Loss: 5.0416\n",
      "Epoch [7/10], Step [17775/68337], Loss: 5.0857\n",
      "Epoch [7/10], Step [17850/68337], Loss: 5.1576\n",
      "Epoch [7/10], Step [17925/68337], Loss: 5.2215\n",
      "Epoch [7/10], Step [18000/68337], Loss: 5.0993\n",
      "Epoch [7/10], Step [18075/68337], Loss: 5.0724\n",
      "Epoch [7/10], Step [18150/68337], Loss: 5.1213\n",
      "Epoch [7/10], Step [18225/68337], Loss: 5.0743\n",
      "Epoch [7/10], Step [18300/68337], Loss: 5.1733\n",
      "Epoch [7/10], Step [18375/68337], Loss: 5.3339\n",
      "Epoch [7/10], Step [18450/68337], Loss: 5.0320\n",
      "Epoch [7/10], Step [18525/68337], Loss: 5.0178\n",
      "Epoch [7/10], Step [18600/68337], Loss: 5.0353\n",
      "Epoch [7/10], Step [18675/68337], Loss: 5.0586\n",
      "Epoch [7/10], Step [18750/68337], Loss: 4.9720\n",
      "Epoch [7/10], Step [18825/68337], Loss: 5.1120\n",
      "Epoch [7/10], Step [18900/68337], Loss: 4.9509\n",
      "Epoch [7/10], Step [18975/68337], Loss: 4.7308\n",
      "Epoch [7/10], Step [19050/68337], Loss: 4.9860\n",
      "Epoch [7/10], Step [19125/68337], Loss: 5.1003\n",
      "Epoch [7/10], Step [19200/68337], Loss: 5.0424\n",
      "Epoch [7/10], Step [19275/68337], Loss: 5.1734\n",
      "Epoch [7/10], Step [19350/68337], Loss: 5.2693\n",
      "Epoch [7/10], Step [19425/68337], Loss: 5.3272\n",
      "Epoch [7/10], Step [19500/68337], Loss: 5.0508\n",
      "Epoch [7/10], Step [19575/68337], Loss: 5.1066\n",
      "Epoch [7/10], Step [19650/68337], Loss: 5.0939\n",
      "Epoch [7/10], Step [19725/68337], Loss: 5.2484\n",
      "Epoch [7/10], Step [19800/68337], Loss: 5.2886\n",
      "Epoch [7/10], Step [19875/68337], Loss: 5.1219\n",
      "Epoch [7/10], Step [19950/68337], Loss: 5.1057\n",
      "Validation perplexity: 125.73405864433262\n",
      "Epoch [7/10], Step [20025/68337], Loss: 5.1513\n",
      "Epoch [7/10], Step [20100/68337], Loss: 5.2716\n",
      "Epoch [7/10], Step [20175/68337], Loss: 5.1904\n",
      "Epoch [7/10], Step [20250/68337], Loss: 5.1413\n",
      "Epoch [7/10], Step [20325/68337], Loss: 4.9820\n",
      "Epoch [7/10], Step [20400/68337], Loss: 5.0330\n",
      "Epoch [7/10], Step [20475/68337], Loss: 4.9619\n",
      "Epoch [7/10], Step [20550/68337], Loss: 5.1732\n",
      "Epoch [7/10], Step [20625/68337], Loss: 5.1340\n",
      "Epoch [7/10], Step [20700/68337], Loss: 5.0593\n",
      "Epoch [7/10], Step [20775/68337], Loss: 5.0893\n",
      "Epoch [7/10], Step [20850/68337], Loss: 5.2363\n",
      "Epoch [7/10], Step [20925/68337], Loss: 5.3804\n",
      "Epoch [7/10], Step [21000/68337], Loss: 5.3276\n",
      "Epoch [7/10], Step [21075/68337], Loss: 5.1629\n",
      "Epoch [7/10], Step [21150/68337], Loss: 5.2063\n",
      "Epoch [7/10], Step [21225/68337], Loss: 5.0179\n",
      "Epoch [7/10], Step [21300/68337], Loss: 5.0525\n",
      "Epoch [7/10], Step [21375/68337], Loss: 5.1125\n",
      "Epoch [7/10], Step [21450/68337], Loss: 5.0273\n",
      "Epoch [7/10], Step [21525/68337], Loss: 4.9081\n",
      "Epoch [7/10], Step [21600/68337], Loss: 5.2115\n",
      "Epoch [7/10], Step [21675/68337], Loss: 4.8385\n",
      "Epoch [7/10], Step [21750/68337], Loss: 5.1755\n",
      "Epoch [7/10], Step [21825/68337], Loss: 5.1547\n",
      "Epoch [7/10], Step [21900/68337], Loss: 5.1454\n",
      "Epoch [7/10], Step [21975/68337], Loss: 5.0733\n",
      "Epoch [7/10], Step [22050/68337], Loss: 5.2021\n",
      "Epoch [7/10], Step [22125/68337], Loss: 5.3265\n",
      "Epoch [7/10], Step [22200/68337], Loss: 4.9307\n",
      "Epoch [7/10], Step [22275/68337], Loss: 5.2058\n",
      "Epoch [7/10], Step [22350/68337], Loss: 4.9144\n",
      "Epoch [7/10], Step [22425/68337], Loss: 4.9781\n",
      "Epoch [7/10], Step [22500/68337], Loss: 5.1863\n",
      "Epoch [7/10], Step [22575/68337], Loss: 5.2230\n",
      "Epoch [7/10], Step [22650/68337], Loss: 5.0883\n",
      "Epoch [7/10], Step [22725/68337], Loss: 5.1252\n",
      "Epoch [7/10], Step [22800/68337], Loss: 5.1065\n",
      "Epoch [7/10], Step [22875/68337], Loss: 5.0895\n",
      "Epoch [7/10], Step [22950/68337], Loss: 4.9131\n",
      "Epoch [7/10], Step [23025/68337], Loss: 5.0059\n",
      "Epoch [7/10], Step [23100/68337], Loss: 5.0464\n",
      "Epoch [7/10], Step [23175/68337], Loss: 5.0620\n",
      "Epoch [7/10], Step [23250/68337], Loss: 4.9974\n",
      "Epoch [7/10], Step [23325/68337], Loss: 5.1064\n",
      "Epoch [7/10], Step [23400/68337], Loss: 5.1913\n",
      "Epoch [7/10], Step [23475/68337], Loss: 5.1432\n",
      "Epoch [7/10], Step [23550/68337], Loss: 5.2495\n",
      "Epoch [7/10], Step [23625/68337], Loss: 5.0872\n",
      "Epoch [7/10], Step [23700/68337], Loss: 4.8868\n",
      "Epoch [7/10], Step [23775/68337], Loss: 5.0194\n",
      "Epoch [7/10], Step [23850/68337], Loss: 5.0450\n",
      "Epoch [7/10], Step [23925/68337], Loss: 5.0270\n",
      "Epoch [7/10], Step [24000/68337], Loss: 5.1562\n",
      "Epoch [7/10], Step [24075/68337], Loss: 5.3183\n",
      "Epoch [7/10], Step [24150/68337], Loss: 4.9132\n",
      "Epoch [7/10], Step [24225/68337], Loss: 4.8800\n",
      "Epoch [7/10], Step [24300/68337], Loss: 5.2565\n",
      "Epoch [7/10], Step [24375/68337], Loss: 5.1703\n",
      "Epoch [7/10], Step [24450/68337], Loss: 5.2157\n",
      "Epoch [7/10], Step [24525/68337], Loss: 5.0814\n",
      "Epoch [7/10], Step [24600/68337], Loss: 5.1153\n",
      "Epoch [7/10], Step [24675/68337], Loss: 4.8944\n",
      "Epoch [7/10], Step [24750/68337], Loss: 4.9125\n",
      "Epoch [7/10], Step [24825/68337], Loss: 5.1824\n",
      "Epoch [7/10], Step [24900/68337], Loss: 5.0583\n",
      "Epoch [7/10], Step [24975/68337], Loss: 5.3374\n",
      "Epoch [7/10], Step [25050/68337], Loss: 5.0597\n",
      "Epoch [7/10], Step [25125/68337], Loss: 5.0631\n",
      "Epoch [7/10], Step [25200/68337], Loss: 5.0594\n",
      "Epoch [7/10], Step [25275/68337], Loss: 5.1992\n",
      "Epoch [7/10], Step [25350/68337], Loss: 5.2505\n",
      "Epoch [7/10], Step [25425/68337], Loss: 5.2081\n",
      "Epoch [7/10], Step [25500/68337], Loss: 5.1241\n",
      "Epoch [7/10], Step [25575/68337], Loss: 5.1207\n",
      "Epoch [7/10], Step [25650/68337], Loss: 4.8657\n",
      "Epoch [7/10], Step [25725/68337], Loss: 4.8154\n",
      "Epoch [7/10], Step [25800/68337], Loss: 5.0103\n",
      "Epoch [7/10], Step [25875/68337], Loss: 4.9683\n",
      "Epoch [7/10], Step [25950/68337], Loss: 5.0266\n",
      "Epoch [7/10], Step [26025/68337], Loss: 5.0324\n",
      "Epoch [7/10], Step [26100/68337], Loss: 5.0148\n",
      "Epoch [7/10], Step [26175/68337], Loss: 5.1418\n",
      "Epoch [7/10], Step [26250/68337], Loss: 4.9679\n",
      "Epoch [7/10], Step [26325/68337], Loss: 5.1594\n",
      "Epoch [7/10], Step [26400/68337], Loss: 4.9281\n",
      "Epoch [7/10], Step [26475/68337], Loss: 5.1526\n",
      "Epoch [7/10], Step [26550/68337], Loss: 5.0809\n",
      "Epoch [7/10], Step [26625/68337], Loss: 5.0457\n",
      "Epoch [7/10], Step [26700/68337], Loss: 5.1434\n",
      "Epoch [7/10], Step [26775/68337], Loss: 5.1217\n",
      "Epoch [7/10], Step [26850/68337], Loss: 5.0022\n",
      "Epoch [7/10], Step [26925/68337], Loss: 5.0089\n",
      "Epoch [7/10], Step [27000/68337], Loss: 5.1196\n",
      "Epoch [7/10], Step [27075/68337], Loss: 5.1424\n",
      "Epoch [7/10], Step [27150/68337], Loss: 4.8715\n",
      "Epoch [7/10], Step [27225/68337], Loss: 4.9801\n",
      "Epoch [7/10], Step [27300/68337], Loss: 5.0705\n",
      "Epoch [7/10], Step [27375/68337], Loss: 5.2437\n",
      "Epoch [7/10], Step [27450/68337], Loss: 5.2741\n",
      "Epoch [7/10], Step [27525/68337], Loss: 5.0340\n",
      "Epoch [7/10], Step [27600/68337], Loss: 5.0855\n",
      "Epoch [7/10], Step [27675/68337], Loss: 5.2623\n",
      "Epoch [7/10], Step [27750/68337], Loss: 5.1246\n",
      "Epoch [7/10], Step [27825/68337], Loss: 4.9182\n",
      "Epoch [7/10], Step [27900/68337], Loss: 5.0433\n",
      "Epoch [7/10], Step [27975/68337], Loss: 5.2020\n",
      "Epoch [7/10], Step [28050/68337], Loss: 5.0284\n",
      "Epoch [7/10], Step [28125/68337], Loss: 5.0716\n",
      "Epoch [7/10], Step [28200/68337], Loss: 4.9398\n",
      "Epoch [7/10], Step [28275/68337], Loss: 5.3810\n",
      "Epoch [7/10], Step [28350/68337], Loss: 5.0328\n",
      "Epoch [7/10], Step [28425/68337], Loss: 5.0715\n",
      "Epoch [7/10], Step [28500/68337], Loss: 5.2921\n",
      "Epoch [7/10], Step [28575/68337], Loss: 5.1293\n",
      "Epoch [7/10], Step [28650/68337], Loss: 5.1026\n",
      "Epoch [7/10], Step [28725/68337], Loss: 5.1324\n",
      "Epoch [7/10], Step [28800/68337], Loss: 5.0948\n",
      "Epoch [7/10], Step [28875/68337], Loss: 5.0509\n",
      "Epoch [7/10], Step [28950/68337], Loss: 5.2325\n",
      "Epoch [7/10], Step [29025/68337], Loss: 5.2046\n",
      "Epoch [7/10], Step [29100/68337], Loss: 5.1809\n",
      "Epoch [7/10], Step [29175/68337], Loss: 5.1187\n",
      "Epoch [7/10], Step [29250/68337], Loss: 5.0899\n",
      "Epoch [7/10], Step [29325/68337], Loss: 5.2378\n",
      "Epoch [7/10], Step [29400/68337], Loss: 5.1762\n",
      "Epoch [7/10], Step [29475/68337], Loss: 5.0660\n",
      "Epoch [7/10], Step [29550/68337], Loss: 5.0383\n",
      "Epoch [7/10], Step [29625/68337], Loss: 4.9132\n",
      "Epoch [7/10], Step [29700/68337], Loss: 4.9921\n",
      "Epoch [7/10], Step [29775/68337], Loss: 5.2095\n",
      "Epoch [7/10], Step [29850/68337], Loss: 5.0793\n",
      "Epoch [7/10], Step [29925/68337], Loss: 4.9876\n",
      "Epoch [7/10], Step [30000/68337], Loss: 5.1372\n",
      "Validation perplexity: 124.85630047488273\n",
      "Epoch [7/10], Step [30075/68337], Loss: 5.2200\n",
      "Epoch [7/10], Step [30150/68337], Loss: 4.9770\n",
      "Epoch [7/10], Step [30225/68337], Loss: 4.9506\n",
      "Epoch [7/10], Step [30300/68337], Loss: 4.9317\n",
      "Epoch [7/10], Step [30375/68337], Loss: 5.0019\n",
      "Epoch [7/10], Step [30450/68337], Loss: 4.9396\n",
      "Epoch [7/10], Step [30525/68337], Loss: 5.1625\n",
      "Epoch [7/10], Step [30600/68337], Loss: 5.0842\n",
      "Epoch [7/10], Step [30675/68337], Loss: 5.3149\n",
      "Epoch [7/10], Step [30750/68337], Loss: 5.1859\n",
      "Epoch [7/10], Step [30825/68337], Loss: 5.1048\n",
      "Epoch [7/10], Step [30900/68337], Loss: 5.1223\n",
      "Epoch [7/10], Step [30975/68337], Loss: 5.1893\n",
      "Epoch [7/10], Step [31050/68337], Loss: 5.0613\n",
      "Epoch [7/10], Step [31125/68337], Loss: 5.1342\n",
      "Epoch [7/10], Step [31200/68337], Loss: 4.8327\n",
      "Epoch [7/10], Step [31275/68337], Loss: 4.9135\n",
      "Epoch [7/10], Step [31350/68337], Loss: 4.9867\n",
      "Epoch [7/10], Step [31425/68337], Loss: 5.1856\n",
      "Epoch [7/10], Step [31500/68337], Loss: 5.2400\n",
      "Epoch [7/10], Step [31575/68337], Loss: 5.0101\n",
      "Epoch [7/10], Step [31650/68337], Loss: 4.8985\n",
      "Epoch [7/10], Step [31725/68337], Loss: 5.0175\n",
      "Epoch [7/10], Step [31800/68337], Loss: 4.9275\n",
      "Epoch [7/10], Step [31875/68337], Loss: 5.0825\n",
      "Epoch [7/10], Step [31950/68337], Loss: 5.0871\n",
      "Epoch [7/10], Step [32025/68337], Loss: 5.1661\n",
      "Epoch [7/10], Step [32100/68337], Loss: 5.1724\n",
      "Epoch [7/10], Step [32175/68337], Loss: 5.0237\n",
      "Epoch [7/10], Step [32250/68337], Loss: 5.1516\n",
      "Epoch [7/10], Step [32325/68337], Loss: 4.9732\n",
      "Epoch [7/10], Step [32400/68337], Loss: 5.2036\n",
      "Epoch [7/10], Step [32475/68337], Loss: 4.9421\n",
      "Epoch [7/10], Step [32550/68337], Loss: 5.0142\n",
      "Epoch [7/10], Step [32625/68337], Loss: 5.3078\n",
      "Epoch [7/10], Step [32700/68337], Loss: 5.0549\n",
      "Epoch [7/10], Step [32775/68337], Loss: 5.1435\n",
      "Epoch [7/10], Step [32850/68337], Loss: 5.1766\n",
      "Epoch [7/10], Step [32925/68337], Loss: 5.1136\n",
      "Epoch [7/10], Step [33000/68337], Loss: 4.8794\n",
      "Epoch [7/10], Step [33075/68337], Loss: 5.0242\n",
      "Epoch [7/10], Step [33150/68337], Loss: 5.1207\n",
      "Epoch [7/10], Step [33225/68337], Loss: 4.9571\n",
      "Epoch [7/10], Step [33300/68337], Loss: 5.0536\n",
      "Epoch [7/10], Step [33375/68337], Loss: 4.9890\n",
      "Epoch [7/10], Step [33450/68337], Loss: 5.3826\n",
      "Epoch [7/10], Step [33525/68337], Loss: 4.9030\n",
      "Epoch [7/10], Step [33600/68337], Loss: 5.2121\n",
      "Epoch [7/10], Step [33675/68337], Loss: 5.0444\n",
      "Epoch [7/10], Step [33750/68337], Loss: 5.1601\n",
      "Epoch [7/10], Step [33825/68337], Loss: 4.9426\n",
      "Epoch [7/10], Step [33900/68337], Loss: 5.0482\n",
      "Epoch [7/10], Step [33975/68337], Loss: 5.2355\n",
      "Epoch [7/10], Step [34050/68337], Loss: 5.2103\n",
      "Epoch [7/10], Step [34125/68337], Loss: 4.9094\n",
      "Epoch [7/10], Step [34200/68337], Loss: 5.1697\n",
      "Epoch [7/10], Step [34275/68337], Loss: 5.0656\n",
      "Epoch [7/10], Step [34350/68337], Loss: 5.0219\n",
      "Epoch [7/10], Step [34425/68337], Loss: 4.9523\n",
      "Epoch [7/10], Step [34500/68337], Loss: 5.0468\n",
      "Epoch [7/10], Step [34575/68337], Loss: 5.2041\n",
      "Epoch [7/10], Step [34650/68337], Loss: 5.1165\n",
      "Epoch [7/10], Step [34725/68337], Loss: 5.1325\n",
      "Epoch [7/10], Step [34800/68337], Loss: 5.1751\n",
      "Epoch [7/10], Step [34875/68337], Loss: 4.9865\n",
      "Epoch [7/10], Step [34950/68337], Loss: 5.0413\n",
      "Epoch [7/10], Step [35025/68337], Loss: 4.8525\n",
      "Epoch [7/10], Step [35100/68337], Loss: 5.0192\n",
      "Epoch [7/10], Step [35175/68337], Loss: 5.0442\n",
      "Epoch [7/10], Step [35250/68337], Loss: 5.2161\n",
      "Epoch [7/10], Step [35325/68337], Loss: 5.1982\n",
      "Epoch [7/10], Step [35400/68337], Loss: 5.1157\n",
      "Epoch [7/10], Step [35475/68337], Loss: 4.9335\n",
      "Epoch [7/10], Step [35550/68337], Loss: 5.1868\n",
      "Epoch [7/10], Step [35625/68337], Loss: 5.2655\n",
      "Epoch [7/10], Step [35700/68337], Loss: 5.3545\n",
      "Epoch [7/10], Step [35775/68337], Loss: 5.0021\n",
      "Epoch [7/10], Step [35850/68337], Loss: 5.0379\n",
      "Epoch [7/10], Step [35925/68337], Loss: 5.0290\n",
      "Epoch [7/10], Step [36000/68337], Loss: 5.3174\n",
      "Epoch [7/10], Step [36075/68337], Loss: 4.8961\n",
      "Epoch [7/10], Step [36150/68337], Loss: 5.1134\n",
      "Epoch [7/10], Step [36225/68337], Loss: 4.9607\n",
      "Epoch [7/10], Step [36300/68337], Loss: 5.2272\n",
      "Epoch [7/10], Step [36375/68337], Loss: 5.1078\n",
      "Epoch [7/10], Step [36450/68337], Loss: 5.0800\n",
      "Epoch [7/10], Step [36525/68337], Loss: 5.2339\n",
      "Epoch [7/10], Step [36600/68337], Loss: 5.2604\n",
      "Epoch [7/10], Step [36675/68337], Loss: 5.0994\n",
      "Epoch [7/10], Step [36750/68337], Loss: 5.0027\n",
      "Epoch [7/10], Step [36825/68337], Loss: 5.1269\n",
      "Epoch [7/10], Step [36900/68337], Loss: 5.0257\n",
      "Epoch [7/10], Step [36975/68337], Loss: 5.1587\n",
      "Epoch [7/10], Step [37050/68337], Loss: 5.1316\n",
      "Epoch [7/10], Step [37125/68337], Loss: 4.7472\n",
      "Epoch [7/10], Step [37200/68337], Loss: 5.1044\n",
      "Epoch [7/10], Step [37275/68337], Loss: 5.0973\n",
      "Epoch [7/10], Step [37350/68337], Loss: 5.1647\n",
      "Epoch [7/10], Step [37425/68337], Loss: 5.1102\n",
      "Epoch [7/10], Step [37500/68337], Loss: 4.9894\n",
      "Epoch [7/10], Step [37575/68337], Loss: 5.0988\n",
      "Epoch [7/10], Step [37650/68337], Loss: 5.2022\n",
      "Epoch [7/10], Step [37725/68337], Loss: 5.1678\n",
      "Epoch [7/10], Step [37800/68337], Loss: 5.0857\n",
      "Epoch [7/10], Step [37875/68337], Loss: 5.0949\n",
      "Epoch [7/10], Step [37950/68337], Loss: 5.1155\n",
      "Epoch [7/10], Step [38025/68337], Loss: 5.1636\n",
      "Epoch [7/10], Step [38100/68337], Loss: 5.2274\n",
      "Epoch [7/10], Step [38175/68337], Loss: 5.1826\n",
      "Epoch [7/10], Step [38250/68337], Loss: 5.0734\n",
      "Epoch [7/10], Step [38325/68337], Loss: 5.1965\n",
      "Epoch [7/10], Step [38400/68337], Loss: 5.1962\n",
      "Epoch [7/10], Step [38475/68337], Loss: 5.2020\n",
      "Epoch [7/10], Step [38550/68337], Loss: 5.0173\n",
      "Epoch [7/10], Step [38625/68337], Loss: 5.1336\n",
      "Epoch [7/10], Step [38700/68337], Loss: 5.0387\n",
      "Epoch [7/10], Step [38775/68337], Loss: 5.0201\n",
      "Epoch [7/10], Step [38850/68337], Loss: 5.0882\n",
      "Epoch [7/10], Step [38925/68337], Loss: 5.1670\n",
      "Epoch [7/10], Step [39000/68337], Loss: 5.1286\n",
      "Epoch [7/10], Step [39075/68337], Loss: 5.1038\n",
      "Epoch [7/10], Step [39150/68337], Loss: 5.1620\n",
      "Epoch [7/10], Step [39225/68337], Loss: 5.0920\n",
      "Epoch [7/10], Step [39300/68337], Loss: 5.1848\n",
      "Epoch [7/10], Step [39375/68337], Loss: 4.8959\n",
      "Epoch [7/10], Step [39450/68337], Loss: 4.9515\n",
      "Epoch [7/10], Step [39525/68337], Loss: 5.0048\n",
      "Epoch [7/10], Step [39600/68337], Loss: 5.3271\n",
      "Epoch [7/10], Step [39675/68337], Loss: 5.1923\n",
      "Epoch [7/10], Step [39750/68337], Loss: 5.1858\n",
      "Epoch [7/10], Step [39825/68337], Loss: 5.2293\n",
      "Epoch [7/10], Step [39900/68337], Loss: 5.1130\n",
      "Epoch [7/10], Step [39975/68337], Loss: 4.9321\n",
      "Validation perplexity: 124.70693489890432\n",
      "Epoch [7/10], Step [40050/68337], Loss: 5.2351\n",
      "Epoch [7/10], Step [40125/68337], Loss: 5.1328\n",
      "Epoch [7/10], Step [40200/68337], Loss: 5.0146\n",
      "Epoch [7/10], Step [40275/68337], Loss: 4.9959\n",
      "Epoch [7/10], Step [40350/68337], Loss: 5.0654\n",
      "Epoch [7/10], Step [40425/68337], Loss: 5.2887\n",
      "Epoch [7/10], Step [40500/68337], Loss: 5.1138\n",
      "Epoch [7/10], Step [40575/68337], Loss: 5.1478\n",
      "Epoch [7/10], Step [40650/68337], Loss: 5.0429\n",
      "Epoch [7/10], Step [40725/68337], Loss: 5.0064\n",
      "Epoch [7/10], Step [40800/68337], Loss: 5.0910\n",
      "Epoch [7/10], Step [40875/68337], Loss: 4.9771\n",
      "Epoch [7/10], Step [40950/68337], Loss: 5.1127\n",
      "Epoch [7/10], Step [41025/68337], Loss: 4.9714\n",
      "Epoch [7/10], Step [41100/68337], Loss: 4.9438\n",
      "Epoch [7/10], Step [41175/68337], Loss: 5.1626\n",
      "Epoch [7/10], Step [41250/68337], Loss: 5.0745\n",
      "Epoch [7/10], Step [41325/68337], Loss: 5.0253\n",
      "Epoch [7/10], Step [41400/68337], Loss: 5.0590\n",
      "Epoch [7/10], Step [41475/68337], Loss: 5.2510\n",
      "Epoch [7/10], Step [41550/68337], Loss: 4.9132\n",
      "Epoch [7/10], Step [41625/68337], Loss: 5.4514\n",
      "Epoch [7/10], Step [41700/68337], Loss: 4.8338\n",
      "Epoch [7/10], Step [41775/68337], Loss: 5.2543\n",
      "Epoch [7/10], Step [41850/68337], Loss: 5.3002\n",
      "Epoch [7/10], Step [41925/68337], Loss: 4.9729\n",
      "Epoch [7/10], Step [42000/68337], Loss: 5.1289\n",
      "Epoch [7/10], Step [42075/68337], Loss: 5.0874\n",
      "Epoch [7/10], Step [42150/68337], Loss: 5.2939\n",
      "Epoch [7/10], Step [42225/68337], Loss: 5.0317\n",
      "Epoch [7/10], Step [42300/68337], Loss: 5.0762\n",
      "Epoch [7/10], Step [42375/68337], Loss: 4.9276\n",
      "Epoch [7/10], Step [42450/68337], Loss: 5.2084\n",
      "Epoch [7/10], Step [42525/68337], Loss: 4.9880\n",
      "Epoch [7/10], Step [42600/68337], Loss: 4.8576\n",
      "Epoch [7/10], Step [42675/68337], Loss: 5.2107\n",
      "Epoch [7/10], Step [42750/68337], Loss: 5.0705\n",
      "Epoch [7/10], Step [42825/68337], Loss: 4.9753\n",
      "Epoch [7/10], Step [42900/68337], Loss: 5.3786\n",
      "Epoch [7/10], Step [42975/68337], Loss: 5.0329\n",
      "Epoch [7/10], Step [43050/68337], Loss: 4.9596\n",
      "Epoch [7/10], Step [43125/68337], Loss: 5.0887\n",
      "Epoch [7/10], Step [43200/68337], Loss: 5.1858\n",
      "Epoch [7/10], Step [43275/68337], Loss: 5.2218\n",
      "Epoch [7/10], Step [43350/68337], Loss: 5.1537\n",
      "Epoch [7/10], Step [43425/68337], Loss: 5.0781\n",
      "Epoch [7/10], Step [43500/68337], Loss: 5.2903\n",
      "Epoch [7/10], Step [43575/68337], Loss: 5.1982\n",
      "Epoch [7/10], Step [43650/68337], Loss: 5.0745\n",
      "Epoch [7/10], Step [43725/68337], Loss: 5.0456\n",
      "Epoch [7/10], Step [43800/68337], Loss: 4.9462\n",
      "Epoch [7/10], Step [43875/68337], Loss: 4.9735\n",
      "Epoch [7/10], Step [43950/68337], Loss: 5.2639\n",
      "Epoch [7/10], Step [44025/68337], Loss: 4.9181\n",
      "Epoch [7/10], Step [44100/68337], Loss: 5.0809\n",
      "Epoch [7/10], Step [44175/68337], Loss: 5.2473\n",
      "Epoch [7/10], Step [44250/68337], Loss: 5.0746\n",
      "Epoch [7/10], Step [44325/68337], Loss: 4.9757\n",
      "Epoch [7/10], Step [44400/68337], Loss: 5.1141\n",
      "Epoch [7/10], Step [44475/68337], Loss: 5.1678\n",
      "Epoch [7/10], Step [44550/68337], Loss: 5.0508\n",
      "Epoch [7/10], Step [44625/68337], Loss: 5.1380\n",
      "Epoch [7/10], Step [44700/68337], Loss: 4.9417\n",
      "Epoch [7/10], Step [44775/68337], Loss: 5.1853\n",
      "Epoch [7/10], Step [44850/68337], Loss: 5.2077\n",
      "Epoch [7/10], Step [44925/68337], Loss: 5.2836\n",
      "Epoch [7/10], Step [45000/68337], Loss: 5.0336\n",
      "Epoch [7/10], Step [45075/68337], Loss: 5.0075\n",
      "Epoch [7/10], Step [45150/68337], Loss: 5.0717\n",
      "Epoch [7/10], Step [45225/68337], Loss: 5.1166\n",
      "Epoch [7/10], Step [45300/68337], Loss: 5.1685\n",
      "Epoch [7/10], Step [45375/68337], Loss: 5.1326\n",
      "Epoch [7/10], Step [45450/68337], Loss: 5.0625\n",
      "Epoch [7/10], Step [45525/68337], Loss: 5.0647\n",
      "Epoch [7/10], Step [45600/68337], Loss: 4.9697\n",
      "Epoch [7/10], Step [45675/68337], Loss: 5.3547\n",
      "Epoch [7/10], Step [45750/68337], Loss: 4.9938\n",
      "Epoch [7/10], Step [45825/68337], Loss: 5.1135\n",
      "Epoch [7/10], Step [45900/68337], Loss: 5.1044\n",
      "Epoch [7/10], Step [45975/68337], Loss: 5.1867\n",
      "Epoch [7/10], Step [46050/68337], Loss: 5.2157\n",
      "Epoch [7/10], Step [46125/68337], Loss: 5.1656\n",
      "Epoch [7/10], Step [46200/68337], Loss: 5.0332\n",
      "Epoch [7/10], Step [46275/68337], Loss: 5.0471\n",
      "Epoch [7/10], Step [46350/68337], Loss: 4.9155\n",
      "Epoch [7/10], Step [46425/68337], Loss: 5.2234\n",
      "Epoch [7/10], Step [46500/68337], Loss: 4.9147\n",
      "Epoch [7/10], Step [46575/68337], Loss: 5.2898\n",
      "Epoch [7/10], Step [46650/68337], Loss: 5.2672\n",
      "Epoch [7/10], Step [46725/68337], Loss: 5.1729\n",
      "Epoch [7/10], Step [46800/68337], Loss: 5.0248\n",
      "Epoch [7/10], Step [46875/68337], Loss: 5.3644\n",
      "Epoch [7/10], Step [46950/68337], Loss: 5.0513\n",
      "Epoch [7/10], Step [47025/68337], Loss: 5.0092\n",
      "Epoch [7/10], Step [47100/68337], Loss: 5.1878\n",
      "Epoch [7/10], Step [47175/68337], Loss: 5.1909\n",
      "Epoch [7/10], Step [47250/68337], Loss: 5.1370\n",
      "Epoch [7/10], Step [47325/68337], Loss: 5.3925\n",
      "Epoch [7/10], Step [47400/68337], Loss: 5.3481\n",
      "Epoch [7/10], Step [47475/68337], Loss: 5.1406\n",
      "Epoch [7/10], Step [47550/68337], Loss: 5.2130\n",
      "Epoch [7/10], Step [47625/68337], Loss: 4.8225\n",
      "Epoch [7/10], Step [47700/68337], Loss: 5.0536\n",
      "Epoch [7/10], Step [47775/68337], Loss: 5.0830\n",
      "Epoch [7/10], Step [47850/68337], Loss: 4.9457\n",
      "Epoch [7/10], Step [47925/68337], Loss: 5.3704\n",
      "Epoch [7/10], Step [48000/68337], Loss: 5.1796\n",
      "Epoch [7/10], Step [48075/68337], Loss: 5.0402\n",
      "Epoch [7/10], Step [48150/68337], Loss: 5.1246\n",
      "Epoch [7/10], Step [48225/68337], Loss: 4.9653\n",
      "Epoch [7/10], Step [48300/68337], Loss: 4.7906\n",
      "Epoch [7/10], Step [48375/68337], Loss: 4.9338\n",
      "Epoch [7/10], Step [48450/68337], Loss: 5.2857\n",
      "Epoch [7/10], Step [48525/68337], Loss: 4.9623\n",
      "Epoch [7/10], Step [48600/68337], Loss: 4.9219\n",
      "Epoch [7/10], Step [48675/68337], Loss: 5.2185\n",
      "Epoch [7/10], Step [48750/68337], Loss: 5.0386\n",
      "Epoch [7/10], Step [48825/68337], Loss: 5.1029\n",
      "Epoch [7/10], Step [48900/68337], Loss: 5.0367\n",
      "Epoch [7/10], Step [48975/68337], Loss: 5.0795\n",
      "Epoch [7/10], Step [49050/68337], Loss: 5.1786\n",
      "Epoch [7/10], Step [49125/68337], Loss: 5.0628\n",
      "Epoch [7/10], Step [49200/68337], Loss: 5.0034\n",
      "Epoch [7/10], Step [49275/68337], Loss: 5.0584\n",
      "Epoch [7/10], Step [49350/68337], Loss: 5.1969\n",
      "Epoch [7/10], Step [49425/68337], Loss: 5.1489\n",
      "Epoch [7/10], Step [49500/68337], Loss: 5.1784\n",
      "Epoch [7/10], Step [49575/68337], Loss: 4.9365\n",
      "Epoch [7/10], Step [49650/68337], Loss: 4.9280\n",
      "Epoch [7/10], Step [49725/68337], Loss: 5.0186\n",
      "Epoch [7/10], Step [49800/68337], Loss: 5.1635\n",
      "Epoch [7/10], Step [49875/68337], Loss: 5.0170\n",
      "Epoch [7/10], Step [49950/68337], Loss: 4.9381\n",
      "Validation perplexity: 125.0676101268917\n",
      "Epoch [7/10], Step [50025/68337], Loss: 5.2641\n",
      "Epoch [7/10], Step [50100/68337], Loss: 5.0780\n",
      "Epoch [7/10], Step [50175/68337], Loss: 5.1300\n",
      "Epoch [7/10], Step [50250/68337], Loss: 5.1534\n",
      "Epoch [7/10], Step [50325/68337], Loss: 5.2073\n",
      "Epoch [7/10], Step [50400/68337], Loss: 5.1014\n",
      "Epoch [7/10], Step [50475/68337], Loss: 5.0469\n",
      "Epoch [7/10], Step [50550/68337], Loss: 5.1570\n",
      "Epoch [7/10], Step [50625/68337], Loss: 5.0750\n",
      "Epoch [7/10], Step [50700/68337], Loss: 5.0313\n",
      "Epoch [7/10], Step [50775/68337], Loss: 5.1545\n",
      "Epoch [7/10], Step [50850/68337], Loss: 5.3132\n",
      "Epoch [7/10], Step [50925/68337], Loss: 5.0210\n",
      "Epoch [7/10], Step [51000/68337], Loss: 5.2445\n",
      "Epoch [7/10], Step [51075/68337], Loss: 5.0948\n",
      "Epoch [7/10], Step [51150/68337], Loss: 5.1778\n",
      "Epoch [7/10], Step [51225/68337], Loss: 5.0984\n",
      "Epoch [7/10], Step [51300/68337], Loss: 5.1722\n",
      "Epoch [7/10], Step [51375/68337], Loss: 5.1474\n",
      "Epoch [7/10], Step [51450/68337], Loss: 5.0295\n",
      "Epoch [7/10], Step [51525/68337], Loss: 5.2272\n",
      "Epoch [7/10], Step [51600/68337], Loss: 5.2252\n",
      "Epoch [7/10], Step [51675/68337], Loss: 5.1154\n",
      "Epoch [7/10], Step [51750/68337], Loss: 5.1303\n",
      "Epoch [7/10], Step [51825/68337], Loss: 5.1391\n",
      "Epoch [7/10], Step [51900/68337], Loss: 5.1251\n",
      "Epoch [7/10], Step [51975/68337], Loss: 5.0775\n",
      "Epoch [7/10], Step [52050/68337], Loss: 4.9144\n",
      "Epoch [7/10], Step [52125/68337], Loss: 5.0166\n",
      "Epoch [7/10], Step [52200/68337], Loss: 5.1287\n",
      "Epoch [7/10], Step [52275/68337], Loss: 5.1771\n",
      "Epoch [7/10], Step [52350/68337], Loss: 5.2017\n",
      "Epoch [7/10], Step [52425/68337], Loss: 5.1528\n",
      "Epoch [7/10], Step [52500/68337], Loss: 5.0553\n",
      "Epoch [7/10], Step [52575/68337], Loss: 5.1684\n",
      "Epoch [7/10], Step [52650/68337], Loss: 5.0753\n",
      "Epoch [7/10], Step [52725/68337], Loss: 5.0476\n",
      "Epoch [7/10], Step [52800/68337], Loss: 4.9719\n",
      "Epoch [7/10], Step [52875/68337], Loss: 5.1134\n",
      "Epoch [7/10], Step [52950/68337], Loss: 5.1783\n",
      "Epoch [7/10], Step [53025/68337], Loss: 5.3065\n",
      "Epoch [7/10], Step [53100/68337], Loss: 5.0243\n",
      "Epoch [7/10], Step [53175/68337], Loss: 5.0360\n",
      "Epoch [7/10], Step [53250/68337], Loss: 4.9514\n",
      "Epoch [7/10], Step [53325/68337], Loss: 4.8729\n",
      "Epoch [7/10], Step [53400/68337], Loss: 5.1796\n",
      "Epoch [7/10], Step [53475/68337], Loss: 5.0398\n",
      "Epoch [7/10], Step [53550/68337], Loss: 5.0614\n",
      "Epoch [7/10], Step [53625/68337], Loss: 5.2908\n",
      "Epoch [7/10], Step [53700/68337], Loss: 5.1509\n",
      "Epoch [7/10], Step [53775/68337], Loss: 5.0426\n",
      "Epoch [7/10], Step [53850/68337], Loss: 5.0764\n",
      "Epoch [7/10], Step [53925/68337], Loss: 5.1516\n",
      "Epoch [7/10], Step [54000/68337], Loss: 5.0441\n",
      "Epoch [7/10], Step [54075/68337], Loss: 5.0758\n",
      "Epoch [7/10], Step [54150/68337], Loss: 5.1231\n",
      "Epoch [7/10], Step [54225/68337], Loss: 4.8634\n",
      "Epoch [7/10], Step [54300/68337], Loss: 5.1235\n",
      "Epoch [7/10], Step [54375/68337], Loss: 5.0451\n",
      "Epoch [7/10], Step [54450/68337], Loss: 5.2001\n",
      "Epoch [7/10], Step [54525/68337], Loss: 4.9857\n",
      "Epoch [7/10], Step [54600/68337], Loss: 5.0308\n",
      "Epoch [7/10], Step [54675/68337], Loss: 5.3824\n",
      "Epoch [7/10], Step [54750/68337], Loss: 5.2757\n",
      "Epoch [7/10], Step [54825/68337], Loss: 5.3024\n",
      "Epoch [7/10], Step [54900/68337], Loss: 4.9920\n",
      "Epoch [7/10], Step [54975/68337], Loss: 5.2283\n",
      "Epoch [7/10], Step [55050/68337], Loss: 5.2244\n",
      "Epoch [7/10], Step [55125/68337], Loss: 5.0393\n",
      "Epoch [7/10], Step [55200/68337], Loss: 5.0142\n",
      "Epoch [7/10], Step [55275/68337], Loss: 5.1530\n",
      "Epoch [7/10], Step [55350/68337], Loss: 5.4101\n",
      "Epoch [7/10], Step [55425/68337], Loss: 5.1085\n",
      "Epoch [7/10], Step [55500/68337], Loss: 5.0181\n",
      "Epoch [7/10], Step [55575/68337], Loss: 5.2787\n",
      "Epoch [7/10], Step [55650/68337], Loss: 5.0493\n",
      "Epoch [7/10], Step [55725/68337], Loss: 5.0989\n",
      "Epoch [7/10], Step [55800/68337], Loss: 4.9847\n",
      "Epoch [7/10], Step [55875/68337], Loss: 4.9981\n",
      "Epoch [7/10], Step [55950/68337], Loss: 5.1333\n",
      "Epoch [7/10], Step [56025/68337], Loss: 5.1045\n",
      "Epoch [7/10], Step [56100/68337], Loss: 5.1315\n",
      "Epoch [7/10], Step [56175/68337], Loss: 5.1698\n",
      "Epoch [7/10], Step [56250/68337], Loss: 5.3481\n",
      "Epoch [7/10], Step [56325/68337], Loss: 5.1950\n",
      "Epoch [7/10], Step [56400/68337], Loss: 5.2884\n",
      "Epoch [7/10], Step [56475/68337], Loss: 5.3304\n",
      "Epoch [7/10], Step [56550/68337], Loss: 5.1252\n",
      "Epoch [7/10], Step [56625/68337], Loss: 4.9015\n",
      "Epoch [7/10], Step [56700/68337], Loss: 5.0575\n",
      "Epoch [7/10], Step [56775/68337], Loss: 5.1261\n",
      "Epoch [7/10], Step [56850/68337], Loss: 5.2668\n",
      "Epoch [7/10], Step [56925/68337], Loss: 5.0518\n",
      "Epoch [7/10], Step [57000/68337], Loss: 5.3015\n",
      "Epoch [7/10], Step [57075/68337], Loss: 5.1422\n",
      "Epoch [7/10], Step [57150/68337], Loss: 5.1664\n",
      "Epoch [7/10], Step [57225/68337], Loss: 4.8648\n",
      "Epoch [7/10], Step [57300/68337], Loss: 4.9540\n",
      "Epoch [7/10], Step [57375/68337], Loss: 5.2209\n",
      "Epoch [7/10], Step [57450/68337], Loss: 5.2035\n",
      "Epoch [7/10], Step [57525/68337], Loss: 5.2153\n",
      "Epoch [7/10], Step [57600/68337], Loss: 5.2070\n",
      "Epoch [7/10], Step [57675/68337], Loss: 4.9924\n",
      "Epoch [7/10], Step [57750/68337], Loss: 5.1544\n",
      "Epoch [7/10], Step [57825/68337], Loss: 5.0752\n",
      "Epoch [7/10], Step [57900/68337], Loss: 5.1981\n",
      "Epoch [7/10], Step [57975/68337], Loss: 4.9886\n",
      "Epoch [7/10], Step [58050/68337], Loss: 5.1118\n",
      "Epoch [7/10], Step [58125/68337], Loss: 5.1988\n",
      "Epoch [7/10], Step [58200/68337], Loss: 4.9131\n",
      "Epoch [7/10], Step [58275/68337], Loss: 5.3888\n",
      "Epoch [7/10], Step [58350/68337], Loss: 5.0175\n",
      "Epoch [7/10], Step [58425/68337], Loss: 5.1449\n",
      "Epoch [7/10], Step [58500/68337], Loss: 5.1991\n",
      "Epoch [7/10], Step [58575/68337], Loss: 5.2083\n",
      "Epoch [7/10], Step [58650/68337], Loss: 5.1492\n",
      "Epoch [7/10], Step [58725/68337], Loss: 5.2338\n",
      "Epoch [7/10], Step [58800/68337], Loss: 5.0150\n",
      "Epoch [7/10], Step [58875/68337], Loss: 5.0420\n",
      "Epoch [7/10], Step [58950/68337], Loss: 4.9829\n",
      "Epoch [7/10], Step [59025/68337], Loss: 4.9325\n",
      "Epoch [7/10], Step [59100/68337], Loss: 5.0111\n",
      "Epoch [7/10], Step [59175/68337], Loss: 5.0434\n",
      "Epoch [7/10], Step [59250/68337], Loss: 5.0506\n",
      "Epoch [7/10], Step [59325/68337], Loss: 5.0832\n",
      "Epoch [7/10], Step [59400/68337], Loss: 5.0374\n",
      "Epoch [7/10], Step [59475/68337], Loss: 5.2645\n",
      "Epoch [7/10], Step [59550/68337], Loss: 5.1620\n",
      "Epoch [7/10], Step [59625/68337], Loss: 4.9881\n",
      "Epoch [7/10], Step [59700/68337], Loss: 5.1751\n",
      "Epoch [7/10], Step [59775/68337], Loss: 5.1568\n",
      "Epoch [7/10], Step [59850/68337], Loss: 5.0479\n",
      "Epoch [7/10], Step [59925/68337], Loss: 5.0013\n",
      "Epoch [7/10], Step [60000/68337], Loss: 5.2685\n",
      "Validation perplexity: 125.01504312355264\n",
      "Epoch [7/10], Step [60075/68337], Loss: 5.1169\n",
      "Epoch [7/10], Step [60150/68337], Loss: 5.0968\n",
      "Epoch [7/10], Step [60225/68337], Loss: 5.0861\n",
      "Epoch [7/10], Step [60300/68337], Loss: 5.0572\n",
      "Epoch [7/10], Step [60375/68337], Loss: 5.0823\n",
      "Epoch [7/10], Step [60450/68337], Loss: 5.1975\n",
      "Epoch [7/10], Step [60525/68337], Loss: 4.8668\n",
      "Epoch [7/10], Step [60600/68337], Loss: 4.9956\n",
      "Epoch [7/10], Step [60675/68337], Loss: 5.3122\n",
      "Epoch [7/10], Step [60750/68337], Loss: 5.3457\n",
      "Epoch [7/10], Step [60825/68337], Loss: 5.3200\n",
      "Epoch [7/10], Step [60900/68337], Loss: 5.0088\n",
      "Epoch [7/10], Step [60975/68337], Loss: 5.1494\n",
      "Epoch [7/10], Step [61050/68337], Loss: 5.2454\n",
      "Epoch [7/10], Step [61125/68337], Loss: 4.9758\n",
      "Epoch [7/10], Step [61200/68337], Loss: 5.0881\n",
      "Epoch [7/10], Step [61275/68337], Loss: 4.8181\n",
      "Epoch [7/10], Step [61350/68337], Loss: 4.9978\n",
      "Epoch [7/10], Step [61425/68337], Loss: 4.9738\n",
      "Epoch [7/10], Step [61500/68337], Loss: 5.1285\n",
      "Epoch [7/10], Step [61575/68337], Loss: 5.0360\n",
      "Epoch [7/10], Step [61650/68337], Loss: 5.0060\n",
      "Epoch [7/10], Step [61725/68337], Loss: 5.2273\n",
      "Epoch [7/10], Step [61800/68337], Loss: 5.1730\n",
      "Epoch [7/10], Step [61875/68337], Loss: 5.2637\n",
      "Epoch [7/10], Step [61950/68337], Loss: 5.0274\n",
      "Epoch [7/10], Step [62025/68337], Loss: 5.2052\n",
      "Epoch [7/10], Step [62100/68337], Loss: 5.0767\n",
      "Epoch [7/10], Step [62175/68337], Loss: 5.0951\n",
      "Epoch [7/10], Step [62250/68337], Loss: 5.3083\n",
      "Epoch [7/10], Step [62325/68337], Loss: 5.0319\n",
      "Epoch [7/10], Step [62400/68337], Loss: 5.0833\n",
      "Epoch [7/10], Step [62475/68337], Loss: 5.2065\n",
      "Epoch [7/10], Step [62550/68337], Loss: 5.1091\n",
      "Epoch [7/10], Step [62625/68337], Loss: 5.1537\n",
      "Epoch [7/10], Step [62700/68337], Loss: 5.1452\n",
      "Epoch [7/10], Step [62775/68337], Loss: 5.0705\n",
      "Epoch [7/10], Step [62850/68337], Loss: 5.1651\n",
      "Epoch [7/10], Step [62925/68337], Loss: 4.9700\n",
      "Epoch [7/10], Step [63000/68337], Loss: 5.1468\n",
      "Epoch [7/10], Step [63075/68337], Loss: 5.1986\n",
      "Epoch [7/10], Step [63150/68337], Loss: 5.1734\n",
      "Epoch [7/10], Step [63225/68337], Loss: 5.2296\n",
      "Epoch [7/10], Step [63300/68337], Loss: 5.2680\n",
      "Epoch [7/10], Step [63375/68337], Loss: 5.1510\n",
      "Epoch [7/10], Step [63450/68337], Loss: 5.0992\n",
      "Epoch [7/10], Step [63525/68337], Loss: 5.0583\n",
      "Epoch [7/10], Step [63600/68337], Loss: 5.1064\n",
      "Epoch [7/10], Step [63675/68337], Loss: 5.1299\n",
      "Epoch [7/10], Step [63750/68337], Loss: 5.1686\n",
      "Epoch [7/10], Step [63825/68337], Loss: 5.2883\n",
      "Epoch [7/10], Step [63900/68337], Loss: 5.0484\n",
      "Epoch [7/10], Step [63975/68337], Loss: 4.9530\n",
      "Epoch [7/10], Step [64050/68337], Loss: 5.3125\n",
      "Epoch [7/10], Step [64125/68337], Loss: 5.1239\n",
      "Epoch [7/10], Step [64200/68337], Loss: 5.1037\n",
      "Epoch [7/10], Step [64275/68337], Loss: 4.8142\n",
      "Epoch [7/10], Step [64350/68337], Loss: 5.2363\n",
      "Epoch [7/10], Step [64425/68337], Loss: 5.1264\n",
      "Epoch [7/10], Step [64500/68337], Loss: 4.8704\n",
      "Epoch [7/10], Step [64575/68337], Loss: 5.1040\n",
      "Epoch [7/10], Step [64650/68337], Loss: 5.2933\n",
      "Epoch [7/10], Step [64725/68337], Loss: 5.2883\n",
      "Epoch [7/10], Step [64800/68337], Loss: 5.0372\n",
      "Epoch [7/10], Step [64875/68337], Loss: 5.1029\n",
      "Epoch [7/10], Step [64950/68337], Loss: 5.2064\n",
      "Epoch [7/10], Step [65025/68337], Loss: 5.1064\n",
      "Epoch [7/10], Step [65100/68337], Loss: 5.1521\n",
      "Epoch [7/10], Step [65175/68337], Loss: 5.2065\n",
      "Epoch [7/10], Step [65250/68337], Loss: 5.2271\n",
      "Epoch [7/10], Step [65325/68337], Loss: 4.9508\n",
      "Epoch [7/10], Step [65400/68337], Loss: 5.2049\n",
      "Epoch [7/10], Step [65475/68337], Loss: 5.1653\n",
      "Epoch [7/10], Step [65550/68337], Loss: 5.0860\n",
      "Epoch [7/10], Step [65625/68337], Loss: 4.9171\n",
      "Epoch [7/10], Step [65700/68337], Loss: 5.1417\n",
      "Epoch [7/10], Step [65775/68337], Loss: 5.1234\n",
      "Epoch [7/10], Step [65850/68337], Loss: 5.0618\n",
      "Epoch [7/10], Step [65925/68337], Loss: 5.1833\n",
      "Epoch [7/10], Step [66000/68337], Loss: 4.9323\n",
      "Epoch [7/10], Step [66075/68337], Loss: 5.1354\n",
      "Epoch [7/10], Step [66150/68337], Loss: 5.1112\n",
      "Epoch [7/10], Step [66225/68337], Loss: 5.0543\n",
      "Epoch [7/10], Step [66300/68337], Loss: 5.1370\n",
      "Epoch [7/10], Step [66375/68337], Loss: 5.1829\n",
      "Epoch [7/10], Step [66450/68337], Loss: 5.0607\n",
      "Epoch [7/10], Step [66525/68337], Loss: 5.1780\n",
      "Epoch [7/10], Step [66600/68337], Loss: 5.2259\n",
      "Epoch [7/10], Step [66675/68337], Loss: 5.4540\n",
      "Epoch [7/10], Step [66750/68337], Loss: 4.9628\n",
      "Epoch [7/10], Step [66825/68337], Loss: 5.0976\n",
      "Epoch [7/10], Step [66900/68337], Loss: 5.0083\n",
      "Epoch [7/10], Step [66975/68337], Loss: 5.1901\n",
      "Epoch [7/10], Step [67050/68337], Loss: 5.1742\n",
      "Epoch [7/10], Step [67125/68337], Loss: 5.1771\n",
      "Epoch [7/10], Step [67200/68337], Loss: 5.1426\n",
      "Epoch [7/10], Step [67275/68337], Loss: 5.0123\n",
      "Epoch [7/10], Step [67350/68337], Loss: 4.8436\n",
      "Epoch [7/10], Step [67425/68337], Loss: 4.9576\n",
      "Epoch [7/10], Step [67500/68337], Loss: 5.1460\n",
      "Epoch [7/10], Step [67575/68337], Loss: 4.9395\n",
      "Epoch [7/10], Step [67650/68337], Loss: 5.1313\n",
      "Epoch [7/10], Step [67725/68337], Loss: 5.2068\n"
     ]
    }
   ],
   "source": [
    "from src.trainComplete import TrainComplete\n",
    "from src.attentionModel import LanguageModelWithAttention\n",
    "\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,\n",
    "                           allowed_special=False, is_attention_training = True)\n",
    "\n",
    "\n",
    "context_length = 32  # Increased context size\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_attention_standard_dropout_ep10_eval10000\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 32,\n",
    "              embedding_dim = embedding_dim,\n",
    "              context_length = context_length,\n",
    "              num_epochs = 10\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd6df4-4a5f-4f08-a9ed-7f1f28f14a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainComplete import TrainComplete\n",
    "from src.attentionModel import LanguageModelWithAttention\n",
    "\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,\n",
    "                           allowed_special=False, is_attention_training = True)\n",
    "\n",
    "\n",
    "context_length = 32  # Increased context size\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_attention_standard_dropout_batchsize16_ep5_eval10000\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 16,\n",
    "              embedding_dim = embedding_dim,\n",
    "              context_length = context_length,\n",
    "              num_epochs = 5\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34208be6-bb03-4f20-a4a2-7ad0591f29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainComplete import TrainComplete\n",
    "from src.attentionModel import LanguageModelWithAttention\n",
    "\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,\n",
    "                           allowed_special=False, is_attention_training = True)\n",
    "\n",
    "\n",
    "context_length = 32  # Increased context size\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_attention_standard_dropout_batchsize64_ep5_eval10000\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 64,\n",
    "              embedding_dim = embedding_dim,\n",
    "              context_length = context_length,\n",
    "              num_epochs = 5\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0abdfa-b07b-424e-8e1c-e407834fe186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cab36-a678-40d6-b0e2-5264f644983c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
