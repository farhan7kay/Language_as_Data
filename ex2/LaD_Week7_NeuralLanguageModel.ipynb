{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3c5c90",
   "metadata": {},
   "source": [
    "# Week 7 - Building and Improving a Simple Language Model\n",
    "\n",
    "Welcome back! In Week 6, we learned how to prepare textual data for training a language model. We generated input-target pairs using a DataLoader. This week, we'll build upon that foundation to implement and improve a simple neural network language model.\n",
    "\n",
    "This notebook was created by Qumeng Sun and Lisa Beinborn. It adapts parts from Sebastian Raschka's notebooks accompanying his book \"Build a Large Language Model (from Scratch)\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead8b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n",
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4ed7a",
   "metadata": {},
   "source": [
    "## 1. Review of data preparation\n",
    "\n",
    "First, let's revisit how we prepared our data last week. We'll load the text data, tokenize it using the GPT-2 tokenizer, and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788d6b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 229893\n",
      "First 10 tokens: [198, 44558, 38340, 314, 198, 198, 41481, 314, 628, 198]\n"
     ]
    }
   ],
   "source": [
    "# Load the text data\n",
    "with open(\"jane_austen_emma.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize the text\n",
    "token_ids = tokenizer.encode(raw_text)\n",
    "\n",
    "print(\"Total number of tokens:\", len(token_ids))\n",
    "print(\"First 10 tokens:\", token_ids[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddcddc",
   "metadata": {},
   "source": [
    "## 2. Preparing dataset and dataloader\n",
    "\n",
    "We'll use the same `GPTDataset` class and `create_dataloader` function that we defined in Week 6 to generate input-target pairs where the target is the input sequence shifted by one token to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f7e4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, context_length):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of context_length\n",
    "        for i in range(0, len(token_ids) - context_length):\n",
    "            input_sequence = token_ids[i:i + context_length]\n",
    "            \n",
    "            #shift to the right\n",
    "            target_sequence = token_ids[i + 1: i + context_length + 1]\n",
    "\n",
    "            # input and output are represented as tensors\n",
    "            self.input_ids.append(torch.tensor(input_sequence))\n",
    "            self.target_ids.append(torch.tensor(target_sequence))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader(txt, batch_size=8, context_length=4, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDataset(txt, tokenizer, context_length)\n",
    "    train, dev, test = torch.utils.data.random_split(dataset, [0.8,0.1,0.1])\n",
    "    \n",
    "    # Create dataloader\n",
    "    train_dataloader = DataLoader(\n",
    "        train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    dev_dataloader = DataLoader(\n",
    "        dev,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_dataloader, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446d5b7",
   "metadata": {},
   "source": [
    "## 3. Training and evaluating a base model\n",
    "\n",
    "We'll start by defining and training a simplistic language model to understand the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf3e3d",
   "metadata": {},
   "source": [
    "### 3.1. Defining the model\n",
    "\n",
    "Our base model will consist of:\n",
    "- **Token Embeddings**: Convert token IDs to dense vectors.\n",
    "- **Positional Embeddings**: Incorporate positional information.\n",
    "- **Linear Layer**: Predict the next token in the sequence.\n",
    "\n",
    "We'll set an appropriate `context_length` during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e43abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_length):\n",
    "        super(SimpleLanguageModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "        \n",
    "        embeddings = token_embeds + position_embeds\n",
    "        logits = self.linear(embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34614df7",
   "metadata": {},
   "source": [
    "### 3.2. Setting up training parameters\n",
    "\n",
    "We'll initialize our model with an appropriate `context_length` and prepare for training.\n",
    "\n",
    "Check the torch documentation for the description of [CrossEntropyLoss](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CrossEntropyLoss) and try to understand what it means that it \"is equivalent to applying LogSoftmax on an input, followed by NLLLoss.\"\n",
    "\n",
    "Check the documentation for the [AdamOptimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) and make sure you understand the role of the lr parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537273b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 128\n",
    "context_length = 32  # Context size for training\n",
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 128\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    raw_text, batch_size=batch_size, \n",
    "    context_length=context_length, shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleLanguageModel(vocab_size, embedding_dim, context_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0728c",
   "metadata": {},
   "source": [
    "### 3.3. Training the model\n",
    "\n",
    "Let's train a very simple model and monitor the loss. This will take a while. \n",
    "Make sure you understand every step of the code at least conceptually and consult the pytorch documentation. If the training process takes too long, test it with a smaller portion of the dataset and/or fewer epochs first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a43d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [0/1436], Loss: 11.1604\n",
      "Epoch [1/2], Step [20/1436], Loss: 10.2425\n",
      "Epoch [1/2], Step [40/1436], Loss: 9.2030\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(token_logits,token_labels)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Apply the backward step (calculate the gradients) \u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Adjust the weights\u001b[39;00m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/lang3_9/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang3_9/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang3_9/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "perplexities = []\n",
    "\n",
    "# Go through learning epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    # Read in data in batches\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply the forward pass\n",
    "        logits = model(x)\n",
    "\n",
    "        # Reshape logits and labels\n",
    "        token_logits = logits.view(-1, vocab_size)\n",
    "        token_labels = y.view(-1)\n",
    "\n",
    "        # To understand what is happening during reshaping, print out logits.shape and token_logits.shape\n",
    "        # and the same for y\n",
    "        #print(logits.shape, token_logits.shape)\n",
    "        #print(y.shape, token_labels.shape)\n",
    "        #print(y[0])\n",
    "        #print(token_labels[0:10])\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(token_logits,token_labels)\n",
    "\n",
    "        # Apply the backward step (calculate the gradients) \n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss over batches\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Monitor progress every twenty batches\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate average cross-entropy loss and perplexity\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    # Monitor developments over learning process\n",
    "    train_losses.append(avg_loss)\n",
    "    perplexities.append(perplexity)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Loss and Perplexity\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss', linestyle='dashed', marker=\"o\")\n",
    "plt.title('Simple Model - Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(perplexities, label='Perplexity', linestyle='dashed', marker=\"o\")\n",
    "plt.title('Simple Model - Perplexity over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0af7df",
   "metadata": {},
   "source": [
    "### 3.4. Evaluating the model\n",
    "\n",
    "Now, we'll compute the perplexity of our simplest model on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dev_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(dev_dataloader)\n",
    "perplexity_simple = math.exp(avg_loss)\n",
    "print(f\"Perplexity of base model: {perplexity_simple:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0d402",
   "metadata": {},
   "source": [
    "## 4. Training with dropout\n",
    "\n",
    "To prevent overfitting and improve generalization, we'll test dropout as a regularization strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5a2df",
   "metadata": {},
   "source": [
    "### 4.1. Adding dropout\n",
    "\n",
    "We'll modify our model to include a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bdc5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_length, dropout=0.2):\n",
    "        super(RegularizedLanguageModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        # This is new!\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "        \n",
    "        embeddings = token_embeds + position_embeds\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        logits = self.linear(embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182d5e3",
   "metadata": {},
   "source": [
    "### 4.2. Retraining the model with dropout\n",
    "\n",
    "We'll re-initialize the model and optimizer, then retrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_reg = []\n",
    "perplexities_reg = []\n",
    "\n",
    "# Re-initialize the model with dropout\n",
    "model = RegularizedLanguageModel(vocab_size, embedding_dim, context_length, dropout=0.2).to(device)\n",
    "\n",
    "# Re-initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Retrain the model\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        \n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    train_losses_reg.append(avg_loss)\n",
    "    perplexities_reg.append(perplexity)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss and perplexity for the model with dropout\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses_reg, label='Training Loss', linestyle=\"dashed\", marker=\"o\")\n",
    "plt.title('Dropout Model - Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(perplexities_reg, label='Perplexity', linestyle=\"dashed\", marker=\"o\")\n",
    "plt.title('Dropout Model - Perplexity over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930c125",
   "metadata": {},
   "source": [
    "### 4.3. Evaluating the dropout model\n",
    "\n",
    "Now, we'll compute the perplexity of our modified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model to evaluation turns off dropout\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dev_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(dev_dataloader)\n",
    "perplexity_regularized = math.exp(avg_loss)\n",
    "print(f\"Regularized Model Perplexity: {perplexity_regularized:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f9fae",
   "metadata": {},
   "source": [
    "## 5. Improving the Model\n",
    "\n",
    "Now, try to further improve the model. For example, you could:\n",
    "- Increase the model depth.\n",
    "- Increase the embedding dimension.\n",
    "- Introduce non-linear activation functions.\n",
    "- Adjust the `context_length`.\n",
    "- Adjust the parameters of the optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac05d28",
   "metadata": {},
   "source": [
    "## 6. Generating text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, start_text, context_length=15, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = tokenizer.encode(start_text)\n",
    "    context = torch.tensor(generated, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(context_length):\n",
    "            if context.size(1) >= context_length:\n",
    "                break\n",
    "            logits = model(context)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            context = torch.cat([context, next_token_id.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    generated_text = tokenizer.decode(context[0].tolist())\n",
    "    return generated_text\n",
    "\n",
    "start_text = \"Emma was\"\n",
    "generated_text = generate_text(model, tokenizer, start_text, context_length=20)\n",
    "print(\"Generated Text:\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ea9bb-4dd8-4411-b995-457c3defdb96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
