{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3c5c90",
   "metadata": {},
   "source": [
    "# Week 7 - Building and Improving a Simple Language Model\n",
    "\n",
    "Welcome back! In Week 6, we learned how to prepare textual data for training a language model. We generated input-target pairs using a DataLoader. This week, we'll build upon that foundation to implement and improve a simple neural network language model.\n",
    "\n",
    "This notebook was created by Qumeng Sun and Lisa Beinborn. It adapts parts from Sebastian Raschka's notebooks accompanying his book \"Build a Large Language Model (from Scratch)\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3aeb197-244c-42a7-8e70-8c240e6c091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'torch>=2.0.1' 'jupyterlab>=4.0' 'tiktoken>=0.5.1' 'numpy>=1.25,<2.0'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead8b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1.post300\n",
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4ed7a",
   "metadata": {},
   "source": [
    "## 1. Review of data preparation\n",
    "\n",
    "First, let's revisit how we prepared our data last week. We'll load the text data, tokenize it using the GPT-2 tokenizer, and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0651954-feb9-4437-93a8-e838c807ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper  import get_cleaned_spanish_text_as_string\n",
    "text_path = \"content/spa_wikipedia_2021_30K-sentences.txt\"\n",
    "path_to_save_folder= \"model/train_data\"\n",
    "\n",
    "raw_text = get_cleaned_spanish_text_as_string(text_path)\n",
    "#enc_text = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788d6b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 1221649\n",
      "First 10 tokens: [1105, 390, 450, 22379, 390, 8235, 1658, 555, 64, 719]\n"
     ]
    }
   ],
   "source": [
    "# Load the text data\n",
    "raw_text = get_cleaned_spanish_text_as_string(text_path)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize the text\n",
    "token_ids = tokenizer.encode(raw_text)\n",
    "\n",
    "print(\"Total number of tokens:\", len(token_ids))\n",
    "print(\"First 10 tokens:\", token_ids[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddcddc",
   "metadata": {},
   "source": [
    "## 2. Preparing dataset and dataloader\n",
    "\n",
    "We'll use the same `GPTDataset` class and `create_dataloader` function that we defined in Week 6 to generate input-target pairs where the target is the input sequence shifted by one token to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7e4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src.dataset import GPTDataset\n",
    "from src.dataset import create_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446d5b7",
   "metadata": {},
   "source": [
    "## 3. Training and evaluating a base model\n",
    "\n",
    "We'll start by defining and training a simplistic language model to understand the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf3e3d",
   "metadata": {},
   "source": [
    "### 3.1. Defining the model\n",
    "\n",
    "Our base model will consist of:\n",
    "- **Token Embeddings**: Convert token IDs to dense vectors.\n",
    "- **Positional Embeddings**: Incorporate positional information.\n",
    "- **Linear Layer**: Predict the next token in the sequence.\n",
    "\n",
    "We'll set an appropriate `context_length` during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e43abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import SimpleLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34614df7",
   "metadata": {},
   "source": [
    "### 3.2. Setting up training parameters\n",
    "\n",
    "We'll initialize our model with an appropriate `context_length` and prepare for training.\n",
    "\n",
    "Check the torch documentation for the description of [CrossEntropyLoss](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CrossEntropyLoss) and try to understand what it means that it \"is equivalent to applying LogSoftmax on an input, followed by NLLLoss.\"\n",
    "\n",
    "Check the documentation for the [AdamOptimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) and make sure you understand the role of the lr parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "537273b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 128\n",
    "context_length = 32  # Context size for training\n",
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 128\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    raw_text, batch_size=batch_size, \n",
    "    context_length=context_length, shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleLanguageModel(vocab_size, embedding_dim, context_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters\n",
    "num_epochs = 1\n",
    "\n",
    "data_loader = train_dataloader\n",
    "train_run_label = \"temp\"\n",
    "print_every = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0728c",
   "metadata": {},
   "source": [
    "### 3.3. Training the model\n",
    "\n",
    "Let's train a very simple model and monitor the loss. This will take a while. \n",
    "Make sure you understand every step of the code at least conceptually and consult the pytorch documentation. If the training process takes too long, test it with a smaller portion of the dataset and/or fewer epochs first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a43d86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [0/7635], Loss: 11.1762\n",
      "Epoch [1/1], Step [1/7635], Loss: 11.1383\n",
      "Epoch [1/1], Step [2/7635], Loss: 11.0960\n",
      "Epoch [1/1], Step [3/7635], Loss: 11.0432\n",
      "Epoch [1/1], Step [4/7635], Loss: 10.9952\n",
      "Epoch [1/1], Step [5/7635], Loss: 10.9477\n",
      "Epoch [1/1], Step [6/7635], Loss: 10.9121\n",
      "Epoch [1/1], Step [7/7635], Loss: 10.8678\n",
      "Epoch [1/1], Step [8/7635], Loss: 10.8273\n",
      "Epoch [1/1], Step [9/7635], Loss: 10.7842\n",
      "Epoch [1/1], Step [10/7635], Loss: 10.7331\n",
      "Epoch [1/1], Step [11/7635], Loss: 10.6953\n",
      "Epoch [1/1], Step [12/7635], Loss: 10.6840\n",
      "Epoch [1/1], Step [13/7635], Loss: 10.5990\n",
      "Epoch [1/1], Step [14/7635], Loss: 10.5769\n",
      "Epoch [1/1], Step [15/7635], Loss: 10.5346\n",
      "Epoch [1/1], Step [16/7635], Loss: 10.5055\n",
      "Epoch [1/1], Step [17/7635], Loss: 10.4527\n",
      "Epoch [1/1], Step [18/7635], Loss: 10.4370\n",
      "Epoch [1/1], Step [19/7635], Loss: 10.3722\n",
      "Epoch [1/1], Step [20/7635], Loss: 10.2893\n",
      "Epoch [1/1], Step [21/7635], Loss: 10.2900\n",
      "Epoch [1/1], Step [22/7635], Loss: 10.2307\n",
      "Epoch [1/1], Step [23/7635], Loss: 10.2062\n",
      "Epoch [1/1], Step [24/7635], Loss: 10.1279\n",
      "Epoch [1/1], Step [25/7635], Loss: 10.0842\n",
      "Epoch [1/1], Step [26/7635], Loss: 10.0568\n",
      "Epoch [1/1], Step [27/7635], Loss: 10.0049\n",
      "Epoch [1/1], Step [28/7635], Loss: 9.9558\n",
      "Epoch [1/1], Step [29/7635], Loss: 9.9560\n",
      "Epoch [1/1], Step [30/7635], Loss: 9.9036\n",
      "Epoch [1/1], Step [31/7635], Loss: 9.8420\n",
      "Epoch [1/1], Step [32/7635], Loss: 9.7849\n",
      "Epoch [1/1], Step [33/7635], Loss: 9.7553\n",
      "Epoch [1/1], Step [34/7635], Loss: 9.7034\n",
      "Epoch [1/1], Step [35/7635], Loss: 9.6458\n",
      "Epoch [1/1], Step [36/7635], Loss: 9.6412\n",
      "Epoch [1/1], Step [37/7635], Loss: 9.5742\n",
      "Epoch [1/1], Step [38/7635], Loss: 9.5066\n",
      "Epoch [1/1], Step [39/7635], Loss: 9.4401\n",
      "Epoch [1/1], Step [40/7635], Loss: 9.4147\n",
      "Epoch [1/1], Step [41/7635], Loss: 9.3586\n",
      "Epoch [1/1], Step [42/7635], Loss: 9.3265\n",
      "Epoch [1/1], Step [43/7635], Loss: 9.2302\n",
      "Epoch [1/1], Step [44/7635], Loss: 9.2278\n",
      "Epoch [1/1], Step [45/7635], Loss: 9.1584\n",
      "Epoch [1/1], Step [46/7635], Loss: 9.1182\n",
      "Epoch [1/1], Step [47/7635], Loss: 9.0577\n",
      "Epoch [1/1], Step [48/7635], Loss: 9.0291\n",
      "Epoch [1/1], Step [49/7635], Loss: 8.9983\n",
      "Epoch [1/1], Step [50/7635], Loss: 8.9449\n",
      "Epoch [1/1], Step [51/7635], Loss: 8.9294\n",
      "Epoch [1/1], Step [52/7635], Loss: 8.8313\n",
      "Epoch [1/1], Step [53/7635], Loss: 8.7989\n",
      "Epoch [1/1], Step [54/7635], Loss: 8.7928\n",
      "Epoch [1/1], Step [55/7635], Loss: 8.6619\n",
      "Epoch [1/1], Step [56/7635], Loss: 8.6131\n",
      "Epoch [1/1], Step [57/7635], Loss: 8.6007\n",
      "Epoch [1/1], Step [58/7635], Loss: 8.5425\n",
      "Epoch [1/1], Step [59/7635], Loss: 8.4870\n",
      "Epoch [1/1], Step [60/7635], Loss: 8.4116\n",
      "Epoch [1/1], Step [61/7635], Loss: 8.4256\n",
      "Epoch [1/1], Step [62/7635], Loss: 8.3368\n",
      "Epoch [1/1], Step [63/7635], Loss: 8.3023\n",
      "Epoch [1/1], Step [64/7635], Loss: 8.2084\n",
      "Epoch [1/1], Step [65/7635], Loss: 8.1555\n",
      "Epoch [1/1], Step [66/7635], Loss: 8.1350\n",
      "Epoch [1/1], Step [67/7635], Loss: 8.0729\n",
      "Epoch [1/1], Step [68/7635], Loss: 8.0940\n",
      "Epoch [1/1], Step [69/7635], Loss: 8.0721\n",
      "Epoch [1/1], Step [70/7635], Loss: 7.9262\n",
      "Epoch [1/1], Step [71/7635], Loss: 7.9080\n",
      "Epoch [1/1], Step [72/7635], Loss: 7.9024\n",
      "Epoch [1/1], Step [73/7635], Loss: 7.8149\n",
      "Epoch [1/1], Step [74/7635], Loss: 7.8268\n",
      "Epoch [1/1], Step [75/7635], Loss: 7.8303\n",
      "Epoch [1/1], Step [76/7635], Loss: 7.7217\n",
      "Epoch [1/1], Step [77/7635], Loss: 7.6212\n",
      "Epoch [1/1], Step [78/7635], Loss: 7.6321\n",
      "Epoch [1/1], Step [79/7635], Loss: 7.6018\n",
      "Epoch [1/1], Step [80/7635], Loss: 7.5738\n",
      "Epoch [1/1], Step [81/7635], Loss: 7.4564\n",
      "Epoch [1/1], Step [82/7635], Loss: 7.4131\n",
      "Epoch [1/1], Step [83/7635], Loss: 7.4250\n",
      "Epoch [1/1], Step [84/7635], Loss: 7.3844\n",
      "Epoch [1/1], Step [85/7635], Loss: 7.3528\n",
      "Epoch [1/1], Step [86/7635], Loss: 7.3148\n",
      "Epoch [1/1], Step [87/7635], Loss: 7.2519\n",
      "Epoch [1/1], Step [88/7635], Loss: 7.2884\n",
      "Epoch [1/1], Step [89/7635], Loss: 7.1440\n",
      "Epoch [1/1], Step [90/7635], Loss: 7.1718\n",
      "Epoch [1/1], Step [91/7635], Loss: 7.1200\n",
      "Epoch [1/1], Step [92/7635], Loss: 7.1498\n",
      "Epoch [1/1], Step [93/7635], Loss: 7.0078\n",
      "Epoch [1/1], Step [94/7635], Loss: 7.0553\n",
      "Epoch [1/1], Step [95/7635], Loss: 6.9824\n",
      "Epoch [1/1], Step [96/7635], Loss: 6.8908\n",
      "Epoch [1/1], Step [97/7635], Loss: 7.0110\n",
      "Epoch [1/1], Step [98/7635], Loss: 6.9923\n",
      "Epoch [1/1], Step [99/7635], Loss: 6.8605\n",
      "Epoch [1/1], Step [100/7635], Loss: 6.8858\n",
      "Epoch [1/1], Step [101/7635], Loss: 6.7589\n",
      "Epoch [1/1], Step [102/7635], Loss: 6.7677\n",
      "Epoch [1/1], Step [103/7635], Loss: 6.7745\n",
      "Epoch [1/1], Step [104/7635], Loss: 6.7648\n",
      "Epoch [1/1], Step [105/7635], Loss: 6.6028\n",
      "Epoch [1/1], Step [106/7635], Loss: 6.7302\n",
      "Epoch [1/1], Step [107/7635], Loss: 6.7172\n",
      "Epoch [1/1], Step [108/7635], Loss: 6.6683\n",
      "Epoch [1/1], Step [109/7635], Loss: 6.6408\n",
      "Epoch [1/1], Step [110/7635], Loss: 6.6672\n",
      "Epoch [1/1], Step [111/7635], Loss: 6.6359\n",
      "Epoch [1/1], Step [112/7635], Loss: 6.5695\n",
      "Epoch [1/1], Step [113/7635], Loss: 6.6175\n",
      "Epoch [1/1], Step [114/7635], Loss: 6.4832\n",
      "Epoch [1/1], Step [115/7635], Loss: 6.4702\n",
      "Epoch [1/1], Step [116/7635], Loss: 6.4950\n",
      "Epoch [1/1], Step [117/7635], Loss: 6.4999\n",
      "Epoch [1/1], Step [118/7635], Loss: 6.4517\n",
      "Epoch [1/1], Step [119/7635], Loss: 6.4138\n",
      "Epoch [1/1], Step [120/7635], Loss: 6.3835\n",
      "Epoch [1/1], Step [121/7635], Loss: 6.3656\n",
      "Epoch [1/1], Step [122/7635], Loss: 6.4225\n",
      "Epoch [1/1], Step [123/7635], Loss: 6.3585\n",
      "Epoch [1/1], Step [124/7635], Loss: 6.3447\n",
      "Epoch [1/1], Step [125/7635], Loss: 6.4066\n",
      "Epoch [1/1], Step [126/7635], Loss: 6.3232\n",
      "Epoch [1/1], Step [127/7635], Loss: 6.3723\n",
      "Epoch [1/1], Step [128/7635], Loss: 6.3390\n",
      "Epoch [1/1], Step [129/7635], Loss: 6.3011\n",
      "Epoch [1/1], Step [130/7635], Loss: 6.3779\n",
      "Epoch [1/1], Step [131/7635], Loss: 6.3157\n",
      "Epoch [1/1], Step [132/7635], Loss: 6.2361\n",
      "Epoch [1/1], Step [133/7635], Loss: 6.2564\n",
      "Epoch [1/1], Step [134/7635], Loss: 6.2311\n",
      "Epoch [1/1], Step [135/7635], Loss: 6.1638\n",
      "Epoch [1/1], Step [136/7635], Loss: 6.1283\n",
      "Epoch [1/1], Step [137/7635], Loss: 6.1219\n",
      "Epoch [1/1], Step [138/7635], Loss: 6.1378\n",
      "Epoch [1/1], Step [139/7635], Loss: 6.1116\n",
      "Epoch [1/1], Step [140/7635], Loss: 6.1564\n",
      "Epoch [1/1], Step [141/7635], Loss: 6.1450\n",
      "Epoch [1/1], Step [142/7635], Loss: 6.0657\n",
      "Epoch [1/1], Step [143/7635], Loss: 6.0312\n",
      "Epoch [1/1], Step [144/7635], Loss: 6.0936\n",
      "Epoch [1/1], Step [145/7635], Loss: 6.0987\n",
      "Epoch [1/1], Step [146/7635], Loss: 5.9826\n",
      "Epoch [1/1], Step [147/7635], Loss: 6.1129\n",
      "Epoch [1/1], Step [148/7635], Loss: 6.0338\n",
      "Epoch [1/1], Step [149/7635], Loss: 6.0344\n",
      "Epoch [1/1], Step [150/7635], Loss: 6.1177\n",
      "Epoch [1/1], Step [151/7635], Loss: 5.9873\n",
      "Epoch [1/1], Step [152/7635], Loss: 6.0739\n",
      "Epoch [1/1], Step [153/7635], Loss: 6.0180\n",
      "Epoch [1/1], Step [154/7635], Loss: 5.9721\n",
      "Epoch [1/1], Step [155/7635], Loss: 5.9250\n",
      "Epoch [1/1], Step [156/7635], Loss: 5.9665\n",
      "Epoch [1/1], Step [157/7635], Loss: 5.9053\n",
      "Epoch [1/1], Step [158/7635], Loss: 5.9429\n",
      "Epoch [1/1], Step [159/7635], Loss: 5.9659\n",
      "Epoch [1/1], Step [160/7635], Loss: 5.9823\n",
      "Epoch [1/1], Step [161/7635], Loss: 5.8881\n",
      "Epoch [1/1], Step [162/7635], Loss: 5.8869\n",
      "Epoch [1/1], Step [163/7635], Loss: 5.9518\n",
      "Epoch [1/1], Step [164/7635], Loss: 6.0494\n",
      "Epoch [1/1], Step [165/7635], Loss: 5.9678\n",
      "Epoch [1/1], Step [166/7635], Loss: 5.8852\n",
      "Epoch [1/1], Step [167/7635], Loss: 5.9399\n",
      "Epoch [1/1], Step [168/7635], Loss: 5.8532\n",
      "Epoch [1/1], Step [169/7635], Loss: 5.8548\n",
      "Epoch [1/1], Step [170/7635], Loss: 5.9094\n",
      "Epoch [1/1], Step [171/7635], Loss: 5.8675\n",
      "Epoch [1/1], Step [172/7635], Loss: 5.8447\n",
      "Epoch [1/1], Step [173/7635], Loss: 5.8830\n",
      "Epoch [1/1], Step [174/7635], Loss: 5.8382\n",
      "Epoch [1/1], Step [175/7635], Loss: 5.8248\n",
      "Epoch [1/1], Step [176/7635], Loss: 5.8156\n",
      "Epoch [1/1], Step [177/7635], Loss: 5.8513\n",
      "Epoch [1/1], Step [178/7635], Loss: 5.7207\n",
      "Epoch [1/1], Step [179/7635], Loss: 5.8771\n",
      "Epoch [1/1], Step [180/7635], Loss: 5.8225\n",
      "Epoch [1/1], Step [181/7635], Loss: 5.8452\n",
      "Epoch [1/1], Step [182/7635], Loss: 5.8082\n",
      "Epoch [1/1], Step [183/7635], Loss: 5.8217\n",
      "Epoch [1/1], Step [184/7635], Loss: 5.8021\n",
      "Epoch [1/1], Step [185/7635], Loss: 5.7986\n",
      "Epoch [1/1], Step [186/7635], Loss: 5.8039\n",
      "Epoch [1/1], Step [187/7635], Loss: 5.6758\n",
      "Epoch [1/1], Step [188/7635], Loss: 5.7549\n",
      "Epoch [1/1], Step [189/7635], Loss: 5.7472\n",
      "Epoch [1/1], Step [190/7635], Loss: 5.7513\n",
      "Epoch [1/1], Step [191/7635], Loss: 5.7862\n",
      "Epoch [1/1], Step [192/7635], Loss: 5.7284\n",
      "Epoch [1/1], Step [193/7635], Loss: 5.7577\n",
      "Epoch [1/1], Step [194/7635], Loss: 5.7156\n",
      "Epoch [1/1], Step [195/7635], Loss: 5.7484\n",
      "Epoch [1/1], Step [196/7635], Loss: 5.7550\n",
      "Epoch [1/1], Step [197/7635], Loss: 5.6903\n",
      "Epoch [1/1], Step [198/7635], Loss: 5.7665\n",
      "Epoch [1/1], Step [199/7635], Loss: 5.7561\n",
      "Epoch [1/1], Step [200/7635], Loss: 5.7065\n",
      "Epoch [1/1], Step [201/7635], Loss: 5.8083\n",
      "Epoch [1/1], Step [202/7635], Loss: 5.7848\n",
      "Epoch [1/1], Step [203/7635], Loss: 5.7164\n",
      "Epoch [1/1], Step [204/7635], Loss: 5.6506\n",
      "Epoch [1/1], Step [205/7635], Loss: 5.6264\n",
      "Epoch [1/1], Step [206/7635], Loss: 5.6374\n",
      "Epoch [1/1], Step [207/7635], Loss: 5.6615\n",
      "Epoch [1/1], Step [208/7635], Loss: 5.6361\n",
      "Epoch [1/1], Step [209/7635], Loss: 5.5337\n",
      "Epoch [1/1], Step [210/7635], Loss: 5.6564\n",
      "Epoch [1/1], Step [211/7635], Loss: 5.7326\n",
      "Epoch [1/1], Step [212/7635], Loss: 5.6991\n",
      "Epoch [1/1], Step [213/7635], Loss: 5.6669\n",
      "Epoch [1/1], Step [214/7635], Loss: 5.7058\n",
      "Epoch [1/1], Step [215/7635], Loss: 5.6192\n",
      "Epoch [1/1], Step [216/7635], Loss: 5.6575\n",
      "Epoch [1/1], Step [217/7635], Loss: 5.5559\n",
      "Epoch [1/1], Step [218/7635], Loss: 5.5266\n",
      "Epoch [1/1], Step [219/7635], Loss: 5.5585\n",
      "Epoch [1/1], Step [220/7635], Loss: 5.6273\n",
      "Epoch [1/1], Step [221/7635], Loss: 5.6064\n",
      "Epoch [1/1], Step [222/7635], Loss: 5.6338\n",
      "Epoch [1/1], Step [223/7635], Loss: 5.4949\n",
      "Epoch [1/1], Step [224/7635], Loss: 5.5036\n",
      "Epoch [1/1], Step [225/7635], Loss: 5.5320\n",
      "Epoch [1/1], Step [226/7635], Loss: 5.5912\n",
      "Epoch [1/1], Step [227/7635], Loss: 5.6582\n",
      "Epoch [1/1], Step [228/7635], Loss: 5.5768\n",
      "Epoch [1/1], Step [229/7635], Loss: 5.6354\n",
      "Epoch [1/1], Step [230/7635], Loss: 5.6063\n",
      "Epoch [1/1], Step [231/7635], Loss: 5.5241\n",
      "Epoch [1/1], Step [232/7635], Loss: 5.4860\n",
      "Epoch [1/1], Step [233/7635], Loss: 5.5238\n",
      "Epoch [1/1], Step [234/7635], Loss: 5.6239\n",
      "Epoch [1/1], Step [235/7635], Loss: 5.5045\n",
      "Epoch [1/1], Step [236/7635], Loss: 5.5789\n",
      "Epoch [1/1], Step [237/7635], Loss: 5.5204\n",
      "Epoch [1/1], Step [238/7635], Loss: 5.6459\n",
      "Epoch [1/1], Step [239/7635], Loss: 5.5293\n",
      "Epoch [1/1], Step [240/7635], Loss: 5.5562\n",
      "Epoch [1/1], Step [241/7635], Loss: 5.5267\n",
      "Epoch [1/1], Step [242/7635], Loss: 5.5018\n",
      "Epoch [1/1], Step [243/7635], Loss: 5.5337\n",
      "Epoch [1/1], Step [244/7635], Loss: 5.5478\n",
      "Epoch [1/1], Step [245/7635], Loss: 5.4965\n",
      "Epoch [1/1], Step [246/7635], Loss: 5.4589\n",
      "Epoch [1/1], Step [247/7635], Loss: 5.6562\n",
      "Epoch [1/1], Step [248/7635], Loss: 5.4840\n",
      "Epoch [1/1], Step [249/7635], Loss: 5.5202\n",
      "Epoch [1/1], Step [250/7635], Loss: 5.5332\n",
      "Epoch [1/1], Step [251/7635], Loss: 5.5501\n",
      "Epoch [1/1], Step [252/7635], Loss: 5.4744\n",
      "Epoch [1/1], Step [253/7635], Loss: 5.4354\n",
      "Epoch [1/1], Step [254/7635], Loss: 5.4343\n",
      "Epoch [1/1], Step [255/7635], Loss: 5.5456\n",
      "Epoch [1/1], Step [256/7635], Loss: 5.4860\n",
      "Epoch [1/1], Step [257/7635], Loss: 5.5397\n",
      "Epoch [1/1], Step [258/7635], Loss: 5.4668\n",
      "Epoch [1/1], Step [259/7635], Loss: 5.4996\n",
      "Epoch [1/1], Step [260/7635], Loss: 5.4060\n",
      "Epoch [1/1], Step [261/7635], Loss: 5.4231\n",
      "Epoch [1/1], Step [262/7635], Loss: 5.4703\n",
      "Epoch [1/1], Step [263/7635], Loss: 5.4710\n",
      "Epoch [1/1], Step [264/7635], Loss: 5.5168\n",
      "Epoch [1/1], Step [265/7635], Loss: 5.5133\n",
      "Epoch [1/1], Step [266/7635], Loss: 5.3906\n",
      "Epoch [1/1], Step [267/7635], Loss: 5.3998\n",
      "Epoch [1/1], Step [268/7635], Loss: 5.4520\n",
      "Epoch [1/1], Step [269/7635], Loss: 5.4371\n",
      "Epoch [1/1], Step [270/7635], Loss: 5.6041\n",
      "Epoch [1/1], Step [271/7635], Loss: 5.3877\n",
      "Epoch [1/1], Step [272/7635], Loss: 5.3968\n",
      "Epoch [1/1], Step [273/7635], Loss: 5.4088\n",
      "Epoch [1/1], Step [274/7635], Loss: 5.4512\n",
      "Epoch [1/1], Step [275/7635], Loss: 5.4420\n",
      "Epoch [1/1], Step [276/7635], Loss: 5.2984\n",
      "Epoch [1/1], Step [277/7635], Loss: 5.4601\n",
      "Epoch [1/1], Step [278/7635], Loss: 5.2778\n",
      "Epoch [1/1], Step [279/7635], Loss: 5.5029\n",
      "Epoch [1/1], Step [280/7635], Loss: 5.4664\n",
      "Epoch [1/1], Step [281/7635], Loss: 5.5212\n",
      "Epoch [1/1], Step [282/7635], Loss: 5.3788\n",
      "Epoch [1/1], Step [283/7635], Loss: 5.4307\n",
      "Epoch [1/1], Step [284/7635], Loss: 5.4563\n",
      "Epoch [1/1], Step [285/7635], Loss: 5.3362\n",
      "Epoch [1/1], Step [286/7635], Loss: 5.4635\n",
      "Epoch [1/1], Step [287/7635], Loss: 5.4780\n",
      "Epoch [1/1], Step [288/7635], Loss: 5.2987\n",
      "Epoch [1/1], Step [289/7635], Loss: 5.4379\n",
      "Epoch [1/1], Step [290/7635], Loss: 5.4721\n",
      "Epoch [1/1], Step [291/7635], Loss: 5.3947\n",
      "Epoch [1/1], Step [292/7635], Loss: 5.2954\n",
      "Epoch [1/1], Step [293/7635], Loss: 5.3436\n",
      "Epoch [1/1], Step [294/7635], Loss: 5.4309\n",
      "Epoch [1/1], Step [295/7635], Loss: 5.4801\n",
      "Epoch [1/1], Step [296/7635], Loss: 5.3431\n",
      "Epoch [1/1], Step [297/7635], Loss: 5.4340\n",
      "Epoch [1/1], Step [298/7635], Loss: 5.3627\n",
      "Epoch [1/1], Step [299/7635], Loss: 5.3676\n",
      "Epoch [1/1], Step [300/7635], Loss: 5.3494\n",
      "Epoch [1/1], Step [301/7635], Loss: 5.2781\n",
      "Epoch [1/1], Step [302/7635], Loss: 5.3950\n",
      "Epoch [1/1], Step [303/7635], Loss: 5.3117\n",
      "Epoch [1/1], Step [304/7635], Loss: 5.4114\n",
      "Epoch [1/1], Step [305/7635], Loss: 5.3084\n",
      "Epoch [1/1], Step [306/7635], Loss: 5.2623\n",
      "Epoch [1/1], Step [307/7635], Loss: 5.4019\n",
      "Epoch [1/1], Step [308/7635], Loss: 5.3509\n",
      "Epoch [1/1], Step [309/7635], Loss: 5.2904\n",
      "Epoch [1/1], Step [310/7635], Loss: 5.1909\n",
      "Epoch [1/1], Step [311/7635], Loss: 5.2266\n",
      "Epoch [1/1], Step [312/7635], Loss: 5.3116\n",
      "Epoch [1/1], Step [313/7635], Loss: 5.2615\n",
      "Epoch [1/1], Step [314/7635], Loss: 5.2606\n",
      "Epoch [1/1], Step [315/7635], Loss: 5.2610\n",
      "Epoch [1/1], Step [316/7635], Loss: 5.3945\n",
      "Epoch [1/1], Step [317/7635], Loss: 5.3783\n",
      "Epoch [1/1], Step [318/7635], Loss: 5.3432\n",
      "Epoch [1/1], Step [319/7635], Loss: 5.4117\n",
      "Epoch [1/1], Step [320/7635], Loss: 5.3193\n",
      "Epoch [1/1], Step [321/7635], Loss: 5.2911\n",
      "Epoch [1/1], Step [322/7635], Loss: 5.1438\n",
      "Epoch [1/1], Step [323/7635], Loss: 5.3436\n",
      "Epoch [1/1], Step [324/7635], Loss: 5.2815\n",
      "Epoch [1/1], Step [325/7635], Loss: 5.2770\n",
      "Epoch [1/1], Step [326/7635], Loss: 5.2061\n",
      "Epoch [1/1], Step [327/7635], Loss: 5.2834\n",
      "Epoch [1/1], Step [328/7635], Loss: 5.2312\n",
      "Epoch [1/1], Step [329/7635], Loss: 5.3209\n",
      "Epoch [1/1], Step [330/7635], Loss: 5.2472\n",
      "Epoch [1/1], Step [331/7635], Loss: 5.3382\n",
      "Epoch [1/1], Step [332/7635], Loss: 5.2791\n",
      "Epoch [1/1], Step [333/7635], Loss: 5.4055\n",
      "Epoch [1/1], Step [334/7635], Loss: 5.3264\n",
      "Epoch [1/1], Step [335/7635], Loss: 5.1800\n",
      "Epoch [1/1], Step [336/7635], Loss: 5.2334\n",
      "Epoch [1/1], Step [337/7635], Loss: 5.1977\n",
      "Epoch [1/1], Step [338/7635], Loss: 5.3244\n",
      "Epoch [1/1], Step [339/7635], Loss: 5.3533\n",
      "Epoch [1/1], Step [340/7635], Loss: 5.2003\n",
      "Epoch [1/1], Step [341/7635], Loss: 5.1814\n",
      "Epoch [1/1], Step [342/7635], Loss: 5.2193\n",
      "Epoch [1/1], Step [343/7635], Loss: 5.2729\n",
      "Epoch [1/1], Step [344/7635], Loss: 5.2070\n",
      "Epoch [1/1], Step [345/7635], Loss: 5.1438\n",
      "Epoch [1/1], Step [346/7635], Loss: 5.1627\n",
      "Epoch [1/1], Step [347/7635], Loss: 5.2793\n",
      "Epoch [1/1], Step [348/7635], Loss: 5.1609\n",
      "Epoch [1/1], Step [349/7635], Loss: 5.2476\n",
      "Epoch [1/1], Step [350/7635], Loss: 5.2211\n",
      "Epoch [1/1], Step [351/7635], Loss: 5.2609\n",
      "Epoch [1/1], Step [352/7635], Loss: 5.1971\n",
      "Epoch [1/1], Step [353/7635], Loss: 5.2150\n",
      "Epoch [1/1], Step [354/7635], Loss: 5.2424\n",
      "Epoch [1/1], Step [355/7635], Loss: 5.1596\n",
      "Epoch [1/1], Step [356/7635], Loss: 5.2234\n",
      "Epoch [1/1], Step [357/7635], Loss: 5.1288\n",
      "Epoch [1/1], Step [358/7635], Loss: 5.1458\n",
      "Epoch [1/1], Step [359/7635], Loss: 5.1094\n",
      "Epoch [1/1], Step [360/7635], Loss: 5.2537\n",
      "Epoch [1/1], Step [361/7635], Loss: 5.2879\n",
      "Epoch [1/1], Step [362/7635], Loss: 5.2046\n",
      "Epoch [1/1], Step [363/7635], Loss: 5.1916\n",
      "Epoch [1/1], Step [364/7635], Loss: 5.1065\n",
      "Epoch [1/1], Step [365/7635], Loss: 5.2537\n",
      "Epoch [1/1], Step [366/7635], Loss: 5.2293\n",
      "Epoch [1/1], Step [367/7635], Loss: 5.2439\n",
      "Epoch [1/1], Step [368/7635], Loss: 5.2491\n",
      "Epoch [1/1], Step [369/7635], Loss: 5.3071\n",
      "Epoch [1/1], Step [370/7635], Loss: 5.1665\n",
      "Epoch [1/1], Step [371/7635], Loss: 5.1419\n",
      "Epoch [1/1], Step [372/7635], Loss: 5.2344\n",
      "Epoch [1/1], Step [373/7635], Loss: 5.1873\n",
      "Epoch [1/1], Step [374/7635], Loss: 5.2438\n",
      "Epoch [1/1], Step [375/7635], Loss: 5.1102\n",
      "Epoch [1/1], Step [376/7635], Loss: 5.1653\n",
      "Epoch [1/1], Step [377/7635], Loss: 5.1795\n",
      "Epoch [1/1], Step [378/7635], Loss: 5.0908\n",
      "Epoch [1/1], Step [379/7635], Loss: 5.1020\n",
      "Epoch [1/1], Step [380/7635], Loss: 5.1440\n",
      "Epoch [1/1], Step [381/7635], Loss: 5.2845\n",
      "Epoch [1/1], Step [382/7635], Loss: 5.1020\n",
      "Epoch [1/1], Step [383/7635], Loss: 5.0009\n",
      "Epoch [1/1], Step [384/7635], Loss: 5.1228\n",
      "Epoch [1/1], Step [385/7635], Loss: 5.1473\n",
      "Epoch [1/1], Step [386/7635], Loss: 5.1183\n",
      "Epoch [1/1], Step [387/7635], Loss: 5.0780\n",
      "Epoch [1/1], Step [388/7635], Loss: 5.1564\n",
      "Epoch [1/1], Step [389/7635], Loss: 5.0500\n",
      "Epoch [1/1], Step [390/7635], Loss: 5.1827\n",
      "Epoch [1/1], Step [391/7635], Loss: 5.2241\n",
      "Epoch [1/1], Step [392/7635], Loss: 5.1173\n",
      "Epoch [1/1], Step [393/7635], Loss: 5.1394\n",
      "Epoch [1/1], Step [394/7635], Loss: 5.1829\n",
      "Epoch [1/1], Step [395/7635], Loss: 5.1087\n",
      "Epoch [1/1], Step [396/7635], Loss: 5.1631\n",
      "Epoch [1/1], Step [397/7635], Loss: 5.2107\n",
      "Epoch [1/1], Step [398/7635], Loss: 5.0835\n",
      "Epoch [1/1], Step [399/7635], Loss: 5.0387\n",
      "Epoch [1/1], Step [400/7635], Loss: 5.1110\n",
      "Epoch [1/1], Step [401/7635], Loss: 5.0949\n",
      "Epoch [1/1], Step [402/7635], Loss: 5.1089\n",
      "Epoch [1/1], Step [403/7635], Loss: 5.1977\n",
      "Epoch [1/1], Step [404/7635], Loss: 5.0792\n",
      "Epoch [1/1], Step [405/7635], Loss: 5.1385\n",
      "Epoch [1/1], Step [406/7635], Loss: 5.0286\n",
      "Epoch [1/1], Step [407/7635], Loss: 5.1998\n",
      "Epoch [1/1], Step [408/7635], Loss: 5.1729\n",
      "Epoch [1/1], Step [409/7635], Loss: 5.0672\n",
      "Epoch [1/1], Step [410/7635], Loss: 5.0530\n",
      "Epoch [1/1], Step [411/7635], Loss: 5.0072\n",
      "Epoch [1/1], Step [412/7635], Loss: 5.0601\n",
      "Epoch [1/1], Step [413/7635], Loss: 5.1159\n",
      "Epoch [1/1], Step [414/7635], Loss: 5.0362\n",
      "Epoch [1/1], Step [415/7635], Loss: 5.0760\n",
      "Epoch [1/1], Step [416/7635], Loss: 5.0310\n",
      "Epoch [1/1], Step [417/7635], Loss: 5.0907\n",
      "Epoch [1/1], Step [418/7635], Loss: 5.1420\n",
      "Epoch [1/1], Step [419/7635], Loss: 5.0256\n",
      "Epoch [1/1], Step [420/7635], Loss: 5.2006\n",
      "Epoch [1/1], Step [421/7635], Loss: 5.1131\n",
      "Epoch [1/1], Step [422/7635], Loss: 4.9948\n",
      "Epoch [1/1], Step [423/7635], Loss: 4.9422\n",
      "Epoch [1/1], Step [424/7635], Loss: 5.1497\n",
      "Epoch [1/1], Step [425/7635], Loss: 5.0599\n",
      "Epoch [1/1], Step [426/7635], Loss: 5.0764\n",
      "Epoch [1/1], Step [427/7635], Loss: 5.1033\n",
      "Epoch [1/1], Step [428/7635], Loss: 5.0866\n",
      "Epoch [1/1], Step [429/7635], Loss: 5.0722\n",
      "Epoch [1/1], Step [430/7635], Loss: 5.1792\n",
      "Epoch [1/1], Step [431/7635], Loss: 5.0580\n",
      "Epoch [1/1], Step [432/7635], Loss: 5.1166\n",
      "Epoch [1/1], Step [433/7635], Loss: 5.0485\n",
      "Epoch [1/1], Step [434/7635], Loss: 5.0860\n",
      "Epoch [1/1], Step [435/7635], Loss: 5.0428\n",
      "Epoch [1/1], Step [436/7635], Loss: 5.0351\n",
      "Epoch [1/1], Step [437/7635], Loss: 5.0603\n",
      "Epoch [1/1], Step [438/7635], Loss: 4.9210\n",
      "Epoch [1/1], Step [439/7635], Loss: 5.0107\n",
      "Epoch [1/1], Step [440/7635], Loss: 5.0827\n",
      "Epoch [1/1], Step [441/7635], Loss: 5.1008\n",
      "Epoch [1/1], Step [442/7635], Loss: 5.0457\n",
      "Epoch [1/1], Step [443/7635], Loss: 5.2397\n",
      "Epoch [1/1], Step [444/7635], Loss: 5.0290\n",
      "Epoch [1/1], Step [445/7635], Loss: 4.9974\n",
      "Epoch [1/1], Step [446/7635], Loss: 5.1331\n",
      "Epoch [1/1], Step [447/7635], Loss: 5.0010\n",
      "Epoch [1/1], Step [448/7635], Loss: 5.0106\n",
      "Epoch [1/1], Step [449/7635], Loss: 5.1267\n",
      "Epoch [1/1], Step [450/7635], Loss: 5.1396\n",
      "Epoch [1/1], Step [451/7635], Loss: 5.0942\n",
      "Epoch [1/1], Step [452/7635], Loss: 4.9423\n",
      "Epoch [1/1], Step [453/7635], Loss: 4.9368\n",
      "Epoch [1/1], Step [454/7635], Loss: 5.0067\n",
      "Epoch [1/1], Step [455/7635], Loss: 4.9747\n",
      "Epoch [1/1], Step [456/7635], Loss: 4.9886\n",
      "Epoch [1/1], Step [457/7635], Loss: 5.0756\n",
      "Epoch [1/1], Step [458/7635], Loss: 4.9684\n",
      "Epoch [1/1], Step [459/7635], Loss: 5.0024\n",
      "Epoch [1/1], Step [460/7635], Loss: 4.9284\n",
      "Epoch [1/1], Step [461/7635], Loss: 4.9501\n",
      "Epoch [1/1], Step [462/7635], Loss: 5.0394\n",
      "Epoch [1/1], Step [463/7635], Loss: 5.0854\n",
      "Epoch [1/1], Step [464/7635], Loss: 5.0000\n",
      "Epoch [1/1], Step [465/7635], Loss: 4.9557\n",
      "Epoch [1/1], Step [466/7635], Loss: 5.1224\n",
      "Epoch [1/1], Step [467/7635], Loss: 4.9451\n",
      "Epoch [1/1], Step [468/7635], Loss: 5.0619\n",
      "Epoch [1/1], Step [469/7635], Loss: 5.0007\n",
      "Epoch [1/1], Step [470/7635], Loss: 4.8649\n",
      "Epoch [1/1], Step [471/7635], Loss: 4.9408\n",
      "Epoch [1/1], Step [472/7635], Loss: 4.9371\n",
      "Epoch [1/1], Step [473/7635], Loss: 4.9170\n",
      "Epoch [1/1], Step [474/7635], Loss: 5.0029\n",
      "Epoch [1/1], Step [475/7635], Loss: 5.0381\n",
      "Epoch [1/1], Step [476/7635], Loss: 5.0217\n",
      "Epoch [1/1], Step [477/7635], Loss: 4.9982\n",
      "Epoch [1/1], Step [478/7635], Loss: 5.0238\n",
      "Epoch [1/1], Step [479/7635], Loss: 5.0919\n",
      "Epoch [1/1], Step [480/7635], Loss: 5.0878\n",
      "Epoch [1/1], Step [481/7635], Loss: 5.0168\n",
      "Epoch [1/1], Step [482/7635], Loss: 5.0360\n",
      "Epoch [1/1], Step [483/7635], Loss: 4.9639\n",
      "Epoch [1/1], Step [484/7635], Loss: 4.9123\n",
      "Epoch [1/1], Step [485/7635], Loss: 4.9276\n",
      "Epoch [1/1], Step [486/7635], Loss: 4.9008\n",
      "Epoch [1/1], Step [487/7635], Loss: 4.9865\n",
      "Epoch [1/1], Step [488/7635], Loss: 4.9650\n",
      "Epoch [1/1], Step [489/7635], Loss: 5.0795\n",
      "Epoch [1/1], Step [490/7635], Loss: 4.9853\n",
      "Epoch [1/1], Step [491/7635], Loss: 4.9764\n",
      "Epoch [1/1], Step [492/7635], Loss: 5.0803\n",
      "Epoch [1/1], Step [493/7635], Loss: 5.0271\n",
      "Epoch [1/1], Step [494/7635], Loss: 4.9727\n",
      "Epoch [1/1], Step [495/7635], Loss: 4.9613\n",
      "Epoch [1/1], Step [496/7635], Loss: 4.9579\n",
      "Epoch [1/1], Step [497/7635], Loss: 4.9547\n",
      "Epoch [1/1], Step [498/7635], Loss: 5.0756\n",
      "Epoch [1/1], Step [499/7635], Loss: 4.9925\n",
      "Epoch [1/1], Step [500/7635], Loss: 5.0384\n",
      "Epoch [1/1], Step [501/7635], Loss: 4.8935\n",
      "Epoch [1/1], Step [502/7635], Loss: 4.9923\n",
      "Epoch [1/1], Step [503/7635], Loss: 4.9465\n",
      "Epoch [1/1], Step [504/7635], Loss: 5.0646\n",
      "Epoch [1/1], Step [505/7635], Loss: 5.0695\n",
      "Epoch [1/1], Step [506/7635], Loss: 5.0846\n",
      "Epoch [1/1], Step [507/7635], Loss: 5.0074\n",
      "Epoch [1/1], Step [508/7635], Loss: 4.9369\n",
      "Epoch [1/1], Step [509/7635], Loss: 4.9131\n",
      "Epoch [1/1], Step [510/7635], Loss: 4.9152\n",
      "Epoch [1/1], Step [511/7635], Loss: 4.9232\n",
      "Epoch [1/1], Step [512/7635], Loss: 5.0685\n",
      "Epoch [1/1], Step [513/7635], Loss: 5.0238\n",
      "Epoch [1/1], Step [514/7635], Loss: 4.9326\n",
      "Epoch [1/1], Step [515/7635], Loss: 4.9616\n",
      "Epoch [1/1], Step [516/7635], Loss: 4.9315\n",
      "Epoch [1/1], Step [517/7635], Loss: 4.9816\n",
      "Epoch [1/1], Step [518/7635], Loss: 4.8643\n",
      "Epoch [1/1], Step [519/7635], Loss: 4.8482\n",
      "Epoch [1/1], Step [520/7635], Loss: 4.9692\n",
      "Epoch [1/1], Step [521/7635], Loss: 4.9809\n",
      "Epoch [1/1], Step [522/7635], Loss: 4.9727\n",
      "Epoch [1/1], Step [523/7635], Loss: 5.0230\n",
      "Epoch [1/1], Step [524/7635], Loss: 4.9049\n",
      "Epoch [1/1], Step [525/7635], Loss: 5.0381\n",
      "Epoch [1/1], Step [526/7635], Loss: 4.9126\n",
      "Epoch [1/1], Step [527/7635], Loss: 4.9555\n",
      "Epoch [1/1], Step [528/7635], Loss: 5.0073\n",
      "Epoch [1/1], Step [529/7635], Loss: 4.9422\n",
      "Epoch [1/1], Step [530/7635], Loss: 4.8610\n",
      "Epoch [1/1], Step [531/7635], Loss: 4.9788\n",
      "Epoch [1/1], Step [532/7635], Loss: 4.8548\n",
      "Epoch [1/1], Step [533/7635], Loss: 4.8067\n",
      "Epoch [1/1], Step [534/7635], Loss: 4.9582\n",
      "Epoch [1/1], Step [535/7635], Loss: 4.9148\n",
      "Epoch [1/1], Step [536/7635], Loss: 4.8335\n",
      "Epoch [1/1], Step [537/7635], Loss: 4.8552\n",
      "Epoch [1/1], Step [538/7635], Loss: 5.0461\n",
      "Epoch [1/1], Step [539/7635], Loss: 4.8385\n",
      "Epoch [1/1], Step [540/7635], Loss: 4.9721\n",
      "Epoch [1/1], Step [541/7635], Loss: 4.8837\n",
      "Epoch [1/1], Step [542/7635], Loss: 4.9615\n",
      "Epoch [1/1], Step [543/7635], Loss: 4.9374\n",
      "Epoch [1/1], Step [544/7635], Loss: 5.0287\n",
      "Epoch [1/1], Step [545/7635], Loss: 4.9207\n",
      "Epoch [1/1], Step [546/7635], Loss: 4.9608\n",
      "Epoch [1/1], Step [547/7635], Loss: 4.9555\n",
      "Epoch [1/1], Step [548/7635], Loss: 4.7808\n",
      "Epoch [1/1], Step [549/7635], Loss: 4.8940\n",
      "Epoch [1/1], Step [550/7635], Loss: 4.9761\n",
      "Epoch [1/1], Step [551/7635], Loss: 4.9443\n",
      "Epoch [1/1], Step [552/7635], Loss: 4.9736\n",
      "Epoch [1/1], Step [553/7635], Loss: 4.9113\n",
      "Epoch [1/1], Step [554/7635], Loss: 4.9572\n",
      "Epoch [1/1], Step [555/7635], Loss: 4.8631\n",
      "Epoch [1/1], Step [556/7635], Loss: 5.0532\n",
      "Epoch [1/1], Step [557/7635], Loss: 4.9002\n",
      "Epoch [1/1], Step [558/7635], Loss: 4.9524\n",
      "Epoch [1/1], Step [559/7635], Loss: 4.9868\n",
      "Epoch [1/1], Step [560/7635], Loss: 4.9753\n",
      "Epoch [1/1], Step [561/7635], Loss: 4.9004\n",
      "Epoch [1/1], Step [562/7635], Loss: 4.8568\n",
      "Epoch [1/1], Step [563/7635], Loss: 4.8610\n",
      "Epoch [1/1], Step [564/7635], Loss: 4.8488\n",
      "Epoch [1/1], Step [565/7635], Loss: 4.9123\n",
      "Epoch [1/1], Step [566/7635], Loss: 4.9030\n",
      "Epoch [1/1], Step [567/7635], Loss: 4.8293\n",
      "Epoch [1/1], Step [568/7635], Loss: 4.8314\n",
      "Epoch [1/1], Step [569/7635], Loss: 4.7956\n",
      "Epoch [1/1], Step [570/7635], Loss: 4.8756\n",
      "Epoch [1/1], Step [571/7635], Loss: 4.9347\n",
      "Epoch [1/1], Step [572/7635], Loss: 4.8539\n",
      "Epoch [1/1], Step [573/7635], Loss: 4.7024\n",
      "Epoch [1/1], Step [574/7635], Loss: 4.8091\n",
      "Epoch [1/1], Step [575/7635], Loss: 4.9113\n",
      "Epoch [1/1], Step [576/7635], Loss: 4.8198\n",
      "Epoch [1/1], Step [577/7635], Loss: 4.8957\n",
      "Epoch [1/1], Step [578/7635], Loss: 4.8233\n",
      "Epoch [1/1], Step [579/7635], Loss: 4.8164\n",
      "Epoch [1/1], Step [580/7635], Loss: 4.8417\n",
      "Epoch [1/1], Step [581/7635], Loss: 4.8876\n",
      "Epoch [1/1], Step [582/7635], Loss: 4.9628\n",
      "Epoch [1/1], Step [583/7635], Loss: 4.9165\n",
      "Epoch [1/1], Step [584/7635], Loss: 4.8223\n",
      "Epoch [1/1], Step [585/7635], Loss: 4.7843\n",
      "Epoch [1/1], Step [586/7635], Loss: 4.9065\n",
      "Epoch [1/1], Step [587/7635], Loss: 4.8439\n",
      "Epoch [1/1], Step [588/7635], Loss: 4.9548\n",
      "Epoch [1/1], Step [589/7635], Loss: 4.8140\n",
      "Epoch [1/1], Step [590/7635], Loss: 4.7662\n",
      "Epoch [1/1], Step [591/7635], Loss: 4.8891\n",
      "Epoch [1/1], Step [592/7635], Loss: 4.8734\n",
      "Epoch [1/1], Step [593/7635], Loss: 4.8293\n",
      "Epoch [1/1], Step [594/7635], Loss: 4.9334\n",
      "Epoch [1/1], Step [595/7635], Loss: 4.9128\n",
      "Epoch [1/1], Step [596/7635], Loss: 4.8457\n",
      "Epoch [1/1], Step [597/7635], Loss: 4.8999\n",
      "Epoch [1/1], Step [598/7635], Loss: 4.9041\n",
      "Epoch [1/1], Step [599/7635], Loss: 4.8318\n",
      "Epoch [1/1], Step [600/7635], Loss: 4.8515\n",
      "Epoch [1/1], Step [601/7635], Loss: 4.9050\n",
      "Epoch [1/1], Step [602/7635], Loss: 4.8352\n",
      "Epoch [1/1], Step [603/7635], Loss: 4.8018\n",
      "Epoch [1/1], Step [604/7635], Loss: 4.9551\n",
      "Epoch [1/1], Step [605/7635], Loss: 4.9394\n",
      "Epoch [1/1], Step [606/7635], Loss: 4.9600\n",
      "Epoch [1/1], Step [607/7635], Loss: 4.8231\n",
      "Epoch [1/1], Step [608/7635], Loss: 4.8355\n",
      "Epoch [1/1], Step [609/7635], Loss: 4.8065\n",
      "Epoch [1/1], Step [610/7635], Loss: 4.8460\n",
      "Epoch [1/1], Step [611/7635], Loss: 4.7588\n",
      "Epoch [1/1], Step [612/7635], Loss: 4.9601\n",
      "Epoch [1/1], Step [613/7635], Loss: 4.8339\n",
      "Epoch [1/1], Step [614/7635], Loss: 4.8408\n",
      "Epoch [1/1], Step [615/7635], Loss: 4.8732\n",
      "Epoch [1/1], Step [616/7635], Loss: 4.7465\n",
      "Epoch [1/1], Step [617/7635], Loss: 4.7601\n",
      "Epoch [1/1], Step [618/7635], Loss: 4.8154\n",
      "Epoch [1/1], Step [619/7635], Loss: 4.8182\n",
      "Epoch [1/1], Step [620/7635], Loss: 4.9467\n",
      "Epoch [1/1], Step [621/7635], Loss: 4.8569\n",
      "Epoch [1/1], Step [622/7635], Loss: 4.8245\n",
      "Epoch [1/1], Step [623/7635], Loss: 4.9611\n",
      "Epoch [1/1], Step [624/7635], Loss: 4.8795\n",
      "Epoch [1/1], Step [625/7635], Loss: 4.7483\n",
      "Epoch [1/1], Step [626/7635], Loss: 4.7658\n",
      "Epoch [1/1], Step [627/7635], Loss: 4.7888\n",
      "Epoch [1/1], Step [628/7635], Loss: 4.7738\n",
      "Epoch [1/1], Step [629/7635], Loss: 4.8623\n",
      "Epoch [1/1], Step [630/7635], Loss: 4.7781\n",
      "Epoch [1/1], Step [631/7635], Loss: 4.8559\n",
      "Epoch [1/1], Step [632/7635], Loss: 4.7862\n",
      "Epoch [1/1], Step [633/7635], Loss: 4.8890\n",
      "Epoch [1/1], Step [634/7635], Loss: 4.8097\n",
      "Epoch [1/1], Step [635/7635], Loss: 4.8159\n",
      "Epoch [1/1], Step [636/7635], Loss: 4.7281\n",
      "Epoch [1/1], Step [637/7635], Loss: 4.7847\n",
      "Epoch [1/1], Step [638/7635], Loss: 4.8690\n",
      "Epoch [1/1], Step [639/7635], Loss: 4.7906\n",
      "Epoch [1/1], Step [640/7635], Loss: 4.8396\n",
      "Epoch [1/1], Step [641/7635], Loss: 4.8026\n",
      "Epoch [1/1], Step [642/7635], Loss: 4.7769\n",
      "Epoch [1/1], Step [643/7635], Loss: 4.8155\n",
      "Epoch [1/1], Step [644/7635], Loss: 4.7266\n",
      "Epoch [1/1], Step [645/7635], Loss: 4.7242\n",
      "Epoch [1/1], Step [646/7635], Loss: 4.7334\n",
      "Epoch [1/1], Step [647/7635], Loss: 4.7795\n",
      "Epoch [1/1], Step [648/7635], Loss: 4.7514\n",
      "Epoch [1/1], Step [649/7635], Loss: 4.7555\n",
      "Epoch [1/1], Step [650/7635], Loss: 4.8157\n",
      "Epoch [1/1], Step [651/7635], Loss: 4.7235\n",
      "Epoch [1/1], Step [652/7635], Loss: 4.7377\n",
      "Epoch [1/1], Step [653/7635], Loss: 4.7629\n",
      "Epoch [1/1], Step [654/7635], Loss: 4.6777\n",
      "Epoch [1/1], Step [655/7635], Loss: 4.8252\n",
      "Epoch [1/1], Step [656/7635], Loss: 4.7287\n",
      "Epoch [1/1], Step [657/7635], Loss: 4.8040\n",
      "Epoch [1/1], Step [658/7635], Loss: 4.7328\n",
      "Epoch [1/1], Step [659/7635], Loss: 4.7283\n",
      "Epoch [1/1], Step [660/7635], Loss: 4.8129\n",
      "Epoch [1/1], Step [661/7635], Loss: 4.9106\n",
      "Epoch [1/1], Step [662/7635], Loss: 4.7551\n",
      "Epoch [1/1], Step [663/7635], Loss: 4.8068\n",
      "Epoch [1/1], Step [664/7635], Loss: 4.7867\n",
      "Epoch [1/1], Step [665/7635], Loss: 4.7310\n",
      "Epoch [1/1], Step [666/7635], Loss: 4.7606\n",
      "Epoch [1/1], Step [667/7635], Loss: 4.7919\n",
      "Epoch [1/1], Step [668/7635], Loss: 4.6949\n",
      "Epoch [1/1], Step [669/7635], Loss: 4.8044\n",
      "Epoch [1/1], Step [670/7635], Loss: 4.7515\n",
      "Epoch [1/1], Step [671/7635], Loss: 4.7148\n",
      "Epoch [1/1], Step [672/7635], Loss: 4.8324\n",
      "Epoch [1/1], Step [673/7635], Loss: 4.7738\n",
      "Epoch [1/1], Step [674/7635], Loss: 4.8151\n",
      "Epoch [1/1], Step [675/7635], Loss: 4.7057\n",
      "Epoch [1/1], Step [676/7635], Loss: 4.7579\n",
      "Epoch [1/1], Step [677/7635], Loss: 4.7431\n",
      "Epoch [1/1], Step [678/7635], Loss: 4.7296\n",
      "Epoch [1/1], Step [679/7635], Loss: 4.8110\n",
      "Epoch [1/1], Step [680/7635], Loss: 4.7838\n",
      "Epoch [1/1], Step [681/7635], Loss: 4.6964\n",
      "Epoch [1/1], Step [682/7635], Loss: 4.8255\n",
      "Epoch [1/1], Step [683/7635], Loss: 4.7727\n",
      "Epoch [1/1], Step [684/7635], Loss: 4.6618\n",
      "Epoch [1/1], Step [685/7635], Loss: 4.7735\n",
      "Epoch [1/1], Step [686/7635], Loss: 4.6505\n",
      "Epoch [1/1], Step [687/7635], Loss: 4.7716\n",
      "Epoch [1/1], Step [688/7635], Loss: 4.7975\n",
      "Epoch [1/1], Step [689/7635], Loss: 4.7069\n",
      "Epoch [1/1], Step [690/7635], Loss: 4.7198\n",
      "Epoch [1/1], Step [691/7635], Loss: 4.7533\n",
      "Epoch [1/1], Step [692/7635], Loss: 4.6528\n",
      "Epoch [1/1], Step [693/7635], Loss: 4.8837\n",
      "Epoch [1/1], Step [694/7635], Loss: 4.8736\n",
      "Epoch [1/1], Step [695/7635], Loss: 4.7787\n",
      "Epoch [1/1], Step [696/7635], Loss: 4.8041\n",
      "Epoch [1/1], Step [697/7635], Loss: 4.7101\n",
      "Epoch [1/1], Step [698/7635], Loss: 4.6431\n",
      "Epoch [1/1], Step [699/7635], Loss: 4.7108\n",
      "Epoch [1/1], Step [700/7635], Loss: 4.6857\n",
      "Epoch [1/1], Step [701/7635], Loss: 4.8136\n",
      "Epoch [1/1], Step [702/7635], Loss: 4.7974\n",
      "Epoch [1/1], Step [703/7635], Loss: 4.7362\n",
      "Epoch [1/1], Step [704/7635], Loss: 4.8091\n",
      "Epoch [1/1], Step [705/7635], Loss: 4.8276\n",
      "Epoch [1/1], Step [706/7635], Loss: 4.7391\n",
      "Epoch [1/1], Step [707/7635], Loss: 4.6759\n",
      "Epoch [1/1], Step [708/7635], Loss: 4.6991\n",
      "Epoch [1/1], Step [709/7635], Loss: 4.7862\n",
      "Epoch [1/1], Step [710/7635], Loss: 4.7140\n",
      "Epoch [1/1], Step [711/7635], Loss: 4.6027\n",
      "Epoch [1/1], Step [712/7635], Loss: 4.6790\n",
      "Epoch [1/1], Step [713/7635], Loss: 4.7868\n",
      "Epoch [1/1], Step [714/7635], Loss: 4.6883\n",
      "Epoch [1/1], Step [715/7635], Loss: 4.7095\n",
      "Epoch [1/1], Step [716/7635], Loss: 4.7665\n",
      "Epoch [1/1], Step [717/7635], Loss: 4.7579\n",
      "Epoch [1/1], Step [718/7635], Loss: 4.6598\n",
      "Epoch [1/1], Step [719/7635], Loss: 4.7442\n",
      "Epoch [1/1], Step [720/7635], Loss: 4.7486\n",
      "Epoch [1/1], Step [721/7635], Loss: 4.6078\n",
      "Epoch [1/1], Step [722/7635], Loss: 4.7386\n",
      "Epoch [1/1], Step [723/7635], Loss: 4.6998\n",
      "Epoch [1/1], Step [724/7635], Loss: 4.6290\n",
      "Epoch [1/1], Step [725/7635], Loss: 4.7055\n",
      "Epoch [1/1], Step [726/7635], Loss: 4.7078\n",
      "Epoch [1/1], Step [727/7635], Loss: 4.7309\n",
      "Epoch [1/1], Step [728/7635], Loss: 4.7064\n",
      "Epoch [1/1], Step [729/7635], Loss: 4.7511\n",
      "Epoch [1/1], Step [730/7635], Loss: 4.6803\n",
      "Epoch [1/1], Step [731/7635], Loss: 4.7871\n",
      "Epoch [1/1], Step [732/7635], Loss: 4.6952\n",
      "Epoch [1/1], Step [733/7635], Loss: 4.7610\n",
      "Epoch [1/1], Step [734/7635], Loss: 4.7371\n",
      "Epoch [1/1], Step [735/7635], Loss: 4.7996\n",
      "Epoch [1/1], Step [736/7635], Loss: 4.7453\n",
      "Epoch [1/1], Step [737/7635], Loss: 4.7308\n",
      "Epoch [1/1], Step [738/7635], Loss: 4.7518\n",
      "Epoch [1/1], Step [739/7635], Loss: 4.6786\n",
      "Epoch [1/1], Step [740/7635], Loss: 4.7724\n",
      "Epoch [1/1], Step [741/7635], Loss: 4.6798\n",
      "Epoch [1/1], Step [742/7635], Loss: 4.6837\n",
      "Epoch [1/1], Step [743/7635], Loss: 4.6785\n",
      "Epoch [1/1], Step [744/7635], Loss: 4.6862\n",
      "Epoch [1/1], Step [745/7635], Loss: 4.6842\n",
      "Epoch [1/1], Step [746/7635], Loss: 4.7738\n",
      "Epoch [1/1], Step [747/7635], Loss: 4.5926\n",
      "Epoch [1/1], Step [748/7635], Loss: 4.7157\n",
      "Epoch [1/1], Step [749/7635], Loss: 4.7004\n",
      "Epoch [1/1], Step [750/7635], Loss: 4.6767\n",
      "Epoch [1/1], Step [751/7635], Loss: 4.7778\n",
      "Epoch [1/1], Step [752/7635], Loss: 4.6850\n",
      "Epoch [1/1], Step [753/7635], Loss: 4.8093\n",
      "Epoch [1/1], Step [754/7635], Loss: 4.7162\n",
      "Epoch [1/1], Step [755/7635], Loss: 4.8150\n",
      "Epoch [1/1], Step [756/7635], Loss: 4.6999\n",
      "Epoch [1/1], Step [757/7635], Loss: 4.6920\n",
      "Epoch [1/1], Step [758/7635], Loss: 4.6631\n",
      "Epoch [1/1], Step [759/7635], Loss: 4.6439\n",
      "Epoch [1/1], Step [760/7635], Loss: 4.6610\n",
      "Epoch [1/1], Step [761/7635], Loss: 4.6655\n",
      "Epoch [1/1], Step [762/7635], Loss: 4.6739\n",
      "Epoch [1/1], Step [763/7635], Loss: 4.7874\n",
      "Epoch [1/1], Step [764/7635], Loss: 4.6711\n",
      "Epoch [1/1], Step [765/7635], Loss: 4.7112\n",
      "Epoch [1/1], Step [766/7635], Loss: 4.6976\n",
      "Epoch [1/1], Step [767/7635], Loss: 4.6904\n",
      "Epoch [1/1], Step [768/7635], Loss: 4.7043\n",
      "Epoch [1/1], Step [769/7635], Loss: 4.6671\n",
      "Epoch [1/1], Step [770/7635], Loss: 4.6392\n",
      "Epoch [1/1], Step [771/7635], Loss: 4.7055\n",
      "Epoch [1/1], Step [772/7635], Loss: 4.6532\n",
      "Epoch [1/1], Step [773/7635], Loss: 4.6113\n",
      "Epoch [1/1], Step [774/7635], Loss: 4.6034\n",
      "Epoch [1/1], Step [775/7635], Loss: 4.7149\n",
      "Epoch [1/1], Step [776/7635], Loss: 4.6928\n",
      "Epoch [1/1], Step [777/7635], Loss: 4.7162\n",
      "Epoch [1/1], Step [778/7635], Loss: 4.7093\n",
      "Epoch [1/1], Step [779/7635], Loss: 4.6642\n",
      "Epoch [1/1], Step [780/7635], Loss: 4.6181\n",
      "Epoch [1/1], Step [781/7635], Loss: 4.6533\n",
      "Epoch [1/1], Step [782/7635], Loss: 4.5619\n",
      "Epoch [1/1], Step [783/7635], Loss: 4.6873\n",
      "Epoch [1/1], Step [784/7635], Loss: 4.6870\n",
      "Epoch [1/1], Step [785/7635], Loss: 4.5660\n",
      "Epoch [1/1], Step [786/7635], Loss: 4.6684\n",
      "Epoch [1/1], Step [787/7635], Loss: 4.7603\n",
      "Epoch [1/1], Step [788/7635], Loss: 4.5891\n",
      "Epoch [1/1], Step [789/7635], Loss: 4.6051\n",
      "Epoch [1/1], Step [790/7635], Loss: 4.7303\n",
      "Epoch [1/1], Step [791/7635], Loss: 4.6753\n",
      "Epoch [1/1], Step [792/7635], Loss: 4.7164\n",
      "Epoch [1/1], Step [793/7635], Loss: 4.8175\n",
      "Epoch [1/1], Step [794/7635], Loss: 4.6998\n",
      "Epoch [1/1], Step [795/7635], Loss: 4.6164\n",
      "Epoch [1/1], Step [796/7635], Loss: 4.7280\n",
      "Epoch [1/1], Step [797/7635], Loss: 4.6576\n",
      "Epoch [1/1], Step [798/7635], Loss: 4.6995\n",
      "Epoch [1/1], Step [799/7635], Loss: 4.5601\n",
      "Epoch [1/1], Step [800/7635], Loss: 4.6195\n",
      "Epoch [1/1], Step [801/7635], Loss: 4.7436\n",
      "Epoch [1/1], Step [802/7635], Loss: 4.7871\n",
      "Epoch [1/1], Step [803/7635], Loss: 4.6216\n",
      "Epoch [1/1], Step [804/7635], Loss: 4.6440\n",
      "Epoch [1/1], Step [805/7635], Loss: 4.7119\n",
      "Epoch [1/1], Step [806/7635], Loss: 4.6674\n",
      "Epoch [1/1], Step [807/7635], Loss: 4.6608\n",
      "Epoch [1/1], Step [808/7635], Loss: 4.7214\n",
      "Epoch [1/1], Step [809/7635], Loss: 4.5779\n",
      "Epoch [1/1], Step [810/7635], Loss: 4.6649\n",
      "Epoch [1/1], Step [811/7635], Loss: 4.6135\n",
      "Epoch [1/1], Step [812/7635], Loss: 4.6550\n",
      "Epoch [1/1], Step [813/7635], Loss: 4.6894\n",
      "Epoch [1/1], Step [814/7635], Loss: 4.6353\n",
      "Epoch [1/1], Step [815/7635], Loss: 4.6529\n",
      "Epoch [1/1], Step [816/7635], Loss: 4.4780\n",
      "Epoch [1/1], Step [817/7635], Loss: 4.7374\n",
      "Epoch [1/1], Step [818/7635], Loss: 4.7110\n",
      "Epoch [1/1], Step [819/7635], Loss: 4.6643\n",
      "Epoch [1/1], Step [820/7635], Loss: 4.7388\n",
      "Epoch [1/1], Step [821/7635], Loss: 4.7065\n",
      "Epoch [1/1], Step [822/7635], Loss: 4.6322\n",
      "Epoch [1/1], Step [823/7635], Loss: 4.8134\n",
      "Epoch [1/1], Step [824/7635], Loss: 4.7003\n",
      "Epoch [1/1], Step [825/7635], Loss: 4.7089\n",
      "Epoch [1/1], Step [826/7635], Loss: 4.6608\n",
      "Epoch [1/1], Step [827/7635], Loss: 4.6237\n",
      "Epoch [1/1], Step [828/7635], Loss: 4.6388\n",
      "Epoch [1/1], Step [829/7635], Loss: 4.6437\n",
      "Epoch [1/1], Step [830/7635], Loss: 4.7379\n",
      "Epoch [1/1], Step [831/7635], Loss: 4.6564\n",
      "Epoch [1/1], Step [832/7635], Loss: 4.6394\n",
      "Epoch [1/1], Step [833/7635], Loss: 4.7016\n",
      "Epoch [1/1], Step [834/7635], Loss: 4.6649\n",
      "Epoch [1/1], Step [835/7635], Loss: 4.6661\n",
      "Epoch [1/1], Step [836/7635], Loss: 4.5991\n",
      "Epoch [1/1], Step [837/7635], Loss: 4.6006\n",
      "Epoch [1/1], Step [838/7635], Loss: 4.6396\n",
      "Epoch [1/1], Step [839/7635], Loss: 4.6094\n",
      "Epoch [1/1], Step [840/7635], Loss: 4.5444\n",
      "Epoch [1/1], Step [841/7635], Loss: 4.5742\n",
      "Epoch [1/1], Step [842/7635], Loss: 4.5897\n",
      "Epoch [1/1], Step [843/7635], Loss: 4.5773\n",
      "Epoch [1/1], Step [844/7635], Loss: 4.6958\n",
      "Epoch [1/1], Step [845/7635], Loss: 4.6477\n",
      "Epoch [1/1], Step [846/7635], Loss: 4.6044\n",
      "Epoch [1/1], Step [847/7635], Loss: 4.5400\n",
      "Epoch [1/1], Step [848/7635], Loss: 4.6508\n",
      "Epoch [1/1], Step [849/7635], Loss: 4.5379\n",
      "Epoch [1/1], Step [850/7635], Loss: 4.5312\n",
      "Epoch [1/1], Step [851/7635], Loss: 4.7305\n",
      "Epoch [1/1], Step [852/7635], Loss: 4.5609\n",
      "Epoch [1/1], Step [853/7635], Loss: 4.6031\n",
      "Epoch [1/1], Step [854/7635], Loss: 4.7292\n",
      "Epoch [1/1], Step [855/7635], Loss: 4.5694\n",
      "Epoch [1/1], Step [856/7635], Loss: 4.5292\n",
      "Epoch [1/1], Step [857/7635], Loss: 4.6743\n",
      "Epoch [1/1], Step [858/7635], Loss: 4.6182\n",
      "Epoch [1/1], Step [859/7635], Loss: 4.7006\n",
      "Epoch [1/1], Step [860/7635], Loss: 4.5946\n",
      "Epoch [1/1], Step [861/7635], Loss: 4.5781\n",
      "Epoch [1/1], Step [862/7635], Loss: 4.6665\n",
      "Epoch [1/1], Step [863/7635], Loss: 4.6718\n",
      "Epoch [1/1], Step [864/7635], Loss: 4.7073\n",
      "Epoch [1/1], Step [865/7635], Loss: 4.7076\n",
      "Epoch [1/1], Step [866/7635], Loss: 4.5912\n",
      "Epoch [1/1], Step [867/7635], Loss: 4.5809\n",
      "Epoch [1/1], Step [868/7635], Loss: 4.6116\n",
      "Epoch [1/1], Step [869/7635], Loss: 4.5868\n",
      "Epoch [1/1], Step [870/7635], Loss: 4.5970\n",
      "Epoch [1/1], Step [871/7635], Loss: 4.7225\n",
      "Epoch [1/1], Step [872/7635], Loss: 4.6762\n",
      "Epoch [1/1], Step [873/7635], Loss: 4.6864\n",
      "Epoch [1/1], Step [874/7635], Loss: 4.6820\n",
      "Epoch [1/1], Step [875/7635], Loss: 4.6262\n",
      "Epoch [1/1], Step [876/7635], Loss: 4.6144\n",
      "Epoch [1/1], Step [877/7635], Loss: 4.5531\n",
      "Epoch [1/1], Step [878/7635], Loss: 4.4836\n",
      "Epoch [1/1], Step [879/7635], Loss: 4.6400\n",
      "Epoch [1/1], Step [880/7635], Loss: 4.6293\n",
      "Epoch [1/1], Step [881/7635], Loss: 4.5892\n",
      "Epoch [1/1], Step [882/7635], Loss: 4.6740\n",
      "Epoch [1/1], Step [883/7635], Loss: 4.5263\n",
      "Epoch [1/1], Step [884/7635], Loss: 4.5981\n",
      "Epoch [1/1], Step [885/7635], Loss: 4.5988\n",
      "Epoch [1/1], Step [886/7635], Loss: 4.6195\n",
      "Epoch [1/1], Step [887/7635], Loss: 4.6686\n",
      "Epoch [1/1], Step [888/7635], Loss: 4.5704\n",
      "Epoch [1/1], Step [889/7635], Loss: 4.6479\n",
      "Epoch [1/1], Step [890/7635], Loss: 4.5003\n",
      "Epoch [1/1], Step [891/7635], Loss: 4.6064\n",
      "Epoch [1/1], Step [892/7635], Loss: 4.5617\n",
      "Epoch [1/1], Step [893/7635], Loss: 4.5463\n",
      "Epoch [1/1], Step [894/7635], Loss: 4.6239\n",
      "Epoch [1/1], Step [895/7635], Loss: 4.6132\n",
      "Epoch [1/1], Step [896/7635], Loss: 4.6291\n",
      "Epoch [1/1], Step [897/7635], Loss: 4.5985\n",
      "Epoch [1/1], Step [898/7635], Loss: 4.6565\n",
      "Epoch [1/1], Step [899/7635], Loss: 4.5193\n",
      "Epoch [1/1], Step [900/7635], Loss: 4.5854\n",
      "Epoch [1/1], Step [901/7635], Loss: 4.6868\n",
      "Epoch [1/1], Step [902/7635], Loss: 4.5302\n",
      "Epoch [1/1], Step [903/7635], Loss: 4.6119\n",
      "Epoch [1/1], Step [904/7635], Loss: 4.6771\n",
      "Epoch [1/1], Step [905/7635], Loss: 4.5739\n",
      "Epoch [1/1], Step [906/7635], Loss: 4.6027\n",
      "Epoch [1/1], Step [907/7635], Loss: 4.5794\n",
      "Epoch [1/1], Step [908/7635], Loss: 4.5214\n",
      "Epoch [1/1], Step [909/7635], Loss: 4.5486\n",
      "Epoch [1/1], Step [910/7635], Loss: 4.5924\n",
      "Epoch [1/1], Step [911/7635], Loss: 4.5421\n",
      "Epoch [1/1], Step [912/7635], Loss: 4.7171\n",
      "Epoch [1/1], Step [913/7635], Loss: 4.6515\n",
      "Epoch [1/1], Step [914/7635], Loss: 4.6404\n",
      "Epoch [1/1], Step [915/7635], Loss: 4.5767\n",
      "Epoch [1/1], Step [916/7635], Loss: 4.5743\n",
      "Epoch [1/1], Step [917/7635], Loss: 4.5845\n",
      "Epoch [1/1], Step [918/7635], Loss: 4.6458\n",
      "Epoch [1/1], Step [919/7635], Loss: 4.5448\n",
      "Epoch [1/1], Step [920/7635], Loss: 4.6141\n",
      "Epoch [1/1], Step [921/7635], Loss: 4.6366\n",
      "Epoch [1/1], Step [922/7635], Loss: 4.5916\n",
      "Epoch [1/1], Step [923/7635], Loss: 4.6508\n",
      "Epoch [1/1], Step [924/7635], Loss: 4.5354\n",
      "Epoch [1/1], Step [925/7635], Loss: 4.5706\n",
      "Epoch [1/1], Step [926/7635], Loss: 4.5381\n",
      "Epoch [1/1], Step [927/7635], Loss: 4.5699\n",
      "Epoch [1/1], Step [928/7635], Loss: 4.6528\n",
      "Epoch [1/1], Step [929/7635], Loss: 4.4823\n",
      "Epoch [1/1], Step [930/7635], Loss: 4.6567\n",
      "Epoch [1/1], Step [931/7635], Loss: 4.6123\n",
      "Epoch [1/1], Step [932/7635], Loss: 4.6177\n",
      "Epoch [1/1], Step [933/7635], Loss: 4.6288\n",
      "Epoch [1/1], Step [934/7635], Loss: 4.6162\n",
      "Epoch [1/1], Step [935/7635], Loss: 4.5928\n",
      "Epoch [1/1], Step [936/7635], Loss: 4.5788\n",
      "Epoch [1/1], Step [937/7635], Loss: 4.5247\n",
      "Epoch [1/1], Step [938/7635], Loss: 4.5277\n",
      "Epoch [1/1], Step [939/7635], Loss: 4.6048\n",
      "Epoch [1/1], Step [940/7635], Loss: 4.5531\n",
      "Epoch [1/1], Step [941/7635], Loss: 4.5879\n",
      "Epoch [1/1], Step [942/7635], Loss: 4.5267\n",
      "Epoch [1/1], Step [943/7635], Loss: 4.4373\n",
      "Epoch [1/1], Step [944/7635], Loss: 4.5677\n",
      "Epoch [1/1], Step [945/7635], Loss: 4.6307\n",
      "Epoch [1/1], Step [946/7635], Loss: 4.5982\n",
      "Epoch [1/1], Step [947/7635], Loss: 4.5857\n",
      "Epoch [1/1], Step [948/7635], Loss: 4.5017\n",
      "Epoch [1/1], Step [949/7635], Loss: 4.5942\n",
      "Epoch [1/1], Step [950/7635], Loss: 4.5712\n",
      "Epoch [1/1], Step [951/7635], Loss: 4.7441\n",
      "Epoch [1/1], Step [952/7635], Loss: 4.5492\n",
      "Epoch [1/1], Step [953/7635], Loss: 4.6415\n",
      "Epoch [1/1], Step [954/7635], Loss: 4.5047\n",
      "Epoch [1/1], Step [955/7635], Loss: 4.6092\n",
      "Epoch [1/1], Step [956/7635], Loss: 4.5855\n",
      "Epoch [1/1], Step [957/7635], Loss: 4.5758\n",
      "Epoch [1/1], Step [958/7635], Loss: 4.5536\n",
      "Epoch [1/1], Step [959/7635], Loss: 4.5772\n",
      "Epoch [1/1], Step [960/7635], Loss: 4.4793\n",
      "Epoch [1/1], Step [961/7635], Loss: 4.5425\n",
      "Epoch [1/1], Step [962/7635], Loss: 4.5525\n",
      "Epoch [1/1], Step [963/7635], Loss: 4.5985\n",
      "Epoch [1/1], Step [964/7635], Loss: 4.5931\n",
      "Epoch [1/1], Step [965/7635], Loss: 4.5553\n",
      "Epoch [1/1], Step [966/7635], Loss: 4.5441\n",
      "Epoch [1/1], Step [967/7635], Loss: 4.5164\n",
      "Epoch [1/1], Step [968/7635], Loss: 4.6591\n",
      "Epoch [1/1], Step [969/7635], Loss: 4.5254\n",
      "Epoch [1/1], Step [970/7635], Loss: 4.5350\n",
      "Epoch [1/1], Step [971/7635], Loss: 4.5866\n",
      "Epoch [1/1], Step [972/7635], Loss: 4.5478\n",
      "Epoch [1/1], Step [973/7635], Loss: 4.5914\n",
      "Epoch [1/1], Step [974/7635], Loss: 4.6493\n",
      "Epoch [1/1], Step [975/7635], Loss: 4.5923\n",
      "Epoch [1/1], Step [976/7635], Loss: 4.4515\n",
      "Epoch [1/1], Step [977/7635], Loss: 4.5257\n",
      "Epoch [1/1], Step [978/7635], Loss: 4.5283\n",
      "Epoch [1/1], Step [979/7635], Loss: 4.6148\n",
      "Epoch [1/1], Step [980/7635], Loss: 4.5813\n",
      "Epoch [1/1], Step [981/7635], Loss: 4.5604\n",
      "Epoch [1/1], Step [982/7635], Loss: 4.5230\n",
      "Epoch [1/1], Step [983/7635], Loss: 4.5102\n",
      "Epoch [1/1], Step [984/7635], Loss: 4.5377\n",
      "Epoch [1/1], Step [985/7635], Loss: 4.6373\n",
      "Epoch [1/1], Step [986/7635], Loss: 4.5639\n",
      "Epoch [1/1], Step [987/7635], Loss: 4.5179\n",
      "Epoch [1/1], Step [988/7635], Loss: 4.4591\n",
      "Epoch [1/1], Step [989/7635], Loss: 4.5833\n",
      "Epoch [1/1], Step [990/7635], Loss: 4.5117\n",
      "Epoch [1/1], Step [991/7635], Loss: 4.7005\n",
      "Epoch [1/1], Step [992/7635], Loss: 4.5686\n",
      "Epoch [1/1], Step [993/7635], Loss: 4.4855\n",
      "Epoch [1/1], Step [994/7635], Loss: 4.4496\n",
      "Epoch [1/1], Step [995/7635], Loss: 4.5435\n",
      "Epoch [1/1], Step [996/7635], Loss: 4.5780\n",
      "Epoch [1/1], Step [997/7635], Loss: 4.5913\n",
      "Epoch [1/1], Step [998/7635], Loss: 4.5440\n",
      "Epoch [1/1], Step [999/7635], Loss: 4.6186\n",
      "Epoch [1/1], Step [1000/7635], Loss: 4.5676\n",
      "Epoch [1/1], Step [1001/7635], Loss: 4.5555\n",
      "Epoch [1/1], Step [1002/7635], Loss: 4.5876\n",
      "Epoch [1/1], Step [1003/7635], Loss: 4.5372\n",
      "Epoch [1/1], Step [1004/7635], Loss: 4.5758\n",
      "Epoch [1/1], Step [1005/7635], Loss: 4.5557\n",
      "Epoch [1/1], Step [1006/7635], Loss: 4.5835\n",
      "Epoch [1/1], Step [1007/7635], Loss: 4.5586\n",
      "Epoch [1/1], Step [1008/7635], Loss: 4.5059\n",
      "Epoch [1/1], Step [1009/7635], Loss: 4.5009\n",
      "Epoch [1/1], Step [1010/7635], Loss: 4.4506\n",
      "Epoch [1/1], Step [1011/7635], Loss: 4.5245\n",
      "Epoch [1/1], Step [1012/7635], Loss: 4.5205\n",
      "Epoch [1/1], Step [1013/7635], Loss: 4.5105\n",
      "Epoch [1/1], Step [1014/7635], Loss: 4.4657\n",
      "Epoch [1/1], Step [1015/7635], Loss: 4.4387\n",
      "Epoch [1/1], Step [1016/7635], Loss: 4.5235\n",
      "Epoch [1/1], Step [1017/7635], Loss: 4.6429\n",
      "Epoch [1/1], Step [1018/7635], Loss: 4.5974\n",
      "Epoch [1/1], Step [1019/7635], Loss: 4.4478\n",
      "Epoch [1/1], Step [1020/7635], Loss: 4.5191\n",
      "Epoch [1/1], Step [1021/7635], Loss: 4.5463\n",
      "Epoch [1/1], Step [1022/7635], Loss: 4.5149\n",
      "Epoch [1/1], Step [1023/7635], Loss: 4.5491\n",
      "Epoch [1/1], Step [1024/7635], Loss: 4.6002\n",
      "Epoch [1/1], Step [1025/7635], Loss: 4.5962\n",
      "Epoch [1/1], Step [1026/7635], Loss: 4.5539\n",
      "Epoch [1/1], Step [1027/7635], Loss: 4.5473\n",
      "Epoch [1/1], Step [1028/7635], Loss: 4.5544\n",
      "Epoch [1/1], Step [1029/7635], Loss: 4.6105\n",
      "Epoch [1/1], Step [1030/7635], Loss: 4.5359\n",
      "Epoch [1/1], Step [1031/7635], Loss: 4.5056\n",
      "Epoch [1/1], Step [1032/7635], Loss: 4.5258\n",
      "Epoch [1/1], Step [1033/7635], Loss: 4.4763\n",
      "Epoch [1/1], Step [1034/7635], Loss: 4.5114\n",
      "Epoch [1/1], Step [1035/7635], Loss: 4.5637\n",
      "Epoch [1/1], Step [1036/7635], Loss: 4.5573\n",
      "Epoch [1/1], Step [1037/7635], Loss: 4.5750\n",
      "Epoch [1/1], Step [1038/7635], Loss: 4.5511\n",
      "Epoch [1/1], Step [1039/7635], Loss: 4.5330\n",
      "Epoch [1/1], Step [1040/7635], Loss: 4.5610\n",
      "Epoch [1/1], Step [1041/7635], Loss: 4.5180\n",
      "Epoch [1/1], Step [1042/7635], Loss: 4.4793\n",
      "Epoch [1/1], Step [1043/7635], Loss: 4.5991\n",
      "Epoch [1/1], Step [1044/7635], Loss: 4.5829\n",
      "Epoch [1/1], Step [1045/7635], Loss: 4.5683\n",
      "Epoch [1/1], Step [1046/7635], Loss: 4.4542\n",
      "Epoch [1/1], Step [1047/7635], Loss: 4.5825\n",
      "Epoch [1/1], Step [1048/7635], Loss: 4.5706\n",
      "Epoch [1/1], Step [1049/7635], Loss: 4.6172\n",
      "Epoch [1/1], Step [1050/7635], Loss: 4.3964\n",
      "Epoch [1/1], Step [1051/7635], Loss: 4.6102\n",
      "Epoch [1/1], Step [1052/7635], Loss: 4.3492\n",
      "Epoch [1/1], Step [1053/7635], Loss: 4.5992\n",
      "Epoch [1/1], Step [1054/7635], Loss: 4.4838\n",
      "Epoch [1/1], Step [1055/7635], Loss: 4.5330\n",
      "Epoch [1/1], Step [1056/7635], Loss: 4.4675\n",
      "Epoch [1/1], Step [1057/7635], Loss: 4.6125\n",
      "Epoch [1/1], Step [1058/7635], Loss: 4.5310\n",
      "Epoch [1/1], Step [1059/7635], Loss: 4.4813\n",
      "Epoch [1/1], Step [1060/7635], Loss: 4.4733\n",
      "Epoch [1/1], Step [1061/7635], Loss: 4.4627\n",
      "Epoch [1/1], Step [1062/7635], Loss: 4.5206\n",
      "Epoch [1/1], Step [1063/7635], Loss: 4.5378\n",
      "Epoch [1/1], Step [1064/7635], Loss: 4.5611\n",
      "Epoch [1/1], Step [1065/7635], Loss: 4.5421\n",
      "Epoch [1/1], Step [1066/7635], Loss: 4.4690\n",
      "Epoch [1/1], Step [1067/7635], Loss: 4.5178\n",
      "Epoch [1/1], Step [1068/7635], Loss: 4.5129\n",
      "Epoch [1/1], Step [1069/7635], Loss: 4.4358\n",
      "Epoch [1/1], Step [1070/7635], Loss: 4.5261\n",
      "Epoch [1/1], Step [1071/7635], Loss: 4.5741\n",
      "Epoch [1/1], Step [1072/7635], Loss: 4.5420\n",
      "Epoch [1/1], Step [1073/7635], Loss: 4.5338\n",
      "Epoch [1/1], Step [1074/7635], Loss: 4.4142\n",
      "Epoch [1/1], Step [1075/7635], Loss: 4.4113\n",
      "Epoch [1/1], Step [1076/7635], Loss: 4.5065\n",
      "Epoch [1/1], Step [1077/7635], Loss: 4.5986\n",
      "Epoch [1/1], Step [1078/7635], Loss: 4.4909\n",
      "Epoch [1/1], Step [1079/7635], Loss: 4.4507\n",
      "Epoch [1/1], Step [1080/7635], Loss: 4.5107\n",
      "Epoch [1/1], Step [1081/7635], Loss: 4.4486\n",
      "Epoch [1/1], Step [1082/7635], Loss: 4.4701\n",
      "Epoch [1/1], Step [1083/7635], Loss: 4.5260\n",
      "Epoch [1/1], Step [1084/7635], Loss: 4.5565\n",
      "Epoch [1/1], Step [1085/7635], Loss: 4.4503\n",
      "Epoch [1/1], Step [1086/7635], Loss: 4.4253\n",
      "Epoch [1/1], Step [1087/7635], Loss: 4.4867\n",
      "Epoch [1/1], Step [1088/7635], Loss: 4.4458\n",
      "Epoch [1/1], Step [1089/7635], Loss: 4.3885\n",
      "Epoch [1/1], Step [1090/7635], Loss: 4.4472\n",
      "Epoch [1/1], Step [1091/7635], Loss: 4.4375\n",
      "Epoch [1/1], Step [1092/7635], Loss: 4.5161\n",
      "Epoch [1/1], Step [1093/7635], Loss: 4.4631\n",
      "Epoch [1/1], Step [1094/7635], Loss: 4.5087\n",
      "Epoch [1/1], Step [1095/7635], Loss: 4.4875\n",
      "Epoch [1/1], Step [1096/7635], Loss: 4.4565\n",
      "Epoch [1/1], Step [1097/7635], Loss: 4.4548\n",
      "Epoch [1/1], Step [1098/7635], Loss: 4.4961\n",
      "Epoch [1/1], Step [1099/7635], Loss: 4.5333\n",
      "Epoch [1/1], Step [1100/7635], Loss: 4.5368\n",
      "Epoch [1/1], Step [1101/7635], Loss: 4.5309\n",
      "Epoch [1/1], Step [1102/7635], Loss: 4.4747\n",
      "Epoch [1/1], Step [1103/7635], Loss: 4.4263\n",
      "Epoch [1/1], Step [1104/7635], Loss: 4.4739\n",
      "Epoch [1/1], Step [1105/7635], Loss: 4.5672\n",
      "Epoch [1/1], Step [1106/7635], Loss: 4.6650\n",
      "Epoch [1/1], Step [1107/7635], Loss: 4.5207\n",
      "Epoch [1/1], Step [1108/7635], Loss: 4.5115\n",
      "Epoch [1/1], Step [1109/7635], Loss: 4.5684\n",
      "Epoch [1/1], Step [1110/7635], Loss: 4.5069\n",
      "Epoch [1/1], Step [1111/7635], Loss: 4.4769\n",
      "Epoch [1/1], Step [1112/7635], Loss: 4.5233\n",
      "Epoch [1/1], Step [1113/7635], Loss: 4.5714\n",
      "Epoch [1/1], Step [1114/7635], Loss: 4.5267\n",
      "Epoch [1/1], Step [1115/7635], Loss: 4.4869\n",
      "Epoch [1/1], Step [1116/7635], Loss: 4.4838\n",
      "Epoch [1/1], Step [1117/7635], Loss: 4.4715\n",
      "Epoch [1/1], Step [1118/7635], Loss: 4.5562\n",
      "Epoch [1/1], Step [1119/7635], Loss: 4.5260\n",
      "Epoch [1/1], Step [1120/7635], Loss: 4.4910\n",
      "Epoch [1/1], Step [1121/7635], Loss: 4.5837\n",
      "Epoch [1/1], Step [1122/7635], Loss: 4.4390\n",
      "Epoch [1/1], Step [1123/7635], Loss: 4.5614\n",
      "Epoch [1/1], Step [1124/7635], Loss: 4.4127\n",
      "Epoch [1/1], Step [1125/7635], Loss: 4.5009\n",
      "Epoch [1/1], Step [1126/7635], Loss: 4.4858\n",
      "Epoch [1/1], Step [1127/7635], Loss: 4.4726\n",
      "Epoch [1/1], Step [1128/7635], Loss: 4.5270\n",
      "Epoch [1/1], Step [1129/7635], Loss: 4.4120\n",
      "Epoch [1/1], Step [1130/7635], Loss: 4.4648\n",
      "Epoch [1/1], Step [1131/7635], Loss: 4.5043\n",
      "Epoch [1/1], Step [1132/7635], Loss: 4.4119\n",
      "Epoch [1/1], Step [1133/7635], Loss: 4.4264\n",
      "Epoch [1/1], Step [1134/7635], Loss: 4.5475\n",
      "Epoch [1/1], Step [1135/7635], Loss: 4.4601\n",
      "Epoch [1/1], Step [1136/7635], Loss: 4.5466\n",
      "Epoch [1/1], Step [1137/7635], Loss: 4.5100\n",
      "Epoch [1/1], Step [1138/7635], Loss: 4.4078\n",
      "Epoch [1/1], Step [1139/7635], Loss: 4.4831\n",
      "Epoch [1/1], Step [1140/7635], Loss: 4.4907\n",
      "Epoch [1/1], Step [1141/7635], Loss: 4.4816\n",
      "Epoch [1/1], Step [1142/7635], Loss: 4.4871\n",
      "Epoch [1/1], Step [1143/7635], Loss: 4.4103\n",
      "Epoch [1/1], Step [1144/7635], Loss: 4.6105\n",
      "Epoch [1/1], Step [1145/7635], Loss: 4.4526\n",
      "Epoch [1/1], Step [1146/7635], Loss: 4.5713\n",
      "Epoch [1/1], Step [1147/7635], Loss: 4.4664\n",
      "Epoch [1/1], Step [1148/7635], Loss: 4.5609\n",
      "Epoch [1/1], Step [1149/7635], Loss: 4.4729\n",
      "Epoch [1/1], Step [1150/7635], Loss: 4.5780\n",
      "Epoch [1/1], Step [1151/7635], Loss: 4.4868\n",
      "Epoch [1/1], Step [1152/7635], Loss: 4.5766\n",
      "Epoch [1/1], Step [1153/7635], Loss: 4.4494\n",
      "Epoch [1/1], Step [1154/7635], Loss: 4.4452\n",
      "Epoch [1/1], Step [1155/7635], Loss: 4.4570\n",
      "Epoch [1/1], Step [1156/7635], Loss: 4.4789\n",
      "Epoch [1/1], Step [1157/7635], Loss: 4.4879\n",
      "Epoch [1/1], Step [1158/7635], Loss: 4.5239\n",
      "Epoch [1/1], Step [1159/7635], Loss: 4.6167\n",
      "Epoch [1/1], Step [1160/7635], Loss: 4.4132\n",
      "Epoch [1/1], Step [1161/7635], Loss: 4.5574\n",
      "Epoch [1/1], Step [1162/7635], Loss: 4.4775\n",
      "Epoch [1/1], Step [1163/7635], Loss: 4.4963\n",
      "Epoch [1/1], Step [1164/7635], Loss: 4.3796\n",
      "Epoch [1/1], Step [1165/7635], Loss: 4.5214\n",
      "Epoch [1/1], Step [1166/7635], Loss: 4.4591\n",
      "Epoch [1/1], Step [1167/7635], Loss: 4.5482\n",
      "Epoch [1/1], Step [1168/7635], Loss: 4.4604\n",
      "Epoch [1/1], Step [1169/7635], Loss: 4.5423\n",
      "Epoch [1/1], Step [1170/7635], Loss: 4.4218\n",
      "Epoch [1/1], Step [1171/7635], Loss: 4.3859\n",
      "Epoch [1/1], Step [1172/7635], Loss: 4.4858\n",
      "Epoch [1/1], Step [1173/7635], Loss: 4.4782\n",
      "Epoch [1/1], Step [1174/7635], Loss: 4.4248\n",
      "Epoch [1/1], Step [1175/7635], Loss: 4.5122\n",
      "Epoch [1/1], Step [1176/7635], Loss: 4.4686\n",
      "Epoch [1/1], Step [1177/7635], Loss: 4.5064\n",
      "Epoch [1/1], Step [1178/7635], Loss: 4.4269\n",
      "Epoch [1/1], Step [1179/7635], Loss: 4.5121\n",
      "Epoch [1/1], Step [1180/7635], Loss: 4.4721\n",
      "Epoch [1/1], Step [1181/7635], Loss: 4.5466\n",
      "Epoch [1/1], Step [1182/7635], Loss: 4.4484\n",
      "Epoch [1/1], Step [1183/7635], Loss: 4.4971\n",
      "Epoch [1/1], Step [1184/7635], Loss: 4.4810\n",
      "Epoch [1/1], Step [1185/7635], Loss: 4.4811\n",
      "Epoch [1/1], Step [1186/7635], Loss: 4.4498\n",
      "Epoch [1/1], Step [1187/7635], Loss: 4.5375\n",
      "Epoch [1/1], Step [1188/7635], Loss: 4.5354\n",
      "Epoch [1/1], Step [1189/7635], Loss: 4.5040\n",
      "Epoch [1/1], Step [1190/7635], Loss: 4.5954\n",
      "Epoch [1/1], Step [1191/7635], Loss: 4.4932\n",
      "Epoch [1/1], Step [1192/7635], Loss: 4.3961\n",
      "Epoch [1/1], Step [1193/7635], Loss: 4.4661\n",
      "Epoch [1/1], Step [1194/7635], Loss: 4.4283\n",
      "Epoch [1/1], Step [1195/7635], Loss: 4.4991\n",
      "Epoch [1/1], Step [1196/7635], Loss: 4.4944\n",
      "Epoch [1/1], Step [1197/7635], Loss: 4.5548\n",
      "Epoch [1/1], Step [1198/7635], Loss: 4.3744\n",
      "Epoch [1/1], Step [1199/7635], Loss: 4.5858\n",
      "Epoch [1/1], Step [1200/7635], Loss: 4.4871\n",
      "Epoch [1/1], Step [1201/7635], Loss: 4.5020\n",
      "Epoch [1/1], Step [1202/7635], Loss: 4.4954\n",
      "Epoch [1/1], Step [1203/7635], Loss: 4.3951\n",
      "Epoch [1/1], Step [1204/7635], Loss: 4.3993\n",
      "Epoch [1/1], Step [1205/7635], Loss: 4.4447\n",
      "Epoch [1/1], Step [1206/7635], Loss: 4.5402\n",
      "Epoch [1/1], Step [1207/7635], Loss: 4.5242\n",
      "Epoch [1/1], Step [1208/7635], Loss: 4.3863\n",
      "Epoch [1/1], Step [1209/7635], Loss: 4.4631\n",
      "Epoch [1/1], Step [1210/7635], Loss: 4.5144\n",
      "Epoch [1/1], Step [1211/7635], Loss: 4.5262\n",
      "Epoch [1/1], Step [1212/7635], Loss: 4.4001\n",
      "Epoch [1/1], Step [1213/7635], Loss: 4.4557\n",
      "Epoch [1/1], Step [1214/7635], Loss: 4.4847\n",
      "Epoch [1/1], Step [1215/7635], Loss: 4.5307\n",
      "Epoch [1/1], Step [1216/7635], Loss: 4.5205\n",
      "Epoch [1/1], Step [1217/7635], Loss: 4.4865\n",
      "Epoch [1/1], Step [1218/7635], Loss: 4.4450\n",
      "Epoch [1/1], Step [1219/7635], Loss: 4.4559\n",
      "Epoch [1/1], Step [1220/7635], Loss: 4.4708\n",
      "Epoch [1/1], Step [1221/7635], Loss: 4.4471\n",
      "Epoch [1/1], Step [1222/7635], Loss: 4.4105\n",
      "Epoch [1/1], Step [1223/7635], Loss: 4.4431\n",
      "Epoch [1/1], Step [1224/7635], Loss: 4.4615\n",
      "Epoch [1/1], Step [1225/7635], Loss: 4.3888\n",
      "Epoch [1/1], Step [1226/7635], Loss: 4.4492\n",
      "Epoch [1/1], Step [1227/7635], Loss: 4.4459\n",
      "Epoch [1/1], Step [1228/7635], Loss: 4.3970\n",
      "Epoch [1/1], Step [1229/7635], Loss: 4.4252\n",
      "Epoch [1/1], Step [1230/7635], Loss: 4.5064\n",
      "Epoch [1/1], Step [1231/7635], Loss: 4.3914\n",
      "Epoch [1/1], Step [1232/7635], Loss: 4.4008\n",
      "Epoch [1/1], Step [1233/7635], Loss: 4.4939\n",
      "Epoch [1/1], Step [1234/7635], Loss: 4.4826\n",
      "Epoch [1/1], Step [1235/7635], Loss: 4.4506\n",
      "Epoch [1/1], Step [1236/7635], Loss: 4.4730\n",
      "Epoch [1/1], Step [1237/7635], Loss: 4.4921\n",
      "Epoch [1/1], Step [1238/7635], Loss: 4.5006\n",
      "Epoch [1/1], Step [1239/7635], Loss: 4.4377\n",
      "Epoch [1/1], Step [1240/7635], Loss: 4.4968\n",
      "Epoch [1/1], Step [1241/7635], Loss: 4.4116\n",
      "Epoch [1/1], Step [1242/7635], Loss: 4.4337\n",
      "Epoch [1/1], Step [1243/7635], Loss: 4.4265\n",
      "Epoch [1/1], Step [1244/7635], Loss: 4.3274\n",
      "Epoch [1/1], Step [1245/7635], Loss: 4.4692\n",
      "Epoch [1/1], Step [1246/7635], Loss: 4.4856\n",
      "Epoch [1/1], Step [1247/7635], Loss: 4.4407\n",
      "Epoch [1/1], Step [1248/7635], Loss: 4.3203\n",
      "Epoch [1/1], Step [1249/7635], Loss: 4.4433\n",
      "Epoch [1/1], Step [1250/7635], Loss: 4.4359\n",
      "Epoch [1/1], Step [1251/7635], Loss: 4.5519\n",
      "Epoch [1/1], Step [1252/7635], Loss: 4.4121\n",
      "Epoch [1/1], Step [1253/7635], Loss: 4.4349\n",
      "Epoch [1/1], Step [1254/7635], Loss: 4.3396\n",
      "Epoch [1/1], Step [1255/7635], Loss: 4.4484\n",
      "Epoch [1/1], Step [1256/7635], Loss: 4.5927\n",
      "Epoch [1/1], Step [1257/7635], Loss: 4.4764\n",
      "Epoch [1/1], Step [1258/7635], Loss: 4.5205\n",
      "Epoch [1/1], Step [1259/7635], Loss: 4.3773\n",
      "Epoch [1/1], Step [1260/7635], Loss: 4.4926\n",
      "Epoch [1/1], Step [1261/7635], Loss: 4.4336\n",
      "Epoch [1/1], Step [1262/7635], Loss: 4.4871\n",
      "Epoch [1/1], Step [1263/7635], Loss: 4.4332\n",
      "Epoch [1/1], Step [1264/7635], Loss: 4.4430\n",
      "Epoch [1/1], Step [1265/7635], Loss: 4.4561\n",
      "Epoch [1/1], Step [1266/7635], Loss: 4.4731\n",
      "Epoch [1/1], Step [1267/7635], Loss: 4.5112\n",
      "Epoch [1/1], Step [1268/7635], Loss: 4.4897\n",
      "Epoch [1/1], Step [1269/7635], Loss: 4.3332\n",
      "Epoch [1/1], Step [1270/7635], Loss: 4.3755\n",
      "Epoch [1/1], Step [1271/7635], Loss: 4.4062\n",
      "Epoch [1/1], Step [1272/7635], Loss: 4.3694\n",
      "Epoch [1/1], Step [1273/7635], Loss: 4.4767\n",
      "Epoch [1/1], Step [1274/7635], Loss: 4.4496\n",
      "Epoch [1/1], Step [1275/7635], Loss: 4.4649\n",
      "Epoch [1/1], Step [1276/7635], Loss: 4.3535\n",
      "Epoch [1/1], Step [1277/7635], Loss: 4.3529\n",
      "Epoch [1/1], Step [1278/7635], Loss: 4.4217\n",
      "Epoch [1/1], Step [1279/7635], Loss: 4.4215\n",
      "Epoch [1/1], Step [1280/7635], Loss: 4.3998\n",
      "Epoch [1/1], Step [1281/7635], Loss: 4.4119\n",
      "Epoch [1/1], Step [1282/7635], Loss: 4.4964\n",
      "Epoch [1/1], Step [1283/7635], Loss: 4.4216\n",
      "Epoch [1/1], Step [1284/7635], Loss: 4.4207\n",
      "Epoch [1/1], Step [1285/7635], Loss: 4.4411\n",
      "Epoch [1/1], Step [1286/7635], Loss: 4.4365\n",
      "Epoch [1/1], Step [1287/7635], Loss: 4.5818\n",
      "Epoch [1/1], Step [1288/7635], Loss: 4.3538\n",
      "Epoch [1/1], Step [1289/7635], Loss: 4.5048\n",
      "Epoch [1/1], Step [1290/7635], Loss: 4.4189\n",
      "Epoch [1/1], Step [1291/7635], Loss: 4.5300\n",
      "Epoch [1/1], Step [1292/7635], Loss: 4.4406\n",
      "Epoch [1/1], Step [1293/7635], Loss: 4.4560\n",
      "Epoch [1/1], Step [1294/7635], Loss: 4.3093\n",
      "Epoch [1/1], Step [1295/7635], Loss: 4.4197\n",
      "Epoch [1/1], Step [1296/7635], Loss: 4.4553\n",
      "Epoch [1/1], Step [1297/7635], Loss: 4.4288\n",
      "Epoch [1/1], Step [1298/7635], Loss: 4.4631\n",
      "Epoch [1/1], Step [1299/7635], Loss: 4.4194\n",
      "Epoch [1/1], Step [1300/7635], Loss: 4.4973\n",
      "Epoch [1/1], Step [1301/7635], Loss: 4.5008\n",
      "Epoch [1/1], Step [1302/7635], Loss: 4.4377\n",
      "Epoch [1/1], Step [1303/7635], Loss: 4.4351\n",
      "Epoch [1/1], Step [1304/7635], Loss: 4.3513\n",
      "Epoch [1/1], Step [1305/7635], Loss: 4.3857\n",
      "Epoch [1/1], Step [1306/7635], Loss: 4.4430\n",
      "Epoch [1/1], Step [1307/7635], Loss: 4.3696\n",
      "Epoch [1/1], Step [1308/7635], Loss: 4.4186\n",
      "Epoch [1/1], Step [1309/7635], Loss: 4.4495\n",
      "Epoch [1/1], Step [1310/7635], Loss: 4.2827\n",
      "Epoch [1/1], Step [1311/7635], Loss: 4.4386\n",
      "Epoch [1/1], Step [1312/7635], Loss: 4.4476\n",
      "Epoch [1/1], Step [1313/7635], Loss: 4.4520\n",
      "Epoch [1/1], Step [1314/7635], Loss: 4.4929\n",
      "Epoch [1/1], Step [1315/7635], Loss: 4.3231\n",
      "Epoch [1/1], Step [1316/7635], Loss: 4.4534\n",
      "Epoch [1/1], Step [1317/7635], Loss: 4.4210\n",
      "Epoch [1/1], Step [1318/7635], Loss: 4.4201\n",
      "Epoch [1/1], Step [1319/7635], Loss: 4.3879\n",
      "Epoch [1/1], Step [1320/7635], Loss: 4.3743\n",
      "Epoch [1/1], Step [1321/7635], Loss: 4.3458\n",
      "Epoch [1/1], Step [1322/7635], Loss: 4.3613\n",
      "Epoch [1/1], Step [1323/7635], Loss: 4.4098\n",
      "Epoch [1/1], Step [1324/7635], Loss: 4.3916\n",
      "Epoch [1/1], Step [1325/7635], Loss: 4.4063\n",
      "Epoch [1/1], Step [1326/7635], Loss: 4.5072\n",
      "Epoch [1/1], Step [1327/7635], Loss: 4.4292\n",
      "Epoch [1/1], Step [1328/7635], Loss: 4.4668\n",
      "Epoch [1/1], Step [1329/7635], Loss: 4.5991\n",
      "Epoch [1/1], Step [1330/7635], Loss: 4.4388\n",
      "Epoch [1/1], Step [1331/7635], Loss: 4.4346\n",
      "Epoch [1/1], Step [1332/7635], Loss: 4.4109\n",
      "Epoch [1/1], Step [1333/7635], Loss: 4.4548\n",
      "Epoch [1/1], Step [1334/7635], Loss: 4.4743\n",
      "Epoch [1/1], Step [1335/7635], Loss: 4.4565\n",
      "Epoch [1/1], Step [1336/7635], Loss: 4.4990\n",
      "Epoch [1/1], Step [1337/7635], Loss: 4.4650\n",
      "Epoch [1/1], Step [1338/7635], Loss: 4.4157\n",
      "Epoch [1/1], Step [1339/7635], Loss: 4.4561\n",
      "Epoch [1/1], Step [1340/7635], Loss: 4.4438\n",
      "Epoch [1/1], Step [1341/7635], Loss: 4.3983\n",
      "Epoch [1/1], Step [1342/7635], Loss: 4.3730\n",
      "Epoch [1/1], Step [1343/7635], Loss: 4.5296\n",
      "Epoch [1/1], Step [1344/7635], Loss: 4.4445\n",
      "Epoch [1/1], Step [1345/7635], Loss: 4.3222\n",
      "Epoch [1/1], Step [1346/7635], Loss: 4.4928\n",
      "Epoch [1/1], Step [1347/7635], Loss: 4.4439\n",
      "Epoch [1/1], Step [1348/7635], Loss: 4.4221\n",
      "Epoch [1/1], Step [1349/7635], Loss: 4.3963\n",
      "Epoch [1/1], Step [1350/7635], Loss: 4.4143\n",
      "Epoch [1/1], Step [1351/7635], Loss: 4.4368\n",
      "Epoch [1/1], Step [1352/7635], Loss: 4.4528\n",
      "Epoch [1/1], Step [1353/7635], Loss: 4.5100\n",
      "Epoch [1/1], Step [1354/7635], Loss: 4.4227\n",
      "Epoch [1/1], Step [1355/7635], Loss: 4.3869\n",
      "Epoch [1/1], Step [1356/7635], Loss: 4.4401\n",
      "Epoch [1/1], Step [1357/7635], Loss: 4.4727\n",
      "Epoch [1/1], Step [1358/7635], Loss: 4.4149\n",
      "Epoch [1/1], Step [1359/7635], Loss: 4.4840\n",
      "Epoch [1/1], Step [1360/7635], Loss: 4.4432\n",
      "Epoch [1/1], Step [1361/7635], Loss: 4.3724\n",
      "Epoch [1/1], Step [1362/7635], Loss: 4.4034\n",
      "Epoch [1/1], Step [1363/7635], Loss: 4.4303\n",
      "Epoch [1/1], Step [1364/7635], Loss: 4.4068\n",
      "Epoch [1/1], Step [1365/7635], Loss: 4.3182\n",
      "Epoch [1/1], Step [1366/7635], Loss: 4.3282\n",
      "Epoch [1/1], Step [1367/7635], Loss: 4.3224\n",
      "Epoch [1/1], Step [1368/7635], Loss: 4.4164\n",
      "Epoch [1/1], Step [1369/7635], Loss: 4.2732\n",
      "Epoch [1/1], Step [1370/7635], Loss: 4.4069\n",
      "Epoch [1/1], Step [1371/7635], Loss: 4.3743\n",
      "Epoch [1/1], Step [1372/7635], Loss: 4.4448\n",
      "Epoch [1/1], Step [1373/7635], Loss: 4.4066\n",
      "Epoch [1/1], Step [1374/7635], Loss: 4.3852\n",
      "Epoch [1/1], Step [1375/7635], Loss: 4.3802\n",
      "Epoch [1/1], Step [1376/7635], Loss: 4.4539\n",
      "Epoch [1/1], Step [1377/7635], Loss: 4.4792\n",
      "Epoch [1/1], Step [1378/7635], Loss: 4.4415\n",
      "Epoch [1/1], Step [1379/7635], Loss: 4.4300\n",
      "Epoch [1/1], Step [1380/7635], Loss: 4.5031\n",
      "Epoch [1/1], Step [1381/7635], Loss: 4.3586\n",
      "Epoch [1/1], Step [1382/7635], Loss: 4.3841\n",
      "Epoch [1/1], Step [1383/7635], Loss: 4.4503\n",
      "Epoch [1/1], Step [1384/7635], Loss: 4.4502\n",
      "Epoch [1/1], Step [1385/7635], Loss: 4.3790\n",
      "Epoch [1/1], Step [1386/7635], Loss: 4.4755\n",
      "Epoch [1/1], Step [1387/7635], Loss: 4.3060\n",
      "Epoch [1/1], Step [1388/7635], Loss: 4.4071\n",
      "Epoch [1/1], Step [1389/7635], Loss: 4.4686\n",
      "Epoch [1/1], Step [1390/7635], Loss: 4.3327\n",
      "Epoch [1/1], Step [1391/7635], Loss: 4.4521\n",
      "Epoch [1/1], Step [1392/7635], Loss: 4.3954\n",
      "Epoch [1/1], Step [1393/7635], Loss: 4.4742\n",
      "Epoch [1/1], Step [1394/7635], Loss: 4.3597\n",
      "Epoch [1/1], Step [1395/7635], Loss: 4.3908\n",
      "Epoch [1/1], Step [1396/7635], Loss: 4.5001\n",
      "Epoch [1/1], Step [1397/7635], Loss: 4.3869\n",
      "Epoch [1/1], Step [1398/7635], Loss: 4.4521\n",
      "Epoch [1/1], Step [1399/7635], Loss: 4.3592\n",
      "Epoch [1/1], Step [1400/7635], Loss: 4.3267\n",
      "Epoch [1/1], Step [1401/7635], Loss: 4.3184\n",
      "Epoch [1/1], Step [1402/7635], Loss: 4.4731\n",
      "Epoch [1/1], Step [1403/7635], Loss: 4.3824\n",
      "Epoch [1/1], Step [1404/7635], Loss: 4.3698\n",
      "Epoch [1/1], Step [1405/7635], Loss: 4.3907\n",
      "Epoch [1/1], Step [1406/7635], Loss: 4.4477\n",
      "Epoch [1/1], Step [1407/7635], Loss: 4.3908\n",
      "Epoch [1/1], Step [1408/7635], Loss: 4.3315\n",
      "Epoch [1/1], Step [1409/7635], Loss: 4.4635\n",
      "Epoch [1/1], Step [1410/7635], Loss: 4.3910\n",
      "Epoch [1/1], Step [1411/7635], Loss: 4.4047\n",
      "Epoch [1/1], Step [1412/7635], Loss: 4.3690\n",
      "Epoch [1/1], Step [1413/7635], Loss: 4.3720\n",
      "Epoch [1/1], Step [1414/7635], Loss: 4.4175\n",
      "Epoch [1/1], Step [1415/7635], Loss: 4.4360\n",
      "Epoch [1/1], Step [1416/7635], Loss: 4.3333\n",
      "Epoch [1/1], Step [1417/7635], Loss: 4.1932\n",
      "Epoch [1/1], Step [1418/7635], Loss: 4.3628\n",
      "Epoch [1/1], Step [1419/7635], Loss: 4.5020\n",
      "Epoch [1/1], Step [1420/7635], Loss: 4.3542\n",
      "Epoch [1/1], Step [1421/7635], Loss: 4.3620\n",
      "Epoch [1/1], Step [1422/7635], Loss: 4.4762\n",
      "Epoch [1/1], Step [1423/7635], Loss: 4.3977\n",
      "Epoch [1/1], Step [1424/7635], Loss: 4.4373\n",
      "Epoch [1/1], Step [1425/7635], Loss: 4.4124\n",
      "Epoch [1/1], Step [1426/7635], Loss: 4.5025\n",
      "Epoch [1/1], Step [1427/7635], Loss: 4.3067\n",
      "Epoch [1/1], Step [1428/7635], Loss: 4.3821\n",
      "Epoch [1/1], Step [1429/7635], Loss: 4.3470\n",
      "Epoch [1/1], Step [1430/7635], Loss: 4.4500\n",
      "Epoch [1/1], Step [1431/7635], Loss: 4.2758\n",
      "Epoch [1/1], Step [1432/7635], Loss: 4.3578\n",
      "Epoch [1/1], Step [1433/7635], Loss: 4.4273\n",
      "Epoch [1/1], Step [1434/7635], Loss: 4.4017\n",
      "Epoch [1/1], Step [1435/7635], Loss: 4.4476\n",
      "Epoch [1/1], Step [1436/7635], Loss: 4.4655\n",
      "Epoch [1/1], Step [1437/7635], Loss: 4.3846\n",
      "Epoch [1/1], Step [1438/7635], Loss: 4.3921\n",
      "Epoch [1/1], Step [1439/7635], Loss: 4.3560\n",
      "Epoch [1/1], Step [1440/7635], Loss: 4.3666\n",
      "Epoch [1/1], Step [1441/7635], Loss: 4.3569\n",
      "Epoch [1/1], Step [1442/7635], Loss: 4.3596\n",
      "Epoch [1/1], Step [1443/7635], Loss: 4.3161\n",
      "Epoch [1/1], Step [1444/7635], Loss: 4.3623\n",
      "Epoch [1/1], Step [1445/7635], Loss: 4.3521\n",
      "Epoch [1/1], Step [1446/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [1447/7635], Loss: 4.3688\n",
      "Epoch [1/1], Step [1448/7635], Loss: 4.4418\n",
      "Epoch [1/1], Step [1449/7635], Loss: 4.4231\n",
      "Epoch [1/1], Step [1450/7635], Loss: 4.4156\n",
      "Epoch [1/1], Step [1451/7635], Loss: 4.3439\n",
      "Epoch [1/1], Step [1452/7635], Loss: 4.3462\n",
      "Epoch [1/1], Step [1453/7635], Loss: 4.3714\n",
      "Epoch [1/1], Step [1454/7635], Loss: 4.4046\n",
      "Epoch [1/1], Step [1455/7635], Loss: 4.4005\n",
      "Epoch [1/1], Step [1456/7635], Loss: 4.3452\n",
      "Epoch [1/1], Step [1457/7635], Loss: 4.4556\n",
      "Epoch [1/1], Step [1458/7635], Loss: 4.4005\n",
      "Epoch [1/1], Step [1459/7635], Loss: 4.3021\n",
      "Epoch [1/1], Step [1460/7635], Loss: 4.4062\n",
      "Epoch [1/1], Step [1461/7635], Loss: 4.3895\n",
      "Epoch [1/1], Step [1462/7635], Loss: 4.3223\n",
      "Epoch [1/1], Step [1463/7635], Loss: 4.3641\n",
      "Epoch [1/1], Step [1464/7635], Loss: 4.3171\n",
      "Epoch [1/1], Step [1465/7635], Loss: 4.3330\n",
      "Epoch [1/1], Step [1466/7635], Loss: 4.3611\n",
      "Epoch [1/1], Step [1467/7635], Loss: 4.3802\n",
      "Epoch [1/1], Step [1468/7635], Loss: 4.3966\n",
      "Epoch [1/1], Step [1469/7635], Loss: 4.3392\n",
      "Epoch [1/1], Step [1470/7635], Loss: 4.4554\n",
      "Epoch [1/1], Step [1471/7635], Loss: 4.3820\n",
      "Epoch [1/1], Step [1472/7635], Loss: 4.3747\n",
      "Epoch [1/1], Step [1473/7635], Loss: 4.3255\n",
      "Epoch [1/1], Step [1474/7635], Loss: 4.4401\n",
      "Epoch [1/1], Step [1475/7635], Loss: 4.3472\n",
      "Epoch [1/1], Step [1476/7635], Loss: 4.3342\n",
      "Epoch [1/1], Step [1477/7635], Loss: 4.3737\n",
      "Epoch [1/1], Step [1478/7635], Loss: 4.4001\n",
      "Epoch [1/1], Step [1479/7635], Loss: 4.3425\n",
      "Epoch [1/1], Step [1480/7635], Loss: 4.3948\n",
      "Epoch [1/1], Step [1481/7635], Loss: 4.3570\n",
      "Epoch [1/1], Step [1482/7635], Loss: 4.3134\n",
      "Epoch [1/1], Step [1483/7635], Loss: 4.3844\n",
      "Epoch [1/1], Step [1484/7635], Loss: 4.4363\n",
      "Epoch [1/1], Step [1485/7635], Loss: 4.3541\n",
      "Epoch [1/1], Step [1486/7635], Loss: 4.3144\n",
      "Epoch [1/1], Step [1487/7635], Loss: 4.3216\n",
      "Epoch [1/1], Step [1488/7635], Loss: 4.3918\n",
      "Epoch [1/1], Step [1489/7635], Loss: 4.3343\n",
      "Epoch [1/1], Step [1490/7635], Loss: 4.3985\n",
      "Epoch [1/1], Step [1491/7635], Loss: 4.3475\n",
      "Epoch [1/1], Step [1492/7635], Loss: 4.2922\n",
      "Epoch [1/1], Step [1493/7635], Loss: 4.4287\n",
      "Epoch [1/1], Step [1494/7635], Loss: 4.3585\n",
      "Epoch [1/1], Step [1495/7635], Loss: 4.3578\n",
      "Epoch [1/1], Step [1496/7635], Loss: 4.4083\n",
      "Epoch [1/1], Step [1497/7635], Loss: 4.3166\n",
      "Epoch [1/1], Step [1498/7635], Loss: 4.3163\n",
      "Epoch [1/1], Step [1499/7635], Loss: 4.3580\n",
      "Epoch [1/1], Step [1500/7635], Loss: 4.3393\n",
      "Epoch [1/1], Step [1501/7635], Loss: 4.4051\n",
      "Epoch [1/1], Step [1502/7635], Loss: 4.4135\n",
      "Epoch [1/1], Step [1503/7635], Loss: 4.3931\n",
      "Epoch [1/1], Step [1504/7635], Loss: 4.4691\n",
      "Epoch [1/1], Step [1505/7635], Loss: 4.3770\n",
      "Epoch [1/1], Step [1506/7635], Loss: 4.3293\n",
      "Epoch [1/1], Step [1507/7635], Loss: 4.4276\n",
      "Epoch [1/1], Step [1508/7635], Loss: 4.3658\n",
      "Epoch [1/1], Step [1509/7635], Loss: 4.3656\n",
      "Epoch [1/1], Step [1510/7635], Loss: 4.4219\n",
      "Epoch [1/1], Step [1511/7635], Loss: 4.4109\n",
      "Epoch [1/1], Step [1512/7635], Loss: 4.3912\n",
      "Epoch [1/1], Step [1513/7635], Loss: 4.4129\n",
      "Epoch [1/1], Step [1514/7635], Loss: 4.3554\n",
      "Epoch [1/1], Step [1515/7635], Loss: 4.3812\n",
      "Epoch [1/1], Step [1516/7635], Loss: 4.3097\n",
      "Epoch [1/1], Step [1517/7635], Loss: 4.4245\n",
      "Epoch [1/1], Step [1518/7635], Loss: 4.2595\n",
      "Epoch [1/1], Step [1519/7635], Loss: 4.3618\n",
      "Epoch [1/1], Step [1520/7635], Loss: 4.3868\n",
      "Epoch [1/1], Step [1521/7635], Loss: 4.2906\n",
      "Epoch [1/1], Step [1522/7635], Loss: 4.4291\n",
      "Epoch [1/1], Step [1523/7635], Loss: 4.4132\n",
      "Epoch [1/1], Step [1524/7635], Loss: 4.3482\n",
      "Epoch [1/1], Step [1525/7635], Loss: 4.3123\n",
      "Epoch [1/1], Step [1526/7635], Loss: 4.4151\n",
      "Epoch [1/1], Step [1527/7635], Loss: 4.3658\n",
      "Epoch [1/1], Step [1528/7635], Loss: 4.3853\n",
      "Epoch [1/1], Step [1529/7635], Loss: 4.3777\n",
      "Epoch [1/1], Step [1530/7635], Loss: 4.3633\n",
      "Epoch [1/1], Step [1531/7635], Loss: 4.5125\n",
      "Epoch [1/1], Step [1532/7635], Loss: 4.3499\n",
      "Epoch [1/1], Step [1533/7635], Loss: 4.4600\n",
      "Epoch [1/1], Step [1534/7635], Loss: 4.2817\n",
      "Epoch [1/1], Step [1535/7635], Loss: 4.2794\n",
      "Epoch [1/1], Step [1536/7635], Loss: 4.4074\n",
      "Epoch [1/1], Step [1537/7635], Loss: 4.2852\n",
      "Epoch [1/1], Step [1538/7635], Loss: 4.5019\n",
      "Epoch [1/1], Step [1539/7635], Loss: 4.4037\n",
      "Epoch [1/1], Step [1540/7635], Loss: 4.3466\n",
      "Epoch [1/1], Step [1541/7635], Loss: 4.2856\n",
      "Epoch [1/1], Step [1542/7635], Loss: 4.2929\n",
      "Epoch [1/1], Step [1543/7635], Loss: 4.4885\n",
      "Epoch [1/1], Step [1544/7635], Loss: 4.3819\n",
      "Epoch [1/1], Step [1545/7635], Loss: 4.3401\n",
      "Epoch [1/1], Step [1546/7635], Loss: 4.3039\n",
      "Epoch [1/1], Step [1547/7635], Loss: 4.3137\n",
      "Epoch [1/1], Step [1548/7635], Loss: 4.3802\n",
      "Epoch [1/1], Step [1549/7635], Loss: 4.3933\n",
      "Epoch [1/1], Step [1550/7635], Loss: 4.4230\n",
      "Epoch [1/1], Step [1551/7635], Loss: 4.3984\n",
      "Epoch [1/1], Step [1552/7635], Loss: 4.2957\n",
      "Epoch [1/1], Step [1553/7635], Loss: 4.4399\n",
      "Epoch [1/1], Step [1554/7635], Loss: 4.2963\n",
      "Epoch [1/1], Step [1555/7635], Loss: 4.4035\n",
      "Epoch [1/1], Step [1556/7635], Loss: 4.3656\n",
      "Epoch [1/1], Step [1557/7635], Loss: 4.2733\n",
      "Epoch [1/1], Step [1558/7635], Loss: 4.3017\n",
      "Epoch [1/1], Step [1559/7635], Loss: 4.4235\n",
      "Epoch [1/1], Step [1560/7635], Loss: 4.4003\n",
      "Epoch [1/1], Step [1561/7635], Loss: 4.3246\n",
      "Epoch [1/1], Step [1562/7635], Loss: 4.3615\n",
      "Epoch [1/1], Step [1563/7635], Loss: 4.2587\n",
      "Epoch [1/1], Step [1564/7635], Loss: 4.3118\n",
      "Epoch [1/1], Step [1565/7635], Loss: 4.3038\n",
      "Epoch [1/1], Step [1566/7635], Loss: 4.3032\n",
      "Epoch [1/1], Step [1567/7635], Loss: 4.2902\n",
      "Epoch [1/1], Step [1568/7635], Loss: 4.2772\n",
      "Epoch [1/1], Step [1569/7635], Loss: 4.4116\n",
      "Epoch [1/1], Step [1570/7635], Loss: 4.3636\n",
      "Epoch [1/1], Step [1571/7635], Loss: 4.3834\n",
      "Epoch [1/1], Step [1572/7635], Loss: 4.3726\n",
      "Epoch [1/1], Step [1573/7635], Loss: 4.3445\n",
      "Epoch [1/1], Step [1574/7635], Loss: 4.2970\n",
      "Epoch [1/1], Step [1575/7635], Loss: 4.3633\n",
      "Epoch [1/1], Step [1576/7635], Loss: 4.3310\n",
      "Epoch [1/1], Step [1577/7635], Loss: 4.3940\n",
      "Epoch [1/1], Step [1578/7635], Loss: 4.3643\n",
      "Epoch [1/1], Step [1579/7635], Loss: 4.3641\n",
      "Epoch [1/1], Step [1580/7635], Loss: 4.3156\n",
      "Epoch [1/1], Step [1581/7635], Loss: 4.3685\n",
      "Epoch [1/1], Step [1582/7635], Loss: 4.2896\n",
      "Epoch [1/1], Step [1583/7635], Loss: 4.4800\n",
      "Epoch [1/1], Step [1584/7635], Loss: 4.3285\n",
      "Epoch [1/1], Step [1585/7635], Loss: 4.3799\n",
      "Epoch [1/1], Step [1586/7635], Loss: 4.3964\n",
      "Epoch [1/1], Step [1587/7635], Loss: 4.3133\n",
      "Epoch [1/1], Step [1588/7635], Loss: 4.3184\n",
      "Epoch [1/1], Step [1589/7635], Loss: 4.2640\n",
      "Epoch [1/1], Step [1590/7635], Loss: 4.3618\n",
      "Epoch [1/1], Step [1591/7635], Loss: 4.3443\n",
      "Epoch [1/1], Step [1592/7635], Loss: 4.2927\n",
      "Epoch [1/1], Step [1593/7635], Loss: 4.3408\n",
      "Epoch [1/1], Step [1594/7635], Loss: 4.3392\n",
      "Epoch [1/1], Step [1595/7635], Loss: 4.3066\n",
      "Epoch [1/1], Step [1596/7635], Loss: 4.4453\n",
      "Epoch [1/1], Step [1597/7635], Loss: 4.2897\n",
      "Epoch [1/1], Step [1598/7635], Loss: 4.2748\n",
      "Epoch [1/1], Step [1599/7635], Loss: 4.2700\n",
      "Epoch [1/1], Step [1600/7635], Loss: 4.3941\n",
      "Epoch [1/1], Step [1601/7635], Loss: 4.4244\n",
      "Epoch [1/1], Step [1602/7635], Loss: 4.3111\n",
      "Epoch [1/1], Step [1603/7635], Loss: 4.3080\n",
      "Epoch [1/1], Step [1604/7635], Loss: 4.3413\n",
      "Epoch [1/1], Step [1605/7635], Loss: 4.4441\n",
      "Epoch [1/1], Step [1606/7635], Loss: 4.4608\n",
      "Epoch [1/1], Step [1607/7635], Loss: 4.2931\n",
      "Epoch [1/1], Step [1608/7635], Loss: 4.3604\n",
      "Epoch [1/1], Step [1609/7635], Loss: 4.3499\n",
      "Epoch [1/1], Step [1610/7635], Loss: 4.3228\n",
      "Epoch [1/1], Step [1611/7635], Loss: 4.3211\n",
      "Epoch [1/1], Step [1612/7635], Loss: 4.3670\n",
      "Epoch [1/1], Step [1613/7635], Loss: 4.3125\n",
      "Epoch [1/1], Step [1614/7635], Loss: 4.3458\n",
      "Epoch [1/1], Step [1615/7635], Loss: 4.2530\n",
      "Epoch [1/1], Step [1616/7635], Loss: 4.2307\n",
      "Epoch [1/1], Step [1617/7635], Loss: 4.4686\n",
      "Epoch [1/1], Step [1618/7635], Loss: 4.3608\n",
      "Epoch [1/1], Step [1619/7635], Loss: 4.4530\n",
      "Epoch [1/1], Step [1620/7635], Loss: 4.2834\n",
      "Epoch [1/1], Step [1621/7635], Loss: 4.2497\n",
      "Epoch [1/1], Step [1622/7635], Loss: 4.3260\n",
      "Epoch [1/1], Step [1623/7635], Loss: 4.3571\n",
      "Epoch [1/1], Step [1624/7635], Loss: 4.3799\n",
      "Epoch [1/1], Step [1625/7635], Loss: 4.2832\n",
      "Epoch [1/1], Step [1626/7635], Loss: 4.3663\n",
      "Epoch [1/1], Step [1627/7635], Loss: 4.4114\n",
      "Epoch [1/1], Step [1628/7635], Loss: 4.3231\n",
      "Epoch [1/1], Step [1629/7635], Loss: 4.3195\n",
      "Epoch [1/1], Step [1630/7635], Loss: 4.3928\n",
      "Epoch [1/1], Step [1631/7635], Loss: 4.3061\n",
      "Epoch [1/1], Step [1632/7635], Loss: 4.3552\n",
      "Epoch [1/1], Step [1633/7635], Loss: 4.3699\n",
      "Epoch [1/1], Step [1634/7635], Loss: 4.2602\n",
      "Epoch [1/1], Step [1635/7635], Loss: 4.3726\n",
      "Epoch [1/1], Step [1636/7635], Loss: 4.3549\n",
      "Epoch [1/1], Step [1637/7635], Loss: 4.4408\n",
      "Epoch [1/1], Step [1638/7635], Loss: 4.3054\n",
      "Epoch [1/1], Step [1639/7635], Loss: 4.4106\n",
      "Epoch [1/1], Step [1640/7635], Loss: 4.3048\n",
      "Epoch [1/1], Step [1641/7635], Loss: 4.3413\n",
      "Epoch [1/1], Step [1642/7635], Loss: 4.2932\n",
      "Epoch [1/1], Step [1643/7635], Loss: 4.3522\n",
      "Epoch [1/1], Step [1644/7635], Loss: 4.3800\n",
      "Epoch [1/1], Step [1645/7635], Loss: 4.2837\n",
      "Epoch [1/1], Step [1646/7635], Loss: 4.3630\n",
      "Epoch [1/1], Step [1647/7635], Loss: 4.3268\n",
      "Epoch [1/1], Step [1648/7635], Loss: 4.4453\n",
      "Epoch [1/1], Step [1649/7635], Loss: 4.2910\n",
      "Epoch [1/1], Step [1650/7635], Loss: 4.3981\n",
      "Epoch [1/1], Step [1651/7635], Loss: 4.3673\n",
      "Epoch [1/1], Step [1652/7635], Loss: 4.3620\n",
      "Epoch [1/1], Step [1653/7635], Loss: 4.3774\n",
      "Epoch [1/1], Step [1654/7635], Loss: 4.1742\n",
      "Epoch [1/1], Step [1655/7635], Loss: 4.3550\n",
      "Epoch [1/1], Step [1656/7635], Loss: 4.3343\n",
      "Epoch [1/1], Step [1657/7635], Loss: 4.3604\n",
      "Epoch [1/1], Step [1658/7635], Loss: 4.3587\n",
      "Epoch [1/1], Step [1659/7635], Loss: 4.3825\n",
      "Epoch [1/1], Step [1660/7635], Loss: 4.3832\n",
      "Epoch [1/1], Step [1661/7635], Loss: 4.3293\n",
      "Epoch [1/1], Step [1662/7635], Loss: 4.3671\n",
      "Epoch [1/1], Step [1663/7635], Loss: 4.3246\n",
      "Epoch [1/1], Step [1664/7635], Loss: 4.4142\n",
      "Epoch [1/1], Step [1665/7635], Loss: 4.2451\n",
      "Epoch [1/1], Step [1666/7635], Loss: 4.3536\n",
      "Epoch [1/1], Step [1667/7635], Loss: 4.3575\n",
      "Epoch [1/1], Step [1668/7635], Loss: 4.3212\n",
      "Epoch [1/1], Step [1669/7635], Loss: 4.3753\n",
      "Epoch [1/1], Step [1670/7635], Loss: 4.3728\n",
      "Epoch [1/1], Step [1671/7635], Loss: 4.3366\n",
      "Epoch [1/1], Step [1672/7635], Loss: 4.3020\n",
      "Epoch [1/1], Step [1673/7635], Loss: 4.3561\n",
      "Epoch [1/1], Step [1674/7635], Loss: 4.3519\n",
      "Epoch [1/1], Step [1675/7635], Loss: 4.3401\n",
      "Epoch [1/1], Step [1676/7635], Loss: 4.3376\n",
      "Epoch [1/1], Step [1677/7635], Loss: 4.3834\n",
      "Epoch [1/1], Step [1678/7635], Loss: 4.3643\n",
      "Epoch [1/1], Step [1679/7635], Loss: 4.3608\n",
      "Epoch [1/1], Step [1680/7635], Loss: 4.2581\n",
      "Epoch [1/1], Step [1681/7635], Loss: 4.3400\n",
      "Epoch [1/1], Step [1682/7635], Loss: 4.3311\n",
      "Epoch [1/1], Step [1683/7635], Loss: 4.2781\n",
      "Epoch [1/1], Step [1684/7635], Loss: 4.3301\n",
      "Epoch [1/1], Step [1685/7635], Loss: 4.2918\n",
      "Epoch [1/1], Step [1686/7635], Loss: 4.2958\n",
      "Epoch [1/1], Step [1687/7635], Loss: 4.3087\n",
      "Epoch [1/1], Step [1688/7635], Loss: 4.3361\n",
      "Epoch [1/1], Step [1689/7635], Loss: 4.2991\n",
      "Epoch [1/1], Step [1690/7635], Loss: 4.2770\n",
      "Epoch [1/1], Step [1691/7635], Loss: 4.3750\n",
      "Epoch [1/1], Step [1692/7635], Loss: 4.2333\n",
      "Epoch [1/1], Step [1693/7635], Loss: 4.3281\n",
      "Epoch [1/1], Step [1694/7635], Loss: 4.3538\n",
      "Epoch [1/1], Step [1695/7635], Loss: 4.2771\n",
      "Epoch [1/1], Step [1696/7635], Loss: 4.3807\n",
      "Epoch [1/1], Step [1697/7635], Loss: 4.2309\n",
      "Epoch [1/1], Step [1698/7635], Loss: 4.3081\n",
      "Epoch [1/1], Step [1699/7635], Loss: 4.2158\n",
      "Epoch [1/1], Step [1700/7635], Loss: 4.2703\n",
      "Epoch [1/1], Step [1701/7635], Loss: 4.2736\n",
      "Epoch [1/1], Step [1702/7635], Loss: 4.4232\n",
      "Epoch [1/1], Step [1703/7635], Loss: 4.3285\n",
      "Epoch [1/1], Step [1704/7635], Loss: 4.3691\n",
      "Epoch [1/1], Step [1705/7635], Loss: 4.2216\n",
      "Epoch [1/1], Step [1706/7635], Loss: 4.3413\n",
      "Epoch [1/1], Step [1707/7635], Loss: 4.3716\n",
      "Epoch [1/1], Step [1708/7635], Loss: 4.2793\n",
      "Epoch [1/1], Step [1709/7635], Loss: 4.3038\n",
      "Epoch [1/1], Step [1710/7635], Loss: 4.2996\n",
      "Epoch [1/1], Step [1711/7635], Loss: 4.4235\n",
      "Epoch [1/1], Step [1712/7635], Loss: 4.3583\n",
      "Epoch [1/1], Step [1713/7635], Loss: 4.3793\n",
      "Epoch [1/1], Step [1714/7635], Loss: 4.3434\n",
      "Epoch [1/1], Step [1715/7635], Loss: 4.3436\n",
      "Epoch [1/1], Step [1716/7635], Loss: 4.3433\n",
      "Epoch [1/1], Step [1717/7635], Loss: 4.2238\n",
      "Epoch [1/1], Step [1718/7635], Loss: 4.3196\n",
      "Epoch [1/1], Step [1719/7635], Loss: 4.3154\n",
      "Epoch [1/1], Step [1720/7635], Loss: 4.3517\n",
      "Epoch [1/1], Step [1721/7635], Loss: 4.4350\n",
      "Epoch [1/1], Step [1722/7635], Loss: 4.3228\n",
      "Epoch [1/1], Step [1723/7635], Loss: 4.2859\n",
      "Epoch [1/1], Step [1724/7635], Loss: 4.3233\n",
      "Epoch [1/1], Step [1725/7635], Loss: 4.2315\n",
      "Epoch [1/1], Step [1726/7635], Loss: 4.2945\n",
      "Epoch [1/1], Step [1727/7635], Loss: 4.3338\n",
      "Epoch [1/1], Step [1728/7635], Loss: 4.3590\n",
      "Epoch [1/1], Step [1729/7635], Loss: 4.4035\n",
      "Epoch [1/1], Step [1730/7635], Loss: 4.3318\n",
      "Epoch [1/1], Step [1731/7635], Loss: 4.3462\n",
      "Epoch [1/1], Step [1732/7635], Loss: 4.3369\n",
      "Epoch [1/1], Step [1733/7635], Loss: 4.3061\n",
      "Epoch [1/1], Step [1734/7635], Loss: 4.3258\n",
      "Epoch [1/1], Step [1735/7635], Loss: 4.2937\n",
      "Epoch [1/1], Step [1736/7635], Loss: 4.3086\n",
      "Epoch [1/1], Step [1737/7635], Loss: 4.3907\n",
      "Epoch [1/1], Step [1738/7635], Loss: 4.2984\n",
      "Epoch [1/1], Step [1739/7635], Loss: 4.3592\n",
      "Epoch [1/1], Step [1740/7635], Loss: 4.3713\n",
      "Epoch [1/1], Step [1741/7635], Loss: 4.1856\n",
      "Epoch [1/1], Step [1742/7635], Loss: 4.2654\n",
      "Epoch [1/1], Step [1743/7635], Loss: 4.3095\n",
      "Epoch [1/1], Step [1744/7635], Loss: 4.3176\n",
      "Epoch [1/1], Step [1745/7635], Loss: 4.3163\n",
      "Epoch [1/1], Step [1746/7635], Loss: 4.3361\n",
      "Epoch [1/1], Step [1747/7635], Loss: 4.3508\n",
      "Epoch [1/1], Step [1748/7635], Loss: 4.2250\n",
      "Epoch [1/1], Step [1749/7635], Loss: 4.2838\n",
      "Epoch [1/1], Step [1750/7635], Loss: 4.3029\n",
      "Epoch [1/1], Step [1751/7635], Loss: 4.2994\n",
      "Epoch [1/1], Step [1752/7635], Loss: 4.3578\n",
      "Epoch [1/1], Step [1753/7635], Loss: 4.3470\n",
      "Epoch [1/1], Step [1754/7635], Loss: 4.2975\n",
      "Epoch [1/1], Step [1755/7635], Loss: 4.2070\n",
      "Epoch [1/1], Step [1756/7635], Loss: 4.2306\n",
      "Epoch [1/1], Step [1757/7635], Loss: 4.2752\n",
      "Epoch [1/1], Step [1758/7635], Loss: 4.2319\n",
      "Epoch [1/1], Step [1759/7635], Loss: 4.2640\n",
      "Epoch [1/1], Step [1760/7635], Loss: 4.3223\n",
      "Epoch [1/1], Step [1761/7635], Loss: 4.3513\n",
      "Epoch [1/1], Step [1762/7635], Loss: 4.3123\n",
      "Epoch [1/1], Step [1763/7635], Loss: 4.3092\n",
      "Epoch [1/1], Step [1764/7635], Loss: 4.3580\n",
      "Epoch [1/1], Step [1765/7635], Loss: 4.3656\n",
      "Epoch [1/1], Step [1766/7635], Loss: 4.2795\n",
      "Epoch [1/1], Step [1767/7635], Loss: 4.2854\n",
      "Epoch [1/1], Step [1768/7635], Loss: 4.2965\n",
      "Epoch [1/1], Step [1769/7635], Loss: 4.3252\n",
      "Epoch [1/1], Step [1770/7635], Loss: 4.2512\n",
      "Epoch [1/1], Step [1771/7635], Loss: 4.2860\n",
      "Epoch [1/1], Step [1772/7635], Loss: 4.4251\n",
      "Epoch [1/1], Step [1773/7635], Loss: 4.2044\n",
      "Epoch [1/1], Step [1774/7635], Loss: 4.2685\n",
      "Epoch [1/1], Step [1775/7635], Loss: 4.3982\n",
      "Epoch [1/1], Step [1776/7635], Loss: 4.3484\n",
      "Epoch [1/1], Step [1777/7635], Loss: 4.2384\n",
      "Epoch [1/1], Step [1778/7635], Loss: 4.3595\n",
      "Epoch [1/1], Step [1779/7635], Loss: 4.2827\n",
      "Epoch [1/1], Step [1780/7635], Loss: 4.2810\n",
      "Epoch [1/1], Step [1781/7635], Loss: 4.2926\n",
      "Epoch [1/1], Step [1782/7635], Loss: 4.3464\n",
      "Epoch [1/1], Step [1783/7635], Loss: 4.3639\n",
      "Epoch [1/1], Step [1784/7635], Loss: 4.3679\n",
      "Epoch [1/1], Step [1785/7635], Loss: 4.3163\n",
      "Epoch [1/1], Step [1786/7635], Loss: 4.3328\n",
      "Epoch [1/1], Step [1787/7635], Loss: 4.3142\n",
      "Epoch [1/1], Step [1788/7635], Loss: 4.2372\n",
      "Epoch [1/1], Step [1789/7635], Loss: 4.2431\n",
      "Epoch [1/1], Step [1790/7635], Loss: 4.2803\n",
      "Epoch [1/1], Step [1791/7635], Loss: 4.2981\n",
      "Epoch [1/1], Step [1792/7635], Loss: 4.3528\n",
      "Epoch [1/1], Step [1793/7635], Loss: 4.3184\n",
      "Epoch [1/1], Step [1794/7635], Loss: 4.2498\n",
      "Epoch [1/1], Step [1795/7635], Loss: 4.2828\n",
      "Epoch [1/1], Step [1796/7635], Loss: 4.3665\n",
      "Epoch [1/1], Step [1797/7635], Loss: 4.1891\n",
      "Epoch [1/1], Step [1798/7635], Loss: 4.3081\n",
      "Epoch [1/1], Step [1799/7635], Loss: 4.3308\n",
      "Epoch [1/1], Step [1800/7635], Loss: 4.2928\n",
      "Epoch [1/1], Step [1801/7635], Loss: 4.2697\n",
      "Epoch [1/1], Step [1802/7635], Loss: 4.2808\n",
      "Epoch [1/1], Step [1803/7635], Loss: 4.3718\n",
      "Epoch [1/1], Step [1804/7635], Loss: 4.2810\n",
      "Epoch [1/1], Step [1805/7635], Loss: 4.3011\n",
      "Epoch [1/1], Step [1806/7635], Loss: 4.3185\n",
      "Epoch [1/1], Step [1807/7635], Loss: 4.3217\n",
      "Epoch [1/1], Step [1808/7635], Loss: 4.3038\n",
      "Epoch [1/1], Step [1809/7635], Loss: 4.2778\n",
      "Epoch [1/1], Step [1810/7635], Loss: 4.3018\n",
      "Epoch [1/1], Step [1811/7635], Loss: 4.2045\n",
      "Epoch [1/1], Step [1812/7635], Loss: 4.3474\n",
      "Epoch [1/1], Step [1813/7635], Loss: 4.2255\n",
      "Epoch [1/1], Step [1814/7635], Loss: 4.2287\n",
      "Epoch [1/1], Step [1815/7635], Loss: 4.3007\n",
      "Epoch [1/1], Step [1816/7635], Loss: 4.3859\n",
      "Epoch [1/1], Step [1817/7635], Loss: 4.2750\n",
      "Epoch [1/1], Step [1818/7635], Loss: 4.2485\n",
      "Epoch [1/1], Step [1819/7635], Loss: 4.3186\n",
      "Epoch [1/1], Step [1820/7635], Loss: 4.2611\n",
      "Epoch [1/1], Step [1821/7635], Loss: 4.1934\n",
      "Epoch [1/1], Step [1822/7635], Loss: 4.2915\n",
      "Epoch [1/1], Step [1823/7635], Loss: 4.3176\n",
      "Epoch [1/1], Step [1824/7635], Loss: 4.3032\n",
      "Epoch [1/1], Step [1825/7635], Loss: 4.2904\n",
      "Epoch [1/1], Step [1826/7635], Loss: 4.3451\n",
      "Epoch [1/1], Step [1827/7635], Loss: 4.2985\n",
      "Epoch [1/1], Step [1828/7635], Loss: 4.3768\n",
      "Epoch [1/1], Step [1829/7635], Loss: 4.3674\n",
      "Epoch [1/1], Step [1830/7635], Loss: 4.2003\n",
      "Epoch [1/1], Step [1831/7635], Loss: 4.2309\n",
      "Epoch [1/1], Step [1832/7635], Loss: 4.3382\n",
      "Epoch [1/1], Step [1833/7635], Loss: 4.2471\n",
      "Epoch [1/1], Step [1834/7635], Loss: 4.2956\n",
      "Epoch [1/1], Step [1835/7635], Loss: 4.3366\n",
      "Epoch [1/1], Step [1836/7635], Loss: 4.2334\n",
      "Epoch [1/1], Step [1837/7635], Loss: 4.2997\n",
      "Epoch [1/1], Step [1838/7635], Loss: 4.3641\n",
      "Epoch [1/1], Step [1839/7635], Loss: 4.2934\n",
      "Epoch [1/1], Step [1840/7635], Loss: 4.3831\n",
      "Epoch [1/1], Step [1841/7635], Loss: 4.3056\n",
      "Epoch [1/1], Step [1842/7635], Loss: 4.1663\n",
      "Epoch [1/1], Step [1843/7635], Loss: 4.1941\n",
      "Epoch [1/1], Step [1844/7635], Loss: 4.2911\n",
      "Epoch [1/1], Step [1845/7635], Loss: 4.3576\n",
      "Epoch [1/1], Step [1846/7635], Loss: 4.2767\n",
      "Epoch [1/1], Step [1847/7635], Loss: 4.3068\n",
      "Epoch [1/1], Step [1848/7635], Loss: 4.2479\n",
      "Epoch [1/1], Step [1849/7635], Loss: 4.2492\n",
      "Epoch [1/1], Step [1850/7635], Loss: 4.2886\n",
      "Epoch [1/1], Step [1851/7635], Loss: 4.3718\n",
      "Epoch [1/1], Step [1852/7635], Loss: 4.2238\n",
      "Epoch [1/1], Step [1853/7635], Loss: 4.2876\n",
      "Epoch [1/1], Step [1854/7635], Loss: 4.2962\n",
      "Epoch [1/1], Step [1855/7635], Loss: 4.3148\n",
      "Epoch [1/1], Step [1856/7635], Loss: 4.3060\n",
      "Epoch [1/1], Step [1857/7635], Loss: 4.2681\n",
      "Epoch [1/1], Step [1858/7635], Loss: 4.2580\n",
      "Epoch [1/1], Step [1859/7635], Loss: 4.3465\n",
      "Epoch [1/1], Step [1860/7635], Loss: 4.3491\n",
      "Epoch [1/1], Step [1861/7635], Loss: 4.3190\n",
      "Epoch [1/1], Step [1862/7635], Loss: 4.2887\n",
      "Epoch [1/1], Step [1863/7635], Loss: 4.2446\n",
      "Epoch [1/1], Step [1864/7635], Loss: 4.3157\n",
      "Epoch [1/1], Step [1865/7635], Loss: 4.2162\n",
      "Epoch [1/1], Step [1866/7635], Loss: 4.3265\n",
      "Epoch [1/1], Step [1867/7635], Loss: 4.2708\n",
      "Epoch [1/1], Step [1868/7635], Loss: 4.3076\n",
      "Epoch [1/1], Step [1869/7635], Loss: 4.2731\n",
      "Epoch [1/1], Step [1870/7635], Loss: 4.2536\n",
      "Epoch [1/1], Step [1871/7635], Loss: 4.3080\n",
      "Epoch [1/1], Step [1872/7635], Loss: 4.3051\n",
      "Epoch [1/1], Step [1873/7635], Loss: 4.3232\n",
      "Epoch [1/1], Step [1874/7635], Loss: 4.2274\n",
      "Epoch [1/1], Step [1875/7635], Loss: 4.4068\n",
      "Epoch [1/1], Step [1876/7635], Loss: 4.3625\n",
      "Epoch [1/1], Step [1877/7635], Loss: 4.2752\n",
      "Epoch [1/1], Step [1878/7635], Loss: 4.3609\n",
      "Epoch [1/1], Step [1879/7635], Loss: 4.2872\n",
      "Epoch [1/1], Step [1880/7635], Loss: 4.3092\n",
      "Epoch [1/1], Step [1881/7635], Loss: 4.2570\n",
      "Epoch [1/1], Step [1882/7635], Loss: 4.2908\n",
      "Epoch [1/1], Step [1883/7635], Loss: 4.3087\n",
      "Epoch [1/1], Step [1884/7635], Loss: 4.2885\n",
      "Epoch [1/1], Step [1885/7635], Loss: 4.2334\n",
      "Epoch [1/1], Step [1886/7635], Loss: 4.3129\n",
      "Epoch [1/1], Step [1887/7635], Loss: 4.2840\n",
      "Epoch [1/1], Step [1888/7635], Loss: 4.3625\n",
      "Epoch [1/1], Step [1889/7635], Loss: 4.3238\n",
      "Epoch [1/1], Step [1890/7635], Loss: 4.2471\n",
      "Epoch [1/1], Step [1891/7635], Loss: 4.3127\n",
      "Epoch [1/1], Step [1892/7635], Loss: 4.1987\n",
      "Epoch [1/1], Step [1893/7635], Loss: 4.3323\n",
      "Epoch [1/1], Step [1894/7635], Loss: 4.2963\n",
      "Epoch [1/1], Step [1895/7635], Loss: 4.2817\n",
      "Epoch [1/1], Step [1896/7635], Loss: 4.3003\n",
      "Epoch [1/1], Step [1897/7635], Loss: 4.3054\n",
      "Epoch [1/1], Step [1898/7635], Loss: 4.2314\n",
      "Epoch [1/1], Step [1899/7635], Loss: 4.3165\n",
      "Epoch [1/1], Step [1900/7635], Loss: 4.3082\n",
      "Epoch [1/1], Step [1901/7635], Loss: 4.3295\n",
      "Epoch [1/1], Step [1902/7635], Loss: 4.2916\n",
      "Epoch [1/1], Step [1903/7635], Loss: 4.3254\n",
      "Epoch [1/1], Step [1904/7635], Loss: 4.2258\n",
      "Epoch [1/1], Step [1905/7635], Loss: 4.3774\n",
      "Epoch [1/1], Step [1906/7635], Loss: 4.2709\n",
      "Epoch [1/1], Step [1907/7635], Loss: 4.3718\n",
      "Epoch [1/1], Step [1908/7635], Loss: 4.2719\n",
      "Epoch [1/1], Step [1909/7635], Loss: 4.2905\n",
      "Epoch [1/1], Step [1910/7635], Loss: 4.2194\n",
      "Epoch [1/1], Step [1911/7635], Loss: 4.2587\n",
      "Epoch [1/1], Step [1912/7635], Loss: 4.2669\n",
      "Epoch [1/1], Step [1913/7635], Loss: 4.2829\n",
      "Epoch [1/1], Step [1914/7635], Loss: 4.2283\n",
      "Epoch [1/1], Step [1915/7635], Loss: 4.2312\n",
      "Epoch [1/1], Step [1916/7635], Loss: 4.2110\n",
      "Epoch [1/1], Step [1917/7635], Loss: 4.2797\n",
      "Epoch [1/1], Step [1918/7635], Loss: 4.3344\n",
      "Epoch [1/1], Step [1919/7635], Loss: 4.2533\n",
      "Epoch [1/1], Step [1920/7635], Loss: 4.3428\n",
      "Epoch [1/1], Step [1921/7635], Loss: 4.1702\n",
      "Epoch [1/1], Step [1922/7635], Loss: 4.1484\n",
      "Epoch [1/1], Step [1923/7635], Loss: 4.1807\n",
      "Epoch [1/1], Step [1924/7635], Loss: 4.2253\n",
      "Epoch [1/1], Step [1925/7635], Loss: 4.2648\n",
      "Epoch [1/1], Step [1926/7635], Loss: 4.2469\n",
      "Epoch [1/1], Step [1927/7635], Loss: 4.3227\n",
      "Epoch [1/1], Step [1928/7635], Loss: 4.2416\n",
      "Epoch [1/1], Step [1929/7635], Loss: 4.2528\n",
      "Epoch [1/1], Step [1930/7635], Loss: 4.2897\n",
      "Epoch [1/1], Step [1931/7635], Loss: 4.4331\n",
      "Epoch [1/1], Step [1932/7635], Loss: 4.2714\n",
      "Epoch [1/1], Step [1933/7635], Loss: 4.2604\n",
      "Epoch [1/1], Step [1934/7635], Loss: 4.2399\n",
      "Epoch [1/1], Step [1935/7635], Loss: 4.2646\n",
      "Epoch [1/1], Step [1936/7635], Loss: 4.3067\n",
      "Epoch [1/1], Step [1937/7635], Loss: 4.3504\n",
      "Epoch [1/1], Step [1938/7635], Loss: 4.1757\n",
      "Epoch [1/1], Step [1939/7635], Loss: 4.1717\n",
      "Epoch [1/1], Step [1940/7635], Loss: 4.3512\n",
      "Epoch [1/1], Step [1941/7635], Loss: 4.3586\n",
      "Epoch [1/1], Step [1942/7635], Loss: 4.2205\n",
      "Epoch [1/1], Step [1943/7635], Loss: 4.2845\n",
      "Epoch [1/1], Step [1944/7635], Loss: 4.3360\n",
      "Epoch [1/1], Step [1945/7635], Loss: 4.1653\n",
      "Epoch [1/1], Step [1946/7635], Loss: 4.1826\n",
      "Epoch [1/1], Step [1947/7635], Loss: 4.2884\n",
      "Epoch [1/1], Step [1948/7635], Loss: 4.2608\n",
      "Epoch [1/1], Step [1949/7635], Loss: 4.2371\n",
      "Epoch [1/1], Step [1950/7635], Loss: 4.2716\n",
      "Epoch [1/1], Step [1951/7635], Loss: 4.2662\n",
      "Epoch [1/1], Step [1952/7635], Loss: 4.2159\n",
      "Epoch [1/1], Step [1953/7635], Loss: 4.2047\n",
      "Epoch [1/1], Step [1954/7635], Loss: 4.2638\n",
      "Epoch [1/1], Step [1955/7635], Loss: 4.3493\n",
      "Epoch [1/1], Step [1956/7635], Loss: 4.2081\n",
      "Epoch [1/1], Step [1957/7635], Loss: 4.3018\n",
      "Epoch [1/1], Step [1958/7635], Loss: 4.2723\n",
      "Epoch [1/1], Step [1959/7635], Loss: 4.2762\n",
      "Epoch [1/1], Step [1960/7635], Loss: 4.2634\n",
      "Epoch [1/1], Step [1961/7635], Loss: 4.2392\n",
      "Epoch [1/1], Step [1962/7635], Loss: 4.3398\n",
      "Epoch [1/1], Step [1963/7635], Loss: 4.2083\n",
      "Epoch [1/1], Step [1964/7635], Loss: 4.4124\n",
      "Epoch [1/1], Step [1965/7635], Loss: 4.3345\n",
      "Epoch [1/1], Step [1966/7635], Loss: 4.2378\n",
      "Epoch [1/1], Step [1967/7635], Loss: 4.2685\n",
      "Epoch [1/1], Step [1968/7635], Loss: 4.2332\n",
      "Epoch [1/1], Step [1969/7635], Loss: 4.2493\n",
      "Epoch [1/1], Step [1970/7635], Loss: 4.2602\n",
      "Epoch [1/1], Step [1971/7635], Loss: 4.2187\n",
      "Epoch [1/1], Step [1972/7635], Loss: 4.2776\n",
      "Epoch [1/1], Step [1973/7635], Loss: 4.3021\n",
      "Epoch [1/1], Step [1974/7635], Loss: 4.2941\n",
      "Epoch [1/1], Step [1975/7635], Loss: 4.3262\n",
      "Epoch [1/1], Step [1976/7635], Loss: 4.2684\n",
      "Epoch [1/1], Step [1977/7635], Loss: 4.2432\n",
      "Epoch [1/1], Step [1978/7635], Loss: 4.3028\n",
      "Epoch [1/1], Step [1979/7635], Loss: 4.3473\n",
      "Epoch [1/1], Step [1980/7635], Loss: 4.2248\n",
      "Epoch [1/1], Step [1981/7635], Loss: 4.2983\n",
      "Epoch [1/1], Step [1982/7635], Loss: 4.2408\n",
      "Epoch [1/1], Step [1983/7635], Loss: 4.1438\n",
      "Epoch [1/1], Step [1984/7635], Loss: 4.3108\n",
      "Epoch [1/1], Step [1985/7635], Loss: 4.2583\n",
      "Epoch [1/1], Step [1986/7635], Loss: 4.2345\n",
      "Epoch [1/1], Step [1987/7635], Loss: 4.3408\n",
      "Epoch [1/1], Step [1988/7635], Loss: 4.2050\n",
      "Epoch [1/1], Step [1989/7635], Loss: 4.3364\n",
      "Epoch [1/1], Step [1990/7635], Loss: 4.2307\n",
      "Epoch [1/1], Step [1991/7635], Loss: 4.2377\n",
      "Epoch [1/1], Step [1992/7635], Loss: 4.2482\n",
      "Epoch [1/1], Step [1993/7635], Loss: 4.2226\n",
      "Epoch [1/1], Step [1994/7635], Loss: 4.2639\n",
      "Epoch [1/1], Step [1995/7635], Loss: 4.2654\n",
      "Epoch [1/1], Step [1996/7635], Loss: 4.3432\n",
      "Epoch [1/1], Step [1997/7635], Loss: 4.3093\n",
      "Epoch [1/1], Step [1998/7635], Loss: 4.2555\n",
      "Epoch [1/1], Step [1999/7635], Loss: 4.3473\n",
      "Epoch [1/1], Step [2000/7635], Loss: 4.3341\n",
      "Epoch [1/1], Step [2001/7635], Loss: 4.1908\n",
      "Epoch [1/1], Step [2002/7635], Loss: 4.2537\n",
      "Epoch [1/1], Step [2003/7635], Loss: 4.2651\n",
      "Epoch [1/1], Step [2004/7635], Loss: 4.2991\n",
      "Epoch [1/1], Step [2005/7635], Loss: 4.2451\n",
      "Epoch [1/1], Step [2006/7635], Loss: 4.2925\n",
      "Epoch [1/1], Step [2007/7635], Loss: 4.3089\n",
      "Epoch [1/1], Step [2008/7635], Loss: 4.2626\n",
      "Epoch [1/1], Step [2009/7635], Loss: 4.2445\n",
      "Epoch [1/1], Step [2010/7635], Loss: 4.3000\n",
      "Epoch [1/1], Step [2011/7635], Loss: 4.2771\n",
      "Epoch [1/1], Step [2012/7635], Loss: 4.2170\n",
      "Epoch [1/1], Step [2013/7635], Loss: 4.3008\n",
      "Epoch [1/1], Step [2014/7635], Loss: 4.2729\n",
      "Epoch [1/1], Step [2015/7635], Loss: 4.1771\n",
      "Epoch [1/1], Step [2016/7635], Loss: 4.2585\n",
      "Epoch [1/1], Step [2017/7635], Loss: 4.1729\n",
      "Epoch [1/1], Step [2018/7635], Loss: 4.3318\n",
      "Epoch [1/1], Step [2019/7635], Loss: 4.3275\n",
      "Epoch [1/1], Step [2020/7635], Loss: 4.2893\n",
      "Epoch [1/1], Step [2021/7635], Loss: 4.2916\n",
      "Epoch [1/1], Step [2022/7635], Loss: 4.3139\n",
      "Epoch [1/1], Step [2023/7635], Loss: 4.2510\n",
      "Epoch [1/1], Step [2024/7635], Loss: 4.2256\n",
      "Epoch [1/1], Step [2025/7635], Loss: 4.2734\n",
      "Epoch [1/1], Step [2026/7635], Loss: 4.3012\n",
      "Epoch [1/1], Step [2027/7635], Loss: 4.3012\n",
      "Epoch [1/1], Step [2028/7635], Loss: 4.1159\n",
      "Epoch [1/1], Step [2029/7635], Loss: 4.2208\n",
      "Epoch [1/1], Step [2030/7635], Loss: 4.2291\n",
      "Epoch [1/1], Step [2031/7635], Loss: 4.3040\n",
      "Epoch [1/1], Step [2032/7635], Loss: 4.2447\n",
      "Epoch [1/1], Step [2033/7635], Loss: 4.2909\n",
      "Epoch [1/1], Step [2034/7635], Loss: 4.2083\n",
      "Epoch [1/1], Step [2035/7635], Loss: 4.2700\n",
      "Epoch [1/1], Step [2036/7635], Loss: 4.3142\n",
      "Epoch [1/1], Step [2037/7635], Loss: 4.2602\n",
      "Epoch [1/1], Step [2038/7635], Loss: 4.2464\n",
      "Epoch [1/1], Step [2039/7635], Loss: 4.2508\n",
      "Epoch [1/1], Step [2040/7635], Loss: 4.3353\n",
      "Epoch [1/1], Step [2041/7635], Loss: 4.2216\n",
      "Epoch [1/1], Step [2042/7635], Loss: 4.1998\n",
      "Epoch [1/1], Step [2043/7635], Loss: 4.2734\n",
      "Epoch [1/1], Step [2044/7635], Loss: 4.2884\n",
      "Epoch [1/1], Step [2045/7635], Loss: 4.1775\n",
      "Epoch [1/1], Step [2046/7635], Loss: 4.3504\n",
      "Epoch [1/1], Step [2047/7635], Loss: 4.2978\n",
      "Epoch [1/1], Step [2048/7635], Loss: 4.2795\n",
      "Epoch [1/1], Step [2049/7635], Loss: 4.3480\n",
      "Epoch [1/1], Step [2050/7635], Loss: 4.2111\n",
      "Epoch [1/1], Step [2051/7635], Loss: 4.2261\n",
      "Epoch [1/1], Step [2052/7635], Loss: 4.2805\n",
      "Epoch [1/1], Step [2053/7635], Loss: 4.2482\n",
      "Epoch [1/1], Step [2054/7635], Loss: 4.2896\n",
      "Epoch [1/1], Step [2055/7635], Loss: 4.2015\n",
      "Epoch [1/1], Step [2056/7635], Loss: 4.2535\n",
      "Epoch [1/1], Step [2057/7635], Loss: 4.2780\n",
      "Epoch [1/1], Step [2058/7635], Loss: 4.2376\n",
      "Epoch [1/1], Step [2059/7635], Loss: 4.3639\n",
      "Epoch [1/1], Step [2060/7635], Loss: 4.2574\n",
      "Epoch [1/1], Step [2061/7635], Loss: 4.3060\n",
      "Epoch [1/1], Step [2062/7635], Loss: 4.3166\n",
      "Epoch [1/1], Step [2063/7635], Loss: 4.2371\n",
      "Epoch [1/1], Step [2064/7635], Loss: 4.3412\n",
      "Epoch [1/1], Step [2065/7635], Loss: 4.3355\n",
      "Epoch [1/1], Step [2066/7635], Loss: 4.1655\n",
      "Epoch [1/1], Step [2067/7635], Loss: 4.2674\n",
      "Epoch [1/1], Step [2068/7635], Loss: 4.1828\n",
      "Epoch [1/1], Step [2069/7635], Loss: 4.2667\n",
      "Epoch [1/1], Step [2070/7635], Loss: 4.1927\n",
      "Epoch [1/1], Step [2071/7635], Loss: 4.2708\n",
      "Epoch [1/1], Step [2072/7635], Loss: 4.2540\n",
      "Epoch [1/1], Step [2073/7635], Loss: 4.2477\n",
      "Epoch [1/1], Step [2074/7635], Loss: 4.3401\n",
      "Epoch [1/1], Step [2075/7635], Loss: 4.2089\n",
      "Epoch [1/1], Step [2076/7635], Loss: 4.2133\n",
      "Epoch [1/1], Step [2077/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [2078/7635], Loss: 4.2520\n",
      "Epoch [1/1], Step [2079/7635], Loss: 4.2749\n",
      "Epoch [1/1], Step [2080/7635], Loss: 4.2593\n",
      "Epoch [1/1], Step [2081/7635], Loss: 4.2340\n",
      "Epoch [1/1], Step [2082/7635], Loss: 4.2399\n",
      "Epoch [1/1], Step [2083/7635], Loss: 4.2929\n",
      "Epoch [1/1], Step [2084/7635], Loss: 4.2123\n",
      "Epoch [1/1], Step [2085/7635], Loss: 4.2451\n",
      "Epoch [1/1], Step [2086/7635], Loss: 4.2618\n",
      "Epoch [1/1], Step [2087/7635], Loss: 4.1924\n",
      "Epoch [1/1], Step [2088/7635], Loss: 4.1854\n",
      "Epoch [1/1], Step [2089/7635], Loss: 4.2998\n",
      "Epoch [1/1], Step [2090/7635], Loss: 4.2547\n",
      "Epoch [1/1], Step [2091/7635], Loss: 4.3242\n",
      "Epoch [1/1], Step [2092/7635], Loss: 4.2705\n",
      "Epoch [1/1], Step [2093/7635], Loss: 4.1768\n",
      "Epoch [1/1], Step [2094/7635], Loss: 4.2658\n",
      "Epoch [1/1], Step [2095/7635], Loss: 4.2440\n",
      "Epoch [1/1], Step [2096/7635], Loss: 4.2478\n",
      "Epoch [1/1], Step [2097/7635], Loss: 4.2374\n",
      "Epoch [1/1], Step [2098/7635], Loss: 4.3279\n",
      "Epoch [1/1], Step [2099/7635], Loss: 4.2140\n",
      "Epoch [1/1], Step [2100/7635], Loss: 4.2028\n",
      "Epoch [1/1], Step [2101/7635], Loss: 4.3370\n",
      "Epoch [1/1], Step [2102/7635], Loss: 4.2272\n",
      "Epoch [1/1], Step [2103/7635], Loss: 4.3568\n",
      "Epoch [1/1], Step [2104/7635], Loss: 4.2525\n",
      "Epoch [1/1], Step [2105/7635], Loss: 4.2706\n",
      "Epoch [1/1], Step [2106/7635], Loss: 4.3048\n",
      "Epoch [1/1], Step [2107/7635], Loss: 4.2953\n",
      "Epoch [1/1], Step [2108/7635], Loss: 4.2008\n",
      "Epoch [1/1], Step [2109/7635], Loss: 4.1908\n",
      "Epoch [1/1], Step [2110/7635], Loss: 4.2268\n",
      "Epoch [1/1], Step [2111/7635], Loss: 4.3130\n",
      "Epoch [1/1], Step [2112/7635], Loss: 4.1453\n",
      "Epoch [1/1], Step [2113/7635], Loss: 4.2161\n",
      "Epoch [1/1], Step [2114/7635], Loss: 4.2340\n",
      "Epoch [1/1], Step [2115/7635], Loss: 4.3117\n",
      "Epoch [1/1], Step [2116/7635], Loss: 4.2434\n",
      "Epoch [1/1], Step [2117/7635], Loss: 4.1607\n",
      "Epoch [1/1], Step [2118/7635], Loss: 4.1699\n",
      "Epoch [1/1], Step [2119/7635], Loss: 4.3570\n",
      "Epoch [1/1], Step [2120/7635], Loss: 4.3466\n",
      "Epoch [1/1], Step [2121/7635], Loss: 4.2893\n",
      "Epoch [1/1], Step [2122/7635], Loss: 4.2037\n",
      "Epoch [1/1], Step [2123/7635], Loss: 4.2057\n",
      "Epoch [1/1], Step [2124/7635], Loss: 4.2693\n",
      "Epoch [1/1], Step [2125/7635], Loss: 4.1994\n",
      "Epoch [1/1], Step [2126/7635], Loss: 4.2385\n",
      "Epoch [1/1], Step [2127/7635], Loss: 4.2455\n",
      "Epoch [1/1], Step [2128/7635], Loss: 4.2094\n",
      "Epoch [1/1], Step [2129/7635], Loss: 4.2320\n",
      "Epoch [1/1], Step [2130/7635], Loss: 4.2576\n",
      "Epoch [1/1], Step [2131/7635], Loss: 4.2301\n",
      "Epoch [1/1], Step [2132/7635], Loss: 4.1632\n",
      "Epoch [1/1], Step [2133/7635], Loss: 4.2427\n",
      "Epoch [1/1], Step [2134/7635], Loss: 4.1653\n",
      "Epoch [1/1], Step [2135/7635], Loss: 4.1967\n",
      "Epoch [1/1], Step [2136/7635], Loss: 4.1990\n",
      "Epoch [1/1], Step [2137/7635], Loss: 4.2709\n",
      "Epoch [1/1], Step [2138/7635], Loss: 4.2099\n",
      "Epoch [1/1], Step [2139/7635], Loss: 4.2581\n",
      "Epoch [1/1], Step [2140/7635], Loss: 4.2353\n",
      "Epoch [1/1], Step [2141/7635], Loss: 4.2740\n",
      "Epoch [1/1], Step [2142/7635], Loss: 4.2582\n",
      "Epoch [1/1], Step [2143/7635], Loss: 4.2005\n",
      "Epoch [1/1], Step [2144/7635], Loss: 4.2168\n",
      "Epoch [1/1], Step [2145/7635], Loss: 4.2375\n",
      "Epoch [1/1], Step [2146/7635], Loss: 4.2428\n",
      "Epoch [1/1], Step [2147/7635], Loss: 4.1831\n",
      "Epoch [1/1], Step [2148/7635], Loss: 4.1758\n",
      "Epoch [1/1], Step [2149/7635], Loss: 4.2074\n",
      "Epoch [1/1], Step [2150/7635], Loss: 4.2491\n",
      "Epoch [1/1], Step [2151/7635], Loss: 4.1617\n",
      "Epoch [1/1], Step [2152/7635], Loss: 4.2295\n",
      "Epoch [1/1], Step [2153/7635], Loss: 4.1965\n",
      "Epoch [1/1], Step [2154/7635], Loss: 4.1931\n",
      "Epoch [1/1], Step [2155/7635], Loss: 4.2182\n",
      "Epoch [1/1], Step [2156/7635], Loss: 4.2371\n",
      "Epoch [1/1], Step [2157/7635], Loss: 4.2178\n",
      "Epoch [1/1], Step [2158/7635], Loss: 4.2105\n",
      "Epoch [1/1], Step [2159/7635], Loss: 4.2424\n",
      "Epoch [1/1], Step [2160/7635], Loss: 4.3853\n",
      "Epoch [1/1], Step [2161/7635], Loss: 4.2928\n",
      "Epoch [1/1], Step [2162/7635], Loss: 4.2032\n",
      "Epoch [1/1], Step [2163/7635], Loss: 4.2112\n",
      "Epoch [1/1], Step [2164/7635], Loss: 4.1205\n",
      "Epoch [1/1], Step [2165/7635], Loss: 4.2073\n",
      "Epoch [1/1], Step [2166/7635], Loss: 4.2218\n",
      "Epoch [1/1], Step [2167/7635], Loss: 4.1109\n",
      "Epoch [1/1], Step [2168/7635], Loss: 4.2992\n",
      "Epoch [1/1], Step [2169/7635], Loss: 4.2582\n",
      "Epoch [1/1], Step [2170/7635], Loss: 4.1942\n",
      "Epoch [1/1], Step [2171/7635], Loss: 4.1306\n",
      "Epoch [1/1], Step [2172/7635], Loss: 4.1957\n",
      "Epoch [1/1], Step [2173/7635], Loss: 4.1767\n",
      "Epoch [1/1], Step [2174/7635], Loss: 4.2098\n",
      "Epoch [1/1], Step [2175/7635], Loss: 4.1068\n",
      "Epoch [1/1], Step [2176/7635], Loss: 4.2850\n",
      "Epoch [1/1], Step [2177/7635], Loss: 4.1926\n",
      "Epoch [1/1], Step [2178/7635], Loss: 4.3214\n",
      "Epoch [1/1], Step [2179/7635], Loss: 4.3016\n",
      "Epoch [1/1], Step [2180/7635], Loss: 4.3227\n",
      "Epoch [1/1], Step [2181/7635], Loss: 4.1085\n",
      "Epoch [1/1], Step [2182/7635], Loss: 4.2062\n",
      "Epoch [1/1], Step [2183/7635], Loss: 4.2872\n",
      "Epoch [1/1], Step [2184/7635], Loss: 4.1957\n",
      "Epoch [1/1], Step [2185/7635], Loss: 4.1724\n",
      "Epoch [1/1], Step [2186/7635], Loss: 4.2232\n",
      "Epoch [1/1], Step [2187/7635], Loss: 4.2062\n",
      "Epoch [1/1], Step [2188/7635], Loss: 4.2861\n",
      "Epoch [1/1], Step [2189/7635], Loss: 4.2385\n",
      "Epoch [1/1], Step [2190/7635], Loss: 4.2817\n",
      "Epoch [1/1], Step [2191/7635], Loss: 4.3175\n",
      "Epoch [1/1], Step [2192/7635], Loss: 4.1620\n",
      "Epoch [1/1], Step [2193/7635], Loss: 4.2504\n",
      "Epoch [1/1], Step [2194/7635], Loss: 4.2180\n",
      "Epoch [1/1], Step [2195/7635], Loss: 4.1570\n",
      "Epoch [1/1], Step [2196/7635], Loss: 4.1741\n",
      "Epoch [1/1], Step [2197/7635], Loss: 4.2503\n",
      "Epoch [1/1], Step [2198/7635], Loss: 4.2038\n",
      "Epoch [1/1], Step [2199/7635], Loss: 4.2256\n",
      "Epoch [1/1], Step [2200/7635], Loss: 4.2526\n",
      "Epoch [1/1], Step [2201/7635], Loss: 4.2022\n",
      "Epoch [1/1], Step [2202/7635], Loss: 4.2620\n",
      "Epoch [1/1], Step [2203/7635], Loss: 4.1903\n",
      "Epoch [1/1], Step [2204/7635], Loss: 4.2773\n",
      "Epoch [1/1], Step [2205/7635], Loss: 4.2740\n",
      "Epoch [1/1], Step [2206/7635], Loss: 4.2946\n",
      "Epoch [1/1], Step [2207/7635], Loss: 4.2078\n",
      "Epoch [1/1], Step [2208/7635], Loss: 4.2177\n",
      "Epoch [1/1], Step [2209/7635], Loss: 4.3490\n",
      "Epoch [1/1], Step [2210/7635], Loss: 4.2636\n",
      "Epoch [1/1], Step [2211/7635], Loss: 4.1031\n",
      "Epoch [1/1], Step [2212/7635], Loss: 4.1901\n",
      "Epoch [1/1], Step [2213/7635], Loss: 4.2084\n",
      "Epoch [1/1], Step [2214/7635], Loss: 4.2423\n",
      "Epoch [1/1], Step [2215/7635], Loss: 4.3445\n",
      "Epoch [1/1], Step [2216/7635], Loss: 4.3307\n",
      "Epoch [1/1], Step [2217/7635], Loss: 4.3028\n",
      "Epoch [1/1], Step [2218/7635], Loss: 4.2857\n",
      "Epoch [1/1], Step [2219/7635], Loss: 4.3184\n",
      "Epoch [1/1], Step [2220/7635], Loss: 4.2487\n",
      "Epoch [1/1], Step [2221/7635], Loss: 4.1398\n",
      "Epoch [1/1], Step [2222/7635], Loss: 4.2068\n",
      "Epoch [1/1], Step [2223/7635], Loss: 4.1918\n",
      "Epoch [1/1], Step [2224/7635], Loss: 4.2988\n",
      "Epoch [1/1], Step [2225/7635], Loss: 4.2582\n",
      "Epoch [1/1], Step [2226/7635], Loss: 4.2678\n",
      "Epoch [1/1], Step [2227/7635], Loss: 4.2633\n",
      "Epoch [1/1], Step [2228/7635], Loss: 4.2000\n",
      "Epoch [1/1], Step [2229/7635], Loss: 4.2350\n",
      "Epoch [1/1], Step [2230/7635], Loss: 4.2877\n",
      "Epoch [1/1], Step [2231/7635], Loss: 4.2209\n",
      "Epoch [1/1], Step [2232/7635], Loss: 4.2461\n",
      "Epoch [1/1], Step [2233/7635], Loss: 4.1631\n",
      "Epoch [1/1], Step [2234/7635], Loss: 4.2096\n",
      "Epoch [1/1], Step [2235/7635], Loss: 4.2673\n",
      "Epoch [1/1], Step [2236/7635], Loss: 4.2254\n",
      "Epoch [1/1], Step [2237/7635], Loss: 4.1890\n",
      "Epoch [1/1], Step [2238/7635], Loss: 4.1745\n",
      "Epoch [1/1], Step [2239/7635], Loss: 4.2277\n",
      "Epoch [1/1], Step [2240/7635], Loss: 4.3338\n",
      "Epoch [1/1], Step [2241/7635], Loss: 4.1995\n",
      "Epoch [1/1], Step [2242/7635], Loss: 4.3379\n",
      "Epoch [1/1], Step [2243/7635], Loss: 4.2478\n",
      "Epoch [1/1], Step [2244/7635], Loss: 4.2677\n",
      "Epoch [1/1], Step [2245/7635], Loss: 4.3085\n",
      "Epoch [1/1], Step [2246/7635], Loss: 4.1830\n",
      "Epoch [1/1], Step [2247/7635], Loss: 4.1782\n",
      "Epoch [1/1], Step [2248/7635], Loss: 4.1023\n",
      "Epoch [1/1], Step [2249/7635], Loss: 4.2817\n",
      "Epoch [1/1], Step [2250/7635], Loss: 4.2055\n",
      "Epoch [1/1], Step [2251/7635], Loss: 4.2295\n",
      "Epoch [1/1], Step [2252/7635], Loss: 4.1841\n",
      "Epoch [1/1], Step [2253/7635], Loss: 4.1366\n",
      "Epoch [1/1], Step [2254/7635], Loss: 4.1426\n",
      "Epoch [1/1], Step [2255/7635], Loss: 4.2661\n",
      "Epoch [1/1], Step [2256/7635], Loss: 4.1683\n",
      "Epoch [1/1], Step [2257/7635], Loss: 4.2557\n",
      "Epoch [1/1], Step [2258/7635], Loss: 4.1969\n",
      "Epoch [1/1], Step [2259/7635], Loss: 4.2547\n",
      "Epoch [1/1], Step [2260/7635], Loss: 4.2142\n",
      "Epoch [1/1], Step [2261/7635], Loss: 4.2536\n",
      "Epoch [1/1], Step [2262/7635], Loss: 4.2770\n",
      "Epoch [1/1], Step [2263/7635], Loss: 4.1913\n",
      "Epoch [1/1], Step [2264/7635], Loss: 4.2013\n",
      "Epoch [1/1], Step [2265/7635], Loss: 4.1874\n",
      "Epoch [1/1], Step [2266/7635], Loss: 4.1793\n",
      "Epoch [1/1], Step [2267/7635], Loss: 4.2483\n",
      "Epoch [1/1], Step [2268/7635], Loss: 4.2842\n",
      "Epoch [1/1], Step [2269/7635], Loss: 4.1875\n",
      "Epoch [1/1], Step [2270/7635], Loss: 4.2709\n",
      "Epoch [1/1], Step [2271/7635], Loss: 4.2689\n",
      "Epoch [1/1], Step [2272/7635], Loss: 4.2422\n",
      "Epoch [1/1], Step [2273/7635], Loss: 4.2702\n",
      "Epoch [1/1], Step [2274/7635], Loss: 4.1272\n",
      "Epoch [1/1], Step [2275/7635], Loss: 4.1549\n",
      "Epoch [1/1], Step [2276/7635], Loss: 4.2149\n",
      "Epoch [1/1], Step [2277/7635], Loss: 4.2895\n",
      "Epoch [1/1], Step [2278/7635], Loss: 4.2207\n",
      "Epoch [1/1], Step [2279/7635], Loss: 4.2061\n",
      "Epoch [1/1], Step [2280/7635], Loss: 4.1874\n",
      "Epoch [1/1], Step [2281/7635], Loss: 4.1579\n",
      "Epoch [1/1], Step [2282/7635], Loss: 4.1973\n",
      "Epoch [1/1], Step [2283/7635], Loss: 4.1492\n",
      "Epoch [1/1], Step [2284/7635], Loss: 4.1757\n",
      "Epoch [1/1], Step [2285/7635], Loss: 4.2546\n",
      "Epoch [1/1], Step [2286/7635], Loss: 4.2341\n",
      "Epoch [1/1], Step [2287/7635], Loss: 4.2620\n",
      "Epoch [1/1], Step [2288/7635], Loss: 4.2159\n",
      "Epoch [1/1], Step [2289/7635], Loss: 4.2060\n",
      "Epoch [1/1], Step [2290/7635], Loss: 4.1858\n",
      "Epoch [1/1], Step [2291/7635], Loss: 4.2224\n",
      "Epoch [1/1], Step [2292/7635], Loss: 4.2297\n",
      "Epoch [1/1], Step [2293/7635], Loss: 4.1859\n",
      "Epoch [1/1], Step [2294/7635], Loss: 4.2091\n",
      "Epoch [1/1], Step [2295/7635], Loss: 4.1416\n",
      "Epoch [1/1], Step [2296/7635], Loss: 4.2279\n",
      "Epoch [1/1], Step [2297/7635], Loss: 4.2214\n",
      "Epoch [1/1], Step [2298/7635], Loss: 4.2173\n",
      "Epoch [1/1], Step [2299/7635], Loss: 4.1548\n",
      "Epoch [1/1], Step [2300/7635], Loss: 4.1812\n",
      "Epoch [1/1], Step [2301/7635], Loss: 4.2470\n",
      "Epoch [1/1], Step [2302/7635], Loss: 4.2258\n",
      "Epoch [1/1], Step [2303/7635], Loss: 4.2089\n",
      "Epoch [1/1], Step [2304/7635], Loss: 4.1832\n",
      "Epoch [1/1], Step [2305/7635], Loss: 4.1545\n",
      "Epoch [1/1], Step [2306/7635], Loss: 4.2416\n",
      "Epoch [1/1], Step [2307/7635], Loss: 4.1897\n",
      "Epoch [1/1], Step [2308/7635], Loss: 4.1281\n",
      "Epoch [1/1], Step [2309/7635], Loss: 4.1084\n",
      "Epoch [1/1], Step [2310/7635], Loss: 4.2233\n",
      "Epoch [1/1], Step [2311/7635], Loss: 4.1770\n",
      "Epoch [1/1], Step [2312/7635], Loss: 4.1870\n",
      "Epoch [1/1], Step [2313/7635], Loss: 4.1907\n",
      "Epoch [1/1], Step [2314/7635], Loss: 4.2264\n",
      "Epoch [1/1], Step [2315/7635], Loss: 4.1686\n",
      "Epoch [1/1], Step [2316/7635], Loss: 4.2354\n",
      "Epoch [1/1], Step [2317/7635], Loss: 4.2320\n",
      "Epoch [1/1], Step [2318/7635], Loss: 4.1931\n",
      "Epoch [1/1], Step [2319/7635], Loss: 4.2280\n",
      "Epoch [1/1], Step [2320/7635], Loss: 4.2869\n",
      "Epoch [1/1], Step [2321/7635], Loss: 4.2672\n",
      "Epoch [1/1], Step [2322/7635], Loss: 4.1883\n",
      "Epoch [1/1], Step [2323/7635], Loss: 4.2182\n",
      "Epoch [1/1], Step [2324/7635], Loss: 4.1518\n",
      "Epoch [1/1], Step [2325/7635], Loss: 4.2221\n",
      "Epoch [1/1], Step [2326/7635], Loss: 4.2424\n",
      "Epoch [1/1], Step [2327/7635], Loss: 4.2284\n",
      "Epoch [1/1], Step [2328/7635], Loss: 4.3058\n",
      "Epoch [1/1], Step [2329/7635], Loss: 4.2036\n",
      "Epoch [1/1], Step [2330/7635], Loss: 4.2216\n",
      "Epoch [1/1], Step [2331/7635], Loss: 4.1214\n",
      "Epoch [1/1], Step [2332/7635], Loss: 4.1850\n",
      "Epoch [1/1], Step [2333/7635], Loss: 4.1995\n",
      "Epoch [1/1], Step [2334/7635], Loss: 4.2205\n",
      "Epoch [1/1], Step [2335/7635], Loss: 4.1907\n",
      "Epoch [1/1], Step [2336/7635], Loss: 4.2262\n",
      "Epoch [1/1], Step [2337/7635], Loss: 4.1816\n",
      "Epoch [1/1], Step [2338/7635], Loss: 4.2030\n",
      "Epoch [1/1], Step [2339/7635], Loss: 4.1895\n",
      "Epoch [1/1], Step [2340/7635], Loss: 4.1795\n",
      "Epoch [1/1], Step [2341/7635], Loss: 4.3002\n",
      "Epoch [1/1], Step [2342/7635], Loss: 4.2477\n",
      "Epoch [1/1], Step [2343/7635], Loss: 4.2274\n",
      "Epoch [1/1], Step [2344/7635], Loss: 4.2106\n",
      "Epoch [1/1], Step [2345/7635], Loss: 4.1760\n",
      "Epoch [1/1], Step [2346/7635], Loss: 4.2118\n",
      "Epoch [1/1], Step [2347/7635], Loss: 4.1957\n",
      "Epoch [1/1], Step [2348/7635], Loss: 4.1927\n",
      "Epoch [1/1], Step [2349/7635], Loss: 4.2481\n",
      "Epoch [1/1], Step [2350/7635], Loss: 4.3164\n",
      "Epoch [1/1], Step [2351/7635], Loss: 4.2105\n",
      "Epoch [1/1], Step [2352/7635], Loss: 4.1582\n",
      "Epoch [1/1], Step [2353/7635], Loss: 4.1688\n",
      "Epoch [1/1], Step [2354/7635], Loss: 4.1801\n",
      "Epoch [1/1], Step [2355/7635], Loss: 4.1015\n",
      "Epoch [1/1], Step [2356/7635], Loss: 4.3141\n",
      "Epoch [1/1], Step [2357/7635], Loss: 4.2532\n",
      "Epoch [1/1], Step [2358/7635], Loss: 4.3503\n",
      "Epoch [1/1], Step [2359/7635], Loss: 4.1498\n",
      "Epoch [1/1], Step [2360/7635], Loss: 4.1639\n",
      "Epoch [1/1], Step [2361/7635], Loss: 4.2555\n",
      "Epoch [1/1], Step [2362/7635], Loss: 4.2764\n",
      "Epoch [1/1], Step [2363/7635], Loss: 4.1691\n",
      "Epoch [1/1], Step [2364/7635], Loss: 4.1784\n",
      "Epoch [1/1], Step [2365/7635], Loss: 4.1697\n",
      "Epoch [1/1], Step [2366/7635], Loss: 4.1295\n",
      "Epoch [1/1], Step [2367/7635], Loss: 4.2083\n",
      "Epoch [1/1], Step [2368/7635], Loss: 4.2228\n",
      "Epoch [1/1], Step [2369/7635], Loss: 4.2377\n",
      "Epoch [1/1], Step [2370/7635], Loss: 4.2448\n",
      "Epoch [1/1], Step [2371/7635], Loss: 4.2708\n",
      "Epoch [1/1], Step [2372/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [2373/7635], Loss: 4.3121\n",
      "Epoch [1/1], Step [2374/7635], Loss: 4.2138\n",
      "Epoch [1/1], Step [2375/7635], Loss: 4.1746\n",
      "Epoch [1/1], Step [2376/7635], Loss: 4.2137\n",
      "Epoch [1/1], Step [2377/7635], Loss: 4.1970\n",
      "Epoch [1/1], Step [2378/7635], Loss: 4.2530\n",
      "Epoch [1/1], Step [2379/7635], Loss: 4.1744\n",
      "Epoch [1/1], Step [2380/7635], Loss: 4.1688\n",
      "Epoch [1/1], Step [2381/7635], Loss: 4.0739\n",
      "Epoch [1/1], Step [2382/7635], Loss: 4.2171\n",
      "Epoch [1/1], Step [2383/7635], Loss: 4.2080\n",
      "Epoch [1/1], Step [2384/7635], Loss: 4.2091\n",
      "Epoch [1/1], Step [2385/7635], Loss: 4.2296\n",
      "Epoch [1/1], Step [2386/7635], Loss: 4.2457\n",
      "Epoch [1/1], Step [2387/7635], Loss: 4.1488\n",
      "Epoch [1/1], Step [2388/7635], Loss: 4.1222\n",
      "Epoch [1/1], Step [2389/7635], Loss: 4.2022\n",
      "Epoch [1/1], Step [2390/7635], Loss: 4.1721\n",
      "Epoch [1/1], Step [2391/7635], Loss: 4.1635\n",
      "Epoch [1/1], Step [2392/7635], Loss: 4.2111\n",
      "Epoch [1/1], Step [2393/7635], Loss: 4.2801\n",
      "Epoch [1/1], Step [2394/7635], Loss: 4.2189\n",
      "Epoch [1/1], Step [2395/7635], Loss: 4.2194\n",
      "Epoch [1/1], Step [2396/7635], Loss: 4.2345\n",
      "Epoch [1/1], Step [2397/7635], Loss: 4.1553\n",
      "Epoch [1/1], Step [2398/7635], Loss: 4.1991\n",
      "Epoch [1/1], Step [2399/7635], Loss: 4.2476\n",
      "Epoch [1/1], Step [2400/7635], Loss: 4.1694\n",
      "Epoch [1/1], Step [2401/7635], Loss: 4.2264\n",
      "Epoch [1/1], Step [2402/7635], Loss: 4.2125\n",
      "Epoch [1/1], Step [2403/7635], Loss: 4.2604\n",
      "Epoch [1/1], Step [2404/7635], Loss: 4.2074\n",
      "Epoch [1/1], Step [2405/7635], Loss: 4.1394\n",
      "Epoch [1/1], Step [2406/7635], Loss: 4.2147\n",
      "Epoch [1/1], Step [2407/7635], Loss: 4.2534\n",
      "Epoch [1/1], Step [2408/7635], Loss: 4.2412\n",
      "Epoch [1/1], Step [2409/7635], Loss: 4.1986\n",
      "Epoch [1/1], Step [2410/7635], Loss: 4.1775\n",
      "Epoch [1/1], Step [2411/7635], Loss: 4.2034\n",
      "Epoch [1/1], Step [2412/7635], Loss: 4.1783\n",
      "Epoch [1/1], Step [2413/7635], Loss: 4.1389\n",
      "Epoch [1/1], Step [2414/7635], Loss: 4.1997\n",
      "Epoch [1/1], Step [2415/7635], Loss: 4.2768\n",
      "Epoch [1/1], Step [2416/7635], Loss: 4.2422\n",
      "Epoch [1/1], Step [2417/7635], Loss: 4.1469\n",
      "Epoch [1/1], Step [2418/7635], Loss: 4.2247\n",
      "Epoch [1/1], Step [2419/7635], Loss: 4.1272\n",
      "Epoch [1/1], Step [2420/7635], Loss: 4.2171\n",
      "Epoch [1/1], Step [2421/7635], Loss: 4.1595\n",
      "Epoch [1/1], Step [2422/7635], Loss: 4.2047\n",
      "Epoch [1/1], Step [2423/7635], Loss: 4.2452\n",
      "Epoch [1/1], Step [2424/7635], Loss: 4.1654\n",
      "Epoch [1/1], Step [2425/7635], Loss: 4.2826\n",
      "Epoch [1/1], Step [2426/7635], Loss: 4.2177\n",
      "Epoch [1/1], Step [2427/7635], Loss: 4.3112\n",
      "Epoch [1/1], Step [2428/7635], Loss: 4.2956\n",
      "Epoch [1/1], Step [2429/7635], Loss: 4.1874\n",
      "Epoch [1/1], Step [2430/7635], Loss: 4.1824\n",
      "Epoch [1/1], Step [2431/7635], Loss: 4.2531\n",
      "Epoch [1/1], Step [2432/7635], Loss: 4.1902\n",
      "Epoch [1/1], Step [2433/7635], Loss: 4.2278\n",
      "Epoch [1/1], Step [2434/7635], Loss: 4.1793\n",
      "Epoch [1/1], Step [2435/7635], Loss: 4.1445\n",
      "Epoch [1/1], Step [2436/7635], Loss: 4.1660\n",
      "Epoch [1/1], Step [2437/7635], Loss: 4.2965\n",
      "Epoch [1/1], Step [2438/7635], Loss: 4.1245\n",
      "Epoch [1/1], Step [2439/7635], Loss: 4.1489\n",
      "Epoch [1/1], Step [2440/7635], Loss: 4.2381\n",
      "Epoch [1/1], Step [2441/7635], Loss: 4.1502\n",
      "Epoch [1/1], Step [2442/7635], Loss: 4.2279\n",
      "Epoch [1/1], Step [2443/7635], Loss: 4.2063\n",
      "Epoch [1/1], Step [2444/7635], Loss: 4.1885\n",
      "Epoch [1/1], Step [2445/7635], Loss: 4.2290\n",
      "Epoch [1/1], Step [2446/7635], Loss: 4.2042\n",
      "Epoch [1/1], Step [2447/7635], Loss: 4.2430\n",
      "Epoch [1/1], Step [2448/7635], Loss: 4.2241\n",
      "Epoch [1/1], Step [2449/7635], Loss: 4.1803\n",
      "Epoch [1/1], Step [2450/7635], Loss: 4.2252\n",
      "Epoch [1/1], Step [2451/7635], Loss: 4.1479\n",
      "Epoch [1/1], Step [2452/7635], Loss: 4.1684\n",
      "Epoch [1/1], Step [2453/7635], Loss: 4.1897\n",
      "Epoch [1/1], Step [2454/7635], Loss: 4.2408\n",
      "Epoch [1/1], Step [2455/7635], Loss: 4.1695\n",
      "Epoch [1/1], Step [2456/7635], Loss: 4.1813\n",
      "Epoch [1/1], Step [2457/7635], Loss: 4.1699\n",
      "Epoch [1/1], Step [2458/7635], Loss: 4.1589\n",
      "Epoch [1/1], Step [2459/7635], Loss: 4.1568\n",
      "Epoch [1/1], Step [2460/7635], Loss: 4.1366\n",
      "Epoch [1/1], Step [2461/7635], Loss: 4.1624\n",
      "Epoch [1/1], Step [2462/7635], Loss: 4.1028\n",
      "Epoch [1/1], Step [2463/7635], Loss: 4.1618\n",
      "Epoch [1/1], Step [2464/7635], Loss: 4.1661\n",
      "Epoch [1/1], Step [2465/7635], Loss: 4.2011\n",
      "Epoch [1/1], Step [2466/7635], Loss: 4.2385\n",
      "Epoch [1/1], Step [2467/7635], Loss: 4.2383\n",
      "Epoch [1/1], Step [2468/7635], Loss: 4.2153\n",
      "Epoch [1/1], Step [2469/7635], Loss: 4.1318\n",
      "Epoch [1/1], Step [2470/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [2471/7635], Loss: 4.2239\n",
      "Epoch [1/1], Step [2472/7635], Loss: 4.1409\n",
      "Epoch [1/1], Step [2473/7635], Loss: 4.2711\n",
      "Epoch [1/1], Step [2474/7635], Loss: 4.0950\n",
      "Epoch [1/1], Step [2475/7635], Loss: 4.3700\n",
      "Epoch [1/1], Step [2476/7635], Loss: 4.1706\n",
      "Epoch [1/1], Step [2477/7635], Loss: 4.1980\n",
      "Epoch [1/1], Step [2478/7635], Loss: 4.1579\n",
      "Epoch [1/1], Step [2479/7635], Loss: 4.2065\n",
      "Epoch [1/1], Step [2480/7635], Loss: 4.1822\n",
      "Epoch [1/1], Step [2481/7635], Loss: 4.2069\n",
      "Epoch [1/1], Step [2482/7635], Loss: 4.1300\n",
      "Epoch [1/1], Step [2483/7635], Loss: 4.2179\n",
      "Epoch [1/1], Step [2484/7635], Loss: 4.1248\n",
      "Epoch [1/1], Step [2485/7635], Loss: 4.1894\n",
      "Epoch [1/1], Step [2486/7635], Loss: 4.2089\n",
      "Epoch [1/1], Step [2487/7635], Loss: 4.1316\n",
      "Epoch [1/1], Step [2488/7635], Loss: 4.1576\n",
      "Epoch [1/1], Step [2489/7635], Loss: 4.2111\n",
      "Epoch [1/1], Step [2490/7635], Loss: 4.2693\n",
      "Epoch [1/1], Step [2491/7635], Loss: 4.1562\n",
      "Epoch [1/1], Step [2492/7635], Loss: 4.1264\n",
      "Epoch [1/1], Step [2493/7635], Loss: 4.1822\n",
      "Epoch [1/1], Step [2494/7635], Loss: 4.2276\n",
      "Epoch [1/1], Step [2495/7635], Loss: 4.2151\n",
      "Epoch [1/1], Step [2496/7635], Loss: 4.2151\n",
      "Epoch [1/1], Step [2497/7635], Loss: 4.1733\n",
      "Epoch [1/1], Step [2498/7635], Loss: 4.1879\n",
      "Epoch [1/1], Step [2499/7635], Loss: 4.1430\n",
      "Epoch [1/1], Step [2500/7635], Loss: 4.1681\n",
      "Epoch [1/1], Step [2501/7635], Loss: 4.2515\n",
      "Epoch [1/1], Step [2502/7635], Loss: 4.1722\n",
      "Epoch [1/1], Step [2503/7635], Loss: 4.1988\n",
      "Epoch [1/1], Step [2504/7635], Loss: 4.0548\n",
      "Epoch [1/1], Step [2505/7635], Loss: 4.1448\n",
      "Epoch [1/1], Step [2506/7635], Loss: 4.1176\n",
      "Epoch [1/1], Step [2507/7635], Loss: 4.1983\n",
      "Epoch [1/1], Step [2508/7635], Loss: 4.2374\n",
      "Epoch [1/1], Step [2509/7635], Loss: 4.2794\n",
      "Epoch [1/1], Step [2510/7635], Loss: 4.2859\n",
      "Epoch [1/1], Step [2511/7635], Loss: 4.1648\n",
      "Epoch [1/1], Step [2512/7635], Loss: 4.2301\n",
      "Epoch [1/1], Step [2513/7635], Loss: 4.1440\n",
      "Epoch [1/1], Step [2514/7635], Loss: 4.1953\n",
      "Epoch [1/1], Step [2515/7635], Loss: 4.1524\n",
      "Epoch [1/1], Step [2516/7635], Loss: 4.1473\n",
      "Epoch [1/1], Step [2517/7635], Loss: 4.1425\n",
      "Epoch [1/1], Step [2518/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [2519/7635], Loss: 4.2153\n",
      "Epoch [1/1], Step [2520/7635], Loss: 4.1930\n",
      "Epoch [1/1], Step [2521/7635], Loss: 4.1766\n",
      "Epoch [1/1], Step [2522/7635], Loss: 4.1861\n",
      "Epoch [1/1], Step [2523/7635], Loss: 4.1720\n",
      "Epoch [1/1], Step [2524/7635], Loss: 4.1526\n",
      "Epoch [1/1], Step [2525/7635], Loss: 4.2215\n",
      "Epoch [1/1], Step [2526/7635], Loss: 4.2735\n",
      "Epoch [1/1], Step [2527/7635], Loss: 4.1289\n",
      "Epoch [1/1], Step [2528/7635], Loss: 4.1659\n",
      "Epoch [1/1], Step [2529/7635], Loss: 4.1364\n",
      "Epoch [1/1], Step [2530/7635], Loss: 4.1742\n",
      "Epoch [1/1], Step [2531/7635], Loss: 4.1860\n",
      "Epoch [1/1], Step [2532/7635], Loss: 4.0988\n",
      "Epoch [1/1], Step [2533/7635], Loss: 4.2200\n",
      "Epoch [1/1], Step [2534/7635], Loss: 4.2189\n",
      "Epoch [1/1], Step [2535/7635], Loss: 4.2615\n",
      "Epoch [1/1], Step [2536/7635], Loss: 4.2533\n",
      "Epoch [1/1], Step [2537/7635], Loss: 4.2215\n",
      "Epoch [1/1], Step [2538/7635], Loss: 4.2363\n",
      "Epoch [1/1], Step [2539/7635], Loss: 4.1874\n",
      "Epoch [1/1], Step [2540/7635], Loss: 4.1219\n",
      "Epoch [1/1], Step [2541/7635], Loss: 4.1204\n",
      "Epoch [1/1], Step [2542/7635], Loss: 4.1980\n",
      "Epoch [1/1], Step [2543/7635], Loss: 4.1565\n",
      "Epoch [1/1], Step [2544/7635], Loss: 4.1727\n",
      "Epoch [1/1], Step [2545/7635], Loss: 4.1300\n",
      "Epoch [1/1], Step [2546/7635], Loss: 4.2077\n",
      "Epoch [1/1], Step [2547/7635], Loss: 4.2202\n",
      "Epoch [1/1], Step [2548/7635], Loss: 4.2511\n",
      "Epoch [1/1], Step [2549/7635], Loss: 4.2246\n",
      "Epoch [1/1], Step [2550/7635], Loss: 4.1925\n",
      "Epoch [1/1], Step [2551/7635], Loss: 4.2240\n",
      "Epoch [1/1], Step [2552/7635], Loss: 4.1915\n",
      "Epoch [1/1], Step [2553/7635], Loss: 4.2014\n",
      "Epoch [1/1], Step [2554/7635], Loss: 4.2472\n",
      "Epoch [1/1], Step [2555/7635], Loss: 4.1552\n",
      "Epoch [1/1], Step [2556/7635], Loss: 4.1580\n",
      "Epoch [1/1], Step [2557/7635], Loss: 4.2563\n",
      "Epoch [1/1], Step [2558/7635], Loss: 4.1364\n",
      "Epoch [1/1], Step [2559/7635], Loss: 4.2297\n",
      "Epoch [1/1], Step [2560/7635], Loss: 4.1999\n",
      "Epoch [1/1], Step [2561/7635], Loss: 4.2150\n",
      "Epoch [1/1], Step [2562/7635], Loss: 4.1967\n",
      "Epoch [1/1], Step [2563/7635], Loss: 4.2543\n",
      "Epoch [1/1], Step [2564/7635], Loss: 4.1914\n",
      "Epoch [1/1], Step [2565/7635], Loss: 4.2090\n",
      "Epoch [1/1], Step [2566/7635], Loss: 4.3168\n",
      "Epoch [1/1], Step [2567/7635], Loss: 4.2346\n",
      "Epoch [1/1], Step [2568/7635], Loss: 4.2561\n",
      "Epoch [1/1], Step [2569/7635], Loss: 4.1885\n",
      "Epoch [1/1], Step [2570/7635], Loss: 4.1701\n",
      "Epoch [1/1], Step [2571/7635], Loss: 4.1880\n",
      "Epoch [1/1], Step [2572/7635], Loss: 4.1439\n",
      "Epoch [1/1], Step [2573/7635], Loss: 4.2024\n",
      "Epoch [1/1], Step [2574/7635], Loss: 4.1444\n",
      "Epoch [1/1], Step [2575/7635], Loss: 4.1586\n",
      "Epoch [1/1], Step [2576/7635], Loss: 4.2083\n",
      "Epoch [1/1], Step [2577/7635], Loss: 4.1404\n",
      "Epoch [1/1], Step [2578/7635], Loss: 4.1693\n",
      "Epoch [1/1], Step [2579/7635], Loss: 4.2357\n",
      "Epoch [1/1], Step [2580/7635], Loss: 4.1950\n",
      "Epoch [1/1], Step [2581/7635], Loss: 4.2038\n",
      "Epoch [1/1], Step [2582/7635], Loss: 4.2200\n",
      "Epoch [1/1], Step [2583/7635], Loss: 4.0863\n",
      "Epoch [1/1], Step [2584/7635], Loss: 4.1561\n",
      "Epoch [1/1], Step [2585/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [2586/7635], Loss: 4.1415\n",
      "Epoch [1/1], Step [2587/7635], Loss: 4.2461\n",
      "Epoch [1/1], Step [2588/7635], Loss: 4.2418\n",
      "Epoch [1/1], Step [2589/7635], Loss: 4.2092\n",
      "Epoch [1/1], Step [2590/7635], Loss: 4.2284\n",
      "Epoch [1/1], Step [2591/7635], Loss: 4.1616\n",
      "Epoch [1/1], Step [2592/7635], Loss: 4.2264\n",
      "Epoch [1/1], Step [2593/7635], Loss: 4.3121\n",
      "Epoch [1/1], Step [2594/7635], Loss: 4.2131\n",
      "Epoch [1/1], Step [2595/7635], Loss: 4.3042\n",
      "Epoch [1/1], Step [2596/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [2597/7635], Loss: 4.1601\n",
      "Epoch [1/1], Step [2598/7635], Loss: 4.1086\n",
      "Epoch [1/1], Step [2599/7635], Loss: 4.1831\n",
      "Epoch [1/1], Step [2600/7635], Loss: 4.1043\n",
      "Epoch [1/1], Step [2601/7635], Loss: 4.1687\n",
      "Epoch [1/1], Step [2602/7635], Loss: 4.0953\n",
      "Epoch [1/1], Step [2603/7635], Loss: 4.0879\n",
      "Epoch [1/1], Step [2604/7635], Loss: 4.1038\n",
      "Epoch [1/1], Step [2605/7635], Loss: 4.2258\n",
      "Epoch [1/1], Step [2606/7635], Loss: 4.1560\n",
      "Epoch [1/1], Step [2607/7635], Loss: 4.2564\n",
      "Epoch [1/1], Step [2608/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [2609/7635], Loss: 4.1551\n",
      "Epoch [1/1], Step [2610/7635], Loss: 4.1960\n",
      "Epoch [1/1], Step [2611/7635], Loss: 4.0975\n",
      "Epoch [1/1], Step [2612/7635], Loss: 4.1152\n",
      "Epoch [1/1], Step [2613/7635], Loss: 4.1779\n",
      "Epoch [1/1], Step [2614/7635], Loss: 4.0959\n",
      "Epoch [1/1], Step [2615/7635], Loss: 4.2121\n",
      "Epoch [1/1], Step [2616/7635], Loss: 4.1909\n",
      "Epoch [1/1], Step [2617/7635], Loss: 4.1689\n",
      "Epoch [1/1], Step [2618/7635], Loss: 4.2684\n",
      "Epoch [1/1], Step [2619/7635], Loss: 4.1596\n",
      "Epoch [1/1], Step [2620/7635], Loss: 4.2150\n",
      "Epoch [1/1], Step [2621/7635], Loss: 4.1695\n",
      "Epoch [1/1], Step [2622/7635], Loss: 4.1566\n",
      "Epoch [1/1], Step [2623/7635], Loss: 4.2262\n",
      "Epoch [1/1], Step [2624/7635], Loss: 4.1564\n",
      "Epoch [1/1], Step [2625/7635], Loss: 4.1802\n",
      "Epoch [1/1], Step [2626/7635], Loss: 4.1227\n",
      "Epoch [1/1], Step [2627/7635], Loss: 4.1606\n",
      "Epoch [1/1], Step [2628/7635], Loss: 4.0952\n",
      "Epoch [1/1], Step [2629/7635], Loss: 4.1646\n",
      "Epoch [1/1], Step [2630/7635], Loss: 4.2170\n",
      "Epoch [1/1], Step [2631/7635], Loss: 4.1780\n",
      "Epoch [1/1], Step [2632/7635], Loss: 4.2485\n",
      "Epoch [1/1], Step [2633/7635], Loss: 4.2298\n",
      "Epoch [1/1], Step [2634/7635], Loss: 4.2307\n",
      "Epoch [1/1], Step [2635/7635], Loss: 4.1333\n",
      "Epoch [1/1], Step [2636/7635], Loss: 4.1229\n",
      "Epoch [1/1], Step [2637/7635], Loss: 4.2531\n",
      "Epoch [1/1], Step [2638/7635], Loss: 4.2159\n",
      "Epoch [1/1], Step [2639/7635], Loss: 4.0807\n",
      "Epoch [1/1], Step [2640/7635], Loss: 4.2035\n",
      "Epoch [1/1], Step [2641/7635], Loss: 4.1498\n",
      "Epoch [1/1], Step [2642/7635], Loss: 4.2265\n",
      "Epoch [1/1], Step [2643/7635], Loss: 4.0791\n",
      "Epoch [1/1], Step [2644/7635], Loss: 4.1958\n",
      "Epoch [1/1], Step [2645/7635], Loss: 4.2058\n",
      "Epoch [1/1], Step [2646/7635], Loss: 4.2536\n",
      "Epoch [1/1], Step [2647/7635], Loss: 4.1962\n",
      "Epoch [1/1], Step [2648/7635], Loss: 4.1762\n",
      "Epoch [1/1], Step [2649/7635], Loss: 4.1487\n",
      "Epoch [1/1], Step [2650/7635], Loss: 4.2106\n",
      "Epoch [1/1], Step [2651/7635], Loss: 4.1807\n",
      "Epoch [1/1], Step [2652/7635], Loss: 4.1617\n",
      "Epoch [1/1], Step [2653/7635], Loss: 4.2474\n",
      "Epoch [1/1], Step [2654/7635], Loss: 4.1820\n",
      "Epoch [1/1], Step [2655/7635], Loss: 4.1741\n",
      "Epoch [1/1], Step [2656/7635], Loss: 4.1670\n",
      "Epoch [1/1], Step [2657/7635], Loss: 4.1149\n",
      "Epoch [1/1], Step [2658/7635], Loss: 4.1183\n",
      "Epoch [1/1], Step [2659/7635], Loss: 4.2088\n",
      "Epoch [1/1], Step [2660/7635], Loss: 4.1872\n",
      "Epoch [1/1], Step [2661/7635], Loss: 4.1285\n",
      "Epoch [1/1], Step [2662/7635], Loss: 4.1765\n",
      "Epoch [1/1], Step [2663/7635], Loss: 4.1715\n",
      "Epoch [1/1], Step [2664/7635], Loss: 4.1747\n",
      "Epoch [1/1], Step [2665/7635], Loss: 4.2452\n",
      "Epoch [1/1], Step [2666/7635], Loss: 4.0576\n",
      "Epoch [1/1], Step [2667/7635], Loss: 4.0952\n",
      "Epoch [1/1], Step [2668/7635], Loss: 4.1761\n",
      "Epoch [1/1], Step [2669/7635], Loss: 4.0650\n",
      "Epoch [1/1], Step [2670/7635], Loss: 4.1994\n",
      "Epoch [1/1], Step [2671/7635], Loss: 4.0467\n",
      "Epoch [1/1], Step [2672/7635], Loss: 4.2309\n",
      "Epoch [1/1], Step [2673/7635], Loss: 4.2462\n",
      "Epoch [1/1], Step [2674/7635], Loss: 4.1511\n",
      "Epoch [1/1], Step [2675/7635], Loss: 4.0731\n",
      "Epoch [1/1], Step [2676/7635], Loss: 4.2305\n",
      "Epoch [1/1], Step [2677/7635], Loss: 4.2110\n",
      "Epoch [1/1], Step [2678/7635], Loss: 4.1525\n",
      "Epoch [1/1], Step [2679/7635], Loss: 4.2647\n",
      "Epoch [1/1], Step [2680/7635], Loss: 4.0547\n",
      "Epoch [1/1], Step [2681/7635], Loss: 4.1903\n",
      "Epoch [1/1], Step [2682/7635], Loss: 4.1397\n",
      "Epoch [1/1], Step [2683/7635], Loss: 4.1611\n",
      "Epoch [1/1], Step [2684/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [2685/7635], Loss: 4.1127\n",
      "Epoch [1/1], Step [2686/7635], Loss: 4.1524\n",
      "Epoch [1/1], Step [2687/7635], Loss: 4.2474\n",
      "Epoch [1/1], Step [2688/7635], Loss: 4.1528\n",
      "Epoch [1/1], Step [2689/7635], Loss: 4.2286\n",
      "Epoch [1/1], Step [2690/7635], Loss: 4.2488\n",
      "Epoch [1/1], Step [2691/7635], Loss: 4.1455\n",
      "Epoch [1/1], Step [2692/7635], Loss: 4.0731\n",
      "Epoch [1/1], Step [2693/7635], Loss: 4.1125\n",
      "Epoch [1/1], Step [2694/7635], Loss: 4.1967\n",
      "Epoch [1/1], Step [2695/7635], Loss: 4.1615\n",
      "Epoch [1/1], Step [2696/7635], Loss: 4.2653\n",
      "Epoch [1/1], Step [2697/7635], Loss: 4.2460\n",
      "Epoch [1/1], Step [2698/7635], Loss: 4.2520\n",
      "Epoch [1/1], Step [2699/7635], Loss: 4.1029\n",
      "Epoch [1/1], Step [2700/7635], Loss: 4.1570\n",
      "Epoch [1/1], Step [2701/7635], Loss: 4.1838\n",
      "Epoch [1/1], Step [2702/7635], Loss: 4.1136\n",
      "Epoch [1/1], Step [2703/7635], Loss: 4.2396\n",
      "Epoch [1/1], Step [2704/7635], Loss: 4.2024\n",
      "Epoch [1/1], Step [2705/7635], Loss: 4.2278\n",
      "Epoch [1/1], Step [2706/7635], Loss: 4.1044\n",
      "Epoch [1/1], Step [2707/7635], Loss: 4.1473\n",
      "Epoch [1/1], Step [2708/7635], Loss: 4.2006\n",
      "Epoch [1/1], Step [2709/7635], Loss: 4.1462\n",
      "Epoch [1/1], Step [2710/7635], Loss: 4.1632\n",
      "Epoch [1/1], Step [2711/7635], Loss: 4.2316\n",
      "Epoch [1/1], Step [2712/7635], Loss: 4.1987\n",
      "Epoch [1/1], Step [2713/7635], Loss: 4.1230\n",
      "Epoch [1/1], Step [2714/7635], Loss: 4.1431\n",
      "Epoch [1/1], Step [2715/7635], Loss: 4.0890\n",
      "Epoch [1/1], Step [2716/7635], Loss: 4.1329\n",
      "Epoch [1/1], Step [2717/7635], Loss: 4.1775\n",
      "Epoch [1/1], Step [2718/7635], Loss: 4.2027\n",
      "Epoch [1/1], Step [2719/7635], Loss: 4.0970\n",
      "Epoch [1/1], Step [2720/7635], Loss: 4.1820\n",
      "Epoch [1/1], Step [2721/7635], Loss: 4.1729\n",
      "Epoch [1/1], Step [2722/7635], Loss: 4.1972\n",
      "Epoch [1/1], Step [2723/7635], Loss: 4.0981\n",
      "Epoch [1/1], Step [2724/7635], Loss: 4.1607\n",
      "Epoch [1/1], Step [2725/7635], Loss: 4.1666\n",
      "Epoch [1/1], Step [2726/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [2727/7635], Loss: 4.1844\n",
      "Epoch [1/1], Step [2728/7635], Loss: 4.1808\n",
      "Epoch [1/1], Step [2729/7635], Loss: 4.1264\n",
      "Epoch [1/1], Step [2730/7635], Loss: 4.1679\n",
      "Epoch [1/1], Step [2731/7635], Loss: 4.2347\n",
      "Epoch [1/1], Step [2732/7635], Loss: 4.1605\n",
      "Epoch [1/1], Step [2733/7635], Loss: 4.1380\n",
      "Epoch [1/1], Step [2734/7635], Loss: 4.0731\n",
      "Epoch [1/1], Step [2735/7635], Loss: 4.2276\n",
      "Epoch [1/1], Step [2736/7635], Loss: 4.1317\n",
      "Epoch [1/1], Step [2737/7635], Loss: 4.1272\n",
      "Epoch [1/1], Step [2738/7635], Loss: 4.1325\n",
      "Epoch [1/1], Step [2739/7635], Loss: 4.2023\n",
      "Epoch [1/1], Step [2740/7635], Loss: 4.1869\n",
      "Epoch [1/1], Step [2741/7635], Loss: 4.1833\n",
      "Epoch [1/1], Step [2742/7635], Loss: 4.1650\n",
      "Epoch [1/1], Step [2743/7635], Loss: 4.1862\n",
      "Epoch [1/1], Step [2744/7635], Loss: 4.0791\n",
      "Epoch [1/1], Step [2745/7635], Loss: 4.1156\n",
      "Epoch [1/1], Step [2746/7635], Loss: 4.2048\n",
      "Epoch [1/1], Step [2747/7635], Loss: 4.1977\n",
      "Epoch [1/1], Step [2748/7635], Loss: 4.1631\n",
      "Epoch [1/1], Step [2749/7635], Loss: 4.1811\n",
      "Epoch [1/1], Step [2750/7635], Loss: 4.2105\n",
      "Epoch [1/1], Step [2751/7635], Loss: 4.1897\n",
      "Epoch [1/1], Step [2752/7635], Loss: 4.1561\n",
      "Epoch [1/1], Step [2753/7635], Loss: 4.1616\n",
      "Epoch [1/1], Step [2754/7635], Loss: 4.1999\n",
      "Epoch [1/1], Step [2755/7635], Loss: 4.1892\n",
      "Epoch [1/1], Step [2756/7635], Loss: 4.1332\n",
      "Epoch [1/1], Step [2757/7635], Loss: 4.1929\n",
      "Epoch [1/1], Step [2758/7635], Loss: 4.2062\n",
      "Epoch [1/1], Step [2759/7635], Loss: 4.1714\n",
      "Epoch [1/1], Step [2760/7635], Loss: 4.2011\n",
      "Epoch [1/1], Step [2761/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [2762/7635], Loss: 4.1782\n",
      "Epoch [1/1], Step [2763/7635], Loss: 4.1583\n",
      "Epoch [1/1], Step [2764/7635], Loss: 4.2263\n",
      "Epoch [1/1], Step [2765/7635], Loss: 4.1581\n",
      "Epoch [1/1], Step [2766/7635], Loss: 4.1176\n",
      "Epoch [1/1], Step [2767/7635], Loss: 4.1647\n",
      "Epoch [1/1], Step [2768/7635], Loss: 4.1680\n",
      "Epoch [1/1], Step [2769/7635], Loss: 4.0987\n",
      "Epoch [1/1], Step [2770/7635], Loss: 4.2135\n",
      "Epoch [1/1], Step [2771/7635], Loss: 4.1964\n",
      "Epoch [1/1], Step [2772/7635], Loss: 4.2474\n",
      "Epoch [1/1], Step [2773/7635], Loss: 4.1292\n",
      "Epoch [1/1], Step [2774/7635], Loss: 4.1051\n",
      "Epoch [1/1], Step [2775/7635], Loss: 4.1689\n",
      "Epoch [1/1], Step [2776/7635], Loss: 4.1492\n",
      "Epoch [1/1], Step [2777/7635], Loss: 4.1697\n",
      "Epoch [1/1], Step [2778/7635], Loss: 4.1602\n",
      "Epoch [1/1], Step [2779/7635], Loss: 4.1307\n",
      "Epoch [1/1], Step [2780/7635], Loss: 4.1493\n",
      "Epoch [1/1], Step [2781/7635], Loss: 4.1022\n",
      "Epoch [1/1], Step [2782/7635], Loss: 4.2133\n",
      "Epoch [1/1], Step [2783/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [2784/7635], Loss: 4.1360\n",
      "Epoch [1/1], Step [2785/7635], Loss: 4.1226\n",
      "Epoch [1/1], Step [2786/7635], Loss: 4.1176\n",
      "Epoch [1/1], Step [2787/7635], Loss: 4.2171\n",
      "Epoch [1/1], Step [2788/7635], Loss: 4.1190\n",
      "Epoch [1/1], Step [2789/7635], Loss: 4.1465\n",
      "Epoch [1/1], Step [2790/7635], Loss: 4.0944\n",
      "Epoch [1/1], Step [2791/7635], Loss: 4.1380\n",
      "Epoch [1/1], Step [2792/7635], Loss: 4.1491\n",
      "Epoch [1/1], Step [2793/7635], Loss: 4.1782\n",
      "Epoch [1/1], Step [2794/7635], Loss: 4.0711\n",
      "Epoch [1/1], Step [2795/7635], Loss: 4.0767\n",
      "Epoch [1/1], Step [2796/7635], Loss: 4.1538\n",
      "Epoch [1/1], Step [2797/7635], Loss: 4.1764\n",
      "Epoch [1/1], Step [2798/7635], Loss: 4.1343\n",
      "Epoch [1/1], Step [2799/7635], Loss: 4.0737\n",
      "Epoch [1/1], Step [2800/7635], Loss: 4.1532\n",
      "Epoch [1/1], Step [2801/7635], Loss: 4.1982\n",
      "Epoch [1/1], Step [2802/7635], Loss: 4.1585\n",
      "Epoch [1/1], Step [2803/7635], Loss: 4.0863\n",
      "Epoch [1/1], Step [2804/7635], Loss: 4.1593\n",
      "Epoch [1/1], Step [2805/7635], Loss: 4.1875\n",
      "Epoch [1/1], Step [2806/7635], Loss: 4.0974\n",
      "Epoch [1/1], Step [2807/7635], Loss: 4.1825\n",
      "Epoch [1/1], Step [2808/7635], Loss: 4.2506\n",
      "Epoch [1/1], Step [2809/7635], Loss: 4.1938\n",
      "Epoch [1/1], Step [2810/7635], Loss: 4.1467\n",
      "Epoch [1/1], Step [2811/7635], Loss: 4.2623\n",
      "Epoch [1/1], Step [2812/7635], Loss: 4.1902\n",
      "Epoch [1/1], Step [2813/7635], Loss: 4.0846\n",
      "Epoch [1/1], Step [2814/7635], Loss: 4.1114\n",
      "Epoch [1/1], Step [2815/7635], Loss: 4.1079\n",
      "Epoch [1/1], Step [2816/7635], Loss: 4.1732\n",
      "Epoch [1/1], Step [2817/7635], Loss: 4.2103\n",
      "Epoch [1/1], Step [2818/7635], Loss: 4.1658\n",
      "Epoch [1/1], Step [2819/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [2820/7635], Loss: 4.1236\n",
      "Epoch [1/1], Step [2821/7635], Loss: 4.1502\n",
      "Epoch [1/1], Step [2822/7635], Loss: 4.1056\n",
      "Epoch [1/1], Step [2823/7635], Loss: 4.1965\n",
      "Epoch [1/1], Step [2824/7635], Loss: 4.1430\n",
      "Epoch [1/1], Step [2825/7635], Loss: 4.1839\n",
      "Epoch [1/1], Step [2826/7635], Loss: 4.1458\n",
      "Epoch [1/1], Step [2827/7635], Loss: 4.1926\n",
      "Epoch [1/1], Step [2828/7635], Loss: 4.1749\n",
      "Epoch [1/1], Step [2829/7635], Loss: 4.1775\n",
      "Epoch [1/1], Step [2830/7635], Loss: 4.1143\n",
      "Epoch [1/1], Step [2831/7635], Loss: 4.2540\n",
      "Epoch [1/1], Step [2832/7635], Loss: 4.2531\n",
      "Epoch [1/1], Step [2833/7635], Loss: 4.1683\n",
      "Epoch [1/1], Step [2834/7635], Loss: 4.2805\n",
      "Epoch [1/1], Step [2835/7635], Loss: 4.1947\n",
      "Epoch [1/1], Step [2836/7635], Loss: 4.1728\n",
      "Epoch [1/1], Step [2837/7635], Loss: 4.1696\n",
      "Epoch [1/1], Step [2838/7635], Loss: 4.1751\n",
      "Epoch [1/1], Step [2839/7635], Loss: 4.1356\n",
      "Epoch [1/1], Step [2840/7635], Loss: 4.1857\n",
      "Epoch [1/1], Step [2841/7635], Loss: 4.2263\n",
      "Epoch [1/1], Step [2842/7635], Loss: 4.1241\n",
      "Epoch [1/1], Step [2843/7635], Loss: 4.2220\n",
      "Epoch [1/1], Step [2844/7635], Loss: 4.1986\n",
      "Epoch [1/1], Step [2845/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [2846/7635], Loss: 4.1977\n",
      "Epoch [1/1], Step [2847/7635], Loss: 4.1529\n",
      "Epoch [1/1], Step [2848/7635], Loss: 4.2311\n",
      "Epoch [1/1], Step [2849/7635], Loss: 4.1451\n",
      "Epoch [1/1], Step [2850/7635], Loss: 4.1774\n",
      "Epoch [1/1], Step [2851/7635], Loss: 4.1022\n",
      "Epoch [1/1], Step [2852/7635], Loss: 4.2575\n",
      "Epoch [1/1], Step [2853/7635], Loss: 4.1528\n",
      "Epoch [1/1], Step [2854/7635], Loss: 4.1862\n",
      "Epoch [1/1], Step [2855/7635], Loss: 4.1380\n",
      "Epoch [1/1], Step [2856/7635], Loss: 4.1669\n",
      "Epoch [1/1], Step [2857/7635], Loss: 4.1025\n",
      "Epoch [1/1], Step [2858/7635], Loss: 4.1997\n",
      "Epoch [1/1], Step [2859/7635], Loss: 4.1622\n",
      "Epoch [1/1], Step [2860/7635], Loss: 4.0975\n",
      "Epoch [1/1], Step [2861/7635], Loss: 4.1348\n",
      "Epoch [1/1], Step [2862/7635], Loss: 4.1548\n",
      "Epoch [1/1], Step [2863/7635], Loss: 4.2411\n",
      "Epoch [1/1], Step [2864/7635], Loss: 4.1229\n",
      "Epoch [1/1], Step [2865/7635], Loss: 4.1816\n",
      "Epoch [1/1], Step [2866/7635], Loss: 4.1777\n",
      "Epoch [1/1], Step [2867/7635], Loss: 4.1615\n",
      "Epoch [1/1], Step [2868/7635], Loss: 4.1290\n",
      "Epoch [1/1], Step [2869/7635], Loss: 4.1718\n",
      "Epoch [1/1], Step [2870/7635], Loss: 4.2125\n",
      "Epoch [1/1], Step [2871/7635], Loss: 4.1391\n",
      "Epoch [1/1], Step [2872/7635], Loss: 4.1562\n",
      "Epoch [1/1], Step [2873/7635], Loss: 4.1467\n",
      "Epoch [1/1], Step [2874/7635], Loss: 4.0695\n",
      "Epoch [1/1], Step [2875/7635], Loss: 4.1469\n",
      "Epoch [1/1], Step [2876/7635], Loss: 4.1567\n",
      "Epoch [1/1], Step [2877/7635], Loss: 4.1696\n",
      "Epoch [1/1], Step [2878/7635], Loss: 4.1335\n",
      "Epoch [1/1], Step [2879/7635], Loss: 4.1097\n",
      "Epoch [1/1], Step [2880/7635], Loss: 4.1619\n",
      "Epoch [1/1], Step [2881/7635], Loss: 4.1495\n",
      "Epoch [1/1], Step [2882/7635], Loss: 4.1043\n",
      "Epoch [1/1], Step [2883/7635], Loss: 4.1587\n",
      "Epoch [1/1], Step [2884/7635], Loss: 4.2022\n",
      "Epoch [1/1], Step [2885/7635], Loss: 4.1138\n",
      "Epoch [1/1], Step [2886/7635], Loss: 4.1902\n",
      "Epoch [1/1], Step [2887/7635], Loss: 4.1232\n",
      "Epoch [1/1], Step [2888/7635], Loss: 4.0683\n",
      "Epoch [1/1], Step [2889/7635], Loss: 4.1340\n",
      "Epoch [1/1], Step [2890/7635], Loss: 4.2294\n",
      "Epoch [1/1], Step [2891/7635], Loss: 4.0777\n",
      "Epoch [1/1], Step [2892/7635], Loss: 4.1487\n",
      "Epoch [1/1], Step [2893/7635], Loss: 4.1131\n",
      "Epoch [1/1], Step [2894/7635], Loss: 4.0819\n",
      "Epoch [1/1], Step [2895/7635], Loss: 4.1381\n",
      "Epoch [1/1], Step [2896/7635], Loss: 4.1370\n",
      "Epoch [1/1], Step [2897/7635], Loss: 4.1956\n",
      "Epoch [1/1], Step [2898/7635], Loss: 4.2139\n",
      "Epoch [1/1], Step [2899/7635], Loss: 4.1452\n",
      "Epoch [1/1], Step [2900/7635], Loss: 4.0932\n",
      "Epoch [1/1], Step [2901/7635], Loss: 4.1270\n",
      "Epoch [1/1], Step [2902/7635], Loss: 4.1839\n",
      "Epoch [1/1], Step [2903/7635], Loss: 4.1229\n",
      "Epoch [1/1], Step [2904/7635], Loss: 4.2035\n",
      "Epoch [1/1], Step [2905/7635], Loss: 4.1684\n",
      "Epoch [1/1], Step [2906/7635], Loss: 4.1722\n",
      "Epoch [1/1], Step [2907/7635], Loss: 4.1534\n",
      "Epoch [1/1], Step [2908/7635], Loss: 4.2051\n",
      "Epoch [1/1], Step [2909/7635], Loss: 4.2392\n",
      "Epoch [1/1], Step [2910/7635], Loss: 4.2257\n",
      "Epoch [1/1], Step [2911/7635], Loss: 4.1741\n",
      "Epoch [1/1], Step [2912/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [2913/7635], Loss: 4.1258\n",
      "Epoch [1/1], Step [2914/7635], Loss: 4.1041\n",
      "Epoch [1/1], Step [2915/7635], Loss: 4.2297\n",
      "Epoch [1/1], Step [2916/7635], Loss: 4.0966\n",
      "Epoch [1/1], Step [2917/7635], Loss: 4.0955\n",
      "Epoch [1/1], Step [2918/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [2919/7635], Loss: 4.1318\n",
      "Epoch [1/1], Step [2920/7635], Loss: 4.0712\n",
      "Epoch [1/1], Step [2921/7635], Loss: 4.0625\n",
      "Epoch [1/1], Step [2922/7635], Loss: 4.0992\n",
      "Epoch [1/1], Step [2923/7635], Loss: 4.1896\n",
      "Epoch [1/1], Step [2924/7635], Loss: 4.1813\n",
      "Epoch [1/1], Step [2925/7635], Loss: 4.1126\n",
      "Epoch [1/1], Step [2926/7635], Loss: 4.2526\n",
      "Epoch [1/1], Step [2927/7635], Loss: 4.1603\n",
      "Epoch [1/1], Step [2928/7635], Loss: 4.0844\n",
      "Epoch [1/1], Step [2929/7635], Loss: 4.1089\n",
      "Epoch [1/1], Step [2930/7635], Loss: 4.1829\n",
      "Epoch [1/1], Step [2931/7635], Loss: 4.1968\n",
      "Epoch [1/1], Step [2932/7635], Loss: 4.1189\n",
      "Epoch [1/1], Step [2933/7635], Loss: 4.0895\n",
      "Epoch [1/1], Step [2934/7635], Loss: 4.1872\n",
      "Epoch [1/1], Step [2935/7635], Loss: 4.2152\n",
      "Epoch [1/1], Step [2936/7635], Loss: 4.1828\n",
      "Epoch [1/1], Step [2937/7635], Loss: 4.1510\n",
      "Epoch [1/1], Step [2938/7635], Loss: 4.1842\n",
      "Epoch [1/1], Step [2939/7635], Loss: 4.1469\n",
      "Epoch [1/1], Step [2940/7635], Loss: 4.1140\n",
      "Epoch [1/1], Step [2941/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [2942/7635], Loss: 4.1469\n",
      "Epoch [1/1], Step [2943/7635], Loss: 4.1089\n",
      "Epoch [1/1], Step [2944/7635], Loss: 4.1252\n",
      "Epoch [1/1], Step [2945/7635], Loss: 4.1468\n",
      "Epoch [1/1], Step [2946/7635], Loss: 4.1675\n",
      "Epoch [1/1], Step [2947/7635], Loss: 4.1995\n",
      "Epoch [1/1], Step [2948/7635], Loss: 4.1187\n",
      "Epoch [1/1], Step [2949/7635], Loss: 4.1674\n",
      "Epoch [1/1], Step [2950/7635], Loss: 4.1006\n",
      "Epoch [1/1], Step [2951/7635], Loss: 4.1488\n",
      "Epoch [1/1], Step [2952/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [2953/7635], Loss: 4.2542\n",
      "Epoch [1/1], Step [2954/7635], Loss: 4.1756\n",
      "Epoch [1/1], Step [2955/7635], Loss: 4.0826\n",
      "Epoch [1/1], Step [2956/7635], Loss: 4.1523\n",
      "Epoch [1/1], Step [2957/7635], Loss: 4.2551\n",
      "Epoch [1/1], Step [2958/7635], Loss: 4.0787\n",
      "Epoch [1/1], Step [2959/7635], Loss: 4.0805\n",
      "Epoch [1/1], Step [2960/7635], Loss: 4.1234\n",
      "Epoch [1/1], Step [2961/7635], Loss: 4.1791\n",
      "Epoch [1/1], Step [2962/7635], Loss: 4.1008\n",
      "Epoch [1/1], Step [2963/7635], Loss: 4.0887\n",
      "Epoch [1/1], Step [2964/7635], Loss: 4.1833\n",
      "Epoch [1/1], Step [2965/7635], Loss: 4.2027\n",
      "Epoch [1/1], Step [2966/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [2967/7635], Loss: 4.0944\n",
      "Epoch [1/1], Step [2968/7635], Loss: 4.2116\n",
      "Epoch [1/1], Step [2969/7635], Loss: 4.0852\n",
      "Epoch [1/1], Step [2970/7635], Loss: 4.1773\n",
      "Epoch [1/1], Step [2971/7635], Loss: 4.1418\n",
      "Epoch [1/1], Step [2972/7635], Loss: 4.1966\n",
      "Epoch [1/1], Step [2973/7635], Loss: 4.0819\n",
      "Epoch [1/1], Step [2974/7635], Loss: 4.0751\n",
      "Epoch [1/1], Step [2975/7635], Loss: 4.1829\n",
      "Epoch [1/1], Step [2976/7635], Loss: 4.0802\n",
      "Epoch [1/1], Step [2977/7635], Loss: 4.2442\n",
      "Epoch [1/1], Step [2978/7635], Loss: 4.1624\n",
      "Epoch [1/1], Step [2979/7635], Loss: 4.0812\n",
      "Epoch [1/1], Step [2980/7635], Loss: 4.0155\n",
      "Epoch [1/1], Step [2981/7635], Loss: 4.0990\n",
      "Epoch [1/1], Step [2982/7635], Loss: 4.1373\n",
      "Epoch [1/1], Step [2983/7635], Loss: 4.1212\n",
      "Epoch [1/1], Step [2984/7635], Loss: 4.0981\n",
      "Epoch [1/1], Step [2985/7635], Loss: 4.1224\n",
      "Epoch [1/1], Step [2986/7635], Loss: 4.2044\n",
      "Epoch [1/1], Step [2987/7635], Loss: 4.1821\n",
      "Epoch [1/1], Step [2988/7635], Loss: 4.1599\n",
      "Epoch [1/1], Step [2989/7635], Loss: 4.1875\n",
      "Epoch [1/1], Step [2990/7635], Loss: 4.1815\n",
      "Epoch [1/1], Step [2991/7635], Loss: 4.1591\n",
      "Epoch [1/1], Step [2992/7635], Loss: 4.1712\n",
      "Epoch [1/1], Step [2993/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [2994/7635], Loss: 4.0690\n",
      "Epoch [1/1], Step [2995/7635], Loss: 4.1241\n",
      "Epoch [1/1], Step [2996/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [2997/7635], Loss: 4.0818\n",
      "Epoch [1/1], Step [2998/7635], Loss: 4.0861\n",
      "Epoch [1/1], Step [2999/7635], Loss: 4.1956\n",
      "Epoch [1/1], Step [3000/7635], Loss: 4.1207\n",
      "Epoch [1/1], Step [3001/7635], Loss: 4.0761\n",
      "Epoch [1/1], Step [3002/7635], Loss: 4.0477\n",
      "Epoch [1/1], Step [3003/7635], Loss: 4.1442\n",
      "Epoch [1/1], Step [3004/7635], Loss: 4.1617\n",
      "Epoch [1/1], Step [3005/7635], Loss: 4.0811\n",
      "Epoch [1/1], Step [3006/7635], Loss: 4.2281\n",
      "Epoch [1/1], Step [3007/7635], Loss: 4.0809\n",
      "Epoch [1/1], Step [3008/7635], Loss: 4.1988\n",
      "Epoch [1/1], Step [3009/7635], Loss: 4.0946\n",
      "Epoch [1/1], Step [3010/7635], Loss: 4.1534\n",
      "Epoch [1/1], Step [3011/7635], Loss: 4.1262\n",
      "Epoch [1/1], Step [3012/7635], Loss: 4.1830\n",
      "Epoch [1/1], Step [3013/7635], Loss: 4.0679\n",
      "Epoch [1/1], Step [3014/7635], Loss: 4.0837\n",
      "Epoch [1/1], Step [3015/7635], Loss: 4.0635\n",
      "Epoch [1/1], Step [3016/7635], Loss: 4.1495\n",
      "Epoch [1/1], Step [3017/7635], Loss: 4.0832\n",
      "Epoch [1/1], Step [3018/7635], Loss: 4.2117\n",
      "Epoch [1/1], Step [3019/7635], Loss: 4.1819\n",
      "Epoch [1/1], Step [3020/7635], Loss: 4.1399\n",
      "Epoch [1/1], Step [3021/7635], Loss: 4.1924\n",
      "Epoch [1/1], Step [3022/7635], Loss: 4.1664\n",
      "Epoch [1/1], Step [3023/7635], Loss: 4.1330\n",
      "Epoch [1/1], Step [3024/7635], Loss: 4.2325\n",
      "Epoch [1/1], Step [3025/7635], Loss: 4.1310\n",
      "Epoch [1/1], Step [3026/7635], Loss: 4.1414\n",
      "Epoch [1/1], Step [3027/7635], Loss: 4.1286\n",
      "Epoch [1/1], Step [3028/7635], Loss: 4.1228\n",
      "Epoch [1/1], Step [3029/7635], Loss: 4.1837\n",
      "Epoch [1/1], Step [3030/7635], Loss: 4.1682\n",
      "Epoch [1/1], Step [3031/7635], Loss: 4.1011\n",
      "Epoch [1/1], Step [3032/7635], Loss: 4.2619\n",
      "Epoch [1/1], Step [3033/7635], Loss: 4.1336\n",
      "Epoch [1/1], Step [3034/7635], Loss: 4.0654\n",
      "Epoch [1/1], Step [3035/7635], Loss: 4.1647\n",
      "Epoch [1/1], Step [3036/7635], Loss: 4.1853\n",
      "Epoch [1/1], Step [3037/7635], Loss: 4.1178\n",
      "Epoch [1/1], Step [3038/7635], Loss: 4.0793\n",
      "Epoch [1/1], Step [3039/7635], Loss: 4.1271\n",
      "Epoch [1/1], Step [3040/7635], Loss: 4.0754\n",
      "Epoch [1/1], Step [3041/7635], Loss: 4.1698\n",
      "Epoch [1/1], Step [3042/7635], Loss: 4.1208\n",
      "Epoch [1/1], Step [3043/7635], Loss: 4.0792\n",
      "Epoch [1/1], Step [3044/7635], Loss: 4.1643\n",
      "Epoch [1/1], Step [3045/7635], Loss: 4.0365\n",
      "Epoch [1/1], Step [3046/7635], Loss: 4.1580\n",
      "Epoch [1/1], Step [3047/7635], Loss: 4.0653\n",
      "Epoch [1/1], Step [3048/7635], Loss: 4.1585\n",
      "Epoch [1/1], Step [3049/7635], Loss: 4.1664\n",
      "Epoch [1/1], Step [3050/7635], Loss: 4.0953\n",
      "Epoch [1/1], Step [3051/7635], Loss: 4.1227\n",
      "Epoch [1/1], Step [3052/7635], Loss: 4.1179\n",
      "Epoch [1/1], Step [3053/7635], Loss: 4.0502\n",
      "Epoch [1/1], Step [3054/7635], Loss: 4.2314\n",
      "Epoch [1/1], Step [3055/7635], Loss: 4.0795\n",
      "Epoch [1/1], Step [3056/7635], Loss: 4.1554\n",
      "Epoch [1/1], Step [3057/7635], Loss: 4.2457\n",
      "Epoch [1/1], Step [3058/7635], Loss: 4.1636\n",
      "Epoch [1/1], Step [3059/7635], Loss: 4.1859\n",
      "Epoch [1/1], Step [3060/7635], Loss: 4.2367\n",
      "Epoch [1/1], Step [3061/7635], Loss: 4.1215\n",
      "Epoch [1/1], Step [3062/7635], Loss: 4.1605\n",
      "Epoch [1/1], Step [3063/7635], Loss: 4.1841\n",
      "Epoch [1/1], Step [3064/7635], Loss: 4.1867\n",
      "Epoch [1/1], Step [3065/7635], Loss: 4.1543\n",
      "Epoch [1/1], Step [3066/7635], Loss: 4.1280\n",
      "Epoch [1/1], Step [3067/7635], Loss: 4.1093\n",
      "Epoch [1/1], Step [3068/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [3069/7635], Loss: 4.0894\n",
      "Epoch [1/1], Step [3070/7635], Loss: 4.1370\n",
      "Epoch [1/1], Step [3071/7635], Loss: 4.1175\n",
      "Epoch [1/1], Step [3072/7635], Loss: 4.1859\n",
      "Epoch [1/1], Step [3073/7635], Loss: 4.0874\n",
      "Epoch [1/1], Step [3074/7635], Loss: 4.1411\n",
      "Epoch [1/1], Step [3075/7635], Loss: 4.0497\n",
      "Epoch [1/1], Step [3076/7635], Loss: 4.1663\n",
      "Epoch [1/1], Step [3077/7635], Loss: 4.0884\n",
      "Epoch [1/1], Step [3078/7635], Loss: 4.0862\n",
      "Epoch [1/1], Step [3079/7635], Loss: 4.2621\n",
      "Epoch [1/1], Step [3080/7635], Loss: 4.1154\n",
      "Epoch [1/1], Step [3081/7635], Loss: 4.2064\n",
      "Epoch [1/1], Step [3082/7635], Loss: 4.1661\n",
      "Epoch [1/1], Step [3083/7635], Loss: 4.0860\n",
      "Epoch [1/1], Step [3084/7635], Loss: 4.1459\n",
      "Epoch [1/1], Step [3085/7635], Loss: 4.1427\n",
      "Epoch [1/1], Step [3086/7635], Loss: 4.1459\n",
      "Epoch [1/1], Step [3087/7635], Loss: 4.1151\n",
      "Epoch [1/1], Step [3088/7635], Loss: 4.1177\n",
      "Epoch [1/1], Step [3089/7635], Loss: 4.0993\n",
      "Epoch [1/1], Step [3090/7635], Loss: 4.0889\n",
      "Epoch [1/1], Step [3091/7635], Loss: 4.0669\n",
      "Epoch [1/1], Step [3092/7635], Loss: 4.1749\n",
      "Epoch [1/1], Step [3093/7635], Loss: 4.1726\n",
      "Epoch [1/1], Step [3094/7635], Loss: 4.1702\n",
      "Epoch [1/1], Step [3095/7635], Loss: 4.0589\n",
      "Epoch [1/1], Step [3096/7635], Loss: 4.0697\n",
      "Epoch [1/1], Step [3097/7635], Loss: 4.2091\n",
      "Epoch [1/1], Step [3098/7635], Loss: 4.0896\n",
      "Epoch [1/1], Step [3099/7635], Loss: 4.1405\n",
      "Epoch [1/1], Step [3100/7635], Loss: 4.0385\n",
      "Epoch [1/1], Step [3101/7635], Loss: 4.2483\n",
      "Epoch [1/1], Step [3102/7635], Loss: 4.2273\n",
      "Epoch [1/1], Step [3103/7635], Loss: 4.1263\n",
      "Epoch [1/1], Step [3104/7635], Loss: 4.1520\n",
      "Epoch [1/1], Step [3105/7635], Loss: 4.1913\n",
      "Epoch [1/1], Step [3106/7635], Loss: 4.2221\n",
      "Epoch [1/1], Step [3107/7635], Loss: 4.1171\n",
      "Epoch [1/1], Step [3108/7635], Loss: 4.2075\n",
      "Epoch [1/1], Step [3109/7635], Loss: 4.2129\n",
      "Epoch [1/1], Step [3110/7635], Loss: 4.0622\n",
      "Epoch [1/1], Step [3111/7635], Loss: 4.1703\n",
      "Epoch [1/1], Step [3112/7635], Loss: 4.0619\n",
      "Epoch [1/1], Step [3113/7635], Loss: 4.1911\n",
      "Epoch [1/1], Step [3114/7635], Loss: 4.1584\n",
      "Epoch [1/1], Step [3115/7635], Loss: 4.2093\n",
      "Epoch [1/1], Step [3116/7635], Loss: 4.0941\n",
      "Epoch [1/1], Step [3117/7635], Loss: 4.0737\n",
      "Epoch [1/1], Step [3118/7635], Loss: 4.0861\n",
      "Epoch [1/1], Step [3119/7635], Loss: 4.1601\n",
      "Epoch [1/1], Step [3120/7635], Loss: 4.0484\n",
      "Epoch [1/1], Step [3121/7635], Loss: 4.1117\n",
      "Epoch [1/1], Step [3122/7635], Loss: 4.1361\n",
      "Epoch [1/1], Step [3123/7635], Loss: 4.0714\n",
      "Epoch [1/1], Step [3124/7635], Loss: 4.0724\n",
      "Epoch [1/1], Step [3125/7635], Loss: 4.1381\n",
      "Epoch [1/1], Step [3126/7635], Loss: 4.1189\n",
      "Epoch [1/1], Step [3127/7635], Loss: 4.1812\n",
      "Epoch [1/1], Step [3128/7635], Loss: 4.1077\n",
      "Epoch [1/1], Step [3129/7635], Loss: 4.1246\n",
      "Epoch [1/1], Step [3130/7635], Loss: 4.1815\n",
      "Epoch [1/1], Step [3131/7635], Loss: 4.1930\n",
      "Epoch [1/1], Step [3132/7635], Loss: 4.2304\n",
      "Epoch [1/1], Step [3133/7635], Loss: 4.1493\n",
      "Epoch [1/1], Step [3134/7635], Loss: 4.2531\n",
      "Epoch [1/1], Step [3135/7635], Loss: 4.1922\n",
      "Epoch [1/1], Step [3136/7635], Loss: 4.0162\n",
      "Epoch [1/1], Step [3137/7635], Loss: 4.0683\n",
      "Epoch [1/1], Step [3138/7635], Loss: 4.0733\n",
      "Epoch [1/1], Step [3139/7635], Loss: 4.0817\n",
      "Epoch [1/1], Step [3140/7635], Loss: 4.1056\n",
      "Epoch [1/1], Step [3141/7635], Loss: 4.1538\n",
      "Epoch [1/1], Step [3142/7635], Loss: 4.1509\n",
      "Epoch [1/1], Step [3143/7635], Loss: 4.1062\n",
      "Epoch [1/1], Step [3144/7635], Loss: 4.1111\n",
      "Epoch [1/1], Step [3145/7635], Loss: 4.2062\n",
      "Epoch [1/1], Step [3146/7635], Loss: 4.1693\n",
      "Epoch [1/1], Step [3147/7635], Loss: 4.1606\n",
      "Epoch [1/1], Step [3148/7635], Loss: 4.1855\n",
      "Epoch [1/1], Step [3149/7635], Loss: 4.1473\n",
      "Epoch [1/1], Step [3150/7635], Loss: 4.0993\n",
      "Epoch [1/1], Step [3151/7635], Loss: 4.1563\n",
      "Epoch [1/1], Step [3152/7635], Loss: 4.1172\n",
      "Epoch [1/1], Step [3153/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [3154/7635], Loss: 4.0682\n",
      "Epoch [1/1], Step [3155/7635], Loss: 4.0632\n",
      "Epoch [1/1], Step [3156/7635], Loss: 4.1141\n",
      "Epoch [1/1], Step [3157/7635], Loss: 4.1823\n",
      "Epoch [1/1], Step [3158/7635], Loss: 3.9966\n",
      "Epoch [1/1], Step [3159/7635], Loss: 4.0781\n",
      "Epoch [1/1], Step [3160/7635], Loss: 4.0786\n",
      "Epoch [1/1], Step [3161/7635], Loss: 4.0696\n",
      "Epoch [1/1], Step [3162/7635], Loss: 4.0146\n",
      "Epoch [1/1], Step [3163/7635], Loss: 4.1022\n",
      "Epoch [1/1], Step [3164/7635], Loss: 4.1389\n",
      "Epoch [1/1], Step [3165/7635], Loss: 4.2086\n",
      "Epoch [1/1], Step [3166/7635], Loss: 4.0845\n",
      "Epoch [1/1], Step [3167/7635], Loss: 4.1909\n",
      "Epoch [1/1], Step [3168/7635], Loss: 4.1760\n",
      "Epoch [1/1], Step [3169/7635], Loss: 4.1452\n",
      "Epoch [1/1], Step [3170/7635], Loss: 4.1521\n",
      "Epoch [1/1], Step [3171/7635], Loss: 4.1100\n",
      "Epoch [1/1], Step [3172/7635], Loss: 4.1919\n",
      "Epoch [1/1], Step [3173/7635], Loss: 4.0438\n",
      "Epoch [1/1], Step [3174/7635], Loss: 4.1233\n",
      "Epoch [1/1], Step [3175/7635], Loss: 4.0662\n",
      "Epoch [1/1], Step [3176/7635], Loss: 4.1686\n",
      "Epoch [1/1], Step [3177/7635], Loss: 4.1388\n",
      "Epoch [1/1], Step [3178/7635], Loss: 4.0268\n",
      "Epoch [1/1], Step [3179/7635], Loss: 4.1913\n",
      "Epoch [1/1], Step [3180/7635], Loss: 4.1049\n",
      "Epoch [1/1], Step [3181/7635], Loss: 4.2030\n",
      "Epoch [1/1], Step [3182/7635], Loss: 4.1648\n",
      "Epoch [1/1], Step [3183/7635], Loss: 4.0431\n",
      "Epoch [1/1], Step [3184/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [3185/7635], Loss: 4.1524\n",
      "Epoch [1/1], Step [3186/7635], Loss: 4.1584\n",
      "Epoch [1/1], Step [3187/7635], Loss: 4.1314\n",
      "Epoch [1/1], Step [3188/7635], Loss: 4.0849\n",
      "Epoch [1/1], Step [3189/7635], Loss: 4.1323\n",
      "Epoch [1/1], Step [3190/7635], Loss: 4.1119\n",
      "Epoch [1/1], Step [3191/7635], Loss: 4.1310\n",
      "Epoch [1/1], Step [3192/7635], Loss: 4.1387\n",
      "Epoch [1/1], Step [3193/7635], Loss: 4.0875\n",
      "Epoch [1/1], Step [3194/7635], Loss: 4.1098\n",
      "Epoch [1/1], Step [3195/7635], Loss: 4.1363\n",
      "Epoch [1/1], Step [3196/7635], Loss: 4.0761\n",
      "Epoch [1/1], Step [3197/7635], Loss: 4.0324\n",
      "Epoch [1/1], Step [3198/7635], Loss: 4.0953\n",
      "Epoch [1/1], Step [3199/7635], Loss: 4.1017\n",
      "Epoch [1/1], Step [3200/7635], Loss: 4.1627\n",
      "Epoch [1/1], Step [3201/7635], Loss: 4.1109\n",
      "Epoch [1/1], Step [3202/7635], Loss: 4.0533\n",
      "Epoch [1/1], Step [3203/7635], Loss: 4.1034\n",
      "Epoch [1/1], Step [3204/7635], Loss: 4.0871\n",
      "Epoch [1/1], Step [3205/7635], Loss: 4.1126\n",
      "Epoch [1/1], Step [3206/7635], Loss: 4.0855\n",
      "Epoch [1/1], Step [3207/7635], Loss: 4.1209\n",
      "Epoch [1/1], Step [3208/7635], Loss: 4.2363\n",
      "Epoch [1/1], Step [3209/7635], Loss: 4.1380\n",
      "Epoch [1/1], Step [3210/7635], Loss: 4.1310\n",
      "Epoch [1/1], Step [3211/7635], Loss: 4.0622\n",
      "Epoch [1/1], Step [3212/7635], Loss: 4.1364\n",
      "Epoch [1/1], Step [3213/7635], Loss: 4.0766\n",
      "Epoch [1/1], Step [3214/7635], Loss: 4.1735\n",
      "Epoch [1/1], Step [3215/7635], Loss: 4.1035\n",
      "Epoch [1/1], Step [3216/7635], Loss: 4.1341\n",
      "Epoch [1/1], Step [3217/7635], Loss: 4.1505\n",
      "Epoch [1/1], Step [3218/7635], Loss: 4.0679\n",
      "Epoch [1/1], Step [3219/7635], Loss: 4.0776\n",
      "Epoch [1/1], Step [3220/7635], Loss: 4.0935\n",
      "Epoch [1/1], Step [3221/7635], Loss: 4.1503\n",
      "Epoch [1/1], Step [3222/7635], Loss: 4.1810\n",
      "Epoch [1/1], Step [3223/7635], Loss: 4.1552\n",
      "Epoch [1/1], Step [3224/7635], Loss: 4.1034\n",
      "Epoch [1/1], Step [3225/7635], Loss: 4.1099\n",
      "Epoch [1/1], Step [3226/7635], Loss: 4.1224\n",
      "Epoch [1/1], Step [3227/7635], Loss: 4.1466\n",
      "Epoch [1/1], Step [3228/7635], Loss: 4.1080\n",
      "Epoch [1/1], Step [3229/7635], Loss: 4.1131\n",
      "Epoch [1/1], Step [3230/7635], Loss: 4.1044\n",
      "Epoch [1/1], Step [3231/7635], Loss: 3.9945\n",
      "Epoch [1/1], Step [3232/7635], Loss: 4.1135\n",
      "Epoch [1/1], Step [3233/7635], Loss: 4.1456\n",
      "Epoch [1/1], Step [3234/7635], Loss: 4.0490\n",
      "Epoch [1/1], Step [3235/7635], Loss: 4.1092\n",
      "Epoch [1/1], Step [3236/7635], Loss: 4.1257\n",
      "Epoch [1/1], Step [3237/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [3238/7635], Loss: 4.1679\n",
      "Epoch [1/1], Step [3239/7635], Loss: 4.0432\n",
      "Epoch [1/1], Step [3240/7635], Loss: 4.1615\n",
      "Epoch [1/1], Step [3241/7635], Loss: 4.1187\n",
      "Epoch [1/1], Step [3242/7635], Loss: 4.1594\n",
      "Epoch [1/1], Step [3243/7635], Loss: 4.0655\n",
      "Epoch [1/1], Step [3244/7635], Loss: 4.1155\n",
      "Epoch [1/1], Step [3245/7635], Loss: 4.1318\n",
      "Epoch [1/1], Step [3246/7635], Loss: 4.0790\n",
      "Epoch [1/1], Step [3247/7635], Loss: 4.1038\n",
      "Epoch [1/1], Step [3248/7635], Loss: 4.1981\n",
      "Epoch [1/1], Step [3249/7635], Loss: 4.0465\n",
      "Epoch [1/1], Step [3250/7635], Loss: 4.2161\n",
      "Epoch [1/1], Step [3251/7635], Loss: 4.1501\n",
      "Epoch [1/1], Step [3252/7635], Loss: 4.0989\n",
      "Epoch [1/1], Step [3253/7635], Loss: 4.0986\n",
      "Epoch [1/1], Step [3254/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [3255/7635], Loss: 3.9723\n",
      "Epoch [1/1], Step [3256/7635], Loss: 4.0924\n",
      "Epoch [1/1], Step [3257/7635], Loss: 4.1728\n",
      "Epoch [1/1], Step [3258/7635], Loss: 4.0892\n",
      "Epoch [1/1], Step [3259/7635], Loss: 4.1644\n",
      "Epoch [1/1], Step [3260/7635], Loss: 4.1551\n",
      "Epoch [1/1], Step [3261/7635], Loss: 4.1491\n",
      "Epoch [1/1], Step [3262/7635], Loss: 4.1290\n",
      "Epoch [1/1], Step [3263/7635], Loss: 4.2026\n",
      "Epoch [1/1], Step [3264/7635], Loss: 4.1480\n",
      "Epoch [1/1], Step [3265/7635], Loss: 4.1201\n",
      "Epoch [1/1], Step [3266/7635], Loss: 4.1213\n",
      "Epoch [1/1], Step [3267/7635], Loss: 4.0620\n",
      "Epoch [1/1], Step [3268/7635], Loss: 4.1038\n",
      "Epoch [1/1], Step [3269/7635], Loss: 4.0407\n",
      "Epoch [1/1], Step [3270/7635], Loss: 4.2568\n",
      "Epoch [1/1], Step [3271/7635], Loss: 4.0960\n",
      "Epoch [1/1], Step [3272/7635], Loss: 4.0622\n",
      "Epoch [1/1], Step [3273/7635], Loss: 4.0397\n",
      "Epoch [1/1], Step [3274/7635], Loss: 4.1264\n",
      "Epoch [1/1], Step [3275/7635], Loss: 4.1227\n",
      "Epoch [1/1], Step [3276/7635], Loss: 4.0212\n",
      "Epoch [1/1], Step [3277/7635], Loss: 4.0897\n",
      "Epoch [1/1], Step [3278/7635], Loss: 4.0706\n",
      "Epoch [1/1], Step [3279/7635], Loss: 4.1767\n",
      "Epoch [1/1], Step [3280/7635], Loss: 4.0822\n",
      "Epoch [1/1], Step [3281/7635], Loss: 3.9638\n",
      "Epoch [1/1], Step [3282/7635], Loss: 4.1476\n",
      "Epoch [1/1], Step [3283/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [3284/7635], Loss: 4.0510\n",
      "Epoch [1/1], Step [3285/7635], Loss: 4.1496\n",
      "Epoch [1/1], Step [3286/7635], Loss: 4.0732\n",
      "Epoch [1/1], Step [3287/7635], Loss: 4.0734\n",
      "Epoch [1/1], Step [3288/7635], Loss: 4.0952\n",
      "Epoch [1/1], Step [3289/7635], Loss: 4.0773\n",
      "Epoch [1/1], Step [3290/7635], Loss: 4.1397\n",
      "Epoch [1/1], Step [3291/7635], Loss: 4.1004\n",
      "Epoch [1/1], Step [3292/7635], Loss: 4.1194\n",
      "Epoch [1/1], Step [3293/7635], Loss: 4.1474\n",
      "Epoch [1/1], Step [3294/7635], Loss: 3.9956\n",
      "Epoch [1/1], Step [3295/7635], Loss: 4.1377\n",
      "Epoch [1/1], Step [3296/7635], Loss: 4.1721\n",
      "Epoch [1/1], Step [3297/7635], Loss: 4.1469\n",
      "Epoch [1/1], Step [3298/7635], Loss: 4.1243\n",
      "Epoch [1/1], Step [3299/7635], Loss: 4.0500\n",
      "Epoch [1/1], Step [3300/7635], Loss: 4.1090\n",
      "Epoch [1/1], Step [3301/7635], Loss: 4.0915\n",
      "Epoch [1/1], Step [3302/7635], Loss: 4.0785\n",
      "Epoch [1/1], Step [3303/7635], Loss: 4.1181\n",
      "Epoch [1/1], Step [3304/7635], Loss: 4.1076\n",
      "Epoch [1/1], Step [3305/7635], Loss: 4.0619\n",
      "Epoch [1/1], Step [3306/7635], Loss: 4.0835\n",
      "Epoch [1/1], Step [3307/7635], Loss: 4.0925\n",
      "Epoch [1/1], Step [3308/7635], Loss: 4.2448\n",
      "Epoch [1/1], Step [3309/7635], Loss: 4.0304\n",
      "Epoch [1/1], Step [3310/7635], Loss: 4.1323\n",
      "Epoch [1/1], Step [3311/7635], Loss: 4.1710\n",
      "Epoch [1/1], Step [3312/7635], Loss: 4.0267\n",
      "Epoch [1/1], Step [3313/7635], Loss: 4.2069\n",
      "Epoch [1/1], Step [3314/7635], Loss: 4.0801\n",
      "Epoch [1/1], Step [3315/7635], Loss: 4.0661\n",
      "Epoch [1/1], Step [3316/7635], Loss: 4.0843\n",
      "Epoch [1/1], Step [3317/7635], Loss: 4.0986\n",
      "Epoch [1/1], Step [3318/7635], Loss: 4.1555\n",
      "Epoch [1/1], Step [3319/7635], Loss: 4.0753\n",
      "Epoch [1/1], Step [3320/7635], Loss: 4.1546\n",
      "Epoch [1/1], Step [3321/7635], Loss: 4.1201\n",
      "Epoch [1/1], Step [3322/7635], Loss: 4.1361\n",
      "Epoch [1/1], Step [3323/7635], Loss: 4.1591\n",
      "Epoch [1/1], Step [3324/7635], Loss: 4.1054\n",
      "Epoch [1/1], Step [3325/7635], Loss: 4.1299\n",
      "Epoch [1/1], Step [3326/7635], Loss: 4.1838\n",
      "Epoch [1/1], Step [3327/7635], Loss: 4.0693\n",
      "Epoch [1/1], Step [3328/7635], Loss: 4.1155\n",
      "Epoch [1/1], Step [3329/7635], Loss: 4.1433\n",
      "Epoch [1/1], Step [3330/7635], Loss: 4.1342\n",
      "Epoch [1/1], Step [3331/7635], Loss: 4.0873\n",
      "Epoch [1/1], Step [3332/7635], Loss: 4.0987\n",
      "Epoch [1/1], Step [3333/7635], Loss: 4.1002\n",
      "Epoch [1/1], Step [3334/7635], Loss: 4.0817\n",
      "Epoch [1/1], Step [3335/7635], Loss: 4.0353\n",
      "Epoch [1/1], Step [3336/7635], Loss: 4.2110\n",
      "Epoch [1/1], Step [3337/7635], Loss: 4.1516\n",
      "Epoch [1/1], Step [3338/7635], Loss: 4.1268\n",
      "Epoch [1/1], Step [3339/7635], Loss: 4.0871\n",
      "Epoch [1/1], Step [3340/7635], Loss: 4.0413\n",
      "Epoch [1/1], Step [3341/7635], Loss: 4.0684\n",
      "Epoch [1/1], Step [3342/7635], Loss: 4.0807\n",
      "Epoch [1/1], Step [3343/7635], Loss: 4.0876\n",
      "Epoch [1/1], Step [3344/7635], Loss: 4.2077\n",
      "Epoch [1/1], Step [3345/7635], Loss: 4.0247\n",
      "Epoch [1/1], Step [3346/7635], Loss: 4.1049\n",
      "Epoch [1/1], Step [3347/7635], Loss: 4.1010\n",
      "Epoch [1/1], Step [3348/7635], Loss: 4.1050\n",
      "Epoch [1/1], Step [3349/7635], Loss: 4.1322\n",
      "Epoch [1/1], Step [3350/7635], Loss: 4.1048\n",
      "Epoch [1/1], Step [3351/7635], Loss: 4.0508\n",
      "Epoch [1/1], Step [3352/7635], Loss: 4.1158\n",
      "Epoch [1/1], Step [3353/7635], Loss: 4.1169\n",
      "Epoch [1/1], Step [3354/7635], Loss: 4.1720\n",
      "Epoch [1/1], Step [3355/7635], Loss: 4.2173\n",
      "Epoch [1/1], Step [3356/7635], Loss: 4.0769\n",
      "Epoch [1/1], Step [3357/7635], Loss: 4.0942\n",
      "Epoch [1/1], Step [3358/7635], Loss: 4.0646\n",
      "Epoch [1/1], Step [3359/7635], Loss: 4.1459\n",
      "Epoch [1/1], Step [3360/7635], Loss: 4.1158\n",
      "Epoch [1/1], Step [3361/7635], Loss: 4.0992\n",
      "Epoch [1/1], Step [3362/7635], Loss: 4.1301\n",
      "Epoch [1/1], Step [3363/7635], Loss: 4.1241\n",
      "Epoch [1/1], Step [3364/7635], Loss: 4.2306\n",
      "Epoch [1/1], Step [3365/7635], Loss: 4.0829\n",
      "Epoch [1/1], Step [3366/7635], Loss: 4.1187\n",
      "Epoch [1/1], Step [3367/7635], Loss: 4.0844\n",
      "Epoch [1/1], Step [3368/7635], Loss: 4.0661\n",
      "Epoch [1/1], Step [3369/7635], Loss: 4.1126\n",
      "Epoch [1/1], Step [3370/7635], Loss: 4.1689\n",
      "Epoch [1/1], Step [3371/7635], Loss: 4.0571\n",
      "Epoch [1/1], Step [3372/7635], Loss: 4.0856\n",
      "Epoch [1/1], Step [3373/7635], Loss: 4.1381\n",
      "Epoch [1/1], Step [3374/7635], Loss: 4.2035\n",
      "Epoch [1/1], Step [3375/7635], Loss: 4.1047\n",
      "Epoch [1/1], Step [3376/7635], Loss: 4.1263\n",
      "Epoch [1/1], Step [3377/7635], Loss: 4.1634\n",
      "Epoch [1/1], Step [3378/7635], Loss: 4.1335\n",
      "Epoch [1/1], Step [3379/7635], Loss: 4.1686\n",
      "Epoch [1/1], Step [3380/7635], Loss: 4.0944\n",
      "Epoch [1/1], Step [3381/7635], Loss: 4.1842\n",
      "Epoch [1/1], Step [3382/7635], Loss: 4.1136\n",
      "Epoch [1/1], Step [3383/7635], Loss: 4.1097\n",
      "Epoch [1/1], Step [3384/7635], Loss: 4.1549\n",
      "Epoch [1/1], Step [3385/7635], Loss: 4.1266\n",
      "Epoch [1/1], Step [3386/7635], Loss: 4.1002\n",
      "Epoch [1/1], Step [3387/7635], Loss: 4.1541\n",
      "Epoch [1/1], Step [3388/7635], Loss: 4.1899\n",
      "Epoch [1/1], Step [3389/7635], Loss: 4.0036\n",
      "Epoch [1/1], Step [3390/7635], Loss: 4.0886\n",
      "Epoch [1/1], Step [3391/7635], Loss: 4.0271\n",
      "Epoch [1/1], Step [3392/7635], Loss: 4.0905\n",
      "Epoch [1/1], Step [3393/7635], Loss: 4.1380\n",
      "Epoch [1/1], Step [3394/7635], Loss: 4.0929\n",
      "Epoch [1/1], Step [3395/7635], Loss: 4.1765\n",
      "Epoch [1/1], Step [3396/7635], Loss: 4.1545\n",
      "Epoch [1/1], Step [3397/7635], Loss: 3.9997\n",
      "Epoch [1/1], Step [3398/7635], Loss: 4.0633\n",
      "Epoch [1/1], Step [3399/7635], Loss: 4.1059\n",
      "Epoch [1/1], Step [3400/7635], Loss: 4.1055\n",
      "Epoch [1/1], Step [3401/7635], Loss: 4.1022\n",
      "Epoch [1/1], Step [3402/7635], Loss: 4.1677\n",
      "Epoch [1/1], Step [3403/7635], Loss: 4.1224\n",
      "Epoch [1/1], Step [3404/7635], Loss: 4.1130\n",
      "Epoch [1/1], Step [3405/7635], Loss: 4.1001\n",
      "Epoch [1/1], Step [3406/7635], Loss: 4.0911\n",
      "Epoch [1/1], Step [3407/7635], Loss: 4.0791\n",
      "Epoch [1/1], Step [3408/7635], Loss: 4.1177\n",
      "Epoch [1/1], Step [3409/7635], Loss: 4.1336\n",
      "Epoch [1/1], Step [3410/7635], Loss: 4.0288\n",
      "Epoch [1/1], Step [3411/7635], Loss: 4.1214\n",
      "Epoch [1/1], Step [3412/7635], Loss: 4.1394\n",
      "Epoch [1/1], Step [3413/7635], Loss: 4.1323\n",
      "Epoch [1/1], Step [3414/7635], Loss: 4.0857\n",
      "Epoch [1/1], Step [3415/7635], Loss: 4.1298\n",
      "Epoch [1/1], Step [3416/7635], Loss: 4.0468\n",
      "Epoch [1/1], Step [3417/7635], Loss: 4.0612\n",
      "Epoch [1/1], Step [3418/7635], Loss: 4.1236\n",
      "Epoch [1/1], Step [3419/7635], Loss: 4.1441\n",
      "Epoch [1/1], Step [3420/7635], Loss: 4.1137\n",
      "Epoch [1/1], Step [3421/7635], Loss: 4.1793\n",
      "Epoch [1/1], Step [3422/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [3423/7635], Loss: 4.1308\n",
      "Epoch [1/1], Step [3424/7635], Loss: 4.1492\n",
      "Epoch [1/1], Step [3425/7635], Loss: 4.1261\n",
      "Epoch [1/1], Step [3426/7635], Loss: 4.1666\n",
      "Epoch [1/1], Step [3427/7635], Loss: 4.0928\n",
      "Epoch [1/1], Step [3428/7635], Loss: 4.0622\n",
      "Epoch [1/1], Step [3429/7635], Loss: 4.0698\n",
      "Epoch [1/1], Step [3430/7635], Loss: 4.0941\n",
      "Epoch [1/1], Step [3431/7635], Loss: 4.1993\n",
      "Epoch [1/1], Step [3432/7635], Loss: 4.1663\n",
      "Epoch [1/1], Step [3433/7635], Loss: 4.0282\n",
      "Epoch [1/1], Step [3434/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [3435/7635], Loss: 4.1647\n",
      "Epoch [1/1], Step [3436/7635], Loss: 4.0856\n",
      "Epoch [1/1], Step [3437/7635], Loss: 4.1318\n",
      "Epoch [1/1], Step [3438/7635], Loss: 4.0990\n",
      "Epoch [1/1], Step [3439/7635], Loss: 4.0880\n",
      "Epoch [1/1], Step [3440/7635], Loss: 4.1783\n",
      "Epoch [1/1], Step [3441/7635], Loss: 4.1373\n",
      "Epoch [1/1], Step [3442/7635], Loss: 4.1114\n",
      "Epoch [1/1], Step [3443/7635], Loss: 4.0294\n",
      "Epoch [1/1], Step [3444/7635], Loss: 4.1379\n",
      "Epoch [1/1], Step [3445/7635], Loss: 4.1215\n",
      "Epoch [1/1], Step [3446/7635], Loss: 4.0489\n",
      "Epoch [1/1], Step [3447/7635], Loss: 4.0706\n",
      "Epoch [1/1], Step [3448/7635], Loss: 4.1472\n",
      "Epoch [1/1], Step [3449/7635], Loss: 4.1554\n",
      "Epoch [1/1], Step [3450/7635], Loss: 4.0760\n",
      "Epoch [1/1], Step [3451/7635], Loss: 4.1082\n",
      "Epoch [1/1], Step [3452/7635], Loss: 4.1551\n",
      "Epoch [1/1], Step [3453/7635], Loss: 4.0912\n",
      "Epoch [1/1], Step [3454/7635], Loss: 4.1554\n",
      "Epoch [1/1], Step [3455/7635], Loss: 4.1506\n",
      "Epoch [1/1], Step [3456/7635], Loss: 4.1410\n",
      "Epoch [1/1], Step [3457/7635], Loss: 4.1642\n",
      "Epoch [1/1], Step [3458/7635], Loss: 4.0575\n",
      "Epoch [1/1], Step [3459/7635], Loss: 4.0747\n",
      "Epoch [1/1], Step [3460/7635], Loss: 4.0598\n",
      "Epoch [1/1], Step [3461/7635], Loss: 4.0865\n",
      "Epoch [1/1], Step [3462/7635], Loss: 4.0720\n",
      "Epoch [1/1], Step [3463/7635], Loss: 4.0892\n",
      "Epoch [1/1], Step [3464/7635], Loss: 4.0927\n",
      "Epoch [1/1], Step [3465/7635], Loss: 4.1050\n",
      "Epoch [1/1], Step [3466/7635], Loss: 4.0542\n",
      "Epoch [1/1], Step [3467/7635], Loss: 4.0593\n",
      "Epoch [1/1], Step [3468/7635], Loss: 4.0611\n",
      "Epoch [1/1], Step [3469/7635], Loss: 4.1467\n",
      "Epoch [1/1], Step [3470/7635], Loss: 4.1348\n",
      "Epoch [1/1], Step [3471/7635], Loss: 4.0530\n",
      "Epoch [1/1], Step [3472/7635], Loss: 3.9955\n",
      "Epoch [1/1], Step [3473/7635], Loss: 4.0931\n",
      "Epoch [1/1], Step [3474/7635], Loss: 4.0702\n",
      "Epoch [1/1], Step [3475/7635], Loss: 4.1291\n",
      "Epoch [1/1], Step [3476/7635], Loss: 4.0332\n",
      "Epoch [1/1], Step [3477/7635], Loss: 4.0678\n",
      "Epoch [1/1], Step [3478/7635], Loss: 4.1628\n",
      "Epoch [1/1], Step [3479/7635], Loss: 4.0599\n",
      "Epoch [1/1], Step [3480/7635], Loss: 4.1140\n",
      "Epoch [1/1], Step [3481/7635], Loss: 4.0007\n",
      "Epoch [1/1], Step [3482/7635], Loss: 4.1513\n",
      "Epoch [1/1], Step [3483/7635], Loss: 4.0753\n",
      "Epoch [1/1], Step [3484/7635], Loss: 4.0912\n",
      "Epoch [1/1], Step [3485/7635], Loss: 4.1303\n",
      "Epoch [1/1], Step [3486/7635], Loss: 4.1919\n",
      "Epoch [1/1], Step [3487/7635], Loss: 4.0014\n",
      "Epoch [1/1], Step [3488/7635], Loss: 4.0971\n",
      "Epoch [1/1], Step [3489/7635], Loss: 4.1320\n",
      "Epoch [1/1], Step [3490/7635], Loss: 4.0486\n",
      "Epoch [1/1], Step [3491/7635], Loss: 3.9846\n",
      "Epoch [1/1], Step [3492/7635], Loss: 4.0586\n",
      "Epoch [1/1], Step [3493/7635], Loss: 4.0745\n",
      "Epoch [1/1], Step [3494/7635], Loss: 4.0506\n",
      "Epoch [1/1], Step [3495/7635], Loss: 3.9859\n",
      "Epoch [1/1], Step [3496/7635], Loss: 4.1827\n",
      "Epoch [1/1], Step [3497/7635], Loss: 4.1524\n",
      "Epoch [1/1], Step [3498/7635], Loss: 4.0179\n",
      "Epoch [1/1], Step [3499/7635], Loss: 4.1502\n",
      "Epoch [1/1], Step [3500/7635], Loss: 4.1156\n",
      "Epoch [1/1], Step [3501/7635], Loss: 4.0385\n",
      "Epoch [1/1], Step [3502/7635], Loss: 4.0550\n",
      "Epoch [1/1], Step [3503/7635], Loss: 4.0831\n",
      "Epoch [1/1], Step [3504/7635], Loss: 4.1672\n",
      "Epoch [1/1], Step [3505/7635], Loss: 4.0770\n",
      "Epoch [1/1], Step [3506/7635], Loss: 4.1483\n",
      "Epoch [1/1], Step [3507/7635], Loss: 4.1131\n",
      "Epoch [1/1], Step [3508/7635], Loss: 4.0402\n",
      "Epoch [1/1], Step [3509/7635], Loss: 4.0690\n",
      "Epoch [1/1], Step [3510/7635], Loss: 4.0610\n",
      "Epoch [1/1], Step [3511/7635], Loss: 4.0684\n",
      "Epoch [1/1], Step [3512/7635], Loss: 4.1521\n",
      "Epoch [1/1], Step [3513/7635], Loss: 4.0703\n",
      "Epoch [1/1], Step [3514/7635], Loss: 4.1440\n",
      "Epoch [1/1], Step [3515/7635], Loss: 3.9792\n",
      "Epoch [1/1], Step [3516/7635], Loss: 4.1065\n",
      "Epoch [1/1], Step [3517/7635], Loss: 4.1525\n",
      "Epoch [1/1], Step [3518/7635], Loss: 4.0982\n",
      "Epoch [1/1], Step [3519/7635], Loss: 4.1058\n",
      "Epoch [1/1], Step [3520/7635], Loss: 4.2214\n",
      "Epoch [1/1], Step [3521/7635], Loss: 4.1222\n",
      "Epoch [1/1], Step [3522/7635], Loss: 4.0643\n",
      "Epoch [1/1], Step [3523/7635], Loss: 4.1149\n",
      "Epoch [1/1], Step [3524/7635], Loss: 4.0472\n",
      "Epoch [1/1], Step [3525/7635], Loss: 4.1701\n",
      "Epoch [1/1], Step [3526/7635], Loss: 4.1903\n",
      "Epoch [1/1], Step [3527/7635], Loss: 4.0775\n",
      "Epoch [1/1], Step [3528/7635], Loss: 4.1525\n",
      "Epoch [1/1], Step [3529/7635], Loss: 4.0811\n",
      "Epoch [1/1], Step [3530/7635], Loss: 4.1632\n",
      "Epoch [1/1], Step [3531/7635], Loss: 4.0115\n",
      "Epoch [1/1], Step [3532/7635], Loss: 4.0081\n",
      "Epoch [1/1], Step [3533/7635], Loss: 4.1193\n",
      "Epoch [1/1], Step [3534/7635], Loss: 4.0646\n",
      "Epoch [1/1], Step [3535/7635], Loss: 4.1523\n",
      "Epoch [1/1], Step [3536/7635], Loss: 4.1213\n",
      "Epoch [1/1], Step [3537/7635], Loss: 4.0862\n",
      "Epoch [1/1], Step [3538/7635], Loss: 4.0806\n",
      "Epoch [1/1], Step [3539/7635], Loss: 4.0587\n",
      "Epoch [1/1], Step [3540/7635], Loss: 4.0583\n",
      "Epoch [1/1], Step [3541/7635], Loss: 4.0703\n",
      "Epoch [1/1], Step [3542/7635], Loss: 4.1729\n",
      "Epoch [1/1], Step [3543/7635], Loss: 4.0902\n",
      "Epoch [1/1], Step [3544/7635], Loss: 4.1313\n",
      "Epoch [1/1], Step [3545/7635], Loss: 4.1282\n",
      "Epoch [1/1], Step [3546/7635], Loss: 4.0860\n",
      "Epoch [1/1], Step [3547/7635], Loss: 4.1031\n",
      "Epoch [1/1], Step [3548/7635], Loss: 4.0746\n",
      "Epoch [1/1], Step [3549/7635], Loss: 4.0536\n",
      "Epoch [1/1], Step [3550/7635], Loss: 4.1375\n",
      "Epoch [1/1], Step [3551/7635], Loss: 4.1625\n",
      "Epoch [1/1], Step [3552/7635], Loss: 4.0872\n",
      "Epoch [1/1], Step [3553/7635], Loss: 4.0844\n",
      "Epoch [1/1], Step [3554/7635], Loss: 4.1996\n",
      "Epoch [1/1], Step [3555/7635], Loss: 4.0511\n",
      "Epoch [1/1], Step [3556/7635], Loss: 4.1359\n",
      "Epoch [1/1], Step [3557/7635], Loss: 4.1233\n",
      "Epoch [1/1], Step [3558/7635], Loss: 4.0381\n",
      "Epoch [1/1], Step [3559/7635], Loss: 4.0222\n",
      "Epoch [1/1], Step [3560/7635], Loss: 4.0536\n",
      "Epoch [1/1], Step [3561/7635], Loss: 4.1352\n",
      "Epoch [1/1], Step [3562/7635], Loss: 4.0421\n",
      "Epoch [1/1], Step [3563/7635], Loss: 4.0780\n",
      "Epoch [1/1], Step [3564/7635], Loss: 4.1391\n",
      "Epoch [1/1], Step [3565/7635], Loss: 4.1610\n",
      "Epoch [1/1], Step [3566/7635], Loss: 4.0788\n",
      "Epoch [1/1], Step [3567/7635], Loss: 4.1090\n",
      "Epoch [1/1], Step [3568/7635], Loss: 4.1455\n",
      "Epoch [1/1], Step [3569/7635], Loss: 4.0906\n",
      "Epoch [1/1], Step [3570/7635], Loss: 4.0898\n",
      "Epoch [1/1], Step [3571/7635], Loss: 4.0211\n",
      "Epoch [1/1], Step [3572/7635], Loss: 4.0906\n",
      "Epoch [1/1], Step [3573/7635], Loss: 4.0451\n",
      "Epoch [1/1], Step [3574/7635], Loss: 4.0988\n",
      "Epoch [1/1], Step [3575/7635], Loss: 4.1118\n",
      "Epoch [1/1], Step [3576/7635], Loss: 4.1276\n",
      "Epoch [1/1], Step [3577/7635], Loss: 4.0337\n",
      "Epoch [1/1], Step [3578/7635], Loss: 4.2155\n",
      "Epoch [1/1], Step [3579/7635], Loss: 4.1232\n",
      "Epoch [1/1], Step [3580/7635], Loss: 4.1123\n",
      "Epoch [1/1], Step [3581/7635], Loss: 4.0601\n",
      "Epoch [1/1], Step [3582/7635], Loss: 4.0139\n",
      "Epoch [1/1], Step [3583/7635], Loss: 4.1343\n",
      "Epoch [1/1], Step [3584/7635], Loss: 4.0754\n",
      "Epoch [1/1], Step [3585/7635], Loss: 4.0980\n",
      "Epoch [1/1], Step [3586/7635], Loss: 4.1169\n",
      "Epoch [1/1], Step [3587/7635], Loss: 4.0369\n",
      "Epoch [1/1], Step [3588/7635], Loss: 4.0694\n",
      "Epoch [1/1], Step [3589/7635], Loss: 4.0822\n",
      "Epoch [1/1], Step [3590/7635], Loss: 4.0956\n",
      "Epoch [1/1], Step [3591/7635], Loss: 4.1050\n",
      "Epoch [1/1], Step [3592/7635], Loss: 4.0478\n",
      "Epoch [1/1], Step [3593/7635], Loss: 4.1278\n",
      "Epoch [1/1], Step [3594/7635], Loss: 4.0984\n",
      "Epoch [1/1], Step [3595/7635], Loss: 4.0969\n",
      "Epoch [1/1], Step [3596/7635], Loss: 4.0858\n",
      "Epoch [1/1], Step [3597/7635], Loss: 4.0957\n",
      "Epoch [1/1], Step [3598/7635], Loss: 4.1701\n",
      "Epoch [1/1], Step [3599/7635], Loss: 4.0397\n",
      "Epoch [1/1], Step [3600/7635], Loss: 4.1805\n",
      "Epoch [1/1], Step [3601/7635], Loss: 4.0928\n",
      "Epoch [1/1], Step [3602/7635], Loss: 4.1452\n",
      "Epoch [1/1], Step [3603/7635], Loss: 4.1342\n",
      "Epoch [1/1], Step [3604/7635], Loss: 4.0756\n",
      "Epoch [1/1], Step [3605/7635], Loss: 4.1079\n",
      "Epoch [1/1], Step [3606/7635], Loss: 4.0331\n",
      "Epoch [1/1], Step [3607/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [3608/7635], Loss: 4.1167\n",
      "Epoch [1/1], Step [3609/7635], Loss: 4.0876\n",
      "Epoch [1/1], Step [3610/7635], Loss: 4.1164\n",
      "Epoch [1/1], Step [3611/7635], Loss: 4.1424\n",
      "Epoch [1/1], Step [3612/7635], Loss: 4.0857\n",
      "Epoch [1/1], Step [3613/7635], Loss: 4.0632\n",
      "Epoch [1/1], Step [3614/7635], Loss: 3.9910\n",
      "Epoch [1/1], Step [3615/7635], Loss: 4.0732\n",
      "Epoch [1/1], Step [3616/7635], Loss: 4.0925\n",
      "Epoch [1/1], Step [3617/7635], Loss: 4.0444\n",
      "Epoch [1/1], Step [3618/7635], Loss: 4.0631\n",
      "Epoch [1/1], Step [3619/7635], Loss: 4.0378\n",
      "Epoch [1/1], Step [3620/7635], Loss: 4.0494\n",
      "Epoch [1/1], Step [3621/7635], Loss: 4.0820\n",
      "Epoch [1/1], Step [3622/7635], Loss: 4.0940\n",
      "Epoch [1/1], Step [3623/7635], Loss: 4.0954\n",
      "Epoch [1/1], Step [3624/7635], Loss: 4.0540\n",
      "Epoch [1/1], Step [3625/7635], Loss: 4.0865\n",
      "Epoch [1/1], Step [3626/7635], Loss: 4.1528\n",
      "Epoch [1/1], Step [3627/7635], Loss: 4.0208\n",
      "Epoch [1/1], Step [3628/7635], Loss: 4.0723\n",
      "Epoch [1/1], Step [3629/7635], Loss: 4.0695\n",
      "Epoch [1/1], Step [3630/7635], Loss: 4.0751\n",
      "Epoch [1/1], Step [3631/7635], Loss: 4.1110\n",
      "Epoch [1/1], Step [3632/7635], Loss: 4.0868\n",
      "Epoch [1/1], Step [3633/7635], Loss: 4.0473\n",
      "Epoch [1/1], Step [3634/7635], Loss: 4.1090\n",
      "Epoch [1/1], Step [3635/7635], Loss: 4.0821\n",
      "Epoch [1/1], Step [3636/7635], Loss: 4.1354\n",
      "Epoch [1/1], Step [3637/7635], Loss: 4.1194\n",
      "Epoch [1/1], Step [3638/7635], Loss: 4.0889\n",
      "Epoch [1/1], Step [3639/7635], Loss: 4.1110\n",
      "Epoch [1/1], Step [3640/7635], Loss: 4.1106\n",
      "Epoch [1/1], Step [3641/7635], Loss: 4.0976\n",
      "Epoch [1/1], Step [3642/7635], Loss: 4.1242\n",
      "Epoch [1/1], Step [3643/7635], Loss: 4.0462\n",
      "Epoch [1/1], Step [3644/7635], Loss: 4.0187\n",
      "Epoch [1/1], Step [3645/7635], Loss: 4.0409\n",
      "Epoch [1/1], Step [3646/7635], Loss: 4.2135\n",
      "Epoch [1/1], Step [3647/7635], Loss: 4.0592\n",
      "Epoch [1/1], Step [3648/7635], Loss: 4.1867\n",
      "Epoch [1/1], Step [3649/7635], Loss: 4.0991\n",
      "Epoch [1/1], Step [3650/7635], Loss: 4.0733\n",
      "Epoch [1/1], Step [3651/7635], Loss: 4.0371\n",
      "Epoch [1/1], Step [3652/7635], Loss: 4.0760\n",
      "Epoch [1/1], Step [3653/7635], Loss: 4.0562\n",
      "Epoch [1/1], Step [3654/7635], Loss: 4.0884\n",
      "Epoch [1/1], Step [3655/7635], Loss: 4.1068\n",
      "Epoch [1/1], Step [3656/7635], Loss: 4.0443\n",
      "Epoch [1/1], Step [3657/7635], Loss: 4.0771\n",
      "Epoch [1/1], Step [3658/7635], Loss: 4.0443\n",
      "Epoch [1/1], Step [3659/7635], Loss: 4.0986\n",
      "Epoch [1/1], Step [3660/7635], Loss: 4.0025\n",
      "Epoch [1/1], Step [3661/7635], Loss: 4.0427\n",
      "Epoch [1/1], Step [3662/7635], Loss: 4.0698\n",
      "Epoch [1/1], Step [3663/7635], Loss: 4.0515\n",
      "Epoch [1/1], Step [3664/7635], Loss: 4.0870\n",
      "Epoch [1/1], Step [3665/7635], Loss: 4.0635\n",
      "Epoch [1/1], Step [3666/7635], Loss: 4.0513\n",
      "Epoch [1/1], Step [3667/7635], Loss: 4.1093\n",
      "Epoch [1/1], Step [3668/7635], Loss: 4.1053\n",
      "Epoch [1/1], Step [3669/7635], Loss: 4.0277\n",
      "Epoch [1/1], Step [3670/7635], Loss: 4.1322\n",
      "Epoch [1/1], Step [3671/7635], Loss: 4.0149\n",
      "Epoch [1/1], Step [3672/7635], Loss: 4.0894\n",
      "Epoch [1/1], Step [3673/7635], Loss: 4.0021\n",
      "Epoch [1/1], Step [3674/7635], Loss: 4.0356\n",
      "Epoch [1/1], Step [3675/7635], Loss: 4.0653\n",
      "Epoch [1/1], Step [3676/7635], Loss: 4.0811\n",
      "Epoch [1/1], Step [3677/7635], Loss: 4.0733\n",
      "Epoch [1/1], Step [3678/7635], Loss: 4.0751\n",
      "Epoch [1/1], Step [3679/7635], Loss: 4.0258\n",
      "Epoch [1/1], Step [3680/7635], Loss: 4.0029\n",
      "Epoch [1/1], Step [3681/7635], Loss: 4.0781\n",
      "Epoch [1/1], Step [3682/7635], Loss: 4.1315\n",
      "Epoch [1/1], Step [3683/7635], Loss: 4.0283\n",
      "Epoch [1/1], Step [3684/7635], Loss: 4.0895\n",
      "Epoch [1/1], Step [3685/7635], Loss: 4.0680\n",
      "Epoch [1/1], Step [3686/7635], Loss: 4.1052\n",
      "Epoch [1/1], Step [3687/7635], Loss: 4.0522\n",
      "Epoch [1/1], Step [3688/7635], Loss: 4.1407\n",
      "Epoch [1/1], Step [3689/7635], Loss: 4.1152\n",
      "Epoch [1/1], Step [3690/7635], Loss: 4.0705\n",
      "Epoch [1/1], Step [3691/7635], Loss: 4.0356\n",
      "Epoch [1/1], Step [3692/7635], Loss: 4.1215\n",
      "Epoch [1/1], Step [3693/7635], Loss: 4.1082\n",
      "Epoch [1/1], Step [3694/7635], Loss: 4.0421\n",
      "Epoch [1/1], Step [3695/7635], Loss: 4.0396\n",
      "Epoch [1/1], Step [3696/7635], Loss: 4.0935\n",
      "Epoch [1/1], Step [3697/7635], Loss: 4.0465\n",
      "Epoch [1/1], Step [3698/7635], Loss: 4.0527\n",
      "Epoch [1/1], Step [3699/7635], Loss: 4.0432\n",
      "Epoch [1/1], Step [3700/7635], Loss: 4.0239\n",
      "Epoch [1/1], Step [3701/7635], Loss: 4.1236\n",
      "Epoch [1/1], Step [3702/7635], Loss: 4.0510\n",
      "Epoch [1/1], Step [3703/7635], Loss: 4.0581\n",
      "Epoch [1/1], Step [3704/7635], Loss: 4.0407\n",
      "Epoch [1/1], Step [3705/7635], Loss: 4.0174\n",
      "Epoch [1/1], Step [3706/7635], Loss: 4.0820\n",
      "Epoch [1/1], Step [3707/7635], Loss: 4.1012\n",
      "Epoch [1/1], Step [3708/7635], Loss: 4.1697\n",
      "Epoch [1/1], Step [3709/7635], Loss: 4.0434\n",
      "Epoch [1/1], Step [3710/7635], Loss: 4.1414\n",
      "Epoch [1/1], Step [3711/7635], Loss: 4.0581\n",
      "Epoch [1/1], Step [3712/7635], Loss: 4.1284\n",
      "Epoch [1/1], Step [3713/7635], Loss: 4.0919\n",
      "Epoch [1/1], Step [3714/7635], Loss: 4.0327\n",
      "Epoch [1/1], Step [3715/7635], Loss: 4.0479\n",
      "Epoch [1/1], Step [3716/7635], Loss: 4.0715\n",
      "Epoch [1/1], Step [3717/7635], Loss: 4.1644\n",
      "Epoch [1/1], Step [3718/7635], Loss: 4.1067\n",
      "Epoch [1/1], Step [3719/7635], Loss: 4.0619\n",
      "Epoch [1/1], Step [3720/7635], Loss: 4.0383\n",
      "Epoch [1/1], Step [3721/7635], Loss: 4.1226\n",
      "Epoch [1/1], Step [3722/7635], Loss: 4.0922\n",
      "Epoch [1/1], Step [3723/7635], Loss: 4.1049\n",
      "Epoch [1/1], Step [3724/7635], Loss: 3.9819\n",
      "Epoch [1/1], Step [3725/7635], Loss: 4.1038\n",
      "Epoch [1/1], Step [3726/7635], Loss: 4.1535\n",
      "Epoch [1/1], Step [3727/7635], Loss: 4.1691\n",
      "Epoch [1/1], Step [3728/7635], Loss: 4.1386\n",
      "Epoch [1/1], Step [3729/7635], Loss: 4.0172\n",
      "Epoch [1/1], Step [3730/7635], Loss: 4.0855\n",
      "Epoch [1/1], Step [3731/7635], Loss: 4.1404\n",
      "Epoch [1/1], Step [3732/7635], Loss: 4.1518\n",
      "Epoch [1/1], Step [3733/7635], Loss: 4.0665\n",
      "Epoch [1/1], Step [3734/7635], Loss: 4.1775\n",
      "Epoch [1/1], Step [3735/7635], Loss: 4.0526\n",
      "Epoch [1/1], Step [3736/7635], Loss: 4.0630\n",
      "Epoch [1/1], Step [3737/7635], Loss: 4.0729\n",
      "Epoch [1/1], Step [3738/7635], Loss: 4.0160\n",
      "Epoch [1/1], Step [3739/7635], Loss: 4.1237\n",
      "Epoch [1/1], Step [3740/7635], Loss: 4.0932\n",
      "Epoch [1/1], Step [3741/7635], Loss: 4.0682\n",
      "Epoch [1/1], Step [3742/7635], Loss: 4.0510\n",
      "Epoch [1/1], Step [3743/7635], Loss: 4.0655\n",
      "Epoch [1/1], Step [3744/7635], Loss: 4.1650\n",
      "Epoch [1/1], Step [3745/7635], Loss: 3.9924\n",
      "Epoch [1/1], Step [3746/7635], Loss: 4.0439\n",
      "Epoch [1/1], Step [3747/7635], Loss: 4.0908\n",
      "Epoch [1/1], Step [3748/7635], Loss: 4.0253\n",
      "Epoch [1/1], Step [3749/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [3750/7635], Loss: 4.0962\n",
      "Epoch [1/1], Step [3751/7635], Loss: 4.0824\n",
      "Epoch [1/1], Step [3752/7635], Loss: 3.9864\n",
      "Epoch [1/1], Step [3753/7635], Loss: 4.0547\n",
      "Epoch [1/1], Step [3754/7635], Loss: 4.0386\n",
      "Epoch [1/1], Step [3755/7635], Loss: 4.0882\n",
      "Epoch [1/1], Step [3756/7635], Loss: 4.1143\n",
      "Epoch [1/1], Step [3757/7635], Loss: 4.0359\n",
      "Epoch [1/1], Step [3758/7635], Loss: 4.0157\n",
      "Epoch [1/1], Step [3759/7635], Loss: 4.0632\n",
      "Epoch [1/1], Step [3760/7635], Loss: 4.1395\n",
      "Epoch [1/1], Step [3761/7635], Loss: 4.0911\n",
      "Epoch [1/1], Step [3762/7635], Loss: 4.0948\n",
      "Epoch [1/1], Step [3763/7635], Loss: 4.0514\n",
      "Epoch [1/1], Step [3764/7635], Loss: 4.0451\n",
      "Epoch [1/1], Step [3765/7635], Loss: 4.0217\n",
      "Epoch [1/1], Step [3766/7635], Loss: 4.0478\n",
      "Epoch [1/1], Step [3767/7635], Loss: 4.1460\n",
      "Epoch [1/1], Step [3768/7635], Loss: 4.0555\n",
      "Epoch [1/1], Step [3769/7635], Loss: 4.1385\n",
      "Epoch [1/1], Step [3770/7635], Loss: 4.0938\n",
      "Epoch [1/1], Step [3771/7635], Loss: 4.0499\n",
      "Epoch [1/1], Step [3772/7635], Loss: 4.0940\n",
      "Epoch [1/1], Step [3773/7635], Loss: 4.0207\n",
      "Epoch [1/1], Step [3774/7635], Loss: 4.0528\n",
      "Epoch [1/1], Step [3775/7635], Loss: 4.0386\n",
      "Epoch [1/1], Step [3776/7635], Loss: 4.0436\n",
      "Epoch [1/1], Step [3777/7635], Loss: 4.1855\n",
      "Epoch [1/1], Step [3778/7635], Loss: 4.0383\n",
      "Epoch [1/1], Step [3779/7635], Loss: 4.1956\n",
      "Epoch [1/1], Step [3780/7635], Loss: 4.0349\n",
      "Epoch [1/1], Step [3781/7635], Loss: 4.0856\n",
      "Epoch [1/1], Step [3782/7635], Loss: 4.0774\n",
      "Epoch [1/1], Step [3783/7635], Loss: 4.0668\n",
      "Epoch [1/1], Step [3784/7635], Loss: 4.0240\n",
      "Epoch [1/1], Step [3785/7635], Loss: 4.2082\n",
      "Epoch [1/1], Step [3786/7635], Loss: 4.0033\n",
      "Epoch [1/1], Step [3787/7635], Loss: 4.0535\n",
      "Epoch [1/1], Step [3788/7635], Loss: 4.0936\n",
      "Epoch [1/1], Step [3789/7635], Loss: 4.0837\n",
      "Epoch [1/1], Step [3790/7635], Loss: 4.0450\n",
      "Epoch [1/1], Step [3791/7635], Loss: 4.0439\n",
      "Epoch [1/1], Step [3792/7635], Loss: 4.1639\n",
      "Epoch [1/1], Step [3793/7635], Loss: 4.0706\n",
      "Epoch [1/1], Step [3794/7635], Loss: 4.1053\n",
      "Epoch [1/1], Step [3795/7635], Loss: 4.0515\n",
      "Epoch [1/1], Step [3796/7635], Loss: 4.0612\n",
      "Epoch [1/1], Step [3797/7635], Loss: 4.1459\n",
      "Epoch [1/1], Step [3798/7635], Loss: 4.0510\n",
      "Epoch [1/1], Step [3799/7635], Loss: 4.0601\n",
      "Epoch [1/1], Step [3800/7635], Loss: 4.0406\n",
      "Epoch [1/1], Step [3801/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [3802/7635], Loss: 4.0163\n",
      "Epoch [1/1], Step [3803/7635], Loss: 4.0967\n",
      "Epoch [1/1], Step [3804/7635], Loss: 3.9257\n",
      "Epoch [1/1], Step [3805/7635], Loss: 4.0779\n",
      "Epoch [1/1], Step [3806/7635], Loss: 4.0181\n",
      "Epoch [1/1], Step [3807/7635], Loss: 4.0385\n",
      "Epoch [1/1], Step [3808/7635], Loss: 4.0289\n",
      "Epoch [1/1], Step [3809/7635], Loss: 4.0443\n",
      "Epoch [1/1], Step [3810/7635], Loss: 4.1322\n",
      "Epoch [1/1], Step [3811/7635], Loss: 4.0703\n",
      "Epoch [1/1], Step [3812/7635], Loss: 4.0519\n",
      "Epoch [1/1], Step [3813/7635], Loss: 4.1333\n",
      "Epoch [1/1], Step [3814/7635], Loss: 4.1515\n",
      "Epoch [1/1], Step [3815/7635], Loss: 4.0411\n",
      "Epoch [1/1], Step [3816/7635], Loss: 4.0888\n",
      "Epoch [1/1], Step [3817/7635], Loss: 4.0568\n",
      "Epoch [1/1], Step [3818/7635], Loss: 4.1085\n",
      "Epoch [1/1], Step [3819/7635], Loss: 4.1769\n",
      "Epoch [1/1], Step [3820/7635], Loss: 4.0939\n",
      "Epoch [1/1], Step [3821/7635], Loss: 4.0808\n",
      "Epoch [1/1], Step [3822/7635], Loss: 4.0165\n",
      "Epoch [1/1], Step [3823/7635], Loss: 4.0264\n",
      "Epoch [1/1], Step [3824/7635], Loss: 4.0819\n",
      "Epoch [1/1], Step [3825/7635], Loss: 4.0065\n",
      "Epoch [1/1], Step [3826/7635], Loss: 4.0497\n",
      "Epoch [1/1], Step [3827/7635], Loss: 4.1967\n",
      "Epoch [1/1], Step [3828/7635], Loss: 3.9875\n",
      "Epoch [1/1], Step [3829/7635], Loss: 4.0175\n",
      "Epoch [1/1], Step [3830/7635], Loss: 4.0773\n",
      "Epoch [1/1], Step [3831/7635], Loss: 4.0443\n",
      "Epoch [1/1], Step [3832/7635], Loss: 4.0901\n",
      "Epoch [1/1], Step [3833/7635], Loss: 4.0276\n",
      "Epoch [1/1], Step [3834/7635], Loss: 4.1158\n",
      "Epoch [1/1], Step [3835/7635], Loss: 4.0503\n",
      "Epoch [1/1], Step [3836/7635], Loss: 4.1065\n",
      "Epoch [1/1], Step [3837/7635], Loss: 4.0268\n",
      "Epoch [1/1], Step [3838/7635], Loss: 4.0644\n",
      "Epoch [1/1], Step [3839/7635], Loss: 4.0392\n",
      "Epoch [1/1], Step [3840/7635], Loss: 4.0314\n",
      "Epoch [1/1], Step [3841/7635], Loss: 4.0433\n",
      "Epoch [1/1], Step [3842/7635], Loss: 4.0388\n",
      "Epoch [1/1], Step [3843/7635], Loss: 4.0306\n",
      "Epoch [1/1], Step [3844/7635], Loss: 4.0362\n",
      "Epoch [1/1], Step [3845/7635], Loss: 4.1271\n",
      "Epoch [1/1], Step [3846/7635], Loss: 4.0847\n",
      "Epoch [1/1], Step [3847/7635], Loss: 4.0404\n",
      "Epoch [1/1], Step [3848/7635], Loss: 4.0923\n",
      "Epoch [1/1], Step [3849/7635], Loss: 4.0155\n",
      "Epoch [1/1], Step [3850/7635], Loss: 4.1725\n",
      "Epoch [1/1], Step [3851/7635], Loss: 4.0778\n",
      "Epoch [1/1], Step [3852/7635], Loss: 4.0782\n",
      "Epoch [1/1], Step [3853/7635], Loss: 4.0365\n",
      "Epoch [1/1], Step [3854/7635], Loss: 4.0607\n",
      "Epoch [1/1], Step [3855/7635], Loss: 3.9813\n",
      "Epoch [1/1], Step [3856/7635], Loss: 4.0870\n",
      "Epoch [1/1], Step [3857/7635], Loss: 4.0757\n",
      "Epoch [1/1], Step [3858/7635], Loss: 4.0615\n",
      "Epoch [1/1], Step [3859/7635], Loss: 4.0765\n",
      "Epoch [1/1], Step [3860/7635], Loss: 4.0250\n",
      "Epoch [1/1], Step [3861/7635], Loss: 4.0335\n",
      "Epoch [1/1], Step [3862/7635], Loss: 4.1427\n",
      "Epoch [1/1], Step [3863/7635], Loss: 4.0648\n",
      "Epoch [1/1], Step [3864/7635], Loss: 4.0360\n",
      "Epoch [1/1], Step [3865/7635], Loss: 4.0125\n",
      "Epoch [1/1], Step [3866/7635], Loss: 4.0217\n",
      "Epoch [1/1], Step [3867/7635], Loss: 4.0541\n",
      "Epoch [1/1], Step [3868/7635], Loss: 4.0677\n",
      "Epoch [1/1], Step [3869/7635], Loss: 4.0791\n",
      "Epoch [1/1], Step [3870/7635], Loss: 4.0840\n",
      "Epoch [1/1], Step [3871/7635], Loss: 4.1140\n",
      "Epoch [1/1], Step [3872/7635], Loss: 3.9913\n",
      "Epoch [1/1], Step [3873/7635], Loss: 4.0558\n",
      "Epoch [1/1], Step [3874/7635], Loss: 4.0563\n",
      "Epoch [1/1], Step [3875/7635], Loss: 4.1056\n",
      "Epoch [1/1], Step [3876/7635], Loss: 3.9719\n",
      "Epoch [1/1], Step [3877/7635], Loss: 4.0767\n",
      "Epoch [1/1], Step [3878/7635], Loss: 4.0843\n",
      "Epoch [1/1], Step [3879/7635], Loss: 4.0750\n",
      "Epoch [1/1], Step [3880/7635], Loss: 4.0099\n",
      "Epoch [1/1], Step [3881/7635], Loss: 4.0899\n",
      "Epoch [1/1], Step [3882/7635], Loss: 4.1143\n",
      "Epoch [1/1], Step [3883/7635], Loss: 4.1228\n",
      "Epoch [1/1], Step [3884/7635], Loss: 4.0584\n",
      "Epoch [1/1], Step [3885/7635], Loss: 4.0249\n",
      "Epoch [1/1], Step [3886/7635], Loss: 4.1538\n",
      "Epoch [1/1], Step [3887/7635], Loss: 4.0517\n",
      "Epoch [1/1], Step [3888/7635], Loss: 4.0665\n",
      "Epoch [1/1], Step [3889/7635], Loss: 4.1179\n",
      "Epoch [1/1], Step [3890/7635], Loss: 4.0070\n",
      "Epoch [1/1], Step [3891/7635], Loss: 4.0427\n",
      "Epoch [1/1], Step [3892/7635], Loss: 4.0513\n",
      "Epoch [1/1], Step [3893/7635], Loss: 4.0221\n",
      "Epoch [1/1], Step [3894/7635], Loss: 4.0426\n",
      "Epoch [1/1], Step [3895/7635], Loss: 4.0304\n",
      "Epoch [1/1], Step [3896/7635], Loss: 4.1097\n",
      "Epoch [1/1], Step [3897/7635], Loss: 4.1177\n",
      "Epoch [1/1], Step [3898/7635], Loss: 4.0409\n",
      "Epoch [1/1], Step [3899/7635], Loss: 4.0183\n",
      "Epoch [1/1], Step [3900/7635], Loss: 3.9896\n",
      "Epoch [1/1], Step [3901/7635], Loss: 4.0006\n",
      "Epoch [1/1], Step [3902/7635], Loss: 4.0464\n",
      "Epoch [1/1], Step [3903/7635], Loss: 4.1113\n",
      "Epoch [1/1], Step [3904/7635], Loss: 4.0549\n",
      "Epoch [1/1], Step [3905/7635], Loss: 4.0786\n",
      "Epoch [1/1], Step [3906/7635], Loss: 4.1473\n",
      "Epoch [1/1], Step [3907/7635], Loss: 4.0837\n",
      "Epoch [1/1], Step [3908/7635], Loss: 4.0070\n",
      "Epoch [1/1], Step [3909/7635], Loss: 4.1114\n",
      "Epoch [1/1], Step [3910/7635], Loss: 4.1521\n",
      "Epoch [1/1], Step [3911/7635], Loss: 4.0523\n",
      "Epoch [1/1], Step [3912/7635], Loss: 4.0535\n",
      "Epoch [1/1], Step [3913/7635], Loss: 4.0116\n",
      "Epoch [1/1], Step [3914/7635], Loss: 4.0228\n",
      "Epoch [1/1], Step [3915/7635], Loss: 4.0138\n",
      "Epoch [1/1], Step [3916/7635], Loss: 4.1195\n",
      "Epoch [1/1], Step [3917/7635], Loss: 4.1123\n",
      "Epoch [1/1], Step [3918/7635], Loss: 4.1419\n",
      "Epoch [1/1], Step [3919/7635], Loss: 4.1252\n",
      "Epoch [1/1], Step [3920/7635], Loss: 4.0998\n",
      "Epoch [1/1], Step [3921/7635], Loss: 4.0389\n",
      "Epoch [1/1], Step [3922/7635], Loss: 3.9804\n",
      "Epoch [1/1], Step [3923/7635], Loss: 4.1705\n",
      "Epoch [1/1], Step [3924/7635], Loss: 4.0530\n",
      "Epoch [1/1], Step [3925/7635], Loss: 4.1288\n",
      "Epoch [1/1], Step [3926/7635], Loss: 4.1593\n",
      "Epoch [1/1], Step [3927/7635], Loss: 4.1347\n",
      "Epoch [1/1], Step [3928/7635], Loss: 3.9778\n",
      "Epoch [1/1], Step [3929/7635], Loss: 4.0353\n",
      "Epoch [1/1], Step [3930/7635], Loss: 4.0042\n",
      "Epoch [1/1], Step [3931/7635], Loss: 4.0759\n",
      "Epoch [1/1], Step [3932/7635], Loss: 4.0406\n",
      "Epoch [1/1], Step [3933/7635], Loss: 4.0156\n",
      "Epoch [1/1], Step [3934/7635], Loss: 4.0830\n",
      "Epoch [1/1], Step [3935/7635], Loss: 4.1145\n",
      "Epoch [1/1], Step [3936/7635], Loss: 4.0509\n",
      "Epoch [1/1], Step [3937/7635], Loss: 4.0895\n",
      "Epoch [1/1], Step [3938/7635], Loss: 4.1181\n",
      "Epoch [1/1], Step [3939/7635], Loss: 4.0510\n",
      "Epoch [1/1], Step [3940/7635], Loss: 4.0038\n",
      "Epoch [1/1], Step [3941/7635], Loss: 4.1008\n",
      "Epoch [1/1], Step [3942/7635], Loss: 4.1007\n",
      "Epoch [1/1], Step [3943/7635], Loss: 3.9931\n",
      "Epoch [1/1], Step [3944/7635], Loss: 4.0509\n",
      "Epoch [1/1], Step [3945/7635], Loss: 4.0348\n",
      "Epoch [1/1], Step [3946/7635], Loss: 3.9952\n",
      "Epoch [1/1], Step [3947/7635], Loss: 4.0372\n",
      "Epoch [1/1], Step [3948/7635], Loss: 4.0248\n",
      "Epoch [1/1], Step [3949/7635], Loss: 4.0993\n",
      "Epoch [1/1], Step [3950/7635], Loss: 4.0453\n",
      "Epoch [1/1], Step [3951/7635], Loss: 4.0775\n",
      "Epoch [1/1], Step [3952/7635], Loss: 4.0297\n",
      "Epoch [1/1], Step [3953/7635], Loss: 4.1134\n",
      "Epoch [1/1], Step [3954/7635], Loss: 4.0685\n",
      "Epoch [1/1], Step [3955/7635], Loss: 4.1225\n",
      "Epoch [1/1], Step [3956/7635], Loss: 4.0259\n",
      "Epoch [1/1], Step [3957/7635], Loss: 4.0841\n",
      "Epoch [1/1], Step [3958/7635], Loss: 4.0621\n",
      "Epoch [1/1], Step [3959/7635], Loss: 4.0729\n",
      "Epoch [1/1], Step [3960/7635], Loss: 4.1007\n",
      "Epoch [1/1], Step [3961/7635], Loss: 4.0103\n",
      "Epoch [1/1], Step [3962/7635], Loss: 4.0426\n",
      "Epoch [1/1], Step [3963/7635], Loss: 4.0496\n",
      "Epoch [1/1], Step [3964/7635], Loss: 4.0547\n",
      "Epoch [1/1], Step [3965/7635], Loss: 4.0115\n",
      "Epoch [1/1], Step [3966/7635], Loss: 4.0730\n",
      "Epoch [1/1], Step [3967/7635], Loss: 4.1486\n",
      "Epoch [1/1], Step [3968/7635], Loss: 4.0274\n",
      "Epoch [1/1], Step [3969/7635], Loss: 4.1363\n",
      "Epoch [1/1], Step [3970/7635], Loss: 4.1364\n",
      "Epoch [1/1], Step [3971/7635], Loss: 4.0430\n",
      "Epoch [1/1], Step [3972/7635], Loss: 4.0248\n",
      "Epoch [1/1], Step [3973/7635], Loss: 4.0889\n",
      "Epoch [1/1], Step [3974/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [3975/7635], Loss: 4.1123\n",
      "Epoch [1/1], Step [3976/7635], Loss: 4.0682\n",
      "Epoch [1/1], Step [3977/7635], Loss: 4.0665\n",
      "Epoch [1/1], Step [3978/7635], Loss: 4.1124\n",
      "Epoch [1/1], Step [3979/7635], Loss: 4.0149\n",
      "Epoch [1/1], Step [3980/7635], Loss: 4.0767\n",
      "Epoch [1/1], Step [3981/7635], Loss: 4.1329\n",
      "Epoch [1/1], Step [3982/7635], Loss: 3.9926\n",
      "Epoch [1/1], Step [3983/7635], Loss: 4.0678\n",
      "Epoch [1/1], Step [3984/7635], Loss: 4.0858\n",
      "Epoch [1/1], Step [3985/7635], Loss: 4.1395\n",
      "Epoch [1/1], Step [3986/7635], Loss: 4.0901\n",
      "Epoch [1/1], Step [3987/7635], Loss: 4.1198\n",
      "Epoch [1/1], Step [3988/7635], Loss: 4.0536\n",
      "Epoch [1/1], Step [3989/7635], Loss: 4.0583\n",
      "Epoch [1/1], Step [3990/7635], Loss: 4.0728\n",
      "Epoch [1/1], Step [3991/7635], Loss: 4.0649\n",
      "Epoch [1/1], Step [3992/7635], Loss: 4.0102\n",
      "Epoch [1/1], Step [3993/7635], Loss: 4.0510\n",
      "Epoch [1/1], Step [3994/7635], Loss: 4.1361\n",
      "Epoch [1/1], Step [3995/7635], Loss: 4.0248\n",
      "Epoch [1/1], Step [3996/7635], Loss: 4.0127\n",
      "Epoch [1/1], Step [3997/7635], Loss: 4.0565\n",
      "Epoch [1/1], Step [3998/7635], Loss: 4.0581\n",
      "Epoch [1/1], Step [3999/7635], Loss: 4.1626\n",
      "Epoch [1/1], Step [4000/7635], Loss: 4.0750\n",
      "Epoch [1/1], Step [4001/7635], Loss: 4.0902\n",
      "Epoch [1/1], Step [4002/7635], Loss: 4.0407\n",
      "Epoch [1/1], Step [4003/7635], Loss: 4.0782\n",
      "Epoch [1/1], Step [4004/7635], Loss: 4.1064\n",
      "Epoch [1/1], Step [4005/7635], Loss: 4.0800\n",
      "Epoch [1/1], Step [4006/7635], Loss: 4.0711\n",
      "Epoch [1/1], Step [4007/7635], Loss: 4.0185\n",
      "Epoch [1/1], Step [4008/7635], Loss: 4.0199\n",
      "Epoch [1/1], Step [4009/7635], Loss: 4.1089\n",
      "Epoch [1/1], Step [4010/7635], Loss: 4.0393\n",
      "Epoch [1/1], Step [4011/7635], Loss: 4.0078\n",
      "Epoch [1/1], Step [4012/7635], Loss: 4.0691\n",
      "Epoch [1/1], Step [4013/7635], Loss: 4.0626\n",
      "Epoch [1/1], Step [4014/7635], Loss: 4.1160\n",
      "Epoch [1/1], Step [4015/7635], Loss: 4.0269\n",
      "Epoch [1/1], Step [4016/7635], Loss: 4.0510\n",
      "Epoch [1/1], Step [4017/7635], Loss: 4.0813\n",
      "Epoch [1/1], Step [4018/7635], Loss: 4.0211\n",
      "Epoch [1/1], Step [4019/7635], Loss: 4.0519\n",
      "Epoch [1/1], Step [4020/7635], Loss: 4.1014\n",
      "Epoch [1/1], Step [4021/7635], Loss: 4.1101\n",
      "Epoch [1/1], Step [4022/7635], Loss: 4.0686\n",
      "Epoch [1/1], Step [4023/7635], Loss: 4.1302\n",
      "Epoch [1/1], Step [4024/7635], Loss: 4.0927\n",
      "Epoch [1/1], Step [4025/7635], Loss: 4.0717\n",
      "Epoch [1/1], Step [4026/7635], Loss: 4.1023\n",
      "Epoch [1/1], Step [4027/7635], Loss: 4.1016\n",
      "Epoch [1/1], Step [4028/7635], Loss: 4.0814\n",
      "Epoch [1/1], Step [4029/7635], Loss: 4.0788\n",
      "Epoch [1/1], Step [4030/7635], Loss: 4.0414\n",
      "Epoch [1/1], Step [4031/7635], Loss: 4.0930\n",
      "Epoch [1/1], Step [4032/7635], Loss: 4.0366\n",
      "Epoch [1/1], Step [4033/7635], Loss: 4.1051\n",
      "Epoch [1/1], Step [4034/7635], Loss: 4.0992\n",
      "Epoch [1/1], Step [4035/7635], Loss: 4.0597\n",
      "Epoch [1/1], Step [4036/7635], Loss: 4.0758\n",
      "Epoch [1/1], Step [4037/7635], Loss: 4.0625\n",
      "Epoch [1/1], Step [4038/7635], Loss: 4.0497\n",
      "Epoch [1/1], Step [4039/7635], Loss: 4.0345\n",
      "Epoch [1/1], Step [4040/7635], Loss: 4.0805\n",
      "Epoch [1/1], Step [4041/7635], Loss: 4.1063\n",
      "Epoch [1/1], Step [4042/7635], Loss: 4.1786\n",
      "Epoch [1/1], Step [4043/7635], Loss: 4.0585\n",
      "Epoch [1/1], Step [4044/7635], Loss: 4.0314\n",
      "Epoch [1/1], Step [4045/7635], Loss: 4.1197\n",
      "Epoch [1/1], Step [4046/7635], Loss: 4.0265\n",
      "Epoch [1/1], Step [4047/7635], Loss: 4.0870\n",
      "Epoch [1/1], Step [4048/7635], Loss: 4.0438\n",
      "Epoch [1/1], Step [4049/7635], Loss: 4.0231\n",
      "Epoch [1/1], Step [4050/7635], Loss: 4.0215\n",
      "Epoch [1/1], Step [4051/7635], Loss: 4.0432\n",
      "Epoch [1/1], Step [4052/7635], Loss: 4.0769\n",
      "Epoch [1/1], Step [4053/7635], Loss: 4.1428\n",
      "Epoch [1/1], Step [4054/7635], Loss: 4.0675\n",
      "Epoch [1/1], Step [4055/7635], Loss: 4.0492\n",
      "Epoch [1/1], Step [4056/7635], Loss: 4.0212\n",
      "Epoch [1/1], Step [4057/7635], Loss: 4.0363\n",
      "Epoch [1/1], Step [4058/7635], Loss: 4.0570\n",
      "Epoch [1/1], Step [4059/7635], Loss: 4.0800\n",
      "Epoch [1/1], Step [4060/7635], Loss: 4.0517\n",
      "Epoch [1/1], Step [4061/7635], Loss: 4.1090\n",
      "Epoch [1/1], Step [4062/7635], Loss: 4.1245\n",
      "Epoch [1/1], Step [4063/7635], Loss: 3.9993\n",
      "Epoch [1/1], Step [4064/7635], Loss: 4.1095\n",
      "Epoch [1/1], Step [4065/7635], Loss: 4.0054\n",
      "Epoch [1/1], Step [4066/7635], Loss: 4.0069\n",
      "Epoch [1/1], Step [4067/7635], Loss: 4.0333\n",
      "Epoch [1/1], Step [4068/7635], Loss: 4.0746\n",
      "Epoch [1/1], Step [4069/7635], Loss: 4.0490\n",
      "Epoch [1/1], Step [4070/7635], Loss: 4.0093\n",
      "Epoch [1/1], Step [4071/7635], Loss: 4.0249\n",
      "Epoch [1/1], Step [4072/7635], Loss: 4.1122\n",
      "Epoch [1/1], Step [4073/7635], Loss: 4.1400\n",
      "Epoch [1/1], Step [4074/7635], Loss: 3.9933\n",
      "Epoch [1/1], Step [4075/7635], Loss: 4.1065\n",
      "Epoch [1/1], Step [4076/7635], Loss: 4.0616\n",
      "Epoch [1/1], Step [4077/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [4078/7635], Loss: 4.1249\n",
      "Epoch [1/1], Step [4079/7635], Loss: 4.0919\n",
      "Epoch [1/1], Step [4080/7635], Loss: 4.0418\n",
      "Epoch [1/1], Step [4081/7635], Loss: 4.0443\n",
      "Epoch [1/1], Step [4082/7635], Loss: 4.0387\n",
      "Epoch [1/1], Step [4083/7635], Loss: 3.9786\n",
      "Epoch [1/1], Step [4084/7635], Loss: 4.0974\n",
      "Epoch [1/1], Step [4085/7635], Loss: 4.0755\n",
      "Epoch [1/1], Step [4086/7635], Loss: 3.9089\n",
      "Epoch [1/1], Step [4087/7635], Loss: 4.0502\n",
      "Epoch [1/1], Step [4088/7635], Loss: 4.0720\n",
      "Epoch [1/1], Step [4089/7635], Loss: 4.0514\n",
      "Epoch [1/1], Step [4090/7635], Loss: 4.1248\n",
      "Epoch [1/1], Step [4091/7635], Loss: 4.0697\n",
      "Epoch [1/1], Step [4092/7635], Loss: 4.0907\n",
      "Epoch [1/1], Step [4093/7635], Loss: 4.0734\n",
      "Epoch [1/1], Step [4094/7635], Loss: 4.0091\n",
      "Epoch [1/1], Step [4095/7635], Loss: 4.0373\n",
      "Epoch [1/1], Step [4096/7635], Loss: 4.1023\n",
      "Epoch [1/1], Step [4097/7635], Loss: 4.0224\n",
      "Epoch [1/1], Step [4098/7635], Loss: 4.0853\n",
      "Epoch [1/1], Step [4099/7635], Loss: 4.1361\n",
      "Epoch [1/1], Step [4100/7635], Loss: 4.0566\n",
      "Epoch [1/1], Step [4101/7635], Loss: 4.1269\n",
      "Epoch [1/1], Step [4102/7635], Loss: 3.9637\n",
      "Epoch [1/1], Step [4103/7635], Loss: 4.1013\n",
      "Epoch [1/1], Step [4104/7635], Loss: 3.9946\n",
      "Epoch [1/1], Step [4105/7635], Loss: 4.0144\n",
      "Epoch [1/1], Step [4106/7635], Loss: 4.0333\n",
      "Epoch [1/1], Step [4107/7635], Loss: 4.0454\n",
      "Epoch [1/1], Step [4108/7635], Loss: 4.0607\n",
      "Epoch [1/1], Step [4109/7635], Loss: 4.0437\n",
      "Epoch [1/1], Step [4110/7635], Loss: 4.0680\n",
      "Epoch [1/1], Step [4111/7635], Loss: 4.0956\n",
      "Epoch [1/1], Step [4112/7635], Loss: 4.0750\n",
      "Epoch [1/1], Step [4113/7635], Loss: 4.0933\n",
      "Epoch [1/1], Step [4114/7635], Loss: 4.1262\n",
      "Epoch [1/1], Step [4115/7635], Loss: 4.1027\n",
      "Epoch [1/1], Step [4116/7635], Loss: 4.0047\n",
      "Epoch [1/1], Step [4117/7635], Loss: 4.0114\n",
      "Epoch [1/1], Step [4118/7635], Loss: 4.1557\n",
      "Epoch [1/1], Step [4119/7635], Loss: 4.0650\n",
      "Epoch [1/1], Step [4120/7635], Loss: 3.9982\n",
      "Epoch [1/1], Step [4121/7635], Loss: 4.0669\n",
      "Epoch [1/1], Step [4122/7635], Loss: 4.0407\n",
      "Epoch [1/1], Step [4123/7635], Loss: 4.0436\n",
      "Epoch [1/1], Step [4124/7635], Loss: 4.0643\n",
      "Epoch [1/1], Step [4125/7635], Loss: 4.0508\n",
      "Epoch [1/1], Step [4126/7635], Loss: 4.0410\n",
      "Epoch [1/1], Step [4127/7635], Loss: 4.0973\n",
      "Epoch [1/1], Step [4128/7635], Loss: 3.9781\n",
      "Epoch [1/1], Step [4129/7635], Loss: 4.0592\n",
      "Epoch [1/1], Step [4130/7635], Loss: 4.0921\n",
      "Epoch [1/1], Step [4131/7635], Loss: 4.0694\n",
      "Epoch [1/1], Step [4132/7635], Loss: 4.1047\n",
      "Epoch [1/1], Step [4133/7635], Loss: 4.0651\n",
      "Epoch [1/1], Step [4134/7635], Loss: 3.9540\n",
      "Epoch [1/1], Step [4135/7635], Loss: 4.1246\n",
      "Epoch [1/1], Step [4136/7635], Loss: 4.0289\n",
      "Epoch [1/1], Step [4137/7635], Loss: 4.0157\n",
      "Epoch [1/1], Step [4138/7635], Loss: 4.1019\n",
      "Epoch [1/1], Step [4139/7635], Loss: 4.0429\n",
      "Epoch [1/1], Step [4140/7635], Loss: 4.0054\n",
      "Epoch [1/1], Step [4141/7635], Loss: 4.1112\n",
      "Epoch [1/1], Step [4142/7635], Loss: 4.0586\n",
      "Epoch [1/1], Step [4143/7635], Loss: 4.0094\n",
      "Epoch [1/1], Step [4144/7635], Loss: 4.0938\n",
      "Epoch [1/1], Step [4145/7635], Loss: 4.0326\n",
      "Epoch [1/1], Step [4146/7635], Loss: 4.0194\n",
      "Epoch [1/1], Step [4147/7635], Loss: 4.0624\n",
      "Epoch [1/1], Step [4148/7635], Loss: 4.0831\n",
      "Epoch [1/1], Step [4149/7635], Loss: 4.0241\n",
      "Epoch [1/1], Step [4150/7635], Loss: 4.0295\n",
      "Epoch [1/1], Step [4151/7635], Loss: 3.9760\n",
      "Epoch [1/1], Step [4152/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [4153/7635], Loss: 4.0377\n",
      "Epoch [1/1], Step [4154/7635], Loss: 4.0177\n",
      "Epoch [1/1], Step [4155/7635], Loss: 4.0930\n",
      "Epoch [1/1], Step [4156/7635], Loss: 4.1275\n",
      "Epoch [1/1], Step [4157/7635], Loss: 4.0194\n",
      "Epoch [1/1], Step [4158/7635], Loss: 4.0541\n",
      "Epoch [1/1], Step [4159/7635], Loss: 4.0470\n",
      "Epoch [1/1], Step [4160/7635], Loss: 4.0635\n",
      "Epoch [1/1], Step [4161/7635], Loss: 4.1120\n",
      "Epoch [1/1], Step [4162/7635], Loss: 4.0780\n",
      "Epoch [1/1], Step [4163/7635], Loss: 4.0990\n",
      "Epoch [1/1], Step [4164/7635], Loss: 4.0658\n",
      "Epoch [1/1], Step [4165/7635], Loss: 4.0053\n",
      "Epoch [1/1], Step [4166/7635], Loss: 4.0484\n",
      "Epoch [1/1], Step [4167/7635], Loss: 4.0358\n",
      "Epoch [1/1], Step [4168/7635], Loss: 4.0457\n",
      "Epoch [1/1], Step [4169/7635], Loss: 4.0554\n",
      "Epoch [1/1], Step [4170/7635], Loss: 4.0955\n",
      "Epoch [1/1], Step [4171/7635], Loss: 4.0360\n",
      "Epoch [1/1], Step [4172/7635], Loss: 4.0061\n",
      "Epoch [1/1], Step [4173/7635], Loss: 4.0385\n",
      "Epoch [1/1], Step [4174/7635], Loss: 4.0881\n",
      "Epoch [1/1], Step [4175/7635], Loss: 4.0945\n",
      "Epoch [1/1], Step [4176/7635], Loss: 3.9640\n",
      "Epoch [1/1], Step [4177/7635], Loss: 4.1288\n",
      "Epoch [1/1], Step [4178/7635], Loss: 4.0000\n",
      "Epoch [1/1], Step [4179/7635], Loss: 4.0480\n",
      "Epoch [1/1], Step [4180/7635], Loss: 4.1342\n",
      "Epoch [1/1], Step [4181/7635], Loss: 4.0513\n",
      "Epoch [1/1], Step [4182/7635], Loss: 4.0743\n",
      "Epoch [1/1], Step [4183/7635], Loss: 4.0388\n",
      "Epoch [1/1], Step [4184/7635], Loss: 3.9994\n",
      "Epoch [1/1], Step [4185/7635], Loss: 4.1282\n",
      "Epoch [1/1], Step [4186/7635], Loss: 4.0296\n",
      "Epoch [1/1], Step [4187/7635], Loss: 4.0491\n",
      "Epoch [1/1], Step [4188/7635], Loss: 4.0270\n",
      "Epoch [1/1], Step [4189/7635], Loss: 4.1094\n",
      "Epoch [1/1], Step [4190/7635], Loss: 4.1450\n",
      "Epoch [1/1], Step [4191/7635], Loss: 4.0347\n",
      "Epoch [1/1], Step [4192/7635], Loss: 4.0207\n",
      "Epoch [1/1], Step [4193/7635], Loss: 4.0548\n",
      "Epoch [1/1], Step [4194/7635], Loss: 4.0149\n",
      "Epoch [1/1], Step [4195/7635], Loss: 4.0939\n",
      "Epoch [1/1], Step [4196/7635], Loss: 4.0275\n",
      "Epoch [1/1], Step [4197/7635], Loss: 4.0538\n",
      "Epoch [1/1], Step [4198/7635], Loss: 4.0659\n",
      "Epoch [1/1], Step [4199/7635], Loss: 4.1167\n",
      "Epoch [1/1], Step [4200/7635], Loss: 4.0571\n",
      "Epoch [1/1], Step [4201/7635], Loss: 4.0398\n",
      "Epoch [1/1], Step [4202/7635], Loss: 4.0317\n",
      "Epoch [1/1], Step [4203/7635], Loss: 4.0775\n",
      "Epoch [1/1], Step [4204/7635], Loss: 4.1570\n",
      "Epoch [1/1], Step [4205/7635], Loss: 4.0443\n",
      "Epoch [1/1], Step [4206/7635], Loss: 4.0200\n",
      "Epoch [1/1], Step [4207/7635], Loss: 4.0380\n",
      "Epoch [1/1], Step [4208/7635], Loss: 4.0725\n",
      "Epoch [1/1], Step [4209/7635], Loss: 4.0959\n",
      "Epoch [1/1], Step [4210/7635], Loss: 3.9939\n",
      "Epoch [1/1], Step [4211/7635], Loss: 4.0644\n",
      "Epoch [1/1], Step [4212/7635], Loss: 4.0788\n",
      "Epoch [1/1], Step [4213/7635], Loss: 4.0784\n",
      "Epoch [1/1], Step [4214/7635], Loss: 4.1085\n",
      "Epoch [1/1], Step [4215/7635], Loss: 4.0720\n",
      "Epoch [1/1], Step [4216/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [4217/7635], Loss: 4.0728\n",
      "Epoch [1/1], Step [4218/7635], Loss: 4.0652\n",
      "Epoch [1/1], Step [4219/7635], Loss: 4.0993\n",
      "Epoch [1/1], Step [4220/7635], Loss: 4.0093\n",
      "Epoch [1/1], Step [4221/7635], Loss: 4.0206\n",
      "Epoch [1/1], Step [4222/7635], Loss: 4.0314\n",
      "Epoch [1/1], Step [4223/7635], Loss: 4.1496\n",
      "Epoch [1/1], Step [4224/7635], Loss: 4.0057\n",
      "Epoch [1/1], Step [4225/7635], Loss: 4.0397\n",
      "Epoch [1/1], Step [4226/7635], Loss: 4.0274\n",
      "Epoch [1/1], Step [4227/7635], Loss: 4.1027\n",
      "Epoch [1/1], Step [4228/7635], Loss: 3.9936\n",
      "Epoch [1/1], Step [4229/7635], Loss: 4.0709\n",
      "Epoch [1/1], Step [4230/7635], Loss: 4.0360\n",
      "Epoch [1/1], Step [4231/7635], Loss: 4.0122\n",
      "Epoch [1/1], Step [4232/7635], Loss: 4.0627\n",
      "Epoch [1/1], Step [4233/7635], Loss: 4.0451\n",
      "Epoch [1/1], Step [4234/7635], Loss: 4.1314\n",
      "Epoch [1/1], Step [4235/7635], Loss: 4.1505\n",
      "Epoch [1/1], Step [4236/7635], Loss: 4.0414\n",
      "Epoch [1/1], Step [4237/7635], Loss: 3.9777\n",
      "Epoch [1/1], Step [4238/7635], Loss: 4.0026\n",
      "Epoch [1/1], Step [4239/7635], Loss: 4.0009\n",
      "Epoch [1/1], Step [4240/7635], Loss: 4.1227\n",
      "Epoch [1/1], Step [4241/7635], Loss: 4.0707\n",
      "Epoch [1/1], Step [4242/7635], Loss: 4.1374\n",
      "Epoch [1/1], Step [4243/7635], Loss: 4.0813\n",
      "Epoch [1/1], Step [4244/7635], Loss: 4.0175\n",
      "Epoch [1/1], Step [4245/7635], Loss: 4.0427\n",
      "Epoch [1/1], Step [4246/7635], Loss: 4.0263\n",
      "Epoch [1/1], Step [4247/7635], Loss: 4.0596\n",
      "Epoch [1/1], Step [4248/7635], Loss: 4.0994\n",
      "Epoch [1/1], Step [4249/7635], Loss: 4.1797\n",
      "Epoch [1/1], Step [4250/7635], Loss: 4.0725\n",
      "Epoch [1/1], Step [4251/7635], Loss: 4.0642\n",
      "Epoch [1/1], Step [4252/7635], Loss: 4.0622\n",
      "Epoch [1/1], Step [4253/7635], Loss: 4.0220\n",
      "Epoch [1/1], Step [4254/7635], Loss: 4.1152\n",
      "Epoch [1/1], Step [4255/7635], Loss: 4.0510\n",
      "Epoch [1/1], Step [4256/7635], Loss: 4.0494\n",
      "Epoch [1/1], Step [4257/7635], Loss: 4.0462\n",
      "Epoch [1/1], Step [4258/7635], Loss: 4.0730\n",
      "Epoch [1/1], Step [4259/7635], Loss: 4.0942\n",
      "Epoch [1/1], Step [4260/7635], Loss: 4.0996\n",
      "Epoch [1/1], Step [4261/7635], Loss: 4.0258\n",
      "Epoch [1/1], Step [4262/7635], Loss: 3.9648\n",
      "Epoch [1/1], Step [4263/7635], Loss: 4.0307\n",
      "Epoch [1/1], Step [4264/7635], Loss: 4.0835\n",
      "Epoch [1/1], Step [4265/7635], Loss: 4.0613\n",
      "Epoch [1/1], Step [4266/7635], Loss: 4.0717\n",
      "Epoch [1/1], Step [4267/7635], Loss: 3.9919\n",
      "Epoch [1/1], Step [4268/7635], Loss: 4.1759\n",
      "Epoch [1/1], Step [4269/7635], Loss: 4.0529\n",
      "Epoch [1/1], Step [4270/7635], Loss: 4.0375\n",
      "Epoch [1/1], Step [4271/7635], Loss: 4.0006\n",
      "Epoch [1/1], Step [4272/7635], Loss: 3.9777\n",
      "Epoch [1/1], Step [4273/7635], Loss: 4.1342\n",
      "Epoch [1/1], Step [4274/7635], Loss: 4.0284\n",
      "Epoch [1/1], Step [4275/7635], Loss: 4.0140\n",
      "Epoch [1/1], Step [4276/7635], Loss: 4.0635\n",
      "Epoch [1/1], Step [4277/7635], Loss: 4.0783\n",
      "Epoch [1/1], Step [4278/7635], Loss: 4.0732\n",
      "Epoch [1/1], Step [4279/7635], Loss: 4.0745\n",
      "Epoch [1/1], Step [4280/7635], Loss: 4.0935\n",
      "Epoch [1/1], Step [4281/7635], Loss: 4.0545\n",
      "Epoch [1/1], Step [4282/7635], Loss: 3.9463\n",
      "Epoch [1/1], Step [4283/7635], Loss: 3.9595\n",
      "Epoch [1/1], Step [4284/7635], Loss: 3.9750\n",
      "Epoch [1/1], Step [4285/7635], Loss: 4.0349\n",
      "Epoch [1/1], Step [4286/7635], Loss: 4.0516\n",
      "Epoch [1/1], Step [4287/7635], Loss: 4.0480\n",
      "Epoch [1/1], Step [4288/7635], Loss: 3.9932\n",
      "Epoch [1/1], Step [4289/7635], Loss: 4.0638\n",
      "Epoch [1/1], Step [4290/7635], Loss: 4.0570\n",
      "Epoch [1/1], Step [4291/7635], Loss: 4.0392\n",
      "Epoch [1/1], Step [4292/7635], Loss: 4.0431\n",
      "Epoch [1/1], Step [4293/7635], Loss: 4.1031\n",
      "Epoch [1/1], Step [4294/7635], Loss: 4.0216\n",
      "Epoch [1/1], Step [4295/7635], Loss: 3.9429\n",
      "Epoch [1/1], Step [4296/7635], Loss: 4.0782\n",
      "Epoch [1/1], Step [4297/7635], Loss: 4.0195\n",
      "Epoch [1/1], Step [4298/7635], Loss: 4.0816\n",
      "Epoch [1/1], Step [4299/7635], Loss: 4.0326\n",
      "Epoch [1/1], Step [4300/7635], Loss: 4.0719\n",
      "Epoch [1/1], Step [4301/7635], Loss: 4.0834\n",
      "Epoch [1/1], Step [4302/7635], Loss: 4.1129\n",
      "Epoch [1/1], Step [4303/7635], Loss: 4.0196\n",
      "Epoch [1/1], Step [4304/7635], Loss: 4.1169\n",
      "Epoch [1/1], Step [4305/7635], Loss: 4.0992\n",
      "Epoch [1/1], Step [4306/7635], Loss: 4.0381\n",
      "Epoch [1/1], Step [4307/7635], Loss: 4.1230\n",
      "Epoch [1/1], Step [4308/7635], Loss: 4.0026\n",
      "Epoch [1/1], Step [4309/7635], Loss: 4.0790\n",
      "Epoch [1/1], Step [4310/7635], Loss: 4.0192\n",
      "Epoch [1/1], Step [4311/7635], Loss: 4.0195\n",
      "Epoch [1/1], Step [4312/7635], Loss: 4.0280\n",
      "Epoch [1/1], Step [4313/7635], Loss: 4.0769\n",
      "Epoch [1/1], Step [4314/7635], Loss: 4.0469\n",
      "Epoch [1/1], Step [4315/7635], Loss: 4.0021\n",
      "Epoch [1/1], Step [4316/7635], Loss: 4.0225\n",
      "Epoch [1/1], Step [4317/7635], Loss: 4.0220\n",
      "Epoch [1/1], Step [4318/7635], Loss: 4.0012\n",
      "Epoch [1/1], Step [4319/7635], Loss: 4.0552\n",
      "Epoch [1/1], Step [4320/7635], Loss: 4.0504\n",
      "Epoch [1/1], Step [4321/7635], Loss: 3.9848\n",
      "Epoch [1/1], Step [4322/7635], Loss: 4.0856\n",
      "Epoch [1/1], Step [4323/7635], Loss: 4.0108\n",
      "Epoch [1/1], Step [4324/7635], Loss: 4.0654\n",
      "Epoch [1/1], Step [4325/7635], Loss: 4.0197\n",
      "Epoch [1/1], Step [4326/7635], Loss: 4.0943\n",
      "Epoch [1/1], Step [4327/7635], Loss: 4.0826\n",
      "Epoch [1/1], Step [4328/7635], Loss: 4.0873\n",
      "Epoch [1/1], Step [4329/7635], Loss: 4.0330\n",
      "Epoch [1/1], Step [4330/7635], Loss: 4.0235\n",
      "Epoch [1/1], Step [4331/7635], Loss: 4.0607\n",
      "Epoch [1/1], Step [4332/7635], Loss: 4.0027\n",
      "Epoch [1/1], Step [4333/7635], Loss: 4.0843\n",
      "Epoch [1/1], Step [4334/7635], Loss: 4.0121\n",
      "Epoch [1/1], Step [4335/7635], Loss: 4.0239\n",
      "Epoch [1/1], Step [4336/7635], Loss: 4.0591\n",
      "Epoch [1/1], Step [4337/7635], Loss: 3.9470\n",
      "Epoch [1/1], Step [4338/7635], Loss: 4.0395\n",
      "Epoch [1/1], Step [4339/7635], Loss: 4.0341\n",
      "Epoch [1/1], Step [4340/7635], Loss: 4.0236\n",
      "Epoch [1/1], Step [4341/7635], Loss: 3.9818\n",
      "Epoch [1/1], Step [4342/7635], Loss: 4.0387\n",
      "Epoch [1/1], Step [4343/7635], Loss: 4.0147\n",
      "Epoch [1/1], Step [4344/7635], Loss: 4.0100\n",
      "Epoch [1/1], Step [4345/7635], Loss: 4.0606\n",
      "Epoch [1/1], Step [4346/7635], Loss: 4.0374\n",
      "Epoch [1/1], Step [4347/7635], Loss: 4.0512\n",
      "Epoch [1/1], Step [4348/7635], Loss: 4.0400\n",
      "Epoch [1/1], Step [4349/7635], Loss: 4.0041\n",
      "Epoch [1/1], Step [4350/7635], Loss: 4.0274\n",
      "Epoch [1/1], Step [4351/7635], Loss: 4.0397\n",
      "Epoch [1/1], Step [4352/7635], Loss: 3.9759\n",
      "Epoch [1/1], Step [4353/7635], Loss: 4.0269\n",
      "Epoch [1/1], Step [4354/7635], Loss: 4.0762\n",
      "Epoch [1/1], Step [4355/7635], Loss: 4.0609\n",
      "Epoch [1/1], Step [4356/7635], Loss: 3.9611\n",
      "Epoch [1/1], Step [4357/7635], Loss: 4.0366\n",
      "Epoch [1/1], Step [4358/7635], Loss: 4.0526\n",
      "Epoch [1/1], Step [4359/7635], Loss: 3.9752\n",
      "Epoch [1/1], Step [4360/7635], Loss: 4.0057\n",
      "Epoch [1/1], Step [4361/7635], Loss: 4.0869\n",
      "Epoch [1/1], Step [4362/7635], Loss: 4.0219\n",
      "Epoch [1/1], Step [4363/7635], Loss: 4.0628\n",
      "Epoch [1/1], Step [4364/7635], Loss: 4.0367\n",
      "Epoch [1/1], Step [4365/7635], Loss: 4.1248\n",
      "Epoch [1/1], Step [4366/7635], Loss: 4.2190\n",
      "Epoch [1/1], Step [4367/7635], Loss: 4.0480\n",
      "Epoch [1/1], Step [4368/7635], Loss: 4.0246\n",
      "Epoch [1/1], Step [4369/7635], Loss: 3.9451\n",
      "Epoch [1/1], Step [4370/7635], Loss: 4.0630\n",
      "Epoch [1/1], Step [4371/7635], Loss: 3.9694\n",
      "Epoch [1/1], Step [4372/7635], Loss: 3.9991\n",
      "Epoch [1/1], Step [4373/7635], Loss: 4.0209\n",
      "Epoch [1/1], Step [4374/7635], Loss: 4.0564\n",
      "Epoch [1/1], Step [4375/7635], Loss: 4.0578\n",
      "Epoch [1/1], Step [4376/7635], Loss: 4.0463\n",
      "Epoch [1/1], Step [4377/7635], Loss: 4.0422\n",
      "Epoch [1/1], Step [4378/7635], Loss: 3.9678\n",
      "Epoch [1/1], Step [4379/7635], Loss: 4.0598\n",
      "Epoch [1/1], Step [4380/7635], Loss: 4.0252\n",
      "Epoch [1/1], Step [4381/7635], Loss: 4.0159\n",
      "Epoch [1/1], Step [4382/7635], Loss: 4.0625\n",
      "Epoch [1/1], Step [4383/7635], Loss: 4.0815\n",
      "Epoch [1/1], Step [4384/7635], Loss: 4.1016\n",
      "Epoch [1/1], Step [4385/7635], Loss: 3.9751\n",
      "Epoch [1/1], Step [4386/7635], Loss: 4.0009\n",
      "Epoch [1/1], Step [4387/7635], Loss: 4.0800\n",
      "Epoch [1/1], Step [4388/7635], Loss: 3.9765\n",
      "Epoch [1/1], Step [4389/7635], Loss: 4.0014\n",
      "Epoch [1/1], Step [4390/7635], Loss: 4.0142\n",
      "Epoch [1/1], Step [4391/7635], Loss: 4.0133\n",
      "Epoch [1/1], Step [4392/7635], Loss: 3.9958\n",
      "Epoch [1/1], Step [4393/7635], Loss: 4.0764\n",
      "Epoch [1/1], Step [4394/7635], Loss: 3.9484\n",
      "Epoch [1/1], Step [4395/7635], Loss: 4.0031\n",
      "Epoch [1/1], Step [4396/7635], Loss: 4.0432\n",
      "Epoch [1/1], Step [4397/7635], Loss: 4.0546\n",
      "Epoch [1/1], Step [4398/7635], Loss: 4.0968\n",
      "Epoch [1/1], Step [4399/7635], Loss: 4.0238\n",
      "Epoch [1/1], Step [4400/7635], Loss: 4.0327\n",
      "Epoch [1/1], Step [4401/7635], Loss: 4.0399\n",
      "Epoch [1/1], Step [4402/7635], Loss: 4.0241\n",
      "Epoch [1/1], Step [4403/7635], Loss: 4.0482\n",
      "Epoch [1/1], Step [4404/7635], Loss: 4.0203\n",
      "Epoch [1/1], Step [4405/7635], Loss: 4.0177\n",
      "Epoch [1/1], Step [4406/7635], Loss: 3.9958\n",
      "Epoch [1/1], Step [4407/7635], Loss: 4.0485\n",
      "Epoch [1/1], Step [4408/7635], Loss: 4.0729\n",
      "Epoch [1/1], Step [4409/7635], Loss: 4.0489\n",
      "Epoch [1/1], Step [4410/7635], Loss: 3.9912\n",
      "Epoch [1/1], Step [4411/7635], Loss: 3.9880\n",
      "Epoch [1/1], Step [4412/7635], Loss: 4.0029\n",
      "Epoch [1/1], Step [4413/7635], Loss: 3.9985\n",
      "Epoch [1/1], Step [4414/7635], Loss: 4.0551\n",
      "Epoch [1/1], Step [4415/7635], Loss: 4.0872\n",
      "Epoch [1/1], Step [4416/7635], Loss: 3.9847\n",
      "Epoch [1/1], Step [4417/7635], Loss: 4.0387\n",
      "Epoch [1/1], Step [4418/7635], Loss: 4.0151\n",
      "Epoch [1/1], Step [4419/7635], Loss: 4.0610\n",
      "Epoch [1/1], Step [4420/7635], Loss: 4.0142\n",
      "Epoch [1/1], Step [4421/7635], Loss: 4.0714\n",
      "Epoch [1/1], Step [4422/7635], Loss: 4.0932\n",
      "Epoch [1/1], Step [4423/7635], Loss: 4.0281\n",
      "Epoch [1/1], Step [4424/7635], Loss: 4.0115\n",
      "Epoch [1/1], Step [4425/7635], Loss: 3.9527\n",
      "Epoch [1/1], Step [4426/7635], Loss: 4.0272\n",
      "Epoch [1/1], Step [4427/7635], Loss: 4.0763\n",
      "Epoch [1/1], Step [4428/7635], Loss: 4.0659\n",
      "Epoch [1/1], Step [4429/7635], Loss: 3.9947\n",
      "Epoch [1/1], Step [4430/7635], Loss: 3.9894\n",
      "Epoch [1/1], Step [4431/7635], Loss: 4.0517\n",
      "Epoch [1/1], Step [4432/7635], Loss: 4.0779\n",
      "Epoch [1/1], Step [4433/7635], Loss: 4.0489\n",
      "Epoch [1/1], Step [4434/7635], Loss: 3.9745\n",
      "Epoch [1/1], Step [4435/7635], Loss: 4.0800\n",
      "Epoch [1/1], Step [4436/7635], Loss: 3.9794\n",
      "Epoch [1/1], Step [4437/7635], Loss: 4.0532\n",
      "Epoch [1/1], Step [4438/7635], Loss: 4.0634\n",
      "Epoch [1/1], Step [4439/7635], Loss: 3.9382\n",
      "Epoch [1/1], Step [4440/7635], Loss: 4.0500\n",
      "Epoch [1/1], Step [4441/7635], Loss: 4.0188\n",
      "Epoch [1/1], Step [4442/7635], Loss: 4.0738\n",
      "Epoch [1/1], Step [4443/7635], Loss: 3.9822\n",
      "Epoch [1/1], Step [4444/7635], Loss: 3.9960\n",
      "Epoch [1/1], Step [4445/7635], Loss: 4.0358\n",
      "Epoch [1/1], Step [4446/7635], Loss: 4.0476\n",
      "Epoch [1/1], Step [4447/7635], Loss: 4.1197\n",
      "Epoch [1/1], Step [4448/7635], Loss: 4.0487\n",
      "Epoch [1/1], Step [4449/7635], Loss: 4.0398\n",
      "Epoch [1/1], Step [4450/7635], Loss: 4.1076\n",
      "Epoch [1/1], Step [4451/7635], Loss: 3.9723\n",
      "Epoch [1/1], Step [4452/7635], Loss: 3.9857\n",
      "Epoch [1/1], Step [4453/7635], Loss: 4.0529\n",
      "Epoch [1/1], Step [4454/7635], Loss: 4.0168\n",
      "Epoch [1/1], Step [4455/7635], Loss: 3.9791\n",
      "Epoch [1/1], Step [4456/7635], Loss: 4.0245\n",
      "Epoch [1/1], Step [4457/7635], Loss: 4.0198\n",
      "Epoch [1/1], Step [4458/7635], Loss: 4.0206\n",
      "Epoch [1/1], Step [4459/7635], Loss: 4.0041\n",
      "Epoch [1/1], Step [4460/7635], Loss: 4.0442\n",
      "Epoch [1/1], Step [4461/7635], Loss: 4.0466\n",
      "Epoch [1/1], Step [4462/7635], Loss: 4.0189\n",
      "Epoch [1/1], Step [4463/7635], Loss: 4.0683\n",
      "Epoch [1/1], Step [4464/7635], Loss: 4.0599\n",
      "Epoch [1/1], Step [4465/7635], Loss: 4.0491\n",
      "Epoch [1/1], Step [4466/7635], Loss: 4.0886\n",
      "Epoch [1/1], Step [4467/7635], Loss: 3.9896\n",
      "Epoch [1/1], Step [4468/7635], Loss: 3.9870\n",
      "Epoch [1/1], Step [4469/7635], Loss: 4.1179\n",
      "Epoch [1/1], Step [4470/7635], Loss: 4.0110\n",
      "Epoch [1/1], Step [4471/7635], Loss: 4.0789\n",
      "Epoch [1/1], Step [4472/7635], Loss: 4.0586\n",
      "Epoch [1/1], Step [4473/7635], Loss: 4.0813\n",
      "Epoch [1/1], Step [4474/7635], Loss: 4.0052\n",
      "Epoch [1/1], Step [4475/7635], Loss: 3.9874\n",
      "Epoch [1/1], Step [4476/7635], Loss: 4.0569\n",
      "Epoch [1/1], Step [4477/7635], Loss: 4.0738\n",
      "Epoch [1/1], Step [4478/7635], Loss: 3.9986\n",
      "Epoch [1/1], Step [4479/7635], Loss: 3.9996\n",
      "Epoch [1/1], Step [4480/7635], Loss: 4.0895\n",
      "Epoch [1/1], Step [4481/7635], Loss: 4.0342\n",
      "Epoch [1/1], Step [4482/7635], Loss: 4.0393\n",
      "Epoch [1/1], Step [4483/7635], Loss: 4.0389\n",
      "Epoch [1/1], Step [4484/7635], Loss: 3.9557\n",
      "Epoch [1/1], Step [4485/7635], Loss: 4.0104\n",
      "Epoch [1/1], Step [4486/7635], Loss: 4.0701\n",
      "Epoch [1/1], Step [4487/7635], Loss: 3.9860\n",
      "Epoch [1/1], Step [4488/7635], Loss: 3.9702\n",
      "Epoch [1/1], Step [4489/7635], Loss: 4.0503\n",
      "Epoch [1/1], Step [4490/7635], Loss: 4.0783\n",
      "Epoch [1/1], Step [4491/7635], Loss: 3.9847\n",
      "Epoch [1/1], Step [4492/7635], Loss: 4.0888\n",
      "Epoch [1/1], Step [4493/7635], Loss: 4.0784\n",
      "Epoch [1/1], Step [4494/7635], Loss: 4.0278\n",
      "Epoch [1/1], Step [4495/7635], Loss: 4.0927\n",
      "Epoch [1/1], Step [4496/7635], Loss: 3.9512\n",
      "Epoch [1/1], Step [4497/7635], Loss: 4.0497\n",
      "Epoch [1/1], Step [4498/7635], Loss: 4.0987\n",
      "Epoch [1/1], Step [4499/7635], Loss: 4.0455\n",
      "Epoch [1/1], Step [4500/7635], Loss: 3.9974\n",
      "Epoch [1/1], Step [4501/7635], Loss: 3.9547\n",
      "Epoch [1/1], Step [4502/7635], Loss: 4.0149\n",
      "Epoch [1/1], Step [4503/7635], Loss: 3.9968\n",
      "Epoch [1/1], Step [4504/7635], Loss: 4.0995\n",
      "Epoch [1/1], Step [4505/7635], Loss: 4.0853\n",
      "Epoch [1/1], Step [4506/7635], Loss: 4.0344\n",
      "Epoch [1/1], Step [4507/7635], Loss: 4.0114\n",
      "Epoch [1/1], Step [4508/7635], Loss: 4.0709\n",
      "Epoch [1/1], Step [4509/7635], Loss: 4.0163\n",
      "Epoch [1/1], Step [4510/7635], Loss: 4.0986\n",
      "Epoch [1/1], Step [4511/7635], Loss: 3.9533\n",
      "Epoch [1/1], Step [4512/7635], Loss: 4.0312\n",
      "Epoch [1/1], Step [4513/7635], Loss: 4.0459\n",
      "Epoch [1/1], Step [4514/7635], Loss: 4.0637\n",
      "Epoch [1/1], Step [4515/7635], Loss: 4.0077\n",
      "Epoch [1/1], Step [4516/7635], Loss: 4.0703\n",
      "Epoch [1/1], Step [4517/7635], Loss: 4.0472\n",
      "Epoch [1/1], Step [4518/7635], Loss: 4.0298\n",
      "Epoch [1/1], Step [4519/7635], Loss: 3.9839\n",
      "Epoch [1/1], Step [4520/7635], Loss: 4.0091\n",
      "Epoch [1/1], Step [4521/7635], Loss: 4.0619\n",
      "Epoch [1/1], Step [4522/7635], Loss: 3.9871\n",
      "Epoch [1/1], Step [4523/7635], Loss: 4.0734\n",
      "Epoch [1/1], Step [4524/7635], Loss: 4.0684\n",
      "Epoch [1/1], Step [4525/7635], Loss: 4.0049\n",
      "Epoch [1/1], Step [4526/7635], Loss: 3.9874\n",
      "Epoch [1/1], Step [4527/7635], Loss: 3.9863\n",
      "Epoch [1/1], Step [4528/7635], Loss: 4.0793\n",
      "Epoch [1/1], Step [4529/7635], Loss: 4.0091\n",
      "Epoch [1/1], Step [4530/7635], Loss: 4.0542\n",
      "Epoch [1/1], Step [4531/7635], Loss: 3.9868\n",
      "Epoch [1/1], Step [4532/7635], Loss: 3.9841\n",
      "Epoch [1/1], Step [4533/7635], Loss: 3.9696\n",
      "Epoch [1/1], Step [4534/7635], Loss: 4.0266\n",
      "Epoch [1/1], Step [4535/7635], Loss: 4.0529\n",
      "Epoch [1/1], Step [4536/7635], Loss: 4.0696\n",
      "Epoch [1/1], Step [4537/7635], Loss: 3.9948\n",
      "Epoch [1/1], Step [4538/7635], Loss: 4.0307\n",
      "Epoch [1/1], Step [4539/7635], Loss: 4.0408\n",
      "Epoch [1/1], Step [4540/7635], Loss: 4.0674\n",
      "Epoch [1/1], Step [4541/7635], Loss: 4.0630\n",
      "Epoch [1/1], Step [4542/7635], Loss: 4.0812\n",
      "Epoch [1/1], Step [4543/7635], Loss: 3.9918\n",
      "Epoch [1/1], Step [4544/7635], Loss: 4.0375\n",
      "Epoch [1/1], Step [4545/7635], Loss: 4.1316\n",
      "Epoch [1/1], Step [4546/7635], Loss: 4.0575\n",
      "Epoch [1/1], Step [4547/7635], Loss: 4.0984\n",
      "Epoch [1/1], Step [4548/7635], Loss: 4.0502\n",
      "Epoch [1/1], Step [4549/7635], Loss: 4.0779\n",
      "Epoch [1/1], Step [4550/7635], Loss: 4.0162\n",
      "Epoch [1/1], Step [4551/7635], Loss: 3.9971\n",
      "Epoch [1/1], Step [4552/7635], Loss: 3.9742\n",
      "Epoch [1/1], Step [4553/7635], Loss: 3.9983\n",
      "Epoch [1/1], Step [4554/7635], Loss: 4.0112\n",
      "Epoch [1/1], Step [4555/7635], Loss: 4.0244\n",
      "Epoch [1/1], Step [4556/7635], Loss: 4.0356\n",
      "Epoch [1/1], Step [4557/7635], Loss: 3.9639\n",
      "Epoch [1/1], Step [4558/7635], Loss: 4.0110\n",
      "Epoch [1/1], Step [4559/7635], Loss: 4.0170\n",
      "Epoch [1/1], Step [4560/7635], Loss: 4.0586\n",
      "Epoch [1/1], Step [4561/7635], Loss: 4.0181\n",
      "Epoch [1/1], Step [4562/7635], Loss: 4.0473\n",
      "Epoch [1/1], Step [4563/7635], Loss: 4.0505\n",
      "Epoch [1/1], Step [4564/7635], Loss: 4.0709\n",
      "Epoch [1/1], Step [4565/7635], Loss: 4.0404\n",
      "Epoch [1/1], Step [4566/7635], Loss: 4.0254\n",
      "Epoch [1/1], Step [4567/7635], Loss: 4.0229\n",
      "Epoch [1/1], Step [4568/7635], Loss: 4.0340\n",
      "Epoch [1/1], Step [4569/7635], Loss: 3.9841\n",
      "Epoch [1/1], Step [4570/7635], Loss: 4.0597\n",
      "Epoch [1/1], Step [4571/7635], Loss: 4.0518\n",
      "Epoch [1/1], Step [4572/7635], Loss: 4.0349\n",
      "Epoch [1/1], Step [4573/7635], Loss: 3.9726\n",
      "Epoch [1/1], Step [4574/7635], Loss: 4.0165\n",
      "Epoch [1/1], Step [4575/7635], Loss: 4.0488\n",
      "Epoch [1/1], Step [4576/7635], Loss: 4.0771\n",
      "Epoch [1/1], Step [4577/7635], Loss: 4.0402\n",
      "Epoch [1/1], Step [4578/7635], Loss: 4.0193\n",
      "Epoch [1/1], Step [4579/7635], Loss: 4.0988\n",
      "Epoch [1/1], Step [4580/7635], Loss: 4.0584\n",
      "Epoch [1/1], Step [4581/7635], Loss: 4.0190\n",
      "Epoch [1/1], Step [4582/7635], Loss: 3.9785\n",
      "Epoch [1/1], Step [4583/7635], Loss: 4.0596\n",
      "Epoch [1/1], Step [4584/7635], Loss: 3.9521\n",
      "Epoch [1/1], Step [4585/7635], Loss: 4.0716\n",
      "Epoch [1/1], Step [4586/7635], Loss: 4.0188\n",
      "Epoch [1/1], Step [4587/7635], Loss: 4.0386\n",
      "Epoch [1/1], Step [4588/7635], Loss: 4.1451\n",
      "Epoch [1/1], Step [4589/7635], Loss: 3.9942\n",
      "Epoch [1/1], Step [4590/7635], Loss: 4.0269\n",
      "Epoch [1/1], Step [4591/7635], Loss: 4.0046\n",
      "Epoch [1/1], Step [4592/7635], Loss: 4.0916\n",
      "Epoch [1/1], Step [4593/7635], Loss: 4.0168\n",
      "Epoch [1/1], Step [4594/7635], Loss: 4.1168\n",
      "Epoch [1/1], Step [4595/7635], Loss: 4.0971\n",
      "Epoch [1/1], Step [4596/7635], Loss: 4.0376\n",
      "Epoch [1/1], Step [4597/7635], Loss: 4.0300\n",
      "Epoch [1/1], Step [4598/7635], Loss: 3.9309\n",
      "Epoch [1/1], Step [4599/7635], Loss: 4.0288\n",
      "Epoch [1/1], Step [4600/7635], Loss: 4.0408\n",
      "Epoch [1/1], Step [4601/7635], Loss: 4.0317\n",
      "Epoch [1/1], Step [4602/7635], Loss: 4.0371\n",
      "Epoch [1/1], Step [4603/7635], Loss: 4.0244\n",
      "Epoch [1/1], Step [4604/7635], Loss: 4.0215\n",
      "Epoch [1/1], Step [4605/7635], Loss: 4.0500\n",
      "Epoch [1/1], Step [4606/7635], Loss: 3.9821\n",
      "Epoch [1/1], Step [4607/7635], Loss: 4.0237\n",
      "Epoch [1/1], Step [4608/7635], Loss: 4.0364\n",
      "Epoch [1/1], Step [4609/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [4610/7635], Loss: 4.0389\n",
      "Epoch [1/1], Step [4611/7635], Loss: 4.0448\n",
      "Epoch [1/1], Step [4612/7635], Loss: 4.0078\n",
      "Epoch [1/1], Step [4613/7635], Loss: 4.0662\n",
      "Epoch [1/1], Step [4614/7635], Loss: 4.0347\n",
      "Epoch [1/1], Step [4615/7635], Loss: 4.0093\n",
      "Epoch [1/1], Step [4616/7635], Loss: 3.9466\n",
      "Epoch [1/1], Step [4617/7635], Loss: 3.9625\n",
      "Epoch [1/1], Step [4618/7635], Loss: 4.0047\n",
      "Epoch [1/1], Step [4619/7635], Loss: 4.0623\n",
      "Epoch [1/1], Step [4620/7635], Loss: 4.0341\n",
      "Epoch [1/1], Step [4621/7635], Loss: 3.9777\n",
      "Epoch [1/1], Step [4622/7635], Loss: 4.0226\n",
      "Epoch [1/1], Step [4623/7635], Loss: 4.0308\n",
      "Epoch [1/1], Step [4624/7635], Loss: 4.0565\n",
      "Epoch [1/1], Step [4625/7635], Loss: 4.0228\n",
      "Epoch [1/1], Step [4626/7635], Loss: 4.0719\n",
      "Epoch [1/1], Step [4627/7635], Loss: 3.9716\n",
      "Epoch [1/1], Step [4628/7635], Loss: 4.0821\n",
      "Epoch [1/1], Step [4629/7635], Loss: 4.0007\n",
      "Epoch [1/1], Step [4630/7635], Loss: 4.0799\n",
      "Epoch [1/1], Step [4631/7635], Loss: 4.1044\n",
      "Epoch [1/1], Step [4632/7635], Loss: 4.0453\n",
      "Epoch [1/1], Step [4633/7635], Loss: 4.0337\n",
      "Epoch [1/1], Step [4634/7635], Loss: 4.0508\n",
      "Epoch [1/1], Step [4635/7635], Loss: 4.0485\n",
      "Epoch [1/1], Step [4636/7635], Loss: 3.9782\n",
      "Epoch [1/1], Step [4637/7635], Loss: 4.0286\n",
      "Epoch [1/1], Step [4638/7635], Loss: 4.0406\n",
      "Epoch [1/1], Step [4639/7635], Loss: 4.0967\n",
      "Epoch [1/1], Step [4640/7635], Loss: 3.9987\n",
      "Epoch [1/1], Step [4641/7635], Loss: 4.0412\n",
      "Epoch [1/1], Step [4642/7635], Loss: 4.0458\n",
      "Epoch [1/1], Step [4643/7635], Loss: 4.0285\n",
      "Epoch [1/1], Step [4644/7635], Loss: 4.0562\n",
      "Epoch [1/1], Step [4645/7635], Loss: 4.0295\n",
      "Epoch [1/1], Step [4646/7635], Loss: 4.0222\n",
      "Epoch [1/1], Step [4647/7635], Loss: 3.9973\n",
      "Epoch [1/1], Step [4648/7635], Loss: 4.0623\n",
      "Epoch [1/1], Step [4649/7635], Loss: 4.0255\n",
      "Epoch [1/1], Step [4650/7635], Loss: 4.0712\n",
      "Epoch [1/1], Step [4651/7635], Loss: 4.0157\n",
      "Epoch [1/1], Step [4652/7635], Loss: 4.0228\n",
      "Epoch [1/1], Step [4653/7635], Loss: 4.0570\n",
      "Epoch [1/1], Step [4654/7635], Loss: 4.0678\n",
      "Epoch [1/1], Step [4655/7635], Loss: 4.0205\n",
      "Epoch [1/1], Step [4656/7635], Loss: 4.0286\n",
      "Epoch [1/1], Step [4657/7635], Loss: 4.0350\n",
      "Epoch [1/1], Step [4658/7635], Loss: 4.0218\n",
      "Epoch [1/1], Step [4659/7635], Loss: 4.0727\n",
      "Epoch [1/1], Step [4660/7635], Loss: 3.9697\n",
      "Epoch [1/1], Step [4661/7635], Loss: 4.0902\n",
      "Epoch [1/1], Step [4662/7635], Loss: 4.0269\n",
      "Epoch [1/1], Step [4663/7635], Loss: 4.0641\n",
      "Epoch [1/1], Step [4664/7635], Loss: 3.9900\n",
      "Epoch [1/1], Step [4665/7635], Loss: 4.0714\n",
      "Epoch [1/1], Step [4666/7635], Loss: 4.1308\n",
      "Epoch [1/1], Step [4667/7635], Loss: 4.0579\n",
      "Epoch [1/1], Step [4668/7635], Loss: 4.1350\n",
      "Epoch [1/1], Step [4669/7635], Loss: 4.0142\n",
      "Epoch [1/1], Step [4670/7635], Loss: 4.0124\n",
      "Epoch [1/1], Step [4671/7635], Loss: 4.0158\n",
      "Epoch [1/1], Step [4672/7635], Loss: 4.0249\n",
      "Epoch [1/1], Step [4673/7635], Loss: 4.0196\n",
      "Epoch [1/1], Step [4674/7635], Loss: 4.0019\n",
      "Epoch [1/1], Step [4675/7635], Loss: 3.9407\n",
      "Epoch [1/1], Step [4676/7635], Loss: 3.9489\n",
      "Epoch [1/1], Step [4677/7635], Loss: 3.9823\n",
      "Epoch [1/1], Step [4678/7635], Loss: 4.0283\n",
      "Epoch [1/1], Step [4679/7635], Loss: 4.0644\n",
      "Epoch [1/1], Step [4680/7635], Loss: 4.0233\n",
      "Epoch [1/1], Step [4681/7635], Loss: 4.0209\n",
      "Epoch [1/1], Step [4682/7635], Loss: 4.0132\n",
      "Epoch [1/1], Step [4683/7635], Loss: 3.9475\n",
      "Epoch [1/1], Step [4684/7635], Loss: 4.0356\n",
      "Epoch [1/1], Step [4685/7635], Loss: 3.9804\n",
      "Epoch [1/1], Step [4686/7635], Loss: 4.0812\n",
      "Epoch [1/1], Step [4687/7635], Loss: 3.9687\n",
      "Epoch [1/1], Step [4688/7635], Loss: 4.0337\n",
      "Epoch [1/1], Step [4689/7635], Loss: 4.0677\n",
      "Epoch [1/1], Step [4690/7635], Loss: 3.9905\n",
      "Epoch [1/1], Step [4691/7635], Loss: 3.8952\n",
      "Epoch [1/1], Step [4692/7635], Loss: 4.0158\n",
      "Epoch [1/1], Step [4693/7635], Loss: 4.0159\n",
      "Epoch [1/1], Step [4694/7635], Loss: 4.0213\n",
      "Epoch [1/1], Step [4695/7635], Loss: 4.0249\n",
      "Epoch [1/1], Step [4696/7635], Loss: 4.0061\n",
      "Epoch [1/1], Step [4697/7635], Loss: 4.0423\n",
      "Epoch [1/1], Step [4698/7635], Loss: 4.0428\n",
      "Epoch [1/1], Step [4699/7635], Loss: 4.0081\n",
      "Epoch [1/1], Step [4700/7635], Loss: 4.0083\n",
      "Epoch [1/1], Step [4701/7635], Loss: 4.0503\n",
      "Epoch [1/1], Step [4702/7635], Loss: 3.9826\n",
      "Epoch [1/1], Step [4703/7635], Loss: 3.9746\n",
      "Epoch [1/1], Step [4704/7635], Loss: 4.0195\n",
      "Epoch [1/1], Step [4705/7635], Loss: 4.0292\n",
      "Epoch [1/1], Step [4706/7635], Loss: 4.0808\n",
      "Epoch [1/1], Step [4707/7635], Loss: 3.9973\n",
      "Epoch [1/1], Step [4708/7635], Loss: 3.9902\n",
      "Epoch [1/1], Step [4709/7635], Loss: 4.0121\n",
      "Epoch [1/1], Step [4710/7635], Loss: 4.0163\n",
      "Epoch [1/1], Step [4711/7635], Loss: 3.9932\n",
      "Epoch [1/1], Step [4712/7635], Loss: 4.0544\n",
      "Epoch [1/1], Step [4713/7635], Loss: 4.0364\n",
      "Epoch [1/1], Step [4714/7635], Loss: 3.9610\n",
      "Epoch [1/1], Step [4715/7635], Loss: 4.0072\n",
      "Epoch [1/1], Step [4716/7635], Loss: 4.0404\n",
      "Epoch [1/1], Step [4717/7635], Loss: 4.0185\n",
      "Epoch [1/1], Step [4718/7635], Loss: 4.0319\n",
      "Epoch [1/1], Step [4719/7635], Loss: 3.9988\n",
      "Epoch [1/1], Step [4720/7635], Loss: 4.0121\n",
      "Epoch [1/1], Step [4721/7635], Loss: 3.9904\n",
      "Epoch [1/1], Step [4722/7635], Loss: 4.0604\n",
      "Epoch [1/1], Step [4723/7635], Loss: 4.0176\n",
      "Epoch [1/1], Step [4724/7635], Loss: 4.0316\n",
      "Epoch [1/1], Step [4725/7635], Loss: 4.1137\n",
      "Epoch [1/1], Step [4726/7635], Loss: 3.9913\n",
      "Epoch [1/1], Step [4727/7635], Loss: 4.0616\n",
      "Epoch [1/1], Step [4728/7635], Loss: 3.9919\n",
      "Epoch [1/1], Step [4729/7635], Loss: 3.9751\n",
      "Epoch [1/1], Step [4730/7635], Loss: 4.0492\n",
      "Epoch [1/1], Step [4731/7635], Loss: 4.0742\n",
      "Epoch [1/1], Step [4732/7635], Loss: 3.9917\n",
      "Epoch [1/1], Step [4733/7635], Loss: 4.0522\n",
      "Epoch [1/1], Step [4734/7635], Loss: 3.9050\n",
      "Epoch [1/1], Step [4735/7635], Loss: 3.9936\n",
      "Epoch [1/1], Step [4736/7635], Loss: 4.0059\n",
      "Epoch [1/1], Step [4737/7635], Loss: 4.0470\n",
      "Epoch [1/1], Step [4738/7635], Loss: 4.0193\n",
      "Epoch [1/1], Step [4739/7635], Loss: 3.9523\n",
      "Epoch [1/1], Step [4740/7635], Loss: 4.0564\n",
      "Epoch [1/1], Step [4741/7635], Loss: 4.0101\n",
      "Epoch [1/1], Step [4742/7635], Loss: 4.0274\n",
      "Epoch [1/1], Step [4743/7635], Loss: 4.0294\n",
      "Epoch [1/1], Step [4744/7635], Loss: 4.0063\n",
      "Epoch [1/1], Step [4745/7635], Loss: 3.9557\n",
      "Epoch [1/1], Step [4746/7635], Loss: 4.0855\n",
      "Epoch [1/1], Step [4747/7635], Loss: 4.0556\n",
      "Epoch [1/1], Step [4748/7635], Loss: 4.0369\n",
      "Epoch [1/1], Step [4749/7635], Loss: 4.0253\n",
      "Epoch [1/1], Step [4750/7635], Loss: 4.0548\n",
      "Epoch [1/1], Step [4751/7635], Loss: 4.0430\n",
      "Epoch [1/1], Step [4752/7635], Loss: 4.0431\n",
      "Epoch [1/1], Step [4753/7635], Loss: 4.0486\n",
      "Epoch [1/1], Step [4754/7635], Loss: 4.0743\n",
      "Epoch [1/1], Step [4755/7635], Loss: 4.0369\n",
      "Epoch [1/1], Step [4756/7635], Loss: 4.0099\n",
      "Epoch [1/1], Step [4757/7635], Loss: 3.9615\n",
      "Epoch [1/1], Step [4758/7635], Loss: 4.0146\n",
      "Epoch [1/1], Step [4759/7635], Loss: 3.9948\n",
      "Epoch [1/1], Step [4760/7635], Loss: 4.0460\n",
      "Epoch [1/1], Step [4761/7635], Loss: 4.0403\n",
      "Epoch [1/1], Step [4762/7635], Loss: 3.9761\n",
      "Epoch [1/1], Step [4763/7635], Loss: 4.0177\n",
      "Epoch [1/1], Step [4764/7635], Loss: 4.0301\n",
      "Epoch [1/1], Step [4765/7635], Loss: 4.0376\n",
      "Epoch [1/1], Step [4766/7635], Loss: 3.9978\n",
      "Epoch [1/1], Step [4767/7635], Loss: 4.0864\n",
      "Epoch [1/1], Step [4768/7635], Loss: 3.9764\n",
      "Epoch [1/1], Step [4769/7635], Loss: 3.9531\n",
      "Epoch [1/1], Step [4770/7635], Loss: 4.0454\n",
      "Epoch [1/1], Step [4771/7635], Loss: 3.9207\n",
      "Epoch [1/1], Step [4772/7635], Loss: 4.0774\n",
      "Epoch [1/1], Step [4773/7635], Loss: 3.9667\n",
      "Epoch [1/1], Step [4774/7635], Loss: 3.9685\n",
      "Epoch [1/1], Step [4775/7635], Loss: 3.9827\n",
      "Epoch [1/1], Step [4776/7635], Loss: 4.0086\n",
      "Epoch [1/1], Step [4777/7635], Loss: 3.9447\n",
      "Epoch [1/1], Step [4778/7635], Loss: 3.9766\n",
      "Epoch [1/1], Step [4779/7635], Loss: 3.9958\n",
      "Epoch [1/1], Step [4780/7635], Loss: 4.0554\n",
      "Epoch [1/1], Step [4781/7635], Loss: 4.0586\n",
      "Epoch [1/1], Step [4782/7635], Loss: 4.0700\n",
      "Epoch [1/1], Step [4783/7635], Loss: 4.0458\n",
      "Epoch [1/1], Step [4784/7635], Loss: 3.9896\n",
      "Epoch [1/1], Step [4785/7635], Loss: 3.9313\n",
      "Epoch [1/1], Step [4786/7635], Loss: 3.9685\n",
      "Epoch [1/1], Step [4787/7635], Loss: 4.1040\n",
      "Epoch [1/1], Step [4788/7635], Loss: 4.0894\n",
      "Epoch [1/1], Step [4789/7635], Loss: 4.0263\n",
      "Epoch [1/1], Step [4790/7635], Loss: 3.9633\n",
      "Epoch [1/1], Step [4791/7635], Loss: 3.9762\n",
      "Epoch [1/1], Step [4792/7635], Loss: 4.0255\n",
      "Epoch [1/1], Step [4793/7635], Loss: 4.0436\n",
      "Epoch [1/1], Step [4794/7635], Loss: 3.9929\n",
      "Epoch [1/1], Step [4795/7635], Loss: 4.0739\n",
      "Epoch [1/1], Step [4796/7635], Loss: 4.0358\n",
      "Epoch [1/1], Step [4797/7635], Loss: 4.0361\n",
      "Epoch [1/1], Step [4798/7635], Loss: 4.0771\n",
      "Epoch [1/1], Step [4799/7635], Loss: 4.0706\n",
      "Epoch [1/1], Step [4800/7635], Loss: 3.9662\n",
      "Epoch [1/1], Step [4801/7635], Loss: 3.9657\n",
      "Epoch [1/1], Step [4802/7635], Loss: 4.0588\n",
      "Epoch [1/1], Step [4803/7635], Loss: 4.0026\n",
      "Epoch [1/1], Step [4804/7635], Loss: 3.9915\n",
      "Epoch [1/1], Step [4805/7635], Loss: 4.0083\n",
      "Epoch [1/1], Step [4806/7635], Loss: 3.9719\n",
      "Epoch [1/1], Step [4807/7635], Loss: 4.0166\n",
      "Epoch [1/1], Step [4808/7635], Loss: 4.0024\n",
      "Epoch [1/1], Step [4809/7635], Loss: 4.0129\n",
      "Epoch [1/1], Step [4810/7635], Loss: 4.0410\n",
      "Epoch [1/1], Step [4811/7635], Loss: 3.9977\n",
      "Epoch [1/1], Step [4812/7635], Loss: 4.0279\n",
      "Epoch [1/1], Step [4813/7635], Loss: 3.9762\n",
      "Epoch [1/1], Step [4814/7635], Loss: 4.1160\n",
      "Epoch [1/1], Step [4815/7635], Loss: 3.9651\n",
      "Epoch [1/1], Step [4816/7635], Loss: 4.0181\n",
      "Epoch [1/1], Step [4817/7635], Loss: 3.9604\n",
      "Epoch [1/1], Step [4818/7635], Loss: 4.0842\n",
      "Epoch [1/1], Step [4819/7635], Loss: 4.0493\n",
      "Epoch [1/1], Step [4820/7635], Loss: 4.0834\n",
      "Epoch [1/1], Step [4821/7635], Loss: 3.9755\n",
      "Epoch [1/1], Step [4822/7635], Loss: 4.0782\n",
      "Epoch [1/1], Step [4823/7635], Loss: 4.0232\n",
      "Epoch [1/1], Step [4824/7635], Loss: 4.0099\n",
      "Epoch [1/1], Step [4825/7635], Loss: 3.9788\n",
      "Epoch [1/1], Step [4826/7635], Loss: 4.0706\n",
      "Epoch [1/1], Step [4827/7635], Loss: 3.9970\n",
      "Epoch [1/1], Step [4828/7635], Loss: 4.0196\n",
      "Epoch [1/1], Step [4829/7635], Loss: 4.0536\n",
      "Epoch [1/1], Step [4830/7635], Loss: 3.9619\n",
      "Epoch [1/1], Step [4831/7635], Loss: 4.0264\n",
      "Epoch [1/1], Step [4832/7635], Loss: 4.1261\n",
      "Epoch [1/1], Step [4833/7635], Loss: 4.0905\n",
      "Epoch [1/1], Step [4834/7635], Loss: 4.0323\n",
      "Epoch [1/1], Step [4835/7635], Loss: 3.9588\n",
      "Epoch [1/1], Step [4836/7635], Loss: 4.0297\n",
      "Epoch [1/1], Step [4837/7635], Loss: 4.0422\n",
      "Epoch [1/1], Step [4838/7635], Loss: 4.0566\n",
      "Epoch [1/1], Step [4839/7635], Loss: 4.0314\n",
      "Epoch [1/1], Step [4840/7635], Loss: 3.9519\n",
      "Epoch [1/1], Step [4841/7635], Loss: 4.0306\n",
      "Epoch [1/1], Step [4842/7635], Loss: 4.0374\n",
      "Epoch [1/1], Step [4843/7635], Loss: 4.0151\n",
      "Epoch [1/1], Step [4844/7635], Loss: 4.0363\n",
      "Epoch [1/1], Step [4845/7635], Loss: 4.0179\n",
      "Epoch [1/1], Step [4846/7635], Loss: 4.0111\n",
      "Epoch [1/1], Step [4847/7635], Loss: 4.0460\n",
      "Epoch [1/1], Step [4848/7635], Loss: 3.9798\n",
      "Epoch [1/1], Step [4849/7635], Loss: 4.0462\n",
      "Epoch [1/1], Step [4850/7635], Loss: 3.9552\n",
      "Epoch [1/1], Step [4851/7635], Loss: 4.0032\n",
      "Epoch [1/1], Step [4852/7635], Loss: 4.0380\n",
      "Epoch [1/1], Step [4853/7635], Loss: 4.0711\n",
      "Epoch [1/1], Step [4854/7635], Loss: 4.0003\n",
      "Epoch [1/1], Step [4855/7635], Loss: 4.0625\n",
      "Epoch [1/1], Step [4856/7635], Loss: 3.9877\n",
      "Epoch [1/1], Step [4857/7635], Loss: 3.9283\n",
      "Epoch [1/1], Step [4858/7635], Loss: 4.0194\n",
      "Epoch [1/1], Step [4859/7635], Loss: 4.0978\n",
      "Epoch [1/1], Step [4860/7635], Loss: 3.9813\n",
      "Epoch [1/1], Step [4861/7635], Loss: 4.0394\n",
      "Epoch [1/1], Step [4862/7635], Loss: 4.0951\n",
      "Epoch [1/1], Step [4863/7635], Loss: 4.0013\n",
      "Epoch [1/1], Step [4864/7635], Loss: 3.9975\n",
      "Epoch [1/1], Step [4865/7635], Loss: 4.0239\n",
      "Epoch [1/1], Step [4866/7635], Loss: 4.0379\n",
      "Epoch [1/1], Step [4867/7635], Loss: 3.9984\n",
      "Epoch [1/1], Step [4868/7635], Loss: 4.0569\n",
      "Epoch [1/1], Step [4869/7635], Loss: 4.0910\n",
      "Epoch [1/1], Step [4870/7635], Loss: 4.0448\n",
      "Epoch [1/1], Step [4871/7635], Loss: 4.0814\n",
      "Epoch [1/1], Step [4872/7635], Loss: 4.0496\n",
      "Epoch [1/1], Step [4873/7635], Loss: 4.0651\n",
      "Epoch [1/1], Step [4874/7635], Loss: 4.1029\n",
      "Epoch [1/1], Step [4875/7635], Loss: 3.9521\n",
      "Epoch [1/1], Step [4876/7635], Loss: 4.0371\n",
      "Epoch [1/1], Step [4877/7635], Loss: 4.0108\n",
      "Epoch [1/1], Step [4878/7635], Loss: 4.0551\n",
      "Epoch [1/1], Step [4879/7635], Loss: 4.0211\n",
      "Epoch [1/1], Step [4880/7635], Loss: 3.9719\n",
      "Epoch [1/1], Step [4881/7635], Loss: 4.0414\n",
      "Epoch [1/1], Step [4882/7635], Loss: 4.0075\n",
      "Epoch [1/1], Step [4883/7635], Loss: 3.9378\n",
      "Epoch [1/1], Step [4884/7635], Loss: 4.0161\n",
      "Epoch [1/1], Step [4885/7635], Loss: 4.0433\n",
      "Epoch [1/1], Step [4886/7635], Loss: 4.0640\n",
      "Epoch [1/1], Step [4887/7635], Loss: 4.0688\n",
      "Epoch [1/1], Step [4888/7635], Loss: 4.0157\n",
      "Epoch [1/1], Step [4889/7635], Loss: 3.9983\n",
      "Epoch [1/1], Step [4890/7635], Loss: 3.9552\n",
      "Epoch [1/1], Step [4891/7635], Loss: 4.0625\n",
      "Epoch [1/1], Step [4892/7635], Loss: 4.0073\n",
      "Epoch [1/1], Step [4893/7635], Loss: 3.9705\n",
      "Epoch [1/1], Step [4894/7635], Loss: 3.9921\n",
      "Epoch [1/1], Step [4895/7635], Loss: 4.0647\n",
      "Epoch [1/1], Step [4896/7635], Loss: 4.0532\n",
      "Epoch [1/1], Step [4897/7635], Loss: 4.0282\n",
      "Epoch [1/1], Step [4898/7635], Loss: 3.9977\n",
      "Epoch [1/1], Step [4899/7635], Loss: 3.9895\n",
      "Epoch [1/1], Step [4900/7635], Loss: 3.8934\n",
      "Epoch [1/1], Step [4901/7635], Loss: 4.0335\n",
      "Epoch [1/1], Step [4902/7635], Loss: 4.0098\n",
      "Epoch [1/1], Step [4903/7635], Loss: 3.9290\n",
      "Epoch [1/1], Step [4904/7635], Loss: 4.0068\n",
      "Epoch [1/1], Step [4905/7635], Loss: 4.0237\n",
      "Epoch [1/1], Step [4906/7635], Loss: 4.0237\n",
      "Epoch [1/1], Step [4907/7635], Loss: 3.9999\n",
      "Epoch [1/1], Step [4908/7635], Loss: 3.9824\n",
      "Epoch [1/1], Step [4909/7635], Loss: 3.9852\n",
      "Epoch [1/1], Step [4910/7635], Loss: 3.9496\n",
      "Epoch [1/1], Step [4911/7635], Loss: 3.9814\n",
      "Epoch [1/1], Step [4912/7635], Loss: 3.9559\n",
      "Epoch [1/1], Step [4913/7635], Loss: 3.9579\n",
      "Epoch [1/1], Step [4914/7635], Loss: 4.0513\n",
      "Epoch [1/1], Step [4915/7635], Loss: 4.0570\n",
      "Epoch [1/1], Step [4916/7635], Loss: 3.9852\n",
      "Epoch [1/1], Step [4917/7635], Loss: 3.9587\n",
      "Epoch [1/1], Step [4918/7635], Loss: 3.9858\n",
      "Epoch [1/1], Step [4919/7635], Loss: 3.9761\n",
      "Epoch [1/1], Step [4920/7635], Loss: 4.0146\n",
      "Epoch [1/1], Step [4921/7635], Loss: 4.0395\n",
      "Epoch [1/1], Step [4922/7635], Loss: 4.0341\n",
      "Epoch [1/1], Step [4923/7635], Loss: 3.9974\n",
      "Epoch [1/1], Step [4924/7635], Loss: 4.0658\n",
      "Epoch [1/1], Step [4925/7635], Loss: 3.9930\n",
      "Epoch [1/1], Step [4926/7635], Loss: 4.0060\n",
      "Epoch [1/1], Step [4927/7635], Loss: 3.9471\n",
      "Epoch [1/1], Step [4928/7635], Loss: 3.9378\n",
      "Epoch [1/1], Step [4929/7635], Loss: 4.1259\n",
      "Epoch [1/1], Step [4930/7635], Loss: 4.0600\n",
      "Epoch [1/1], Step [4931/7635], Loss: 4.0297\n",
      "Epoch [1/1], Step [4932/7635], Loss: 3.9962\n",
      "Epoch [1/1], Step [4933/7635], Loss: 3.9812\n",
      "Epoch [1/1], Step [4934/7635], Loss: 4.0174\n",
      "Epoch [1/1], Step [4935/7635], Loss: 4.0338\n",
      "Epoch [1/1], Step [4936/7635], Loss: 4.0744\n",
      "Epoch [1/1], Step [4937/7635], Loss: 3.9987\n",
      "Epoch [1/1], Step [4938/7635], Loss: 4.0125\n",
      "Epoch [1/1], Step [4939/7635], Loss: 3.9464\n",
      "Epoch [1/1], Step [4940/7635], Loss: 3.9258\n",
      "Epoch [1/1], Step [4941/7635], Loss: 4.0186\n",
      "Epoch [1/1], Step [4942/7635], Loss: 3.9556\n",
      "Epoch [1/1], Step [4943/7635], Loss: 4.0857\n",
      "Epoch [1/1], Step [4944/7635], Loss: 4.1043\n",
      "Epoch [1/1], Step [4945/7635], Loss: 4.1031\n",
      "Epoch [1/1], Step [4946/7635], Loss: 3.9548\n",
      "Epoch [1/1], Step [4947/7635], Loss: 3.9962\n",
      "Epoch [1/1], Step [4948/7635], Loss: 4.0829\n",
      "Epoch [1/1], Step [4949/7635], Loss: 4.0330\n",
      "Epoch [1/1], Step [4950/7635], Loss: 4.0445\n",
      "Epoch [1/1], Step [4951/7635], Loss: 4.0429\n",
      "Epoch [1/1], Step [4952/7635], Loss: 4.0403\n",
      "Epoch [1/1], Step [4953/7635], Loss: 4.0105\n",
      "Epoch [1/1], Step [4954/7635], Loss: 3.9857\n",
      "Epoch [1/1], Step [4955/7635], Loss: 4.0447\n",
      "Epoch [1/1], Step [4956/7635], Loss: 3.9691\n",
      "Epoch [1/1], Step [4957/7635], Loss: 3.9582\n",
      "Epoch [1/1], Step [4958/7635], Loss: 3.9067\n",
      "Epoch [1/1], Step [4959/7635], Loss: 3.9688\n",
      "Epoch [1/1], Step [4960/7635], Loss: 4.0860\n",
      "Epoch [1/1], Step [4961/7635], Loss: 3.9892\n",
      "Epoch [1/1], Step [4962/7635], Loss: 4.0262\n",
      "Epoch [1/1], Step [4963/7635], Loss: 3.9999\n",
      "Epoch [1/1], Step [4964/7635], Loss: 3.9833\n",
      "Epoch [1/1], Step [4965/7635], Loss: 4.0232\n",
      "Epoch [1/1], Step [4966/7635], Loss: 4.0599\n",
      "Epoch [1/1], Step [4967/7635], Loss: 3.9635\n",
      "Epoch [1/1], Step [4968/7635], Loss: 3.9737\n",
      "Epoch [1/1], Step [4969/7635], Loss: 4.0521\n",
      "Epoch [1/1], Step [4970/7635], Loss: 4.0602\n",
      "Epoch [1/1], Step [4971/7635], Loss: 4.0426\n",
      "Epoch [1/1], Step [4972/7635], Loss: 3.9560\n",
      "Epoch [1/1], Step [4973/7635], Loss: 3.9829\n",
      "Epoch [1/1], Step [4974/7635], Loss: 3.9713\n",
      "Epoch [1/1], Step [4975/7635], Loss: 4.0119\n",
      "Epoch [1/1], Step [4976/7635], Loss: 4.0694\n",
      "Epoch [1/1], Step [4977/7635], Loss: 4.0007\n",
      "Epoch [1/1], Step [4978/7635], Loss: 4.0034\n",
      "Epoch [1/1], Step [4979/7635], Loss: 4.0569\n",
      "Epoch [1/1], Step [4980/7635], Loss: 4.0195\n",
      "Epoch [1/1], Step [4981/7635], Loss: 4.0505\n",
      "Epoch [1/1], Step [4982/7635], Loss: 4.0509\n",
      "Epoch [1/1], Step [4983/7635], Loss: 4.0016\n",
      "Epoch [1/1], Step [4984/7635], Loss: 3.9766\n",
      "Epoch [1/1], Step [4985/7635], Loss: 3.9884\n",
      "Epoch [1/1], Step [4986/7635], Loss: 3.8821\n",
      "Epoch [1/1], Step [4987/7635], Loss: 4.1030\n",
      "Epoch [1/1], Step [4988/7635], Loss: 4.0297\n",
      "Epoch [1/1], Step [4989/7635], Loss: 4.0687\n",
      "Epoch [1/1], Step [4990/7635], Loss: 4.0158\n",
      "Epoch [1/1], Step [4991/7635], Loss: 4.0132\n",
      "Epoch [1/1], Step [4992/7635], Loss: 4.0206\n",
      "Epoch [1/1], Step [4993/7635], Loss: 4.0720\n",
      "Epoch [1/1], Step [4994/7635], Loss: 3.9862\n",
      "Epoch [1/1], Step [4995/7635], Loss: 3.9790\n",
      "Epoch [1/1], Step [4996/7635], Loss: 4.0438\n",
      "Epoch [1/1], Step [4997/7635], Loss: 4.0207\n",
      "Epoch [1/1], Step [4998/7635], Loss: 3.9913\n",
      "Epoch [1/1], Step [4999/7635], Loss: 3.9939\n",
      "Epoch [1/1], Step [5000/7635], Loss: 4.0536\n",
      "Epoch [1/1], Step [5001/7635], Loss: 3.9858\n",
      "Epoch [1/1], Step [5002/7635], Loss: 3.9777\n",
      "Epoch [1/1], Step [5003/7635], Loss: 3.9861\n",
      "Epoch [1/1], Step [5004/7635], Loss: 4.0406\n",
      "Epoch [1/1], Step [5005/7635], Loss: 3.9229\n",
      "Epoch [1/1], Step [5006/7635], Loss: 3.9806\n",
      "Epoch [1/1], Step [5007/7635], Loss: 4.0475\n",
      "Epoch [1/1], Step [5008/7635], Loss: 4.0815\n",
      "Epoch [1/1], Step [5009/7635], Loss: 4.1070\n",
      "Epoch [1/1], Step [5010/7635], Loss: 4.0155\n",
      "Epoch [1/1], Step [5011/7635], Loss: 4.0251\n",
      "Epoch [1/1], Step [5012/7635], Loss: 3.9873\n",
      "Epoch [1/1], Step [5013/7635], Loss: 3.9997\n",
      "Epoch [1/1], Step [5014/7635], Loss: 3.9890\n",
      "Epoch [1/1], Step [5015/7635], Loss: 3.9851\n",
      "Epoch [1/1], Step [5016/7635], Loss: 3.9305\n",
      "Epoch [1/1], Step [5017/7635], Loss: 3.9618\n",
      "Epoch [1/1], Step [5018/7635], Loss: 4.0047\n",
      "Epoch [1/1], Step [5019/7635], Loss: 4.0774\n",
      "Epoch [1/1], Step [5020/7635], Loss: 3.9963\n",
      "Epoch [1/1], Step [5021/7635], Loss: 4.0337\n",
      "Epoch [1/1], Step [5022/7635], Loss: 4.0055\n",
      "Epoch [1/1], Step [5023/7635], Loss: 3.9878\n",
      "Epoch [1/1], Step [5024/7635], Loss: 3.9614\n",
      "Epoch [1/1], Step [5025/7635], Loss: 4.0050\n",
      "Epoch [1/1], Step [5026/7635], Loss: 4.0235\n",
      "Epoch [1/1], Step [5027/7635], Loss: 4.0985\n",
      "Epoch [1/1], Step [5028/7635], Loss: 4.0324\n",
      "Epoch [1/1], Step [5029/7635], Loss: 3.9815\n",
      "Epoch [1/1], Step [5030/7635], Loss: 3.9883\n",
      "Epoch [1/1], Step [5031/7635], Loss: 3.9809\n",
      "Epoch [1/1], Step [5032/7635], Loss: 4.0181\n",
      "Epoch [1/1], Step [5033/7635], Loss: 4.0065\n",
      "Epoch [1/1], Step [5034/7635], Loss: 3.9376\n",
      "Epoch [1/1], Step [5035/7635], Loss: 4.0738\n",
      "Epoch [1/1], Step [5036/7635], Loss: 3.9896\n",
      "Epoch [1/1], Step [5037/7635], Loss: 3.9642\n",
      "Epoch [1/1], Step [5038/7635], Loss: 3.9969\n",
      "Epoch [1/1], Step [5039/7635], Loss: 4.0107\n",
      "Epoch [1/1], Step [5040/7635], Loss: 4.0163\n",
      "Epoch [1/1], Step [5041/7635], Loss: 3.9357\n",
      "Epoch [1/1], Step [5042/7635], Loss: 4.0393\n",
      "Epoch [1/1], Step [5043/7635], Loss: 4.0043\n",
      "Epoch [1/1], Step [5044/7635], Loss: 4.0418\n",
      "Epoch [1/1], Step [5045/7635], Loss: 4.0603\n",
      "Epoch [1/1], Step [5046/7635], Loss: 3.9201\n",
      "Epoch [1/1], Step [5047/7635], Loss: 3.9723\n",
      "Epoch [1/1], Step [5048/7635], Loss: 4.1183\n",
      "Epoch [1/1], Step [5049/7635], Loss: 4.0109\n",
      "Epoch [1/1], Step [5050/7635], Loss: 4.0947\n",
      "Epoch [1/1], Step [5051/7635], Loss: 4.0868\n",
      "Epoch [1/1], Step [5052/7635], Loss: 3.9966\n",
      "Epoch [1/1], Step [5053/7635], Loss: 4.0040\n",
      "Epoch [1/1], Step [5054/7635], Loss: 3.9837\n",
      "Epoch [1/1], Step [5055/7635], Loss: 4.0193\n",
      "Epoch [1/1], Step [5056/7635], Loss: 4.0418\n",
      "Epoch [1/1], Step [5057/7635], Loss: 3.9996\n",
      "Epoch [1/1], Step [5058/7635], Loss: 3.9914\n",
      "Epoch [1/1], Step [5059/7635], Loss: 3.9801\n",
      "Epoch [1/1], Step [5060/7635], Loss: 3.9710\n",
      "Epoch [1/1], Step [5061/7635], Loss: 4.1417\n",
      "Epoch [1/1], Step [5062/7635], Loss: 3.9846\n",
      "Epoch [1/1], Step [5063/7635], Loss: 4.0355\n",
      "Epoch [1/1], Step [5064/7635], Loss: 3.9327\n",
      "Epoch [1/1], Step [5065/7635], Loss: 3.9759\n",
      "Epoch [1/1], Step [5066/7635], Loss: 4.0583\n",
      "Epoch [1/1], Step [5067/7635], Loss: 3.9981\n",
      "Epoch [1/1], Step [5068/7635], Loss: 4.0076\n",
      "Epoch [1/1], Step [5069/7635], Loss: 4.0528\n",
      "Epoch [1/1], Step [5070/7635], Loss: 4.0709\n",
      "Epoch [1/1], Step [5071/7635], Loss: 3.9331\n",
      "Epoch [1/1], Step [5072/7635], Loss: 3.9898\n",
      "Epoch [1/1], Step [5073/7635], Loss: 4.0279\n",
      "Epoch [1/1], Step [5074/7635], Loss: 3.9502\n",
      "Epoch [1/1], Step [5075/7635], Loss: 4.0156\n",
      "Epoch [1/1], Step [5076/7635], Loss: 4.0121\n",
      "Epoch [1/1], Step [5077/7635], Loss: 4.0131\n",
      "Epoch [1/1], Step [5078/7635], Loss: 4.0110\n",
      "Epoch [1/1], Step [5079/7635], Loss: 3.9298\n",
      "Epoch [1/1], Step [5080/7635], Loss: 4.0270\n",
      "Epoch [1/1], Step [5081/7635], Loss: 4.0374\n",
      "Epoch [1/1], Step [5082/7635], Loss: 4.0067\n",
      "Epoch [1/1], Step [5083/7635], Loss: 4.0046\n",
      "Epoch [1/1], Step [5084/7635], Loss: 4.0829\n",
      "Epoch [1/1], Step [5085/7635], Loss: 3.9644\n",
      "Epoch [1/1], Step [5086/7635], Loss: 4.0408\n",
      "Epoch [1/1], Step [5087/7635], Loss: 4.0530\n",
      "Epoch [1/1], Step [5088/7635], Loss: 4.0793\n",
      "Epoch [1/1], Step [5089/7635], Loss: 3.9961\n",
      "Epoch [1/1], Step [5090/7635], Loss: 3.9751\n",
      "Epoch [1/1], Step [5091/7635], Loss: 4.0706\n",
      "Epoch [1/1], Step [5092/7635], Loss: 3.9830\n",
      "Epoch [1/1], Step [5093/7635], Loss: 4.0858\n",
      "Epoch [1/1], Step [5094/7635], Loss: 3.9788\n",
      "Epoch [1/1], Step [5095/7635], Loss: 4.0133\n",
      "Epoch [1/1], Step [5096/7635], Loss: 4.0125\n",
      "Epoch [1/1], Step [5097/7635], Loss: 4.0327\n",
      "Epoch [1/1], Step [5098/7635], Loss: 3.9641\n",
      "Epoch [1/1], Step [5099/7635], Loss: 4.0316\n",
      "Epoch [1/1], Step [5100/7635], Loss: 4.0042\n",
      "Epoch [1/1], Step [5101/7635], Loss: 4.0437\n",
      "Epoch [1/1], Step [5102/7635], Loss: 3.9480\n",
      "Epoch [1/1], Step [5103/7635], Loss: 4.0529\n",
      "Epoch [1/1], Step [5104/7635], Loss: 3.9850\n",
      "Epoch [1/1], Step [5105/7635], Loss: 4.0274\n",
      "Epoch [1/1], Step [5106/7635], Loss: 4.0251\n",
      "Epoch [1/1], Step [5107/7635], Loss: 4.0181\n",
      "Epoch [1/1], Step [5108/7635], Loss: 4.0078\n",
      "Epoch [1/1], Step [5109/7635], Loss: 3.9721\n",
      "Epoch [1/1], Step [5110/7635], Loss: 4.0077\n",
      "Epoch [1/1], Step [5111/7635], Loss: 4.0812\n",
      "Epoch [1/1], Step [5112/7635], Loss: 3.9999\n",
      "Epoch [1/1], Step [5113/7635], Loss: 3.9831\n",
      "Epoch [1/1], Step [5114/7635], Loss: 4.0261\n",
      "Epoch [1/1], Step [5115/7635], Loss: 3.9672\n",
      "Epoch [1/1], Step [5116/7635], Loss: 4.0938\n",
      "Epoch [1/1], Step [5117/7635], Loss: 4.0615\n",
      "Epoch [1/1], Step [5118/7635], Loss: 4.0340\n",
      "Epoch [1/1], Step [5119/7635], Loss: 4.0117\n",
      "Epoch [1/1], Step [5120/7635], Loss: 4.0839\n",
      "Epoch [1/1], Step [5121/7635], Loss: 4.0509\n",
      "Epoch [1/1], Step [5122/7635], Loss: 4.0412\n",
      "Epoch [1/1], Step [5123/7635], Loss: 3.9976\n",
      "Epoch [1/1], Step [5124/7635], Loss: 3.9662\n",
      "Epoch [1/1], Step [5125/7635], Loss: 4.0003\n",
      "Epoch [1/1], Step [5126/7635], Loss: 4.0202\n",
      "Epoch [1/1], Step [5127/7635], Loss: 3.9886\n",
      "Epoch [1/1], Step [5128/7635], Loss: 4.0326\n",
      "Epoch [1/1], Step [5129/7635], Loss: 4.0040\n",
      "Epoch [1/1], Step [5130/7635], Loss: 3.9906\n",
      "Epoch [1/1], Step [5131/7635], Loss: 3.9840\n",
      "Epoch [1/1], Step [5132/7635], Loss: 4.0310\n",
      "Epoch [1/1], Step [5133/7635], Loss: 4.0549\n",
      "Epoch [1/1], Step [5134/7635], Loss: 4.0278\n",
      "Epoch [1/1], Step [5135/7635], Loss: 4.0557\n",
      "Epoch [1/1], Step [5136/7635], Loss: 3.9866\n",
      "Epoch [1/1], Step [5137/7635], Loss: 4.0216\n",
      "Epoch [1/1], Step [5138/7635], Loss: 4.0041\n",
      "Epoch [1/1], Step [5139/7635], Loss: 4.1225\n",
      "Epoch [1/1], Step [5140/7635], Loss: 4.0684\n",
      "Epoch [1/1], Step [5141/7635], Loss: 4.0478\n",
      "Epoch [1/1], Step [5142/7635], Loss: 4.0375\n",
      "Epoch [1/1], Step [5143/7635], Loss: 4.0101\n",
      "Epoch [1/1], Step [5144/7635], Loss: 3.9743\n",
      "Epoch [1/1], Step [5145/7635], Loss: 3.9679\n",
      "Epoch [1/1], Step [5146/7635], Loss: 4.0148\n",
      "Epoch [1/1], Step [5147/7635], Loss: 4.0446\n",
      "Epoch [1/1], Step [5148/7635], Loss: 3.9606\n",
      "Epoch [1/1], Step [5149/7635], Loss: 4.0353\n",
      "Epoch [1/1], Step [5150/7635], Loss: 3.9598\n",
      "Epoch [1/1], Step [5151/7635], Loss: 4.0231\n",
      "Epoch [1/1], Step [5152/7635], Loss: 3.9640\n",
      "Epoch [1/1], Step [5153/7635], Loss: 3.9686\n",
      "Epoch [1/1], Step [5154/7635], Loss: 4.0757\n",
      "Epoch [1/1], Step [5155/7635], Loss: 4.0028\n",
      "Epoch [1/1], Step [5156/7635], Loss: 4.0303\n",
      "Epoch [1/1], Step [5157/7635], Loss: 3.9584\n",
      "Epoch [1/1], Step [5158/7635], Loss: 3.9747\n",
      "Epoch [1/1], Step [5159/7635], Loss: 4.0324\n",
      "Epoch [1/1], Step [5160/7635], Loss: 4.0256\n",
      "Epoch [1/1], Step [5161/7635], Loss: 3.9697\n",
      "Epoch [1/1], Step [5162/7635], Loss: 3.9945\n",
      "Epoch [1/1], Step [5163/7635], Loss: 3.9955\n",
      "Epoch [1/1], Step [5164/7635], Loss: 4.0468\n",
      "Epoch [1/1], Step [5165/7635], Loss: 4.0406\n",
      "Epoch [1/1], Step [5166/7635], Loss: 3.9767\n",
      "Epoch [1/1], Step [5167/7635], Loss: 4.0127\n",
      "Epoch [1/1], Step [5168/7635], Loss: 3.9419\n",
      "Epoch [1/1], Step [5169/7635], Loss: 3.9767\n",
      "Epoch [1/1], Step [5170/7635], Loss: 4.0517\n",
      "Epoch [1/1], Step [5171/7635], Loss: 4.0011\n",
      "Epoch [1/1], Step [5172/7635], Loss: 3.9533\n",
      "Epoch [1/1], Step [5173/7635], Loss: 3.9340\n",
      "Epoch [1/1], Step [5174/7635], Loss: 3.9428\n",
      "Epoch [1/1], Step [5175/7635], Loss: 4.0064\n",
      "Epoch [1/1], Step [5176/7635], Loss: 4.0189\n",
      "Epoch [1/1], Step [5177/7635], Loss: 4.0290\n",
      "Epoch [1/1], Step [5178/7635], Loss: 4.0232\n",
      "Epoch [1/1], Step [5179/7635], Loss: 4.0413\n",
      "Epoch [1/1], Step [5180/7635], Loss: 3.9815\n",
      "Epoch [1/1], Step [5181/7635], Loss: 4.0316\n",
      "Epoch [1/1], Step [5182/7635], Loss: 4.0297\n",
      "Epoch [1/1], Step [5183/7635], Loss: 3.9499\n",
      "Epoch [1/1], Step [5184/7635], Loss: 4.0724\n",
      "Epoch [1/1], Step [5185/7635], Loss: 4.0205\n",
      "Epoch [1/1], Step [5186/7635], Loss: 4.0312\n",
      "Epoch [1/1], Step [5187/7635], Loss: 4.0618\n",
      "Epoch [1/1], Step [5188/7635], Loss: 4.0281\n",
      "Epoch [1/1], Step [5189/7635], Loss: 4.0105\n",
      "Epoch [1/1], Step [5190/7635], Loss: 4.0045\n",
      "Epoch [1/1], Step [5191/7635], Loss: 4.0017\n",
      "Epoch [1/1], Step [5192/7635], Loss: 3.9303\n",
      "Epoch [1/1], Step [5193/7635], Loss: 3.9771\n",
      "Epoch [1/1], Step [5194/7635], Loss: 3.9756\n",
      "Epoch [1/1], Step [5195/7635], Loss: 3.9662\n",
      "Epoch [1/1], Step [5196/7635], Loss: 3.9242\n",
      "Epoch [1/1], Step [5197/7635], Loss: 3.9869\n",
      "Epoch [1/1], Step [5198/7635], Loss: 4.0829\n",
      "Epoch [1/1], Step [5199/7635], Loss: 3.9283\n",
      "Epoch [1/1], Step [5200/7635], Loss: 4.0005\n",
      "Epoch [1/1], Step [5201/7635], Loss: 4.0346\n",
      "Epoch [1/1], Step [5202/7635], Loss: 3.9594\n",
      "Epoch [1/1], Step [5203/7635], Loss: 4.0154\n",
      "Epoch [1/1], Step [5204/7635], Loss: 4.0157\n",
      "Epoch [1/1], Step [5205/7635], Loss: 4.0261\n",
      "Epoch [1/1], Step [5206/7635], Loss: 3.9492\n",
      "Epoch [1/1], Step [5207/7635], Loss: 4.0282\n",
      "Epoch [1/1], Step [5208/7635], Loss: 4.0737\n",
      "Epoch [1/1], Step [5209/7635], Loss: 3.9793\n",
      "Epoch [1/1], Step [5210/7635], Loss: 3.9521\n",
      "Epoch [1/1], Step [5211/7635], Loss: 4.0041\n",
      "Epoch [1/1], Step [5212/7635], Loss: 3.9710\n",
      "Epoch [1/1], Step [5213/7635], Loss: 3.9292\n",
      "Epoch [1/1], Step [5214/7635], Loss: 4.0019\n",
      "Epoch [1/1], Step [5215/7635], Loss: 3.9965\n",
      "Epoch [1/1], Step [5216/7635], Loss: 3.9650\n",
      "Epoch [1/1], Step [5217/7635], Loss: 3.9965\n",
      "Epoch [1/1], Step [5218/7635], Loss: 3.9523\n",
      "Epoch [1/1], Step [5219/7635], Loss: 4.0355\n",
      "Epoch [1/1], Step [5220/7635], Loss: 3.9606\n",
      "Epoch [1/1], Step [5221/7635], Loss: 3.9902\n",
      "Epoch [1/1], Step [5222/7635], Loss: 3.9963\n",
      "Epoch [1/1], Step [5223/7635], Loss: 4.0916\n",
      "Epoch [1/1], Step [5224/7635], Loss: 3.9743\n",
      "Epoch [1/1], Step [5225/7635], Loss: 4.0139\n",
      "Epoch [1/1], Step [5226/7635], Loss: 3.9551\n",
      "Epoch [1/1], Step [5227/7635], Loss: 3.9399\n",
      "Epoch [1/1], Step [5228/7635], Loss: 4.0429\n",
      "Epoch [1/1], Step [5229/7635], Loss: 3.9620\n",
      "Epoch [1/1], Step [5230/7635], Loss: 4.0011\n",
      "Epoch [1/1], Step [5231/7635], Loss: 4.0256\n",
      "Epoch [1/1], Step [5232/7635], Loss: 3.9733\n",
      "Epoch [1/1], Step [5233/7635], Loss: 4.0214\n",
      "Epoch [1/1], Step [5234/7635], Loss: 4.0242\n",
      "Epoch [1/1], Step [5235/7635], Loss: 4.0441\n",
      "Epoch [1/1], Step [5236/7635], Loss: 3.9724\n",
      "Epoch [1/1], Step [5237/7635], Loss: 3.9459\n",
      "Epoch [1/1], Step [5238/7635], Loss: 4.0450\n",
      "Epoch [1/1], Step [5239/7635], Loss: 4.0271\n",
      "Epoch [1/1], Step [5240/7635], Loss: 4.0452\n",
      "Epoch [1/1], Step [5241/7635], Loss: 4.0488\n",
      "Epoch [1/1], Step [5242/7635], Loss: 4.0017\n",
      "Epoch [1/1], Step [5243/7635], Loss: 4.0521\n",
      "Epoch [1/1], Step [5244/7635], Loss: 3.9485\n",
      "Epoch [1/1], Step [5245/7635], Loss: 3.9551\n",
      "Epoch [1/1], Step [5246/7635], Loss: 4.0025\n",
      "Epoch [1/1], Step [5247/7635], Loss: 3.9941\n",
      "Epoch [1/1], Step [5248/7635], Loss: 3.9591\n",
      "Epoch [1/1], Step [5249/7635], Loss: 3.9084\n",
      "Epoch [1/1], Step [5250/7635], Loss: 4.0199\n",
      "Epoch [1/1], Step [5251/7635], Loss: 3.9345\n",
      "Epoch [1/1], Step [5252/7635], Loss: 3.9222\n",
      "Epoch [1/1], Step [5253/7635], Loss: 4.0128\n",
      "Epoch [1/1], Step [5254/7635], Loss: 3.9415\n",
      "Epoch [1/1], Step [5255/7635], Loss: 3.9148\n",
      "Epoch [1/1], Step [5256/7635], Loss: 3.9993\n",
      "Epoch [1/1], Step [5257/7635], Loss: 3.9680\n",
      "Epoch [1/1], Step [5258/7635], Loss: 3.9794\n",
      "Epoch [1/1], Step [5259/7635], Loss: 3.9805\n",
      "Epoch [1/1], Step [5260/7635], Loss: 4.0516\n",
      "Epoch [1/1], Step [5261/7635], Loss: 4.0489\n",
      "Epoch [1/1], Step [5262/7635], Loss: 4.0285\n",
      "Epoch [1/1], Step [5263/7635], Loss: 3.9487\n",
      "Epoch [1/1], Step [5264/7635], Loss: 4.0130\n",
      "Epoch [1/1], Step [5265/7635], Loss: 4.0284\n",
      "Epoch [1/1], Step [5266/7635], Loss: 3.9925\n",
      "Epoch [1/1], Step [5267/7635], Loss: 4.0694\n",
      "Epoch [1/1], Step [5268/7635], Loss: 4.0261\n",
      "Epoch [1/1], Step [5269/7635], Loss: 4.0318\n",
      "Epoch [1/1], Step [5270/7635], Loss: 3.9758\n",
      "Epoch [1/1], Step [5271/7635], Loss: 3.9256\n",
      "Epoch [1/1], Step [5272/7635], Loss: 4.0236\n",
      "Epoch [1/1], Step [5273/7635], Loss: 3.9836\n",
      "Epoch [1/1], Step [5274/7635], Loss: 3.9718\n",
      "Epoch [1/1], Step [5275/7635], Loss: 3.9702\n",
      "Epoch [1/1], Step [5276/7635], Loss: 3.9663\n",
      "Epoch [1/1], Step [5277/7635], Loss: 4.0271\n",
      "Epoch [1/1], Step [5278/7635], Loss: 4.0109\n",
      "Epoch [1/1], Step [5279/7635], Loss: 4.0223\n",
      "Epoch [1/1], Step [5280/7635], Loss: 3.9509\n",
      "Epoch [1/1], Step [5281/7635], Loss: 4.0168\n",
      "Epoch [1/1], Step [5282/7635], Loss: 4.0060\n",
      "Epoch [1/1], Step [5283/7635], Loss: 3.9878\n",
      "Epoch [1/1], Step [5284/7635], Loss: 3.9858\n",
      "Epoch [1/1], Step [5285/7635], Loss: 4.0034\n",
      "Epoch [1/1], Step [5286/7635], Loss: 3.9945\n",
      "Epoch [1/1], Step [5287/7635], Loss: 3.9824\n",
      "Epoch [1/1], Step [5288/7635], Loss: 4.0036\n",
      "Epoch [1/1], Step [5289/7635], Loss: 4.0694\n",
      "Epoch [1/1], Step [5290/7635], Loss: 3.9311\n",
      "Epoch [1/1], Step [5291/7635], Loss: 4.0022\n",
      "Epoch [1/1], Step [5292/7635], Loss: 3.9776\n",
      "Epoch [1/1], Step [5293/7635], Loss: 3.9870\n",
      "Epoch [1/1], Step [5294/7635], Loss: 3.9455\n",
      "Epoch [1/1], Step [5295/7635], Loss: 4.0012\n",
      "Epoch [1/1], Step [5296/7635], Loss: 3.9546\n",
      "Epoch [1/1], Step [5297/7635], Loss: 3.9255\n",
      "Epoch [1/1], Step [5298/7635], Loss: 3.9428\n",
      "Epoch [1/1], Step [5299/7635], Loss: 3.9910\n",
      "Epoch [1/1], Step [5300/7635], Loss: 4.0190\n",
      "Epoch [1/1], Step [5301/7635], Loss: 3.9323\n",
      "Epoch [1/1], Step [5302/7635], Loss: 3.9963\n",
      "Epoch [1/1], Step [5303/7635], Loss: 4.0016\n",
      "Epoch [1/1], Step [5304/7635], Loss: 3.9993\n",
      "Epoch [1/1], Step [5305/7635], Loss: 4.0183\n",
      "Epoch [1/1], Step [5306/7635], Loss: 3.9723\n",
      "Epoch [1/1], Step [5307/7635], Loss: 3.9935\n",
      "Epoch [1/1], Step [5308/7635], Loss: 3.9654\n",
      "Epoch [1/1], Step [5309/7635], Loss: 4.0303\n",
      "Epoch [1/1], Step [5310/7635], Loss: 3.9019\n",
      "Epoch [1/1], Step [5311/7635], Loss: 4.0199\n",
      "Epoch [1/1], Step [5312/7635], Loss: 4.0024\n",
      "Epoch [1/1], Step [5313/7635], Loss: 3.9164\n",
      "Epoch [1/1], Step [5314/7635], Loss: 3.9852\n",
      "Epoch [1/1], Step [5315/7635], Loss: 3.9630\n",
      "Epoch [1/1], Step [5316/7635], Loss: 3.9751\n",
      "Epoch [1/1], Step [5317/7635], Loss: 3.9044\n",
      "Epoch [1/1], Step [5318/7635], Loss: 3.9297\n",
      "Epoch [1/1], Step [5319/7635], Loss: 3.9122\n",
      "Epoch [1/1], Step [5320/7635], Loss: 3.9964\n",
      "Epoch [1/1], Step [5321/7635], Loss: 4.0529\n",
      "Epoch [1/1], Step [5322/7635], Loss: 3.9588\n",
      "Epoch [1/1], Step [5323/7635], Loss: 4.0425\n",
      "Epoch [1/1], Step [5324/7635], Loss: 3.9660\n",
      "Epoch [1/1], Step [5325/7635], Loss: 4.0337\n",
      "Epoch [1/1], Step [5326/7635], Loss: 4.0647\n",
      "Epoch [1/1], Step [5327/7635], Loss: 3.9744\n",
      "Epoch [1/1], Step [5328/7635], Loss: 4.0148\n",
      "Epoch [1/1], Step [5329/7635], Loss: 4.0477\n",
      "Epoch [1/1], Step [5330/7635], Loss: 3.9995\n",
      "Epoch [1/1], Step [5331/7635], Loss: 3.9797\n",
      "Epoch [1/1], Step [5332/7635], Loss: 3.9489\n",
      "Epoch [1/1], Step [5333/7635], Loss: 3.9677\n",
      "Epoch [1/1], Step [5334/7635], Loss: 3.9521\n",
      "Epoch [1/1], Step [5335/7635], Loss: 3.9343\n",
      "Epoch [1/1], Step [5336/7635], Loss: 4.0488\n",
      "Epoch [1/1], Step [5337/7635], Loss: 3.9753\n",
      "Epoch [1/1], Step [5338/7635], Loss: 3.9885\n",
      "Epoch [1/1], Step [5339/7635], Loss: 3.9915\n",
      "Epoch [1/1], Step [5340/7635], Loss: 3.9410\n",
      "Epoch [1/1], Step [5341/7635], Loss: 4.0051\n",
      "Epoch [1/1], Step [5342/7635], Loss: 3.9879\n",
      "Epoch [1/1], Step [5343/7635], Loss: 4.0016\n",
      "Epoch [1/1], Step [5344/7635], Loss: 3.9338\n",
      "Epoch [1/1], Step [5345/7635], Loss: 4.0204\n",
      "Epoch [1/1], Step [5346/7635], Loss: 4.0837\n",
      "Epoch [1/1], Step [5347/7635], Loss: 3.9667\n",
      "Epoch [1/1], Step [5348/7635], Loss: 3.9790\n",
      "Epoch [1/1], Step [5349/7635], Loss: 3.9672\n",
      "Epoch [1/1], Step [5350/7635], Loss: 3.9300\n",
      "Epoch [1/1], Step [5351/7635], Loss: 4.0350\n",
      "Epoch [1/1], Step [5352/7635], Loss: 4.0450\n",
      "Epoch [1/1], Step [5353/7635], Loss: 3.9969\n",
      "Epoch [1/1], Step [5354/7635], Loss: 3.9125\n",
      "Epoch [1/1], Step [5355/7635], Loss: 4.0029\n",
      "Epoch [1/1], Step [5356/7635], Loss: 4.0183\n",
      "Epoch [1/1], Step [5357/7635], Loss: 3.9883\n",
      "Epoch [1/1], Step [5358/7635], Loss: 4.0283\n",
      "Epoch [1/1], Step [5359/7635], Loss: 3.9705\n",
      "Epoch [1/1], Step [5360/7635], Loss: 3.9754\n",
      "Epoch [1/1], Step [5361/7635], Loss: 4.0199\n",
      "Epoch [1/1], Step [5362/7635], Loss: 4.0412\n",
      "Epoch [1/1], Step [5363/7635], Loss: 4.0464\n",
      "Epoch [1/1], Step [5364/7635], Loss: 3.9564\n",
      "Epoch [1/1], Step [5365/7635], Loss: 4.0268\n",
      "Epoch [1/1], Step [5366/7635], Loss: 4.0223\n",
      "Epoch [1/1], Step [5367/7635], Loss: 4.0428\n",
      "Epoch [1/1], Step [5368/7635], Loss: 4.0566\n",
      "Epoch [1/1], Step [5369/7635], Loss: 3.9407\n",
      "Epoch [1/1], Step [5370/7635], Loss: 3.9817\n",
      "Epoch [1/1], Step [5371/7635], Loss: 4.0698\n",
      "Epoch [1/1], Step [5372/7635], Loss: 3.9573\n",
      "Epoch [1/1], Step [5373/7635], Loss: 4.0827\n",
      "Epoch [1/1], Step [5374/7635], Loss: 4.0099\n",
      "Epoch [1/1], Step [5375/7635], Loss: 3.9444\n",
      "Epoch [1/1], Step [5376/7635], Loss: 3.9714\n",
      "Epoch [1/1], Step [5377/7635], Loss: 3.9046\n",
      "Epoch [1/1], Step [5378/7635], Loss: 3.9577\n",
      "Epoch [1/1], Step [5379/7635], Loss: 4.0558\n",
      "Epoch [1/1], Step [5380/7635], Loss: 3.9875\n",
      "Epoch [1/1], Step [5381/7635], Loss: 4.0355\n",
      "Epoch [1/1], Step [5382/7635], Loss: 3.9859\n",
      "Epoch [1/1], Step [5383/7635], Loss: 3.8925\n",
      "Epoch [1/1], Step [5384/7635], Loss: 4.0277\n",
      "Epoch [1/1], Step [5385/7635], Loss: 3.9651\n",
      "Epoch [1/1], Step [5386/7635], Loss: 4.0745\n",
      "Epoch [1/1], Step [5387/7635], Loss: 3.9785\n",
      "Epoch [1/1], Step [5388/7635], Loss: 4.0309\n",
      "Epoch [1/1], Step [5389/7635], Loss: 4.0155\n",
      "Epoch [1/1], Step [5390/7635], Loss: 4.0080\n",
      "Epoch [1/1], Step [5391/7635], Loss: 3.9457\n",
      "Epoch [1/1], Step [5392/7635], Loss: 3.9931\n",
      "Epoch [1/1], Step [5393/7635], Loss: 4.0420\n",
      "Epoch [1/1], Step [5394/7635], Loss: 3.9486\n",
      "Epoch [1/1], Step [5395/7635], Loss: 3.8919\n",
      "Epoch [1/1], Step [5396/7635], Loss: 3.9472\n",
      "Epoch [1/1], Step [5397/7635], Loss: 4.0622\n",
      "Epoch [1/1], Step [5398/7635], Loss: 3.9815\n",
      "Epoch [1/1], Step [5399/7635], Loss: 3.9950\n",
      "Epoch [1/1], Step [5400/7635], Loss: 4.0716\n",
      "Epoch [1/1], Step [5401/7635], Loss: 3.9400\n",
      "Epoch [1/1], Step [5402/7635], Loss: 4.0317\n",
      "Epoch [1/1], Step [5403/7635], Loss: 4.0189\n",
      "Epoch [1/1], Step [5404/7635], Loss: 3.9382\n",
      "Epoch [1/1], Step [5405/7635], Loss: 4.0471\n",
      "Epoch [1/1], Step [5406/7635], Loss: 4.0617\n",
      "Epoch [1/1], Step [5407/7635], Loss: 3.9618\n",
      "Epoch [1/1], Step [5408/7635], Loss: 4.0119\n",
      "Epoch [1/1], Step [5409/7635], Loss: 3.9391\n",
      "Epoch [1/1], Step [5410/7635], Loss: 4.0243\n",
      "Epoch [1/1], Step [5411/7635], Loss: 3.9931\n",
      "Epoch [1/1], Step [5412/7635], Loss: 3.9675\n",
      "Epoch [1/1], Step [5413/7635], Loss: 4.0123\n",
      "Epoch [1/1], Step [5414/7635], Loss: 3.9877\n",
      "Epoch [1/1], Step [5415/7635], Loss: 3.9816\n",
      "Epoch [1/1], Step [5416/7635], Loss: 3.9403\n",
      "Epoch [1/1], Step [5417/7635], Loss: 3.9256\n",
      "Epoch [1/1], Step [5418/7635], Loss: 3.9218\n",
      "Epoch [1/1], Step [5419/7635], Loss: 3.9830\n",
      "Epoch [1/1], Step [5420/7635], Loss: 3.9844\n",
      "Epoch [1/1], Step [5421/7635], Loss: 4.0449\n",
      "Epoch [1/1], Step [5422/7635], Loss: 4.0370\n",
      "Epoch [1/1], Step [5423/7635], Loss: 3.9652\n",
      "Epoch [1/1], Step [5424/7635], Loss: 4.0182\n",
      "Epoch [1/1], Step [5425/7635], Loss: 4.0076\n",
      "Epoch [1/1], Step [5426/7635], Loss: 3.9965\n",
      "Epoch [1/1], Step [5427/7635], Loss: 3.9759\n",
      "Epoch [1/1], Step [5428/7635], Loss: 3.9970\n",
      "Epoch [1/1], Step [5429/7635], Loss: 4.0235\n",
      "Epoch [1/1], Step [5430/7635], Loss: 3.9186\n",
      "Epoch [1/1], Step [5431/7635], Loss: 4.0268\n",
      "Epoch [1/1], Step [5432/7635], Loss: 3.9414\n",
      "Epoch [1/1], Step [5433/7635], Loss: 3.9732\n",
      "Epoch [1/1], Step [5434/7635], Loss: 4.0317\n",
      "Epoch [1/1], Step [5435/7635], Loss: 3.9842\n",
      "Epoch [1/1], Step [5436/7635], Loss: 4.0144\n",
      "Epoch [1/1], Step [5437/7635], Loss: 4.0394\n",
      "Epoch [1/1], Step [5438/7635], Loss: 4.0254\n",
      "Epoch [1/1], Step [5439/7635], Loss: 4.0436\n",
      "Epoch [1/1], Step [5440/7635], Loss: 4.0296\n",
      "Epoch [1/1], Step [5441/7635], Loss: 3.9408\n",
      "Epoch [1/1], Step [5442/7635], Loss: 4.0224\n",
      "Epoch [1/1], Step [5443/7635], Loss: 3.9809\n",
      "Epoch [1/1], Step [5444/7635], Loss: 3.9340\n",
      "Epoch [1/1], Step [5445/7635], Loss: 3.9755\n",
      "Epoch [1/1], Step [5446/7635], Loss: 4.0236\n",
      "Epoch [1/1], Step [5447/7635], Loss: 3.9787\n",
      "Epoch [1/1], Step [5448/7635], Loss: 3.9797\n",
      "Epoch [1/1], Step [5449/7635], Loss: 4.0103\n",
      "Epoch [1/1], Step [5450/7635], Loss: 3.9728\n",
      "Epoch [1/1], Step [5451/7635], Loss: 3.9921\n",
      "Epoch [1/1], Step [5452/7635], Loss: 4.0075\n",
      "Epoch [1/1], Step [5453/7635], Loss: 4.0051\n",
      "Epoch [1/1], Step [5454/7635], Loss: 4.0218\n",
      "Epoch [1/1], Step [5455/7635], Loss: 4.0356\n",
      "Epoch [1/1], Step [5456/7635], Loss: 3.9620\n",
      "Epoch [1/1], Step [5457/7635], Loss: 3.9976\n",
      "Epoch [1/1], Step [5458/7635], Loss: 3.9550\n",
      "Epoch [1/1], Step [5459/7635], Loss: 3.9877\n",
      "Epoch [1/1], Step [5460/7635], Loss: 3.9218\n",
      "Epoch [1/1], Step [5461/7635], Loss: 4.0030\n",
      "Epoch [1/1], Step [5462/7635], Loss: 4.0027\n",
      "Epoch [1/1], Step [5463/7635], Loss: 3.9909\n",
      "Epoch [1/1], Step [5464/7635], Loss: 3.9659\n",
      "Epoch [1/1], Step [5465/7635], Loss: 4.0816\n",
      "Epoch [1/1], Step [5466/7635], Loss: 3.9690\n",
      "Epoch [1/1], Step [5467/7635], Loss: 3.9167\n",
      "Epoch [1/1], Step [5468/7635], Loss: 3.9938\n",
      "Epoch [1/1], Step [5469/7635], Loss: 3.9422\n",
      "Epoch [1/1], Step [5470/7635], Loss: 3.9874\n",
      "Epoch [1/1], Step [5471/7635], Loss: 3.8919\n",
      "Epoch [1/1], Step [5472/7635], Loss: 3.9778\n",
      "Epoch [1/1], Step [5473/7635], Loss: 4.0321\n",
      "Epoch [1/1], Step [5474/7635], Loss: 4.0296\n",
      "Epoch [1/1], Step [5475/7635], Loss: 4.0097\n",
      "Epoch [1/1], Step [5476/7635], Loss: 3.9503\n",
      "Epoch [1/1], Step [5477/7635], Loss: 4.0072\n",
      "Epoch [1/1], Step [5478/7635], Loss: 3.9708\n",
      "Epoch [1/1], Step [5479/7635], Loss: 3.9538\n",
      "Epoch [1/1], Step [5480/7635], Loss: 3.9871\n",
      "Epoch [1/1], Step [5481/7635], Loss: 4.0629\n",
      "Epoch [1/1], Step [5482/7635], Loss: 3.8883\n",
      "Epoch [1/1], Step [5483/7635], Loss: 3.9447\n",
      "Epoch [1/1], Step [5484/7635], Loss: 3.9645\n",
      "Epoch [1/1], Step [5485/7635], Loss: 3.9928\n",
      "Epoch [1/1], Step [5486/7635], Loss: 4.0174\n",
      "Epoch [1/1], Step [5487/7635], Loss: 3.9995\n",
      "Epoch [1/1], Step [5488/7635], Loss: 4.0244\n",
      "Epoch [1/1], Step [5489/7635], Loss: 3.9753\n",
      "Epoch [1/1], Step [5490/7635], Loss: 3.9623\n",
      "Epoch [1/1], Step [5491/7635], Loss: 3.9824\n",
      "Epoch [1/1], Step [5492/7635], Loss: 3.9431\n",
      "Epoch [1/1], Step [5493/7635], Loss: 3.9799\n",
      "Epoch [1/1], Step [5494/7635], Loss: 3.9906\n",
      "Epoch [1/1], Step [5495/7635], Loss: 4.0141\n",
      "Epoch [1/1], Step [5496/7635], Loss: 4.0017\n",
      "Epoch [1/1], Step [5497/7635], Loss: 4.0145\n",
      "Epoch [1/1], Step [5498/7635], Loss: 3.9532\n",
      "Epoch [1/1], Step [5499/7635], Loss: 4.0529\n",
      "Epoch [1/1], Step [5500/7635], Loss: 3.8973\n",
      "Epoch [1/1], Step [5501/7635], Loss: 4.0023\n",
      "Epoch [1/1], Step [5502/7635], Loss: 3.9762\n",
      "Epoch [1/1], Step [5503/7635], Loss: 4.0533\n",
      "Epoch [1/1], Step [5504/7635], Loss: 4.0340\n",
      "Epoch [1/1], Step [5505/7635], Loss: 3.9716\n",
      "Epoch [1/1], Step [5506/7635], Loss: 3.9663\n",
      "Epoch [1/1], Step [5507/7635], Loss: 3.9636\n",
      "Epoch [1/1], Step [5508/7635], Loss: 3.9812\n",
      "Epoch [1/1], Step [5509/7635], Loss: 3.9177\n",
      "Epoch [1/1], Step [5510/7635], Loss: 3.9909\n",
      "Epoch [1/1], Step [5511/7635], Loss: 3.9153\n",
      "Epoch [1/1], Step [5512/7635], Loss: 3.9771\n",
      "Epoch [1/1], Step [5513/7635], Loss: 3.9699\n",
      "Epoch [1/1], Step [5514/7635], Loss: 4.0557\n",
      "Epoch [1/1], Step [5515/7635], Loss: 3.9853\n",
      "Epoch [1/1], Step [5516/7635], Loss: 4.0089\n",
      "Epoch [1/1], Step [5517/7635], Loss: 4.0028\n",
      "Epoch [1/1], Step [5518/7635], Loss: 3.9367\n",
      "Epoch [1/1], Step [5519/7635], Loss: 4.0034\n",
      "Epoch [1/1], Step [5520/7635], Loss: 4.0442\n",
      "Epoch [1/1], Step [5521/7635], Loss: 3.9977\n",
      "Epoch [1/1], Step [5522/7635], Loss: 4.0107\n",
      "Epoch [1/1], Step [5523/7635], Loss: 3.9394\n",
      "Epoch [1/1], Step [5524/7635], Loss: 3.9809\n",
      "Epoch [1/1], Step [5525/7635], Loss: 3.9329\n",
      "Epoch [1/1], Step [5526/7635], Loss: 3.9382\n",
      "Epoch [1/1], Step [5527/7635], Loss: 3.9742\n",
      "Epoch [1/1], Step [5528/7635], Loss: 3.9987\n",
      "Epoch [1/1], Step [5529/7635], Loss: 3.8794\n",
      "Epoch [1/1], Step [5530/7635], Loss: 4.0503\n",
      "Epoch [1/1], Step [5531/7635], Loss: 3.9623\n",
      "Epoch [1/1], Step [5532/7635], Loss: 3.9280\n",
      "Epoch [1/1], Step [5533/7635], Loss: 3.9601\n",
      "Epoch [1/1], Step [5534/7635], Loss: 4.0195\n",
      "Epoch [1/1], Step [5535/7635], Loss: 4.0353\n",
      "Epoch [1/1], Step [5536/7635], Loss: 4.0303\n",
      "Epoch [1/1], Step [5537/7635], Loss: 3.9544\n",
      "Epoch [1/1], Step [5538/7635], Loss: 4.0237\n",
      "Epoch [1/1], Step [5539/7635], Loss: 4.0187\n",
      "Epoch [1/1], Step [5540/7635], Loss: 4.0316\n",
      "Epoch [1/1], Step [5541/7635], Loss: 4.0237\n",
      "Epoch [1/1], Step [5542/7635], Loss: 3.8808\n",
      "Epoch [1/1], Step [5543/7635], Loss: 3.9607\n",
      "Epoch [1/1], Step [5544/7635], Loss: 3.9237\n",
      "Epoch [1/1], Step [5545/7635], Loss: 4.0132\n",
      "Epoch [1/1], Step [5546/7635], Loss: 3.9796\n",
      "Epoch [1/1], Step [5547/7635], Loss: 3.9740\n",
      "Epoch [1/1], Step [5548/7635], Loss: 4.0300\n",
      "Epoch [1/1], Step [5549/7635], Loss: 3.9516\n",
      "Epoch [1/1], Step [5550/7635], Loss: 3.9705\n",
      "Epoch [1/1], Step [5551/7635], Loss: 3.9243\n",
      "Epoch [1/1], Step [5552/7635], Loss: 4.0160\n",
      "Epoch [1/1], Step [5553/7635], Loss: 3.9542\n",
      "Epoch [1/1], Step [5554/7635], Loss: 3.9770\n",
      "Epoch [1/1], Step [5555/7635], Loss: 3.9688\n",
      "Epoch [1/1], Step [5556/7635], Loss: 4.0248\n",
      "Epoch [1/1], Step [5557/7635], Loss: 3.9733\n",
      "Epoch [1/1], Step [5558/7635], Loss: 3.9827\n",
      "Epoch [1/1], Step [5559/7635], Loss: 3.9003\n",
      "Epoch [1/1], Step [5560/7635], Loss: 4.0196\n",
      "Epoch [1/1], Step [5561/7635], Loss: 3.9239\n",
      "Epoch [1/1], Step [5562/7635], Loss: 3.9924\n",
      "Epoch [1/1], Step [5563/7635], Loss: 3.9967\n",
      "Epoch [1/1], Step [5564/7635], Loss: 4.0347\n",
      "Epoch [1/1], Step [5565/7635], Loss: 4.0312\n",
      "Epoch [1/1], Step [5566/7635], Loss: 4.0098\n",
      "Epoch [1/1], Step [5567/7635], Loss: 4.0101\n",
      "Epoch [1/1], Step [5568/7635], Loss: 4.0987\n",
      "Epoch [1/1], Step [5569/7635], Loss: 3.9845\n",
      "Epoch [1/1], Step [5570/7635], Loss: 4.0534\n",
      "Epoch [1/1], Step [5571/7635], Loss: 3.9802\n",
      "Epoch [1/1], Step [5572/7635], Loss: 4.0260\n",
      "Epoch [1/1], Step [5573/7635], Loss: 4.0540\n",
      "Epoch [1/1], Step [5574/7635], Loss: 4.0049\n",
      "Epoch [1/1], Step [5575/7635], Loss: 3.9279\n",
      "Epoch [1/1], Step [5576/7635], Loss: 3.9694\n",
      "Epoch [1/1], Step [5577/7635], Loss: 3.9988\n",
      "Epoch [1/1], Step [5578/7635], Loss: 3.9457\n",
      "Epoch [1/1], Step [5579/7635], Loss: 4.0169\n",
      "Epoch [1/1], Step [5580/7635], Loss: 3.9727\n",
      "Epoch [1/1], Step [5581/7635], Loss: 3.9598\n",
      "Epoch [1/1], Step [5582/7635], Loss: 4.0498\n",
      "Epoch [1/1], Step [5583/7635], Loss: 3.9735\n",
      "Epoch [1/1], Step [5584/7635], Loss: 3.9327\n",
      "Epoch [1/1], Step [5585/7635], Loss: 3.9347\n",
      "Epoch [1/1], Step [5586/7635], Loss: 3.9542\n",
      "Epoch [1/1], Step [5587/7635], Loss: 4.0858\n",
      "Epoch [1/1], Step [5588/7635], Loss: 4.0167\n",
      "Epoch [1/1], Step [5589/7635], Loss: 4.0313\n",
      "Epoch [1/1], Step [5590/7635], Loss: 3.9972\n",
      "Epoch [1/1], Step [5591/7635], Loss: 3.9474\n",
      "Epoch [1/1], Step [5592/7635], Loss: 3.9986\n",
      "Epoch [1/1], Step [5593/7635], Loss: 3.9688\n",
      "Epoch [1/1], Step [5594/7635], Loss: 3.9644\n",
      "Epoch [1/1], Step [5595/7635], Loss: 3.9249\n",
      "Epoch [1/1], Step [5596/7635], Loss: 4.0218\n",
      "Epoch [1/1], Step [5597/7635], Loss: 3.9883\n",
      "Epoch [1/1], Step [5598/7635], Loss: 4.0294\n",
      "Epoch [1/1], Step [5599/7635], Loss: 3.9794\n",
      "Epoch [1/1], Step [5600/7635], Loss: 3.9841\n",
      "Epoch [1/1], Step [5601/7635], Loss: 3.9801\n",
      "Epoch [1/1], Step [5602/7635], Loss: 3.9097\n",
      "Epoch [1/1], Step [5603/7635], Loss: 3.9664\n",
      "Epoch [1/1], Step [5604/7635], Loss: 4.0384\n",
      "Epoch [1/1], Step [5605/7635], Loss: 3.9824\n",
      "Epoch [1/1], Step [5606/7635], Loss: 3.9086\n",
      "Epoch [1/1], Step [5607/7635], Loss: 3.9575\n",
      "Epoch [1/1], Step [5608/7635], Loss: 4.0390\n",
      "Epoch [1/1], Step [5609/7635], Loss: 3.9522\n",
      "Epoch [1/1], Step [5610/7635], Loss: 4.0455\n",
      "Epoch [1/1], Step [5611/7635], Loss: 4.0379\n",
      "Epoch [1/1], Step [5612/7635], Loss: 3.9736\n",
      "Epoch [1/1], Step [5613/7635], Loss: 4.0469\n",
      "Epoch [1/1], Step [5614/7635], Loss: 3.9219\n",
      "Epoch [1/1], Step [5615/7635], Loss: 3.9594\n",
      "Epoch [1/1], Step [5616/7635], Loss: 3.9943\n",
      "Epoch [1/1], Step [5617/7635], Loss: 4.0146\n",
      "Epoch [1/1], Step [5618/7635], Loss: 3.9227\n",
      "Epoch [1/1], Step [5619/7635], Loss: 3.9908\n",
      "Epoch [1/1], Step [5620/7635], Loss: 3.9382\n",
      "Epoch [1/1], Step [5621/7635], Loss: 4.0625\n",
      "Epoch [1/1], Step [5622/7635], Loss: 3.9799\n",
      "Epoch [1/1], Step [5623/7635], Loss: 3.9615\n",
      "Epoch [1/1], Step [5624/7635], Loss: 3.9020\n",
      "Epoch [1/1], Step [5625/7635], Loss: 4.0597\n",
      "Epoch [1/1], Step [5626/7635], Loss: 3.9961\n",
      "Epoch [1/1], Step [5627/7635], Loss: 4.0103\n",
      "Epoch [1/1], Step [5628/7635], Loss: 4.0486\n",
      "Epoch [1/1], Step [5629/7635], Loss: 4.0524\n",
      "Epoch [1/1], Step [5630/7635], Loss: 3.9838\n",
      "Epoch [1/1], Step [5631/7635], Loss: 3.8940\n",
      "Epoch [1/1], Step [5632/7635], Loss: 4.0371\n",
      "Epoch [1/1], Step [5633/7635], Loss: 4.0299\n",
      "Epoch [1/1], Step [5634/7635], Loss: 3.9569\n",
      "Epoch [1/1], Step [5635/7635], Loss: 3.9897\n",
      "Epoch [1/1], Step [5636/7635], Loss: 3.9904\n",
      "Epoch [1/1], Step [5637/7635], Loss: 3.9807\n",
      "Epoch [1/1], Step [5638/7635], Loss: 3.9643\n",
      "Epoch [1/1], Step [5639/7635], Loss: 3.9413\n",
      "Epoch [1/1], Step [5640/7635], Loss: 4.0280\n",
      "Epoch [1/1], Step [5641/7635], Loss: 4.0063\n",
      "Epoch [1/1], Step [5642/7635], Loss: 3.9794\n",
      "Epoch [1/1], Step [5643/7635], Loss: 3.9731\n",
      "Epoch [1/1], Step [5644/7635], Loss: 3.9975\n",
      "Epoch [1/1], Step [5645/7635], Loss: 4.0131\n",
      "Epoch [1/1], Step [5646/7635], Loss: 4.0550\n",
      "Epoch [1/1], Step [5647/7635], Loss: 3.9901\n",
      "Epoch [1/1], Step [5648/7635], Loss: 3.8886\n",
      "Epoch [1/1], Step [5649/7635], Loss: 4.0129\n",
      "Epoch [1/1], Step [5650/7635], Loss: 4.0191\n",
      "Epoch [1/1], Step [5651/7635], Loss: 3.9891\n",
      "Epoch [1/1], Step [5652/7635], Loss: 4.0155\n",
      "Epoch [1/1], Step [5653/7635], Loss: 3.9829\n",
      "Epoch [1/1], Step [5654/7635], Loss: 4.0003\n",
      "Epoch [1/1], Step [5655/7635], Loss: 4.0364\n",
      "Epoch [1/1], Step [5656/7635], Loss: 4.0158\n",
      "Epoch [1/1], Step [5657/7635], Loss: 3.9709\n",
      "Epoch [1/1], Step [5658/7635], Loss: 4.0059\n",
      "Epoch [1/1], Step [5659/7635], Loss: 3.9964\n",
      "Epoch [1/1], Step [5660/7635], Loss: 4.0230\n",
      "Epoch [1/1], Step [5661/7635], Loss: 3.9603\n",
      "Epoch [1/1], Step [5662/7635], Loss: 4.0334\n",
      "Epoch [1/1], Step [5663/7635], Loss: 3.9870\n",
      "Epoch [1/1], Step [5664/7635], Loss: 3.9141\n",
      "Epoch [1/1], Step [5665/7635], Loss: 4.0434\n",
      "Epoch [1/1], Step [5666/7635], Loss: 3.9828\n",
      "Epoch [1/1], Step [5667/7635], Loss: 4.0238\n",
      "Epoch [1/1], Step [5668/7635], Loss: 3.9611\n",
      "Epoch [1/1], Step [5669/7635], Loss: 4.0591\n",
      "Epoch [1/1], Step [5670/7635], Loss: 4.0247\n",
      "Epoch [1/1], Step [5671/7635], Loss: 3.9615\n",
      "Epoch [1/1], Step [5672/7635], Loss: 3.9989\n",
      "Epoch [1/1], Step [5673/7635], Loss: 3.9566\n",
      "Epoch [1/1], Step [5674/7635], Loss: 3.9487\n",
      "Epoch [1/1], Step [5675/7635], Loss: 4.0297\n",
      "Epoch [1/1], Step [5676/7635], Loss: 4.0116\n",
      "Epoch [1/1], Step [5677/7635], Loss: 4.0074\n",
      "Epoch [1/1], Step [5678/7635], Loss: 4.0195\n",
      "Epoch [1/1], Step [5679/7635], Loss: 3.9388\n",
      "Epoch [1/1], Step [5680/7635], Loss: 3.9227\n",
      "Epoch [1/1], Step [5681/7635], Loss: 3.9583\n",
      "Epoch [1/1], Step [5682/7635], Loss: 3.9830\n",
      "Epoch [1/1], Step [5683/7635], Loss: 3.9307\n",
      "Epoch [1/1], Step [5684/7635], Loss: 3.9520\n",
      "Epoch [1/1], Step [5685/7635], Loss: 3.9181\n",
      "Epoch [1/1], Step [5686/7635], Loss: 3.9608\n",
      "Epoch [1/1], Step [5687/7635], Loss: 4.0030\n",
      "Epoch [1/1], Step [5688/7635], Loss: 4.0347\n",
      "Epoch [1/1], Step [5689/7635], Loss: 4.0048\n",
      "Epoch [1/1], Step [5690/7635], Loss: 4.0144\n",
      "Epoch [1/1], Step [5691/7635], Loss: 3.9251\n",
      "Epoch [1/1], Step [5692/7635], Loss: 3.9390\n",
      "Epoch [1/1], Step [5693/7635], Loss: 4.0276\n",
      "Epoch [1/1], Step [5694/7635], Loss: 4.0003\n",
      "Epoch [1/1], Step [5695/7635], Loss: 3.9968\n",
      "Epoch [1/1], Step [5696/7635], Loss: 4.0293\n",
      "Epoch [1/1], Step [5697/7635], Loss: 3.9896\n",
      "Epoch [1/1], Step [5698/7635], Loss: 3.9884\n",
      "Epoch [1/1], Step [5699/7635], Loss: 3.9778\n",
      "Epoch [1/1], Step [5700/7635], Loss: 3.9951\n",
      "Epoch [1/1], Step [5701/7635], Loss: 3.9988\n",
      "Epoch [1/1], Step [5702/7635], Loss: 3.9551\n",
      "Epoch [1/1], Step [5703/7635], Loss: 3.9731\n",
      "Epoch [1/1], Step [5704/7635], Loss: 4.0299\n",
      "Epoch [1/1], Step [5705/7635], Loss: 3.9482\n",
      "Epoch [1/1], Step [5706/7635], Loss: 3.9794\n",
      "Epoch [1/1], Step [5707/7635], Loss: 3.9664\n",
      "Epoch [1/1], Step [5708/7635], Loss: 3.9741\n",
      "Epoch [1/1], Step [5709/7635], Loss: 3.9903\n",
      "Epoch [1/1], Step [5710/7635], Loss: 3.9435\n",
      "Epoch [1/1], Step [5711/7635], Loss: 3.9916\n",
      "Epoch [1/1], Step [5712/7635], Loss: 3.9855\n",
      "Epoch [1/1], Step [5713/7635], Loss: 3.9499\n",
      "Epoch [1/1], Step [5714/7635], Loss: 4.0514\n",
      "Epoch [1/1], Step [5715/7635], Loss: 4.0250\n",
      "Epoch [1/1], Step [5716/7635], Loss: 3.9934\n",
      "Epoch [1/1], Step [5717/7635], Loss: 3.9847\n",
      "Epoch [1/1], Step [5718/7635], Loss: 3.9715\n",
      "Epoch [1/1], Step [5719/7635], Loss: 3.9431\n",
      "Epoch [1/1], Step [5720/7635], Loss: 3.9471\n",
      "Epoch [1/1], Step [5721/7635], Loss: 3.9827\n",
      "Epoch [1/1], Step [5722/7635], Loss: 4.0119\n",
      "Epoch [1/1], Step [5723/7635], Loss: 4.0388\n",
      "Epoch [1/1], Step [5724/7635], Loss: 3.9524\n",
      "Epoch [1/1], Step [5725/7635], Loss: 4.0162\n",
      "Epoch [1/1], Step [5726/7635], Loss: 3.9220\n",
      "Epoch [1/1], Step [5727/7635], Loss: 3.9246\n",
      "Epoch [1/1], Step [5728/7635], Loss: 3.9903\n",
      "Epoch [1/1], Step [5729/7635], Loss: 3.9580\n",
      "Epoch [1/1], Step [5730/7635], Loss: 4.0089\n",
      "Epoch [1/1], Step [5731/7635], Loss: 3.9096\n",
      "Epoch [1/1], Step [5732/7635], Loss: 4.0464\n",
      "Epoch [1/1], Step [5733/7635], Loss: 3.9465\n",
      "Epoch [1/1], Step [5734/7635], Loss: 4.0339\n",
      "Epoch [1/1], Step [5735/7635], Loss: 3.9634\n",
      "Epoch [1/1], Step [5736/7635], Loss: 3.9026\n",
      "Epoch [1/1], Step [5737/7635], Loss: 4.0217\n",
      "Epoch [1/1], Step [5738/7635], Loss: 3.9586\n",
      "Epoch [1/1], Step [5739/7635], Loss: 3.9718\n",
      "Epoch [1/1], Step [5740/7635], Loss: 3.9935\n",
      "Epoch [1/1], Step [5741/7635], Loss: 3.9714\n",
      "Epoch [1/1], Step [5742/7635], Loss: 3.9704\n",
      "Epoch [1/1], Step [5743/7635], Loss: 3.9466\n",
      "Epoch [1/1], Step [5744/7635], Loss: 4.0538\n",
      "Epoch [1/1], Step [5745/7635], Loss: 3.9458\n",
      "Epoch [1/1], Step [5746/7635], Loss: 3.9878\n",
      "Epoch [1/1], Step [5747/7635], Loss: 4.0042\n",
      "Epoch [1/1], Step [5748/7635], Loss: 3.9677\n",
      "Epoch [1/1], Step [5749/7635], Loss: 3.9332\n",
      "Epoch [1/1], Step [5750/7635], Loss: 3.9725\n",
      "Epoch [1/1], Step [5751/7635], Loss: 3.9579\n",
      "Epoch [1/1], Step [5752/7635], Loss: 3.9229\n",
      "Epoch [1/1], Step [5753/7635], Loss: 4.0167\n",
      "Epoch [1/1], Step [5754/7635], Loss: 3.9705\n",
      "Epoch [1/1], Step [5755/7635], Loss: 3.9871\n",
      "Epoch [1/1], Step [5756/7635], Loss: 4.0319\n",
      "Epoch [1/1], Step [5757/7635], Loss: 3.9241\n",
      "Epoch [1/1], Step [5758/7635], Loss: 3.9824\n",
      "Epoch [1/1], Step [5759/7635], Loss: 3.9051\n",
      "Epoch [1/1], Step [5760/7635], Loss: 3.9603\n",
      "Epoch [1/1], Step [5761/7635], Loss: 3.9580\n",
      "Epoch [1/1], Step [5762/7635], Loss: 4.0247\n",
      "Epoch [1/1], Step [5763/7635], Loss: 4.0447\n",
      "Epoch [1/1], Step [5764/7635], Loss: 4.0245\n",
      "Epoch [1/1], Step [5765/7635], Loss: 3.9475\n",
      "Epoch [1/1], Step [5766/7635], Loss: 4.0770\n",
      "Epoch [1/1], Step [5767/7635], Loss: 3.9491\n",
      "Epoch [1/1], Step [5768/7635], Loss: 3.9825\n",
      "Epoch [1/1], Step [5769/7635], Loss: 4.0294\n",
      "Epoch [1/1], Step [5770/7635], Loss: 4.0019\n",
      "Epoch [1/1], Step [5771/7635], Loss: 3.9432\n",
      "Epoch [1/1], Step [5772/7635], Loss: 3.9932\n",
      "Epoch [1/1], Step [5773/7635], Loss: 3.9559\n",
      "Epoch [1/1], Step [5774/7635], Loss: 3.9945\n",
      "Epoch [1/1], Step [5775/7635], Loss: 3.9531\n",
      "Epoch [1/1], Step [5776/7635], Loss: 3.9181\n",
      "Epoch [1/1], Step [5777/7635], Loss: 3.9932\n",
      "Epoch [1/1], Step [5778/7635], Loss: 4.0071\n",
      "Epoch [1/1], Step [5779/7635], Loss: 3.9894\n",
      "Epoch [1/1], Step [5780/7635], Loss: 3.9385\n",
      "Epoch [1/1], Step [5781/7635], Loss: 3.9389\n",
      "Epoch [1/1], Step [5782/7635], Loss: 3.9044\n",
      "Epoch [1/1], Step [5783/7635], Loss: 3.9971\n",
      "Epoch [1/1], Step [5784/7635], Loss: 3.9547\n",
      "Epoch [1/1], Step [5785/7635], Loss: 3.9893\n",
      "Epoch [1/1], Step [5786/7635], Loss: 4.0010\n",
      "Epoch [1/1], Step [5787/7635], Loss: 4.0689\n",
      "Epoch [1/1], Step [5788/7635], Loss: 3.9678\n",
      "Epoch [1/1], Step [5789/7635], Loss: 3.9763\n",
      "Epoch [1/1], Step [5790/7635], Loss: 3.9288\n",
      "Epoch [1/1], Step [5791/7635], Loss: 4.0605\n",
      "Epoch [1/1], Step [5792/7635], Loss: 3.9891\n",
      "Epoch [1/1], Step [5793/7635], Loss: 4.0308\n",
      "Epoch [1/1], Step [5794/7635], Loss: 3.9076\n",
      "Epoch [1/1], Step [5795/7635], Loss: 3.9443\n",
      "Epoch [1/1], Step [5796/7635], Loss: 4.0339\n",
      "Epoch [1/1], Step [5797/7635], Loss: 3.9765\n",
      "Epoch [1/1], Step [5798/7635], Loss: 4.0247\n",
      "Epoch [1/1], Step [5799/7635], Loss: 3.9535\n",
      "Epoch [1/1], Step [5800/7635], Loss: 3.9316\n",
      "Epoch [1/1], Step [5801/7635], Loss: 3.9626\n",
      "Epoch [1/1], Step [5802/7635], Loss: 3.9502\n",
      "Epoch [1/1], Step [5803/7635], Loss: 4.0791\n",
      "Epoch [1/1], Step [5804/7635], Loss: 3.9417\n",
      "Epoch [1/1], Step [5805/7635], Loss: 3.9508\n",
      "Epoch [1/1], Step [5806/7635], Loss: 3.9498\n",
      "Epoch [1/1], Step [5807/7635], Loss: 4.0128\n",
      "Epoch [1/1], Step [5808/7635], Loss: 3.9236\n",
      "Epoch [1/1], Step [5809/7635], Loss: 3.9957\n",
      "Epoch [1/1], Step [5810/7635], Loss: 4.0588\n",
      "Epoch [1/1], Step [5811/7635], Loss: 4.0240\n",
      "Epoch [1/1], Step [5812/7635], Loss: 4.0019\n",
      "Epoch [1/1], Step [5813/7635], Loss: 3.9372\n",
      "Epoch [1/1], Step [5814/7635], Loss: 3.9575\n",
      "Epoch [1/1], Step [5815/7635], Loss: 4.0562\n",
      "Epoch [1/1], Step [5816/7635], Loss: 3.9347\n",
      "Epoch [1/1], Step [5817/7635], Loss: 3.9585\n",
      "Epoch [1/1], Step [5818/7635], Loss: 4.0112\n",
      "Epoch [1/1], Step [5819/7635], Loss: 3.9291\n",
      "Epoch [1/1], Step [5820/7635], Loss: 3.9659\n",
      "Epoch [1/1], Step [5821/7635], Loss: 4.0135\n",
      "Epoch [1/1], Step [5822/7635], Loss: 3.9742\n",
      "Epoch [1/1], Step [5823/7635], Loss: 4.0301\n",
      "Epoch [1/1], Step [5824/7635], Loss: 3.9929\n",
      "Epoch [1/1], Step [5825/7635], Loss: 3.9754\n",
      "Epoch [1/1], Step [5826/7635], Loss: 3.9135\n",
      "Epoch [1/1], Step [5827/7635], Loss: 3.9941\n",
      "Epoch [1/1], Step [5828/7635], Loss: 4.0017\n",
      "Epoch [1/1], Step [5829/7635], Loss: 3.8649\n",
      "Epoch [1/1], Step [5830/7635], Loss: 4.0203\n",
      "Epoch [1/1], Step [5831/7635], Loss: 3.9740\n",
      "Epoch [1/1], Step [5832/7635], Loss: 3.9962\n",
      "Epoch [1/1], Step [5833/7635], Loss: 3.9912\n",
      "Epoch [1/1], Step [5834/7635], Loss: 3.9508\n",
      "Epoch [1/1], Step [5835/7635], Loss: 3.9892\n",
      "Epoch [1/1], Step [5836/7635], Loss: 4.1185\n",
      "Epoch [1/1], Step [5837/7635], Loss: 4.0028\n",
      "Epoch [1/1], Step [5838/7635], Loss: 3.9572\n",
      "Epoch [1/1], Step [5839/7635], Loss: 3.8878\n",
      "Epoch [1/1], Step [5840/7635], Loss: 3.9432\n",
      "Epoch [1/1], Step [5841/7635], Loss: 4.0219\n",
      "Epoch [1/1], Step [5842/7635], Loss: 4.0084\n",
      "Epoch [1/1], Step [5843/7635], Loss: 4.0232\n",
      "Epoch [1/1], Step [5844/7635], Loss: 4.0092\n",
      "Epoch [1/1], Step [5845/7635], Loss: 3.9674\n",
      "Epoch [1/1], Step [5846/7635], Loss: 3.9787\n",
      "Epoch [1/1], Step [5847/7635], Loss: 3.9411\n",
      "Epoch [1/1], Step [5848/7635], Loss: 4.0003\n",
      "Epoch [1/1], Step [5849/7635], Loss: 3.9828\n",
      "Epoch [1/1], Step [5850/7635], Loss: 4.0200\n",
      "Epoch [1/1], Step [5851/7635], Loss: 3.9577\n",
      "Epoch [1/1], Step [5852/7635], Loss: 3.9164\n",
      "Epoch [1/1], Step [5853/7635], Loss: 4.0212\n",
      "Epoch [1/1], Step [5854/7635], Loss: 3.9378\n",
      "Epoch [1/1], Step [5855/7635], Loss: 3.9794\n",
      "Epoch [1/1], Step [5856/7635], Loss: 4.0071\n",
      "Epoch [1/1], Step [5857/7635], Loss: 3.9402\n",
      "Epoch [1/1], Step [5858/7635], Loss: 3.9563\n",
      "Epoch [1/1], Step [5859/7635], Loss: 3.9598\n",
      "Epoch [1/1], Step [5860/7635], Loss: 3.9204\n",
      "Epoch [1/1], Step [5861/7635], Loss: 4.0013\n",
      "Epoch [1/1], Step [5862/7635], Loss: 4.0013\n",
      "Epoch [1/1], Step [5863/7635], Loss: 3.9755\n",
      "Epoch [1/1], Step [5864/7635], Loss: 3.9301\n",
      "Epoch [1/1], Step [5865/7635], Loss: 3.9495\n",
      "Epoch [1/1], Step [5866/7635], Loss: 3.9102\n",
      "Epoch [1/1], Step [5867/7635], Loss: 4.0247\n",
      "Epoch [1/1], Step [5868/7635], Loss: 3.9346\n",
      "Epoch [1/1], Step [5869/7635], Loss: 3.9574\n",
      "Epoch [1/1], Step [5870/7635], Loss: 3.9708\n",
      "Epoch [1/1], Step [5871/7635], Loss: 4.0022\n",
      "Epoch [1/1], Step [5872/7635], Loss: 3.9905\n",
      "Epoch [1/1], Step [5873/7635], Loss: 3.9885\n",
      "Epoch [1/1], Step [5874/7635], Loss: 4.0060\n",
      "Epoch [1/1], Step [5875/7635], Loss: 3.9321\n",
      "Epoch [1/1], Step [5876/7635], Loss: 3.9254\n",
      "Epoch [1/1], Step [5877/7635], Loss: 4.0110\n",
      "Epoch [1/1], Step [5878/7635], Loss: 4.0254\n",
      "Epoch [1/1], Step [5879/7635], Loss: 3.9467\n",
      "Epoch [1/1], Step [5880/7635], Loss: 3.9937\n",
      "Epoch [1/1], Step [5881/7635], Loss: 3.9308\n",
      "Epoch [1/1], Step [5882/7635], Loss: 4.0583\n",
      "Epoch [1/1], Step [5883/7635], Loss: 3.9989\n",
      "Epoch [1/1], Step [5884/7635], Loss: 3.9214\n",
      "Epoch [1/1], Step [5885/7635], Loss: 3.9566\n",
      "Epoch [1/1], Step [5886/7635], Loss: 3.8769\n",
      "Epoch [1/1], Step [5887/7635], Loss: 3.9737\n",
      "Epoch [1/1], Step [5888/7635], Loss: 3.9111\n",
      "Epoch [1/1], Step [5889/7635], Loss: 3.9494\n",
      "Epoch [1/1], Step [5890/7635], Loss: 3.9893\n",
      "Epoch [1/1], Step [5891/7635], Loss: 3.9958\n",
      "Epoch [1/1], Step [5892/7635], Loss: 3.9650\n",
      "Epoch [1/1], Step [5893/7635], Loss: 3.9684\n",
      "Epoch [1/1], Step [5894/7635], Loss: 4.0096\n",
      "Epoch [1/1], Step [5895/7635], Loss: 4.0102\n",
      "Epoch [1/1], Step [5896/7635], Loss: 3.9866\n",
      "Epoch [1/1], Step [5897/7635], Loss: 3.9383\n",
      "Epoch [1/1], Step [5898/7635], Loss: 3.9434\n",
      "Epoch [1/1], Step [5899/7635], Loss: 3.9644\n",
      "Epoch [1/1], Step [5900/7635], Loss: 3.8858\n",
      "Epoch [1/1], Step [5901/7635], Loss: 3.9749\n",
      "Epoch [1/1], Step [5902/7635], Loss: 4.0072\n",
      "Epoch [1/1], Step [5903/7635], Loss: 3.9613\n",
      "Epoch [1/1], Step [5904/7635], Loss: 3.9898\n",
      "Epoch [1/1], Step [5905/7635], Loss: 3.9889\n",
      "Epoch [1/1], Step [5906/7635], Loss: 3.9193\n",
      "Epoch [1/1], Step [5907/7635], Loss: 4.0071\n",
      "Epoch [1/1], Step [5908/7635], Loss: 3.9252\n",
      "Epoch [1/1], Step [5909/7635], Loss: 4.0384\n",
      "Epoch [1/1], Step [5910/7635], Loss: 3.8519\n",
      "Epoch [1/1], Step [5911/7635], Loss: 3.9833\n",
      "Epoch [1/1], Step [5912/7635], Loss: 3.8953\n",
      "Epoch [1/1], Step [5913/7635], Loss: 4.0459\n",
      "Epoch [1/1], Step [5914/7635], Loss: 3.9458\n",
      "Epoch [1/1], Step [5915/7635], Loss: 4.0020\n",
      "Epoch [1/1], Step [5916/7635], Loss: 4.0437\n",
      "Epoch [1/1], Step [5917/7635], Loss: 4.0187\n",
      "Epoch [1/1], Step [5918/7635], Loss: 3.9919\n",
      "Epoch [1/1], Step [5919/7635], Loss: 4.0102\n",
      "Epoch [1/1], Step [5920/7635], Loss: 3.9990\n",
      "Epoch [1/1], Step [5921/7635], Loss: 3.9824\n",
      "Epoch [1/1], Step [5922/7635], Loss: 3.9909\n",
      "Epoch [1/1], Step [5923/7635], Loss: 3.9920\n",
      "Epoch [1/1], Step [5924/7635], Loss: 3.9569\n",
      "Epoch [1/1], Step [5925/7635], Loss: 3.9452\n",
      "Epoch [1/1], Step [5926/7635], Loss: 3.9569\n",
      "Epoch [1/1], Step [5927/7635], Loss: 3.9433\n",
      "Epoch [1/1], Step [5928/7635], Loss: 3.9727\n",
      "Epoch [1/1], Step [5929/7635], Loss: 3.8977\n",
      "Epoch [1/1], Step [5930/7635], Loss: 4.0196\n",
      "Epoch [1/1], Step [5931/7635], Loss: 3.9392\n",
      "Epoch [1/1], Step [5932/7635], Loss: 3.9516\n",
      "Epoch [1/1], Step [5933/7635], Loss: 3.9761\n",
      "Epoch [1/1], Step [5934/7635], Loss: 4.0217\n",
      "Epoch [1/1], Step [5935/7635], Loss: 4.0012\n",
      "Epoch [1/1], Step [5936/7635], Loss: 4.0009\n",
      "Epoch [1/1], Step [5937/7635], Loss: 4.0082\n",
      "Epoch [1/1], Step [5938/7635], Loss: 4.0011\n",
      "Epoch [1/1], Step [5939/7635], Loss: 3.9757\n",
      "Epoch [1/1], Step [5940/7635], Loss: 3.9731\n",
      "Epoch [1/1], Step [5941/7635], Loss: 3.9435\n",
      "Epoch [1/1], Step [5942/7635], Loss: 3.9499\n",
      "Epoch [1/1], Step [5943/7635], Loss: 3.9408\n",
      "Epoch [1/1], Step [5944/7635], Loss: 3.9477\n",
      "Epoch [1/1], Step [5945/7635], Loss: 3.9679\n",
      "Epoch [1/1], Step [5946/7635], Loss: 3.9674\n",
      "Epoch [1/1], Step [5947/7635], Loss: 3.9709\n",
      "Epoch [1/1], Step [5948/7635], Loss: 3.9427\n",
      "Epoch [1/1], Step [5949/7635], Loss: 3.9841\n",
      "Epoch [1/1], Step [5950/7635], Loss: 3.9615\n",
      "Epoch [1/1], Step [5951/7635], Loss: 3.9463\n",
      "Epoch [1/1], Step [5952/7635], Loss: 3.9114\n",
      "Epoch [1/1], Step [5953/7635], Loss: 3.9629\n",
      "Epoch [1/1], Step [5954/7635], Loss: 3.9708\n",
      "Epoch [1/1], Step [5955/7635], Loss: 3.9339\n",
      "Epoch [1/1], Step [5956/7635], Loss: 3.9752\n",
      "Epoch [1/1], Step [5957/7635], Loss: 3.9450\n",
      "Epoch [1/1], Step [5958/7635], Loss: 3.9923\n",
      "Epoch [1/1], Step [5959/7635], Loss: 3.9137\n",
      "Epoch [1/1], Step [5960/7635], Loss: 4.0314\n",
      "Epoch [1/1], Step [5961/7635], Loss: 4.0000\n",
      "Epoch [1/1], Step [5962/7635], Loss: 3.8731\n",
      "Epoch [1/1], Step [5963/7635], Loss: 3.9587\n",
      "Epoch [1/1], Step [5964/7635], Loss: 3.9665\n",
      "Epoch [1/1], Step [5965/7635], Loss: 3.9955\n",
      "Epoch [1/1], Step [5966/7635], Loss: 3.9809\n",
      "Epoch [1/1], Step [5967/7635], Loss: 3.9588\n",
      "Epoch [1/1], Step [5968/7635], Loss: 4.0148\n",
      "Epoch [1/1], Step [5969/7635], Loss: 3.9608\n",
      "Epoch [1/1], Step [5970/7635], Loss: 4.0273\n",
      "Epoch [1/1], Step [5971/7635], Loss: 3.8891\n",
      "Epoch [1/1], Step [5972/7635], Loss: 4.0115\n",
      "Epoch [1/1], Step [5973/7635], Loss: 4.0115\n",
      "Epoch [1/1], Step [5974/7635], Loss: 3.9389\n",
      "Epoch [1/1], Step [5975/7635], Loss: 4.0076\n",
      "Epoch [1/1], Step [5976/7635], Loss: 3.9538\n",
      "Epoch [1/1], Step [5977/7635], Loss: 3.9540\n",
      "Epoch [1/1], Step [5978/7635], Loss: 3.9479\n",
      "Epoch [1/1], Step [5979/7635], Loss: 3.9166\n",
      "Epoch [1/1], Step [5980/7635], Loss: 3.9203\n",
      "Epoch [1/1], Step [5981/7635], Loss: 3.9121\n",
      "Epoch [1/1], Step [5982/7635], Loss: 3.9933\n",
      "Epoch [1/1], Step [5983/7635], Loss: 3.8792\n",
      "Epoch [1/1], Step [5984/7635], Loss: 3.9732\n",
      "Epoch [1/1], Step [5985/7635], Loss: 4.0058\n",
      "Epoch [1/1], Step [5986/7635], Loss: 3.9390\n",
      "Epoch [1/1], Step [5987/7635], Loss: 3.9490\n",
      "Epoch [1/1], Step [5988/7635], Loss: 3.9681\n",
      "Epoch [1/1], Step [5989/7635], Loss: 3.9757\n",
      "Epoch [1/1], Step [5990/7635], Loss: 4.0484\n",
      "Epoch [1/1], Step [5991/7635], Loss: 3.9826\n",
      "Epoch [1/1], Step [5992/7635], Loss: 4.0275\n",
      "Epoch [1/1], Step [5993/7635], Loss: 3.9649\n",
      "Epoch [1/1], Step [5994/7635], Loss: 4.0162\n",
      "Epoch [1/1], Step [5995/7635], Loss: 3.9568\n",
      "Epoch [1/1], Step [5996/7635], Loss: 3.9222\n",
      "Epoch [1/1], Step [5997/7635], Loss: 3.9956\n",
      "Epoch [1/1], Step [5998/7635], Loss: 4.0358\n",
      "Epoch [1/1], Step [5999/7635], Loss: 3.9856\n",
      "Epoch [1/1], Step [6000/7635], Loss: 3.9073\n",
      "Epoch [1/1], Step [6001/7635], Loss: 4.0722\n",
      "Epoch [1/1], Step [6002/7635], Loss: 3.9680\n",
      "Epoch [1/1], Step [6003/7635], Loss: 3.8769\n",
      "Epoch [1/1], Step [6004/7635], Loss: 4.0039\n",
      "Epoch [1/1], Step [6005/7635], Loss: 3.8970\n",
      "Epoch [1/1], Step [6006/7635], Loss: 3.9343\n",
      "Epoch [1/1], Step [6007/7635], Loss: 4.0223\n",
      "Epoch [1/1], Step [6008/7635], Loss: 3.9965\n",
      "Epoch [1/1], Step [6009/7635], Loss: 4.0117\n",
      "Epoch [1/1], Step [6010/7635], Loss: 3.9348\n",
      "Epoch [1/1], Step [6011/7635], Loss: 3.9341\n",
      "Epoch [1/1], Step [6012/7635], Loss: 3.9028\n",
      "Epoch [1/1], Step [6013/7635], Loss: 3.9669\n",
      "Epoch [1/1], Step [6014/7635], Loss: 3.9186\n",
      "Epoch [1/1], Step [6015/7635], Loss: 4.0236\n",
      "Epoch [1/1], Step [6016/7635], Loss: 3.9399\n",
      "Epoch [1/1], Step [6017/7635], Loss: 3.9491\n",
      "Epoch [1/1], Step [6018/7635], Loss: 3.9578\n",
      "Epoch [1/1], Step [6019/7635], Loss: 3.9049\n",
      "Epoch [1/1], Step [6020/7635], Loss: 4.0052\n",
      "Epoch [1/1], Step [6021/7635], Loss: 3.9637\n",
      "Epoch [1/1], Step [6022/7635], Loss: 4.0251\n",
      "Epoch [1/1], Step [6023/7635], Loss: 3.9669\n",
      "Epoch [1/1], Step [6024/7635], Loss: 3.9955\n",
      "Epoch [1/1], Step [6025/7635], Loss: 3.9230\n",
      "Epoch [1/1], Step [6026/7635], Loss: 3.9297\n",
      "Epoch [1/1], Step [6027/7635], Loss: 4.0220\n",
      "Epoch [1/1], Step [6028/7635], Loss: 3.9431\n",
      "Epoch [1/1], Step [6029/7635], Loss: 4.0165\n",
      "Epoch [1/1], Step [6030/7635], Loss: 3.9910\n",
      "Epoch [1/1], Step [6031/7635], Loss: 4.0085\n",
      "Epoch [1/1], Step [6032/7635], Loss: 3.9951\n",
      "Epoch [1/1], Step [6033/7635], Loss: 3.9688\n",
      "Epoch [1/1], Step [6034/7635], Loss: 3.9624\n",
      "Epoch [1/1], Step [6035/7635], Loss: 3.9427\n",
      "Epoch [1/1], Step [6036/7635], Loss: 3.9500\n",
      "Epoch [1/1], Step [6037/7635], Loss: 3.9405\n",
      "Epoch [1/1], Step [6038/7635], Loss: 4.0193\n",
      "Epoch [1/1], Step [6039/7635], Loss: 3.9328\n",
      "Epoch [1/1], Step [6040/7635], Loss: 3.9255\n",
      "Epoch [1/1], Step [6041/7635], Loss: 4.0410\n",
      "Epoch [1/1], Step [6042/7635], Loss: 4.0395\n",
      "Epoch [1/1], Step [6043/7635], Loss: 3.9410\n",
      "Epoch [1/1], Step [6044/7635], Loss: 3.9550\n",
      "Epoch [1/1], Step [6045/7635], Loss: 4.0463\n",
      "Epoch [1/1], Step [6046/7635], Loss: 3.9632\n",
      "Epoch [1/1], Step [6047/7635], Loss: 3.9740\n",
      "Epoch [1/1], Step [6048/7635], Loss: 4.0439\n",
      "Epoch [1/1], Step [6049/7635], Loss: 3.9849\n",
      "Epoch [1/1], Step [6050/7635], Loss: 3.9862\n",
      "Epoch [1/1], Step [6051/7635], Loss: 3.9781\n",
      "Epoch [1/1], Step [6052/7635], Loss: 3.9373\n",
      "Epoch [1/1], Step [6053/7635], Loss: 4.0119\n",
      "Epoch [1/1], Step [6054/7635], Loss: 4.0200\n",
      "Epoch [1/1], Step [6055/7635], Loss: 4.0088\n",
      "Epoch [1/1], Step [6056/7635], Loss: 3.9187\n",
      "Epoch [1/1], Step [6057/7635], Loss: 4.0495\n",
      "Epoch [1/1], Step [6058/7635], Loss: 3.9384\n",
      "Epoch [1/1], Step [6059/7635], Loss: 3.9450\n",
      "Epoch [1/1], Step [6060/7635], Loss: 3.9649\n",
      "Epoch [1/1], Step [6061/7635], Loss: 3.9612\n",
      "Epoch [1/1], Step [6062/7635], Loss: 3.8981\n",
      "Epoch [1/1], Step [6063/7635], Loss: 3.9793\n",
      "Epoch [1/1], Step [6064/7635], Loss: 3.9524\n",
      "Epoch [1/1], Step [6065/7635], Loss: 3.9747\n",
      "Epoch [1/1], Step [6066/7635], Loss: 3.9564\n",
      "Epoch [1/1], Step [6067/7635], Loss: 4.0097\n",
      "Epoch [1/1], Step [6068/7635], Loss: 3.9748\n",
      "Epoch [1/1], Step [6069/7635], Loss: 3.8837\n",
      "Epoch [1/1], Step [6070/7635], Loss: 3.9795\n",
      "Epoch [1/1], Step [6071/7635], Loss: 4.0118\n",
      "Epoch [1/1], Step [6072/7635], Loss: 3.9985\n",
      "Epoch [1/1], Step [6073/7635], Loss: 3.9521\n",
      "Epoch [1/1], Step [6074/7635], Loss: 3.9784\n",
      "Epoch [1/1], Step [6075/7635], Loss: 3.9260\n",
      "Epoch [1/1], Step [6076/7635], Loss: 3.9660\n",
      "Epoch [1/1], Step [6077/7635], Loss: 3.9077\n",
      "Epoch [1/1], Step [6078/7635], Loss: 4.0099\n",
      "Epoch [1/1], Step [6079/7635], Loss: 4.0505\n",
      "Epoch [1/1], Step [6080/7635], Loss: 4.0378\n",
      "Epoch [1/1], Step [6081/7635], Loss: 3.9751\n",
      "Epoch [1/1], Step [6082/7635], Loss: 3.9705\n",
      "Epoch [1/1], Step [6083/7635], Loss: 3.9956\n",
      "Epoch [1/1], Step [6084/7635], Loss: 3.9717\n",
      "Epoch [1/1], Step [6085/7635], Loss: 3.9421\n",
      "Epoch [1/1], Step [6086/7635], Loss: 3.9268\n",
      "Epoch [1/1], Step [6087/7635], Loss: 4.0036\n",
      "Epoch [1/1], Step [6088/7635], Loss: 3.9498\n",
      "Epoch [1/1], Step [6089/7635], Loss: 3.9145\n",
      "Epoch [1/1], Step [6090/7635], Loss: 3.9514\n",
      "Epoch [1/1], Step [6091/7635], Loss: 3.9781\n",
      "Epoch [1/1], Step [6092/7635], Loss: 3.9749\n",
      "Epoch [1/1], Step [6093/7635], Loss: 4.0223\n",
      "Epoch [1/1], Step [6094/7635], Loss: 3.9552\n",
      "Epoch [1/1], Step [6095/7635], Loss: 4.0289\n",
      "Epoch [1/1], Step [6096/7635], Loss: 3.9954\n",
      "Epoch [1/1], Step [6097/7635], Loss: 3.9678\n",
      "Epoch [1/1], Step [6098/7635], Loss: 3.9111\n",
      "Epoch [1/1], Step [6099/7635], Loss: 3.9341\n",
      "Epoch [1/1], Step [6100/7635], Loss: 3.9570\n",
      "Epoch [1/1], Step [6101/7635], Loss: 4.0030\n",
      "Epoch [1/1], Step [6102/7635], Loss: 4.0165\n",
      "Epoch [1/1], Step [6103/7635], Loss: 3.9750\n",
      "Epoch [1/1], Step [6104/7635], Loss: 4.0405\n",
      "Epoch [1/1], Step [6105/7635], Loss: 4.0179\n",
      "Epoch [1/1], Step [6106/7635], Loss: 3.9767\n",
      "Epoch [1/1], Step [6107/7635], Loss: 3.9113\n",
      "Epoch [1/1], Step [6108/7635], Loss: 3.9321\n",
      "Epoch [1/1], Step [6109/7635], Loss: 3.9953\n",
      "Epoch [1/1], Step [6110/7635], Loss: 3.9818\n",
      "Epoch [1/1], Step [6111/7635], Loss: 3.9729\n",
      "Epoch [1/1], Step [6112/7635], Loss: 3.9934\n",
      "Epoch [1/1], Step [6113/7635], Loss: 3.9059\n",
      "Epoch [1/1], Step [6114/7635], Loss: 4.0054\n",
      "Epoch [1/1], Step [6115/7635], Loss: 3.9352\n",
      "Epoch [1/1], Step [6116/7635], Loss: 3.9313\n",
      "Epoch [1/1], Step [6117/7635], Loss: 3.9389\n",
      "Epoch [1/1], Step [6118/7635], Loss: 3.9182\n",
      "Epoch [1/1], Step [6119/7635], Loss: 3.9632\n",
      "Epoch [1/1], Step [6120/7635], Loss: 4.0028\n",
      "Epoch [1/1], Step [6121/7635], Loss: 4.0030\n",
      "Epoch [1/1], Step [6122/7635], Loss: 3.9269\n",
      "Epoch [1/1], Step [6123/7635], Loss: 3.9438\n",
      "Epoch [1/1], Step [6124/7635], Loss: 3.9161\n",
      "Epoch [1/1], Step [6125/7635], Loss: 3.8990\n",
      "Epoch [1/1], Step [6126/7635], Loss: 3.9551\n",
      "Epoch [1/1], Step [6127/7635], Loss: 3.9506\n",
      "Epoch [1/1], Step [6128/7635], Loss: 4.0212\n",
      "Epoch [1/1], Step [6129/7635], Loss: 3.9336\n",
      "Epoch [1/1], Step [6130/7635], Loss: 3.9695\n",
      "Epoch [1/1], Step [6131/7635], Loss: 4.0036\n",
      "Epoch [1/1], Step [6132/7635], Loss: 4.0030\n",
      "Epoch [1/1], Step [6133/7635], Loss: 4.0254\n",
      "Epoch [1/1], Step [6134/7635], Loss: 3.9903\n",
      "Epoch [1/1], Step [6135/7635], Loss: 3.9867\n",
      "Epoch [1/1], Step [6136/7635], Loss: 3.9253\n",
      "Epoch [1/1], Step [6137/7635], Loss: 3.9699\n",
      "Epoch [1/1], Step [6138/7635], Loss: 3.8743\n",
      "Epoch [1/1], Step [6139/7635], Loss: 4.0017\n",
      "Epoch [1/1], Step [6140/7635], Loss: 3.9821\n",
      "Epoch [1/1], Step [6141/7635], Loss: 4.0049\n",
      "Epoch [1/1], Step [6142/7635], Loss: 3.9765\n",
      "Epoch [1/1], Step [6143/7635], Loss: 3.9095\n",
      "Epoch [1/1], Step [6144/7635], Loss: 3.9791\n",
      "Epoch [1/1], Step [6145/7635], Loss: 3.9899\n",
      "Epoch [1/1], Step [6146/7635], Loss: 3.9773\n",
      "Epoch [1/1], Step [6147/7635], Loss: 3.9371\n",
      "Epoch [1/1], Step [6148/7635], Loss: 3.9959\n",
      "Epoch [1/1], Step [6149/7635], Loss: 3.9994\n",
      "Epoch [1/1], Step [6150/7635], Loss: 3.9767\n",
      "Epoch [1/1], Step [6151/7635], Loss: 4.0031\n",
      "Epoch [1/1], Step [6152/7635], Loss: 4.0135\n",
      "Epoch [1/1], Step [6153/7635], Loss: 3.9481\n",
      "Epoch [1/1], Step [6154/7635], Loss: 3.9346\n",
      "Epoch [1/1], Step [6155/7635], Loss: 4.0052\n",
      "Epoch [1/1], Step [6156/7635], Loss: 3.9781\n",
      "Epoch [1/1], Step [6157/7635], Loss: 4.0046\n",
      "Epoch [1/1], Step [6158/7635], Loss: 3.9552\n",
      "Epoch [1/1], Step [6159/7635], Loss: 4.0106\n",
      "Epoch [1/1], Step [6160/7635], Loss: 3.8740\n",
      "Epoch [1/1], Step [6161/7635], Loss: 3.9582\n",
      "Epoch [1/1], Step [6162/7635], Loss: 3.9495\n",
      "Epoch [1/1], Step [6163/7635], Loss: 3.9660\n",
      "Epoch [1/1], Step [6164/7635], Loss: 4.0210\n",
      "Epoch [1/1], Step [6165/7635], Loss: 3.9317\n",
      "Epoch [1/1], Step [6166/7635], Loss: 3.9443\n",
      "Epoch [1/1], Step [6167/7635], Loss: 3.9736\n",
      "Epoch [1/1], Step [6168/7635], Loss: 3.9964\n",
      "Epoch [1/1], Step [6169/7635], Loss: 3.9806\n",
      "Epoch [1/1], Step [6170/7635], Loss: 3.9520\n",
      "Epoch [1/1], Step [6171/7635], Loss: 3.9394\n",
      "Epoch [1/1], Step [6172/7635], Loss: 3.9374\n",
      "Epoch [1/1], Step [6173/7635], Loss: 3.9085\n",
      "Epoch [1/1], Step [6174/7635], Loss: 3.9024\n",
      "Epoch [1/1], Step [6175/7635], Loss: 4.0157\n",
      "Epoch [1/1], Step [6176/7635], Loss: 4.0046\n",
      "Epoch [1/1], Step [6177/7635], Loss: 3.9578\n",
      "Epoch [1/1], Step [6178/7635], Loss: 3.9593\n",
      "Epoch [1/1], Step [6179/7635], Loss: 3.9346\n",
      "Epoch [1/1], Step [6180/7635], Loss: 4.0384\n",
      "Epoch [1/1], Step [6181/7635], Loss: 4.0372\n",
      "Epoch [1/1], Step [6182/7635], Loss: 4.0199\n",
      "Epoch [1/1], Step [6183/7635], Loss: 3.9926\n",
      "Epoch [1/1], Step [6184/7635], Loss: 3.9464\n",
      "Epoch [1/1], Step [6185/7635], Loss: 3.9839\n",
      "Epoch [1/1], Step [6186/7635], Loss: 3.9672\n",
      "Epoch [1/1], Step [6187/7635], Loss: 3.9555\n",
      "Epoch [1/1], Step [6188/7635], Loss: 3.8855\n",
      "Epoch [1/1], Step [6189/7635], Loss: 4.0186\n",
      "Epoch [1/1], Step [6190/7635], Loss: 3.9724\n",
      "Epoch [1/1], Step [6191/7635], Loss: 3.9050\n",
      "Epoch [1/1], Step [6192/7635], Loss: 3.9456\n",
      "Epoch [1/1], Step [6193/7635], Loss: 3.9273\n",
      "Epoch [1/1], Step [6194/7635], Loss: 3.9837\n",
      "Epoch [1/1], Step [6195/7635], Loss: 3.9988\n",
      "Epoch [1/1], Step [6196/7635], Loss: 3.9909\n",
      "Epoch [1/1], Step [6197/7635], Loss: 3.9969\n",
      "Epoch [1/1], Step [6198/7635], Loss: 3.9230\n",
      "Epoch [1/1], Step [6199/7635], Loss: 3.8684\n",
      "Epoch [1/1], Step [6200/7635], Loss: 3.9504\n",
      "Epoch [1/1], Step [6201/7635], Loss: 3.9530\n",
      "Epoch [1/1], Step [6202/7635], Loss: 3.9539\n",
      "Epoch [1/1], Step [6203/7635], Loss: 3.9322\n",
      "Epoch [1/1], Step [6204/7635], Loss: 3.9425\n",
      "Epoch [1/1], Step [6205/7635], Loss: 3.9810\n",
      "Epoch [1/1], Step [6206/7635], Loss: 3.9913\n",
      "Epoch [1/1], Step [6207/7635], Loss: 3.9808\n",
      "Epoch [1/1], Step [6208/7635], Loss: 3.9317\n",
      "Epoch [1/1], Step [6209/7635], Loss: 4.0032\n",
      "Epoch [1/1], Step [6210/7635], Loss: 3.9174\n",
      "Epoch [1/1], Step [6211/7635], Loss: 3.8610\n",
      "Epoch [1/1], Step [6212/7635], Loss: 3.9379\n",
      "Epoch [1/1], Step [6213/7635], Loss: 4.0006\n",
      "Epoch [1/1], Step [6214/7635], Loss: 4.0186\n",
      "Epoch [1/1], Step [6215/7635], Loss: 4.0011\n",
      "Epoch [1/1], Step [6216/7635], Loss: 3.9659\n",
      "Epoch [1/1], Step [6217/7635], Loss: 3.9461\n",
      "Epoch [1/1], Step [6218/7635], Loss: 3.9597\n",
      "Epoch [1/1], Step [6219/7635], Loss: 4.0051\n",
      "Epoch [1/1], Step [6220/7635], Loss: 3.9488\n",
      "Epoch [1/1], Step [6221/7635], Loss: 4.0056\n",
      "Epoch [1/1], Step [6222/7635], Loss: 3.9351\n",
      "Epoch [1/1], Step [6223/7635], Loss: 3.9665\n",
      "Epoch [1/1], Step [6224/7635], Loss: 3.8748\n",
      "Epoch [1/1], Step [6225/7635], Loss: 3.9279\n",
      "Epoch [1/1], Step [6226/7635], Loss: 3.9676\n",
      "Epoch [1/1], Step [6227/7635], Loss: 3.9258\n",
      "Epoch [1/1], Step [6228/7635], Loss: 3.9943\n",
      "Epoch [1/1], Step [6229/7635], Loss: 3.9419\n",
      "Epoch [1/1], Step [6230/7635], Loss: 4.0662\n",
      "Epoch [1/1], Step [6231/7635], Loss: 3.9860\n",
      "Epoch [1/1], Step [6232/7635], Loss: 3.9570\n",
      "Epoch [1/1], Step [6233/7635], Loss: 3.9649\n",
      "Epoch [1/1], Step [6234/7635], Loss: 4.0523\n",
      "Epoch [1/1], Step [6235/7635], Loss: 4.0438\n",
      "Epoch [1/1], Step [6236/7635], Loss: 3.9948\n",
      "Epoch [1/1], Step [6237/7635], Loss: 3.9477\n",
      "Epoch [1/1], Step [6238/7635], Loss: 3.9206\n",
      "Epoch [1/1], Step [6239/7635], Loss: 3.9459\n",
      "Epoch [1/1], Step [6240/7635], Loss: 3.9455\n",
      "Epoch [1/1], Step [6241/7635], Loss: 3.9692\n",
      "Epoch [1/1], Step [6242/7635], Loss: 4.0265\n",
      "Epoch [1/1], Step [6243/7635], Loss: 3.9779\n",
      "Epoch [1/1], Step [6244/7635], Loss: 3.9773\n",
      "Epoch [1/1], Step [6245/7635], Loss: 3.9158\n",
      "Epoch [1/1], Step [6246/7635], Loss: 3.9605\n",
      "Epoch [1/1], Step [6247/7635], Loss: 4.0477\n",
      "Epoch [1/1], Step [6248/7635], Loss: 4.0249\n",
      "Epoch [1/1], Step [6249/7635], Loss: 3.9175\n",
      "Epoch [1/1], Step [6250/7635], Loss: 3.9337\n",
      "Epoch [1/1], Step [6251/7635], Loss: 3.9320\n",
      "Epoch [1/1], Step [6252/7635], Loss: 4.0441\n",
      "Epoch [1/1], Step [6253/7635], Loss: 3.9769\n",
      "Epoch [1/1], Step [6254/7635], Loss: 3.9586\n",
      "Epoch [1/1], Step [6255/7635], Loss: 3.9754\n",
      "Epoch [1/1], Step [6256/7635], Loss: 3.9607\n",
      "Epoch [1/1], Step [6257/7635], Loss: 3.9743\n",
      "Epoch [1/1], Step [6258/7635], Loss: 3.9072\n",
      "Epoch [1/1], Step [6259/7635], Loss: 3.9738\n",
      "Epoch [1/1], Step [6260/7635], Loss: 3.9402\n",
      "Epoch [1/1], Step [6261/7635], Loss: 4.0059\n",
      "Epoch [1/1], Step [6262/7635], Loss: 3.9514\n",
      "Epoch [1/1], Step [6263/7635], Loss: 3.9856\n",
      "Epoch [1/1], Step [6264/7635], Loss: 3.9470\n",
      "Epoch [1/1], Step [6265/7635], Loss: 3.9424\n",
      "Epoch [1/1], Step [6266/7635], Loss: 3.9887\n",
      "Epoch [1/1], Step [6267/7635], Loss: 3.9687\n",
      "Epoch [1/1], Step [6268/7635], Loss: 3.9607\n",
      "Epoch [1/1], Step [6269/7635], Loss: 3.9495\n",
      "Epoch [1/1], Step [6270/7635], Loss: 3.9717\n",
      "Epoch [1/1], Step [6271/7635], Loss: 4.0134\n",
      "Epoch [1/1], Step [6272/7635], Loss: 3.9257\n",
      "Epoch [1/1], Step [6273/7635], Loss: 3.9988\n",
      "Epoch [1/1], Step [6274/7635], Loss: 3.9812\n",
      "Epoch [1/1], Step [6275/7635], Loss: 4.0908\n",
      "Epoch [1/1], Step [6276/7635], Loss: 3.9956\n",
      "Epoch [1/1], Step [6277/7635], Loss: 3.9548\n",
      "Epoch [1/1], Step [6278/7635], Loss: 3.9286\n",
      "Epoch [1/1], Step [6279/7635], Loss: 3.9776\n",
      "Epoch [1/1], Step [6280/7635], Loss: 3.9679\n",
      "Epoch [1/1], Step [6281/7635], Loss: 3.9208\n",
      "Epoch [1/1], Step [6282/7635], Loss: 3.9305\n",
      "Epoch [1/1], Step [6283/7635], Loss: 4.0109\n",
      "Epoch [1/1], Step [6284/7635], Loss: 4.0433\n",
      "Epoch [1/1], Step [6285/7635], Loss: 3.9419\n",
      "Epoch [1/1], Step [6286/7635], Loss: 3.9575\n",
      "Epoch [1/1], Step [6287/7635], Loss: 3.9664\n",
      "Epoch [1/1], Step [6288/7635], Loss: 4.0105\n",
      "Epoch [1/1], Step [6289/7635], Loss: 3.9663\n",
      "Epoch [1/1], Step [6290/7635], Loss: 4.0538\n",
      "Epoch [1/1], Step [6291/7635], Loss: 3.9510\n",
      "Epoch [1/1], Step [6292/7635], Loss: 3.9255\n",
      "Epoch [1/1], Step [6293/7635], Loss: 3.8905\n",
      "Epoch [1/1], Step [6294/7635], Loss: 3.9634\n",
      "Epoch [1/1], Step [6295/7635], Loss: 3.9490\n",
      "Epoch [1/1], Step [6296/7635], Loss: 3.9128\n",
      "Epoch [1/1], Step [6297/7635], Loss: 3.9527\n",
      "Epoch [1/1], Step [6298/7635], Loss: 4.0325\n",
      "Epoch [1/1], Step [6299/7635], Loss: 4.0274\n",
      "Epoch [1/1], Step [6300/7635], Loss: 3.9314\n",
      "Epoch [1/1], Step [6301/7635], Loss: 3.9928\n",
      "Epoch [1/1], Step [6302/7635], Loss: 3.9410\n",
      "Epoch [1/1], Step [6303/7635], Loss: 3.9705\n",
      "Epoch [1/1], Step [6304/7635], Loss: 3.8988\n",
      "Epoch [1/1], Step [6305/7635], Loss: 3.9193\n",
      "Epoch [1/1], Step [6306/7635], Loss: 3.9760\n",
      "Epoch [1/1], Step [6307/7635], Loss: 3.9827\n",
      "Epoch [1/1], Step [6308/7635], Loss: 3.9780\n",
      "Epoch [1/1], Step [6309/7635], Loss: 3.9268\n",
      "Epoch [1/1], Step [6310/7635], Loss: 3.9357\n",
      "Epoch [1/1], Step [6311/7635], Loss: 3.9707\n",
      "Epoch [1/1], Step [6312/7635], Loss: 4.0002\n",
      "Epoch [1/1], Step [6313/7635], Loss: 3.9105\n",
      "Epoch [1/1], Step [6314/7635], Loss: 3.9434\n",
      "Epoch [1/1], Step [6315/7635], Loss: 3.9028\n",
      "Epoch [1/1], Step [6316/7635], Loss: 3.9270\n",
      "Epoch [1/1], Step [6317/7635], Loss: 3.8579\n",
      "Epoch [1/1], Step [6318/7635], Loss: 3.9550\n",
      "Epoch [1/1], Step [6319/7635], Loss: 3.9369\n",
      "Epoch [1/1], Step [6320/7635], Loss: 4.0044\n",
      "Epoch [1/1], Step [6321/7635], Loss: 3.9801\n",
      "Epoch [1/1], Step [6322/7635], Loss: 3.9657\n",
      "Epoch [1/1], Step [6323/7635], Loss: 4.0503\n",
      "Epoch [1/1], Step [6324/7635], Loss: 3.9374\n",
      "Epoch [1/1], Step [6325/7635], Loss: 3.9707\n",
      "Epoch [1/1], Step [6326/7635], Loss: 3.9097\n",
      "Epoch [1/1], Step [6327/7635], Loss: 3.9301\n",
      "Epoch [1/1], Step [6328/7635], Loss: 3.8992\n",
      "Epoch [1/1], Step [6329/7635], Loss: 3.9660\n",
      "Epoch [1/1], Step [6330/7635], Loss: 3.9660\n",
      "Epoch [1/1], Step [6331/7635], Loss: 3.9865\n",
      "Epoch [1/1], Step [6332/7635], Loss: 3.9187\n",
      "Epoch [1/1], Step [6333/7635], Loss: 3.9383\n",
      "Epoch [1/1], Step [6334/7635], Loss: 3.9323\n",
      "Epoch [1/1], Step [6335/7635], Loss: 3.9205\n",
      "Epoch [1/1], Step [6336/7635], Loss: 3.9528\n",
      "Epoch [1/1], Step [6337/7635], Loss: 3.9526\n",
      "Epoch [1/1], Step [6338/7635], Loss: 3.9869\n",
      "Epoch [1/1], Step [6339/7635], Loss: 4.0053\n",
      "Epoch [1/1], Step [6340/7635], Loss: 3.9459\n",
      "Epoch [1/1], Step [6341/7635], Loss: 3.9358\n",
      "Epoch [1/1], Step [6342/7635], Loss: 3.9847\n",
      "Epoch [1/1], Step [6343/7635], Loss: 3.8683\n",
      "Epoch [1/1], Step [6344/7635], Loss: 3.9632\n",
      "Epoch [1/1], Step [6345/7635], Loss: 3.9500\n",
      "Epoch [1/1], Step [6346/7635], Loss: 4.0084\n",
      "Epoch [1/1], Step [6347/7635], Loss: 3.9590\n",
      "Epoch [1/1], Step [6348/7635], Loss: 3.9583\n",
      "Epoch [1/1], Step [6349/7635], Loss: 3.9539\n",
      "Epoch [1/1], Step [6350/7635], Loss: 3.9872\n",
      "Epoch [1/1], Step [6351/7635], Loss: 3.9735\n",
      "Epoch [1/1], Step [6352/7635], Loss: 3.9093\n",
      "Epoch [1/1], Step [6353/7635], Loss: 3.9442\n",
      "Epoch [1/1], Step [6354/7635], Loss: 3.9057\n",
      "Epoch [1/1], Step [6355/7635], Loss: 3.9574\n",
      "Epoch [1/1], Step [6356/7635], Loss: 4.0450\n",
      "Epoch [1/1], Step [6357/7635], Loss: 4.0047\n",
      "Epoch [1/1], Step [6358/7635], Loss: 3.9714\n",
      "Epoch [1/1], Step [6359/7635], Loss: 3.9756\n",
      "Epoch [1/1], Step [6360/7635], Loss: 4.0317\n",
      "Epoch [1/1], Step [6361/7635], Loss: 3.9620\n",
      "Epoch [1/1], Step [6362/7635], Loss: 3.9092\n",
      "Epoch [1/1], Step [6363/7635], Loss: 3.9625\n",
      "Epoch [1/1], Step [6364/7635], Loss: 3.9818\n",
      "Epoch [1/1], Step [6365/7635], Loss: 3.9668\n",
      "Epoch [1/1], Step [6366/7635], Loss: 3.9973\n",
      "Epoch [1/1], Step [6367/7635], Loss: 3.9274\n",
      "Epoch [1/1], Step [6368/7635], Loss: 3.9882\n",
      "Epoch [1/1], Step [6369/7635], Loss: 3.9361\n",
      "Epoch [1/1], Step [6370/7635], Loss: 3.9118\n",
      "Epoch [1/1], Step [6371/7635], Loss: 3.9408\n",
      "Epoch [1/1], Step [6372/7635], Loss: 3.9346\n",
      "Epoch [1/1], Step [6373/7635], Loss: 3.9637\n",
      "Epoch [1/1], Step [6374/7635], Loss: 3.9508\n",
      "Epoch [1/1], Step [6375/7635], Loss: 3.9701\n",
      "Epoch [1/1], Step [6376/7635], Loss: 3.9618\n",
      "Epoch [1/1], Step [6377/7635], Loss: 3.9272\n",
      "Epoch [1/1], Step [6378/7635], Loss: 3.8766\n",
      "Epoch [1/1], Step [6379/7635], Loss: 3.8955\n",
      "Epoch [1/1], Step [6380/7635], Loss: 3.9127\n",
      "Epoch [1/1], Step [6381/7635], Loss: 3.9643\n",
      "Epoch [1/1], Step [6382/7635], Loss: 3.9715\n",
      "Epoch [1/1], Step [6383/7635], Loss: 3.9447\n",
      "Epoch [1/1], Step [6384/7635], Loss: 3.9416\n",
      "Epoch [1/1], Step [6385/7635], Loss: 3.9666\n",
      "Epoch [1/1], Step [6386/7635], Loss: 4.0126\n",
      "Epoch [1/1], Step [6387/7635], Loss: 3.9527\n",
      "Epoch [1/1], Step [6388/7635], Loss: 3.9802\n",
      "Epoch [1/1], Step [6389/7635], Loss: 4.0033\n",
      "Epoch [1/1], Step [6390/7635], Loss: 3.9991\n",
      "Epoch [1/1], Step [6391/7635], Loss: 3.9502\n",
      "Epoch [1/1], Step [6392/7635], Loss: 3.8790\n",
      "Epoch [1/1], Step [6393/7635], Loss: 3.9322\n",
      "Epoch [1/1], Step [6394/7635], Loss: 3.9210\n",
      "Epoch [1/1], Step [6395/7635], Loss: 4.0285\n",
      "Epoch [1/1], Step [6396/7635], Loss: 3.9833\n",
      "Epoch [1/1], Step [6397/7635], Loss: 3.9841\n",
      "Epoch [1/1], Step [6398/7635], Loss: 3.9808\n",
      "Epoch [1/1], Step [6399/7635], Loss: 3.9304\n",
      "Epoch [1/1], Step [6400/7635], Loss: 4.0162\n",
      "Epoch [1/1], Step [6401/7635], Loss: 3.8651\n",
      "Epoch [1/1], Step [6402/7635], Loss: 3.9688\n",
      "Epoch [1/1], Step [6403/7635], Loss: 3.9512\n",
      "Epoch [1/1], Step [6404/7635], Loss: 3.8982\n",
      "Epoch [1/1], Step [6405/7635], Loss: 3.9830\n",
      "Epoch [1/1], Step [6406/7635], Loss: 3.9445\n",
      "Epoch [1/1], Step [6407/7635], Loss: 3.9783\n",
      "Epoch [1/1], Step [6408/7635], Loss: 3.9851\n",
      "Epoch [1/1], Step [6409/7635], Loss: 3.9898\n",
      "Epoch [1/1], Step [6410/7635], Loss: 3.9068\n",
      "Epoch [1/1], Step [6411/7635], Loss: 4.0024\n",
      "Epoch [1/1], Step [6412/7635], Loss: 3.9664\n",
      "Epoch [1/1], Step [6413/7635], Loss: 3.9620\n",
      "Epoch [1/1], Step [6414/7635], Loss: 4.0051\n",
      "Epoch [1/1], Step [6415/7635], Loss: 3.9336\n",
      "Epoch [1/1], Step [6416/7635], Loss: 3.9634\n",
      "Epoch [1/1], Step [6417/7635], Loss: 3.9713\n",
      "Epoch [1/1], Step [6418/7635], Loss: 3.8703\n",
      "Epoch [1/1], Step [6419/7635], Loss: 3.8739\n",
      "Epoch [1/1], Step [6420/7635], Loss: 3.9743\n",
      "Epoch [1/1], Step [6421/7635], Loss: 4.0242\n",
      "Epoch [1/1], Step [6422/7635], Loss: 3.8936\n",
      "Epoch [1/1], Step [6423/7635], Loss: 3.9442\n",
      "Epoch [1/1], Step [6424/7635], Loss: 4.0072\n",
      "Epoch [1/1], Step [6425/7635], Loss: 3.9895\n",
      "Epoch [1/1], Step [6426/7635], Loss: 3.9885\n",
      "Epoch [1/1], Step [6427/7635], Loss: 3.9999\n",
      "Epoch [1/1], Step [6428/7635], Loss: 3.8833\n",
      "Epoch [1/1], Step [6429/7635], Loss: 4.0278\n",
      "Epoch [1/1], Step [6430/7635], Loss: 3.9710\n",
      "Epoch [1/1], Step [6431/7635], Loss: 3.9371\n",
      "Epoch [1/1], Step [6432/7635], Loss: 4.0350\n",
      "Epoch [1/1], Step [6433/7635], Loss: 4.0351\n",
      "Epoch [1/1], Step [6434/7635], Loss: 3.9609\n",
      "Epoch [1/1], Step [6435/7635], Loss: 3.9794\n",
      "Epoch [1/1], Step [6436/7635], Loss: 3.9581\n",
      "Epoch [1/1], Step [6437/7635], Loss: 3.9370\n",
      "Epoch [1/1], Step [6438/7635], Loss: 3.9307\n",
      "Epoch [1/1], Step [6439/7635], Loss: 3.8867\n",
      "Epoch [1/1], Step [6440/7635], Loss: 3.9575\n",
      "Epoch [1/1], Step [6441/7635], Loss: 3.8969\n",
      "Epoch [1/1], Step [6442/7635], Loss: 3.8910\n",
      "Epoch [1/1], Step [6443/7635], Loss: 4.0384\n",
      "Epoch [1/1], Step [6444/7635], Loss: 3.9708\n",
      "Epoch [1/1], Step [6445/7635], Loss: 3.9510\n",
      "Epoch [1/1], Step [6446/7635], Loss: 3.9888\n",
      "Epoch [1/1], Step [6447/7635], Loss: 3.9197\n",
      "Epoch [1/1], Step [6448/7635], Loss: 4.0032\n",
      "Epoch [1/1], Step [6449/7635], Loss: 3.9759\n",
      "Epoch [1/1], Step [6450/7635], Loss: 3.9599\n",
      "Epoch [1/1], Step [6451/7635], Loss: 4.0825\n",
      "Epoch [1/1], Step [6452/7635], Loss: 3.9009\n",
      "Epoch [1/1], Step [6453/7635], Loss: 3.9330\n",
      "Epoch [1/1], Step [6454/7635], Loss: 3.8939\n",
      "Epoch [1/1], Step [6455/7635], Loss: 3.9878\n",
      "Epoch [1/1], Step [6456/7635], Loss: 3.9847\n",
      "Epoch [1/1], Step [6457/7635], Loss: 3.9202\n",
      "Epoch [1/1], Step [6458/7635], Loss: 3.9521\n",
      "Epoch [1/1], Step [6459/7635], Loss: 3.9297\n",
      "Epoch [1/1], Step [6460/7635], Loss: 3.9414\n",
      "Epoch [1/1], Step [6461/7635], Loss: 3.9885\n",
      "Epoch [1/1], Step [6462/7635], Loss: 3.9556\n",
      "Epoch [1/1], Step [6463/7635], Loss: 3.8995\n",
      "Epoch [1/1], Step [6464/7635], Loss: 3.9844\n",
      "Epoch [1/1], Step [6465/7635], Loss: 3.9777\n",
      "Epoch [1/1], Step [6466/7635], Loss: 3.9757\n",
      "Epoch [1/1], Step [6467/7635], Loss: 3.9596\n",
      "Epoch [1/1], Step [6468/7635], Loss: 3.9375\n",
      "Epoch [1/1], Step [6469/7635], Loss: 3.9383\n",
      "Epoch [1/1], Step [6470/7635], Loss: 3.8688\n",
      "Epoch [1/1], Step [6471/7635], Loss: 3.9799\n",
      "Epoch [1/1], Step [6472/7635], Loss: 3.9188\n",
      "Epoch [1/1], Step [6473/7635], Loss: 3.9283\n",
      "Epoch [1/1], Step [6474/7635], Loss: 3.8866\n",
      "Epoch [1/1], Step [6475/7635], Loss: 3.9433\n",
      "Epoch [1/1], Step [6476/7635], Loss: 3.9948\n",
      "Epoch [1/1], Step [6477/7635], Loss: 3.9115\n",
      "Epoch [1/1], Step [6478/7635], Loss: 3.9696\n",
      "Epoch [1/1], Step [6479/7635], Loss: 3.9279\n",
      "Epoch [1/1], Step [6480/7635], Loss: 3.9705\n",
      "Epoch [1/1], Step [6481/7635], Loss: 3.8965\n",
      "Epoch [1/1], Step [6482/7635], Loss: 3.9603\n",
      "Epoch [1/1], Step [6483/7635], Loss: 3.9989\n",
      "Epoch [1/1], Step [6484/7635], Loss: 3.8857\n",
      "Epoch [1/1], Step [6485/7635], Loss: 3.9853\n",
      "Epoch [1/1], Step [6486/7635], Loss: 3.9017\n",
      "Epoch [1/1], Step [6487/7635], Loss: 3.8708\n",
      "Epoch [1/1], Step [6488/7635], Loss: 3.9228\n",
      "Epoch [1/1], Step [6489/7635], Loss: 3.9204\n",
      "Epoch [1/1], Step [6490/7635], Loss: 3.9602\n",
      "Epoch [1/1], Step [6491/7635], Loss: 3.9636\n",
      "Epoch [1/1], Step [6492/7635], Loss: 3.9538\n",
      "Epoch [1/1], Step [6493/7635], Loss: 4.0123\n",
      "Epoch [1/1], Step [6494/7635], Loss: 3.9704\n",
      "Epoch [1/1], Step [6495/7635], Loss: 3.9995\n",
      "Epoch [1/1], Step [6496/7635], Loss: 3.9262\n",
      "Epoch [1/1], Step [6497/7635], Loss: 3.9702\n",
      "Epoch [1/1], Step [6498/7635], Loss: 3.9643\n",
      "Epoch [1/1], Step [6499/7635], Loss: 3.9572\n",
      "Epoch [1/1], Step [6500/7635], Loss: 4.0195\n",
      "Epoch [1/1], Step [6501/7635], Loss: 3.9522\n",
      "Epoch [1/1], Step [6502/7635], Loss: 4.0055\n",
      "Epoch [1/1], Step [6503/7635], Loss: 3.9753\n",
      "Epoch [1/1], Step [6504/7635], Loss: 3.9249\n",
      "Epoch [1/1], Step [6505/7635], Loss: 3.8766\n",
      "Epoch [1/1], Step [6506/7635], Loss: 3.9459\n",
      "Epoch [1/1], Step [6507/7635], Loss: 3.9853\n",
      "Epoch [1/1], Step [6508/7635], Loss: 3.9720\n",
      "Epoch [1/1], Step [6509/7635], Loss: 3.9282\n",
      "Epoch [1/1], Step [6510/7635], Loss: 3.9221\n",
      "Epoch [1/1], Step [6511/7635], Loss: 3.8839\n",
      "Epoch [1/1], Step [6512/7635], Loss: 3.9228\n",
      "Epoch [1/1], Step [6513/7635], Loss: 3.9353\n",
      "Epoch [1/1], Step [6514/7635], Loss: 3.9719\n",
      "Epoch [1/1], Step [6515/7635], Loss: 3.9051\n",
      "Epoch [1/1], Step [6516/7635], Loss: 4.0241\n",
      "Epoch [1/1], Step [6517/7635], Loss: 4.0025\n",
      "Epoch [1/1], Step [6518/7635], Loss: 3.9216\n",
      "Epoch [1/1], Step [6519/7635], Loss: 3.9641\n",
      "Epoch [1/1], Step [6520/7635], Loss: 3.9221\n",
      "Epoch [1/1], Step [6521/7635], Loss: 3.9563\n",
      "Epoch [1/1], Step [6522/7635], Loss: 3.9602\n",
      "Epoch [1/1], Step [6523/7635], Loss: 3.9730\n",
      "Epoch [1/1], Step [6524/7635], Loss: 3.9500\n",
      "Epoch [1/1], Step [6525/7635], Loss: 3.9186\n",
      "Epoch [1/1], Step [6526/7635], Loss: 3.9399\n",
      "Epoch [1/1], Step [6527/7635], Loss: 3.9149\n",
      "Epoch [1/1], Step [6528/7635], Loss: 3.9210\n",
      "Epoch [1/1], Step [6529/7635], Loss: 3.9697\n",
      "Epoch [1/1], Step [6530/7635], Loss: 3.9447\n",
      "Epoch [1/1], Step [6531/7635], Loss: 3.9932\n",
      "Epoch [1/1], Step [6532/7635], Loss: 3.8668\n",
      "Epoch [1/1], Step [6533/7635], Loss: 3.9771\n",
      "Epoch [1/1], Step [6534/7635], Loss: 3.8519\n",
      "Epoch [1/1], Step [6535/7635], Loss: 3.8764\n",
      "Epoch [1/1], Step [6536/7635], Loss: 3.9518\n",
      "Epoch [1/1], Step [6537/7635], Loss: 3.8839\n",
      "Epoch [1/1], Step [6538/7635], Loss: 3.8935\n",
      "Epoch [1/1], Step [6539/7635], Loss: 4.0044\n",
      "Epoch [1/1], Step [6540/7635], Loss: 3.9446\n",
      "Epoch [1/1], Step [6541/7635], Loss: 3.9602\n",
      "Epoch [1/1], Step [6542/7635], Loss: 3.8962\n",
      "Epoch [1/1], Step [6543/7635], Loss: 3.9826\n",
      "Epoch [1/1], Step [6544/7635], Loss: 3.9866\n",
      "Epoch [1/1], Step [6545/7635], Loss: 3.9219\n",
      "Epoch [1/1], Step [6546/7635], Loss: 3.9217\n",
      "Epoch [1/1], Step [6547/7635], Loss: 3.9670\n",
      "Epoch [1/1], Step [6548/7635], Loss: 3.8459\n",
      "Epoch [1/1], Step [6549/7635], Loss: 3.9843\n",
      "Epoch [1/1], Step [6550/7635], Loss: 3.9647\n",
      "Epoch [1/1], Step [6551/7635], Loss: 3.9606\n",
      "Epoch [1/1], Step [6552/7635], Loss: 3.9756\n",
      "Epoch [1/1], Step [6553/7635], Loss: 3.9468\n",
      "Epoch [1/1], Step [6554/7635], Loss: 3.9927\n",
      "Epoch [1/1], Step [6555/7635], Loss: 4.0010\n",
      "Epoch [1/1], Step [6556/7635], Loss: 3.8679\n",
      "Epoch [1/1], Step [6557/7635], Loss: 3.9620\n",
      "Epoch [1/1], Step [6558/7635], Loss: 3.9657\n",
      "Epoch [1/1], Step [6559/7635], Loss: 4.0099\n",
      "Epoch [1/1], Step [6560/7635], Loss: 3.9918\n",
      "Epoch [1/1], Step [6561/7635], Loss: 3.9191\n",
      "Epoch [1/1], Step [6562/7635], Loss: 3.9040\n",
      "Epoch [1/1], Step [6563/7635], Loss: 3.9430\n",
      "Epoch [1/1], Step [6564/7635], Loss: 4.0692\n",
      "Epoch [1/1], Step [6565/7635], Loss: 4.0173\n",
      "Epoch [1/1], Step [6566/7635], Loss: 3.9521\n",
      "Epoch [1/1], Step [6567/7635], Loss: 3.9033\n",
      "Epoch [1/1], Step [6568/7635], Loss: 3.9020\n",
      "Epoch [1/1], Step [6569/7635], Loss: 3.9424\n",
      "Epoch [1/1], Step [6570/7635], Loss: 3.9789\n",
      "Epoch [1/1], Step [6571/7635], Loss: 3.9771\n",
      "Epoch [1/1], Step [6572/7635], Loss: 4.0011\n",
      "Epoch [1/1], Step [6573/7635], Loss: 3.9504\n",
      "Epoch [1/1], Step [6574/7635], Loss: 3.9282\n",
      "Epoch [1/1], Step [6575/7635], Loss: 3.9603\n",
      "Epoch [1/1], Step [6576/7635], Loss: 3.9574\n",
      "Epoch [1/1], Step [6577/7635], Loss: 3.9943\n",
      "Epoch [1/1], Step [6578/7635], Loss: 4.0050\n",
      "Epoch [1/1], Step [6579/7635], Loss: 3.9882\n",
      "Epoch [1/1], Step [6580/7635], Loss: 4.0249\n",
      "Epoch [1/1], Step [6581/7635], Loss: 3.9407\n",
      "Epoch [1/1], Step [6582/7635], Loss: 3.9544\n",
      "Epoch [1/1], Step [6583/7635], Loss: 3.9702\n",
      "Epoch [1/1], Step [6584/7635], Loss: 3.9507\n",
      "Epoch [1/1], Step [6585/7635], Loss: 3.9096\n",
      "Epoch [1/1], Step [6586/7635], Loss: 3.8947\n",
      "Epoch [1/1], Step [6587/7635], Loss: 3.9606\n",
      "Epoch [1/1], Step [6588/7635], Loss: 3.9510\n",
      "Epoch [1/1], Step [6589/7635], Loss: 3.9279\n",
      "Epoch [1/1], Step [6590/7635], Loss: 3.9618\n",
      "Epoch [1/1], Step [6591/7635], Loss: 3.8931\n",
      "Epoch [1/1], Step [6592/7635], Loss: 4.0023\n",
      "Epoch [1/1], Step [6593/7635], Loss: 3.9227\n",
      "Epoch [1/1], Step [6594/7635], Loss: 4.0598\n",
      "Epoch [1/1], Step [6595/7635], Loss: 3.8692\n",
      "Epoch [1/1], Step [6596/7635], Loss: 3.9807\n",
      "Epoch [1/1], Step [6597/7635], Loss: 3.9907\n",
      "Epoch [1/1], Step [6598/7635], Loss: 3.9866\n",
      "Epoch [1/1], Step [6599/7635], Loss: 3.9639\n",
      "Epoch [1/1], Step [6600/7635], Loss: 3.8776\n",
      "Epoch [1/1], Step [6601/7635], Loss: 3.8408\n",
      "Epoch [1/1], Step [6602/7635], Loss: 4.0158\n",
      "Epoch [1/1], Step [6603/7635], Loss: 3.9490\n",
      "Epoch [1/1], Step [6604/7635], Loss: 3.9905\n",
      "Epoch [1/1], Step [6605/7635], Loss: 3.9347\n",
      "Epoch [1/1], Step [6606/7635], Loss: 3.9684\n",
      "Epoch [1/1], Step [6607/7635], Loss: 3.9727\n",
      "Epoch [1/1], Step [6608/7635], Loss: 4.0020\n",
      "Epoch [1/1], Step [6609/7635], Loss: 3.9753\n",
      "Epoch [1/1], Step [6610/7635], Loss: 3.9574\n",
      "Epoch [1/1], Step [6611/7635], Loss: 3.9731\n",
      "Epoch [1/1], Step [6612/7635], Loss: 3.9881\n",
      "Epoch [1/1], Step [6613/7635], Loss: 3.9638\n",
      "Epoch [1/1], Step [6614/7635], Loss: 3.9429\n",
      "Epoch [1/1], Step [6615/7635], Loss: 3.9443\n",
      "Epoch [1/1], Step [6616/7635], Loss: 3.8902\n",
      "Epoch [1/1], Step [6617/7635], Loss: 4.0245\n",
      "Epoch [1/1], Step [6618/7635], Loss: 3.9567\n",
      "Epoch [1/1], Step [6619/7635], Loss: 3.9064\n",
      "Epoch [1/1], Step [6620/7635], Loss: 4.0110\n",
      "Epoch [1/1], Step [6621/7635], Loss: 3.9617\n",
      "Epoch [1/1], Step [6622/7635], Loss: 3.8879\n",
      "Epoch [1/1], Step [6623/7635], Loss: 3.9491\n",
      "Epoch [1/1], Step [6624/7635], Loss: 3.9456\n",
      "Epoch [1/1], Step [6625/7635], Loss: 3.8408\n",
      "Epoch [1/1], Step [6626/7635], Loss: 3.9771\n",
      "Epoch [1/1], Step [6627/7635], Loss: 3.9644\n",
      "Epoch [1/1], Step [6628/7635], Loss: 3.9845\n",
      "Epoch [1/1], Step [6629/7635], Loss: 4.0012\n",
      "Epoch [1/1], Step [6630/7635], Loss: 3.9354\n",
      "Epoch [1/1], Step [6631/7635], Loss: 3.9176\n",
      "Epoch [1/1], Step [6632/7635], Loss: 3.9061\n",
      "Epoch [1/1], Step [6633/7635], Loss: 3.9410\n",
      "Epoch [1/1], Step [6634/7635], Loss: 3.9131\n",
      "Epoch [1/1], Step [6635/7635], Loss: 3.9078\n",
      "Epoch [1/1], Step [6636/7635], Loss: 3.9725\n",
      "Epoch [1/1], Step [6637/7635], Loss: 3.8965\n",
      "Epoch [1/1], Step [6638/7635], Loss: 3.9370\n",
      "Epoch [1/1], Step [6639/7635], Loss: 3.9819\n",
      "Epoch [1/1], Step [6640/7635], Loss: 4.0086\n",
      "Epoch [1/1], Step [6641/7635], Loss: 4.0469\n",
      "Epoch [1/1], Step [6642/7635], Loss: 3.9901\n",
      "Epoch [1/1], Step [6643/7635], Loss: 3.9018\n",
      "Epoch [1/1], Step [6644/7635], Loss: 3.8830\n",
      "Epoch [1/1], Step [6645/7635], Loss: 3.9293\n",
      "Epoch [1/1], Step [6646/7635], Loss: 3.9643\n",
      "Epoch [1/1], Step [6647/7635], Loss: 4.0477\n",
      "Epoch [1/1], Step [6648/7635], Loss: 3.9148\n",
      "Epoch [1/1], Step [6649/7635], Loss: 3.9100\n",
      "Epoch [1/1], Step [6650/7635], Loss: 3.9861\n",
      "Epoch [1/1], Step [6651/7635], Loss: 3.9068\n",
      "Epoch [1/1], Step [6652/7635], Loss: 3.9567\n",
      "Epoch [1/1], Step [6653/7635], Loss: 3.9885\n",
      "Epoch [1/1], Step [6654/7635], Loss: 3.8903\n",
      "Epoch [1/1], Step [6655/7635], Loss: 3.9467\n",
      "Epoch [1/1], Step [6656/7635], Loss: 3.9868\n",
      "Epoch [1/1], Step [6657/7635], Loss: 3.9334\n",
      "Epoch [1/1], Step [6658/7635], Loss: 3.9687\n",
      "Epoch [1/1], Step [6659/7635], Loss: 4.0054\n",
      "Epoch [1/1], Step [6660/7635], Loss: 4.0081\n",
      "Epoch [1/1], Step [6661/7635], Loss: 3.9552\n",
      "Epoch [1/1], Step [6662/7635], Loss: 3.9613\n",
      "Epoch [1/1], Step [6663/7635], Loss: 3.9628\n",
      "Epoch [1/1], Step [6664/7635], Loss: 3.9563\n",
      "Epoch [1/1], Step [6665/7635], Loss: 3.9778\n",
      "Epoch [1/1], Step [6666/7635], Loss: 3.9322\n",
      "Epoch [1/1], Step [6667/7635], Loss: 3.9766\n",
      "Epoch [1/1], Step [6668/7635], Loss: 4.0247\n",
      "Epoch [1/1], Step [6669/7635], Loss: 3.9519\n",
      "Epoch [1/1], Step [6670/7635], Loss: 3.9594\n",
      "Epoch [1/1], Step [6671/7635], Loss: 3.9431\n",
      "Epoch [1/1], Step [6672/7635], Loss: 3.9453\n",
      "Epoch [1/1], Step [6673/7635], Loss: 3.9528\n",
      "Epoch [1/1], Step [6674/7635], Loss: 3.8747\n",
      "Epoch [1/1], Step [6675/7635], Loss: 3.9728\n",
      "Epoch [1/1], Step [6676/7635], Loss: 3.9860\n",
      "Epoch [1/1], Step [6677/7635], Loss: 3.9518\n",
      "Epoch [1/1], Step [6678/7635], Loss: 3.9436\n",
      "Epoch [1/1], Step [6679/7635], Loss: 4.0078\n",
      "Epoch [1/1], Step [6680/7635], Loss: 4.0023\n",
      "Epoch [1/1], Step [6681/7635], Loss: 3.9206\n",
      "Epoch [1/1], Step [6682/7635], Loss: 4.0073\n",
      "Epoch [1/1], Step [6683/7635], Loss: 3.9602\n",
      "Epoch [1/1], Step [6684/7635], Loss: 3.8986\n",
      "Epoch [1/1], Step [6685/7635], Loss: 3.9345\n",
      "Epoch [1/1], Step [6686/7635], Loss: 3.9603\n",
      "Epoch [1/1], Step [6687/7635], Loss: 3.9678\n",
      "Epoch [1/1], Step [6688/7635], Loss: 3.9678\n",
      "Epoch [1/1], Step [6689/7635], Loss: 3.9635\n",
      "Epoch [1/1], Step [6690/7635], Loss: 3.9902\n",
      "Epoch [1/1], Step [6691/7635], Loss: 3.9342\n",
      "Epoch [1/1], Step [6692/7635], Loss: 3.9652\n",
      "Epoch [1/1], Step [6693/7635], Loss: 3.9740\n",
      "Epoch [1/1], Step [6694/7635], Loss: 3.8888\n",
      "Epoch [1/1], Step [6695/7635], Loss: 3.9638\n",
      "Epoch [1/1], Step [6696/7635], Loss: 3.8509\n",
      "Epoch [1/1], Step [6697/7635], Loss: 3.9387\n",
      "Epoch [1/1], Step [6698/7635], Loss: 3.9608\n",
      "Epoch [1/1], Step [6699/7635], Loss: 3.9587\n",
      "Epoch [1/1], Step [6700/7635], Loss: 3.9773\n",
      "Epoch [1/1], Step [6701/7635], Loss: 3.8674\n",
      "Epoch [1/1], Step [6702/7635], Loss: 3.9305\n",
      "Epoch [1/1], Step [6703/7635], Loss: 3.9345\n",
      "Epoch [1/1], Step [6704/7635], Loss: 3.9884\n",
      "Epoch [1/1], Step [6705/7635], Loss: 3.9153\n",
      "Epoch [1/1], Step [6706/7635], Loss: 3.9199\n",
      "Epoch [1/1], Step [6707/7635], Loss: 3.9320\n",
      "Epoch [1/1], Step [6708/7635], Loss: 4.0020\n",
      "Epoch [1/1], Step [6709/7635], Loss: 4.0082\n",
      "Epoch [1/1], Step [6710/7635], Loss: 3.9486\n",
      "Epoch [1/1], Step [6711/7635], Loss: 3.9549\n",
      "Epoch [1/1], Step [6712/7635], Loss: 3.9434\n",
      "Epoch [1/1], Step [6713/7635], Loss: 3.9421\n",
      "Epoch [1/1], Step [6714/7635], Loss: 3.9435\n",
      "Epoch [1/1], Step [6715/7635], Loss: 3.9511\n",
      "Epoch [1/1], Step [6716/7635], Loss: 3.9162\n",
      "Epoch [1/1], Step [6717/7635], Loss: 3.9251\n",
      "Epoch [1/1], Step [6718/7635], Loss: 4.0366\n",
      "Epoch [1/1], Step [6719/7635], Loss: 3.9790\n",
      "Epoch [1/1], Step [6720/7635], Loss: 3.9213\n",
      "Epoch [1/1], Step [6721/7635], Loss: 3.9052\n",
      "Epoch [1/1], Step [6722/7635], Loss: 3.9671\n",
      "Epoch [1/1], Step [6723/7635], Loss: 3.9198\n",
      "Epoch [1/1], Step [6724/7635], Loss: 3.9852\n",
      "Epoch [1/1], Step [6725/7635], Loss: 3.9538\n",
      "Epoch [1/1], Step [6726/7635], Loss: 3.9455\n",
      "Epoch [1/1], Step [6727/7635], Loss: 4.0305\n",
      "Epoch [1/1], Step [6728/7635], Loss: 3.9723\n",
      "Epoch [1/1], Step [6729/7635], Loss: 3.9369\n",
      "Epoch [1/1], Step [6730/7635], Loss: 3.8676\n",
      "Epoch [1/1], Step [6731/7635], Loss: 3.9525\n",
      "Epoch [1/1], Step [6732/7635], Loss: 3.9786\n",
      "Epoch [1/1], Step [6733/7635], Loss: 3.9789\n",
      "Epoch [1/1], Step [6734/7635], Loss: 3.9539\n",
      "Epoch [1/1], Step [6735/7635], Loss: 3.9298\n",
      "Epoch [1/1], Step [6736/7635], Loss: 3.9653\n",
      "Epoch [1/1], Step [6737/7635], Loss: 3.9886\n",
      "Epoch [1/1], Step [6738/7635], Loss: 3.9352\n",
      "Epoch [1/1], Step [6739/7635], Loss: 4.0328\n",
      "Epoch [1/1], Step [6740/7635], Loss: 3.8945\n",
      "Epoch [1/1], Step [6741/7635], Loss: 3.9577\n",
      "Epoch [1/1], Step [6742/7635], Loss: 3.9187\n",
      "Epoch [1/1], Step [6743/7635], Loss: 3.8927\n",
      "Epoch [1/1], Step [6744/7635], Loss: 3.9706\n",
      "Epoch [1/1], Step [6745/7635], Loss: 4.0120\n",
      "Epoch [1/1], Step [6746/7635], Loss: 3.9662\n",
      "Epoch [1/1], Step [6747/7635], Loss: 3.9498\n",
      "Epoch [1/1], Step [6748/7635], Loss: 3.9193\n",
      "Epoch [1/1], Step [6749/7635], Loss: 3.9309\n",
      "Epoch [1/1], Step [6750/7635], Loss: 3.8908\n",
      "Epoch [1/1], Step [6751/7635], Loss: 3.9202\n",
      "Epoch [1/1], Step [6752/7635], Loss: 3.9344\n",
      "Epoch [1/1], Step [6753/7635], Loss: 3.9510\n",
      "Epoch [1/1], Step [6754/7635], Loss: 3.9618\n",
      "Epoch [1/1], Step [6755/7635], Loss: 3.9884\n",
      "Epoch [1/1], Step [6756/7635], Loss: 3.8755\n",
      "Epoch [1/1], Step [6757/7635], Loss: 3.9891\n",
      "Epoch [1/1], Step [6758/7635], Loss: 3.9414\n",
      "Epoch [1/1], Step [6759/7635], Loss: 3.9936\n",
      "Epoch [1/1], Step [6760/7635], Loss: 3.8918\n",
      "Epoch [1/1], Step [6761/7635], Loss: 3.9721\n",
      "Epoch [1/1], Step [6762/7635], Loss: 3.9531\n",
      "Epoch [1/1], Step [6763/7635], Loss: 3.9412\n",
      "Epoch [1/1], Step [6764/7635], Loss: 3.9323\n",
      "Epoch [1/1], Step [6765/7635], Loss: 3.9977\n",
      "Epoch [1/1], Step [6766/7635], Loss: 4.0047\n",
      "Epoch [1/1], Step [6767/7635], Loss: 3.8652\n",
      "Epoch [1/1], Step [6768/7635], Loss: 3.9365\n",
      "Epoch [1/1], Step [6769/7635], Loss: 3.9893\n",
      "Epoch [1/1], Step [6770/7635], Loss: 3.9585\n",
      "Epoch [1/1], Step [6771/7635], Loss: 3.9023\n",
      "Epoch [1/1], Step [6772/7635], Loss: 3.9183\n",
      "Epoch [1/1], Step [6773/7635], Loss: 3.9537\n",
      "Epoch [1/1], Step [6774/7635], Loss: 3.9975\n",
      "Epoch [1/1], Step [6775/7635], Loss: 3.9405\n",
      "Epoch [1/1], Step [6776/7635], Loss: 3.9129\n",
      "Epoch [1/1], Step [6777/7635], Loss: 4.0064\n",
      "Epoch [1/1], Step [6778/7635], Loss: 3.9606\n",
      "Epoch [1/1], Step [6779/7635], Loss: 3.9676\n",
      "Epoch [1/1], Step [6780/7635], Loss: 3.9271\n",
      "Epoch [1/1], Step [6781/7635], Loss: 4.0009\n",
      "Epoch [1/1], Step [6782/7635], Loss: 3.9281\n",
      "Epoch [1/1], Step [6783/7635], Loss: 3.8864\n",
      "Epoch [1/1], Step [6784/7635], Loss: 3.8789\n",
      "Epoch [1/1], Step [6785/7635], Loss: 4.0540\n",
      "Epoch [1/1], Step [6786/7635], Loss: 3.9382\n",
      "Epoch [1/1], Step [6787/7635], Loss: 4.0242\n",
      "Epoch [1/1], Step [6788/7635], Loss: 3.9382\n",
      "Epoch [1/1], Step [6789/7635], Loss: 3.9573\n",
      "Epoch [1/1], Step [6790/7635], Loss: 3.9948\n",
      "Epoch [1/1], Step [6791/7635], Loss: 3.9316\n",
      "Epoch [1/1], Step [6792/7635], Loss: 3.9919\n",
      "Epoch [1/1], Step [6793/7635], Loss: 3.9383\n",
      "Epoch [1/1], Step [6794/7635], Loss: 4.0016\n",
      "Epoch [1/1], Step [6795/7635], Loss: 3.9103\n",
      "Epoch [1/1], Step [6796/7635], Loss: 3.8744\n",
      "Epoch [1/1], Step [6797/7635], Loss: 3.9124\n",
      "Epoch [1/1], Step [6798/7635], Loss: 3.9341\n",
      "Epoch [1/1], Step [6799/7635], Loss: 3.9603\n",
      "Epoch [1/1], Step [6800/7635], Loss: 3.9434\n",
      "Epoch [1/1], Step [6801/7635], Loss: 3.9738\n",
      "Epoch [1/1], Step [6802/7635], Loss: 3.9090\n",
      "Epoch [1/1], Step [6803/7635], Loss: 3.9428\n",
      "Epoch [1/1], Step [6804/7635], Loss: 3.9009\n",
      "Epoch [1/1], Step [6805/7635], Loss: 3.8749\n",
      "Epoch [1/1], Step [6806/7635], Loss: 3.9824\n",
      "Epoch [1/1], Step [6807/7635], Loss: 3.9443\n",
      "Epoch [1/1], Step [6808/7635], Loss: 4.0058\n",
      "Epoch [1/1], Step [6809/7635], Loss: 3.8689\n",
      "Epoch [1/1], Step [6810/7635], Loss: 3.9111\n",
      "Epoch [1/1], Step [6811/7635], Loss: 3.9173\n",
      "Epoch [1/1], Step [6812/7635], Loss: 3.9873\n",
      "Epoch [1/1], Step [6813/7635], Loss: 3.9365\n",
      "Epoch [1/1], Step [6814/7635], Loss: 3.9215\n",
      "Epoch [1/1], Step [6815/7635], Loss: 3.8816\n",
      "Epoch [1/1], Step [6816/7635], Loss: 3.9532\n",
      "Epoch [1/1], Step [6817/7635], Loss: 3.9129\n",
      "Epoch [1/1], Step [6818/7635], Loss: 3.9817\n",
      "Epoch [1/1], Step [6819/7635], Loss: 3.9391\n",
      "Epoch [1/1], Step [6820/7635], Loss: 3.9982\n",
      "Epoch [1/1], Step [6821/7635], Loss: 3.9367\n",
      "Epoch [1/1], Step [6822/7635], Loss: 3.8830\n",
      "Epoch [1/1], Step [6823/7635], Loss: 4.0270\n",
      "Epoch [1/1], Step [6824/7635], Loss: 3.9352\n",
      "Epoch [1/1], Step [6825/7635], Loss: 3.9921\n",
      "Epoch [1/1], Step [6826/7635], Loss: 3.9275\n",
      "Epoch [1/1], Step [6827/7635], Loss: 3.9086\n",
      "Epoch [1/1], Step [6828/7635], Loss: 3.9678\n",
      "Epoch [1/1], Step [6829/7635], Loss: 3.9470\n",
      "Epoch [1/1], Step [6830/7635], Loss: 3.9555\n",
      "Epoch [1/1], Step [6831/7635], Loss: 3.8997\n",
      "Epoch [1/1], Step [6832/7635], Loss: 3.9412\n",
      "Epoch [1/1], Step [6833/7635], Loss: 3.9662\n",
      "Epoch [1/1], Step [6834/7635], Loss: 3.9401\n",
      "Epoch [1/1], Step [6835/7635], Loss: 3.9764\n",
      "Epoch [1/1], Step [6836/7635], Loss: 3.8584\n",
      "Epoch [1/1], Step [6837/7635], Loss: 3.9077\n",
      "Epoch [1/1], Step [6838/7635], Loss: 4.0136\n",
      "Epoch [1/1], Step [6839/7635], Loss: 3.9606\n",
      "Epoch [1/1], Step [6840/7635], Loss: 3.9622\n",
      "Epoch [1/1], Step [6841/7635], Loss: 4.0071\n",
      "Epoch [1/1], Step [6842/7635], Loss: 3.9363\n",
      "Epoch [1/1], Step [6843/7635], Loss: 3.8927\n",
      "Epoch [1/1], Step [6844/7635], Loss: 3.9625\n",
      "Epoch [1/1], Step [6845/7635], Loss: 3.9820\n",
      "Epoch [1/1], Step [6846/7635], Loss: 3.9666\n",
      "Epoch [1/1], Step [6847/7635], Loss: 3.9995\n",
      "Epoch [1/1], Step [6848/7635], Loss: 3.9542\n",
      "Epoch [1/1], Step [6849/7635], Loss: 4.0224\n",
      "Epoch [1/1], Step [6850/7635], Loss: 3.9672\n",
      "Epoch [1/1], Step [6851/7635], Loss: 3.9204\n",
      "Epoch [1/1], Step [6852/7635], Loss: 3.9204\n",
      "Epoch [1/1], Step [6853/7635], Loss: 3.8639\n",
      "Epoch [1/1], Step [6854/7635], Loss: 3.9086\n",
      "Epoch [1/1], Step [6855/7635], Loss: 3.9178\n",
      "Epoch [1/1], Step [6856/7635], Loss: 3.9182\n",
      "Epoch [1/1], Step [6857/7635], Loss: 3.8842\n",
      "Epoch [1/1], Step [6858/7635], Loss: 3.9565\n",
      "Epoch [1/1], Step [6859/7635], Loss: 3.9563\n",
      "Epoch [1/1], Step [6860/7635], Loss: 3.9638\n",
      "Epoch [1/1], Step [6861/7635], Loss: 3.9353\n",
      "Epoch [1/1], Step [6862/7635], Loss: 3.8519\n",
      "Epoch [1/1], Step [6863/7635], Loss: 3.9092\n",
      "Epoch [1/1], Step [6864/7635], Loss: 3.9165\n",
      "Epoch [1/1], Step [6865/7635], Loss: 3.9900\n",
      "Epoch [1/1], Step [6866/7635], Loss: 3.9562\n",
      "Epoch [1/1], Step [6867/7635], Loss: 3.9248\n",
      "Epoch [1/1], Step [6868/7635], Loss: 3.9855\n",
      "Epoch [1/1], Step [6869/7635], Loss: 4.0269\n",
      "Epoch [1/1], Step [6870/7635], Loss: 3.8954\n",
      "Epoch [1/1], Step [6871/7635], Loss: 4.0006\n",
      "Epoch [1/1], Step [6872/7635], Loss: 3.9478\n",
      "Epoch [1/1], Step [6873/7635], Loss: 3.9469\n",
      "Epoch [1/1], Step [6874/7635], Loss: 3.8587\n",
      "Epoch [1/1], Step [6875/7635], Loss: 3.9709\n",
      "Epoch [1/1], Step [6876/7635], Loss: 3.9974\n",
      "Epoch [1/1], Step [6877/7635], Loss: 3.9624\n",
      "Epoch [1/1], Step [6878/7635], Loss: 3.9575\n",
      "Epoch [1/1], Step [6879/7635], Loss: 3.8594\n",
      "Epoch [1/1], Step [6880/7635], Loss: 3.9609\n",
      "Epoch [1/1], Step [6881/7635], Loss: 3.8924\n",
      "Epoch [1/1], Step [6882/7635], Loss: 3.9246\n",
      "Epoch [1/1], Step [6883/7635], Loss: 4.0088\n",
      "Epoch [1/1], Step [6884/7635], Loss: 3.9604\n",
      "Epoch [1/1], Step [6885/7635], Loss: 4.0080\n",
      "Epoch [1/1], Step [6886/7635], Loss: 4.0036\n",
      "Epoch [1/1], Step [6887/7635], Loss: 3.9554\n",
      "Epoch [1/1], Step [6888/7635], Loss: 3.9610\n",
      "Epoch [1/1], Step [6889/7635], Loss: 3.9578\n",
      "Epoch [1/1], Step [6890/7635], Loss: 3.9475\n",
      "Epoch [1/1], Step [6891/7635], Loss: 3.9385\n",
      "Epoch [1/1], Step [6892/7635], Loss: 3.9549\n",
      "Epoch [1/1], Step [6893/7635], Loss: 3.9593\n",
      "Epoch [1/1], Step [6894/7635], Loss: 3.9559\n",
      "Epoch [1/1], Step [6895/7635], Loss: 3.9317\n",
      "Epoch [1/1], Step [6896/7635], Loss: 3.8980\n",
      "Epoch [1/1], Step [6897/7635], Loss: 3.9095\n",
      "Epoch [1/1], Step [6898/7635], Loss: 3.8983\n",
      "Epoch [1/1], Step [6899/7635], Loss: 3.9596\n",
      "Epoch [1/1], Step [6900/7635], Loss: 3.9873\n",
      "Epoch [1/1], Step [6901/7635], Loss: 3.9097\n",
      "Epoch [1/1], Step [6902/7635], Loss: 3.9756\n",
      "Epoch [1/1], Step [6903/7635], Loss: 4.0082\n",
      "Epoch [1/1], Step [6904/7635], Loss: 3.9681\n",
      "Epoch [1/1], Step [6905/7635], Loss: 3.9757\n",
      "Epoch [1/1], Step [6906/7635], Loss: 3.9240\n",
      "Epoch [1/1], Step [6907/7635], Loss: 4.0096\n",
      "Epoch [1/1], Step [6908/7635], Loss: 3.9515\n",
      "Epoch [1/1], Step [6909/7635], Loss: 3.9179\n",
      "Epoch [1/1], Step [6910/7635], Loss: 3.8935\n",
      "Epoch [1/1], Step [6911/7635], Loss: 3.9607\n",
      "Epoch [1/1], Step [6912/7635], Loss: 3.9120\n",
      "Epoch [1/1], Step [6913/7635], Loss: 3.9275\n",
      "Epoch [1/1], Step [6914/7635], Loss: 3.9516\n",
      "Epoch [1/1], Step [6915/7635], Loss: 3.9378\n",
      "Epoch [1/1], Step [6916/7635], Loss: 3.9575\n",
      "Epoch [1/1], Step [6917/7635], Loss: 3.9122\n",
      "Epoch [1/1], Step [6918/7635], Loss: 3.9145\n",
      "Epoch [1/1], Step [6919/7635], Loss: 3.9109\n",
      "Epoch [1/1], Step [6920/7635], Loss: 3.9481\n",
      "Epoch [1/1], Step [6921/7635], Loss: 4.0519\n",
      "Epoch [1/1], Step [6922/7635], Loss: 3.9202\n",
      "Epoch [1/1], Step [6923/7635], Loss: 3.9451\n",
      "Epoch [1/1], Step [6924/7635], Loss: 3.9444\n",
      "Epoch [1/1], Step [6925/7635], Loss: 3.9521\n",
      "Epoch [1/1], Step [6926/7635], Loss: 3.9473\n",
      "Epoch [1/1], Step [6927/7635], Loss: 3.9351\n",
      "Epoch [1/1], Step [6928/7635], Loss: 3.9712\n",
      "Epoch [1/1], Step [6929/7635], Loss: 3.9416\n",
      "Epoch [1/1], Step [6930/7635], Loss: 3.9619\n",
      "Epoch [1/1], Step [6931/7635], Loss: 3.9612\n",
      "Epoch [1/1], Step [6932/7635], Loss: 3.8680\n",
      "Epoch [1/1], Step [6933/7635], Loss: 3.9759\n",
      "Epoch [1/1], Step [6934/7635], Loss: 3.9960\n",
      "Epoch [1/1], Step [6935/7635], Loss: 3.9018\n",
      "Epoch [1/1], Step [6936/7635], Loss: 4.0299\n",
      "Epoch [1/1], Step [6937/7635], Loss: 3.9655\n",
      "Epoch [1/1], Step [6938/7635], Loss: 3.9024\n",
      "Epoch [1/1], Step [6939/7635], Loss: 3.9392\n",
      "Epoch [1/1], Step [6940/7635], Loss: 4.0412\n",
      "Epoch [1/1], Step [6941/7635], Loss: 3.9201\n",
      "Epoch [1/1], Step [6942/7635], Loss: 3.8889\n",
      "Epoch [1/1], Step [6943/7635], Loss: 3.9275\n",
      "Epoch [1/1], Step [6944/7635], Loss: 3.9503\n",
      "Epoch [1/1], Step [6945/7635], Loss: 3.9512\n",
      "Epoch [1/1], Step [6946/7635], Loss: 3.9170\n",
      "Epoch [1/1], Step [6947/7635], Loss: 3.9740\n",
      "Epoch [1/1], Step [6948/7635], Loss: 3.9723\n",
      "Epoch [1/1], Step [6949/7635], Loss: 3.9615\n",
      "Epoch [1/1], Step [6950/7635], Loss: 3.9365\n",
      "Epoch [1/1], Step [6951/7635], Loss: 3.9390\n",
      "Epoch [1/1], Step [6952/7635], Loss: 3.9320\n",
      "Epoch [1/1], Step [6953/7635], Loss: 3.9656\n",
      "Epoch [1/1], Step [6954/7635], Loss: 3.9473\n",
      "Epoch [1/1], Step [6955/7635], Loss: 3.9161\n",
      "Epoch [1/1], Step [6956/7635], Loss: 3.9144\n",
      "Epoch [1/1], Step [6957/7635], Loss: 3.9513\n",
      "Epoch [1/1], Step [6958/7635], Loss: 3.9213\n",
      "Epoch [1/1], Step [6959/7635], Loss: 3.8794\n",
      "Epoch [1/1], Step [6960/7635], Loss: 4.0211\n",
      "Epoch [1/1], Step [6961/7635], Loss: 3.9137\n",
      "Epoch [1/1], Step [6962/7635], Loss: 3.9117\n",
      "Epoch [1/1], Step [6963/7635], Loss: 3.9049\n",
      "Epoch [1/1], Step [6964/7635], Loss: 3.9698\n",
      "Epoch [1/1], Step [6965/7635], Loss: 3.8939\n",
      "Epoch [1/1], Step [6966/7635], Loss: 3.9313\n",
      "Epoch [1/1], Step [6967/7635], Loss: 3.8762\n",
      "Epoch [1/1], Step [6968/7635], Loss: 3.9511\n",
      "Epoch [1/1], Step [6969/7635], Loss: 3.9616\n",
      "Epoch [1/1], Step [6970/7635], Loss: 3.9278\n",
      "Epoch [1/1], Step [6971/7635], Loss: 3.9223\n",
      "Epoch [1/1], Step [6972/7635], Loss: 3.8939\n",
      "Epoch [1/1], Step [6973/7635], Loss: 3.9222\n",
      "Epoch [1/1], Step [6974/7635], Loss: 3.9277\n",
      "Epoch [1/1], Step [6975/7635], Loss: 3.8886\n",
      "Epoch [1/1], Step [6976/7635], Loss: 3.9504\n",
      "Epoch [1/1], Step [6977/7635], Loss: 3.9472\n",
      "Epoch [1/1], Step [6978/7635], Loss: 3.9152\n",
      "Epoch [1/1], Step [6979/7635], Loss: 3.8830\n",
      "Epoch [1/1], Step [6980/7635], Loss: 3.9579\n",
      "Epoch [1/1], Step [6981/7635], Loss: 3.9422\n",
      "Epoch [1/1], Step [6982/7635], Loss: 3.9365\n",
      "Epoch [1/1], Step [6983/7635], Loss: 3.9053\n",
      "Epoch [1/1], Step [6984/7635], Loss: 3.9631\n",
      "Epoch [1/1], Step [6985/7635], Loss: 3.9445\n",
      "Epoch [1/1], Step [6986/7635], Loss: 3.9956\n",
      "Epoch [1/1], Step [6987/7635], Loss: 3.9299\n",
      "Epoch [1/1], Step [6988/7635], Loss: 3.9652\n",
      "Epoch [1/1], Step [6989/7635], Loss: 3.9821\n",
      "Epoch [1/1], Step [6990/7635], Loss: 3.9546\n",
      "Epoch [1/1], Step [6991/7635], Loss: 3.8855\n",
      "Epoch [1/1], Step [6992/7635], Loss: 3.9770\n",
      "Epoch [1/1], Step [6993/7635], Loss: 3.9387\n",
      "Epoch [1/1], Step [6994/7635], Loss: 3.9755\n",
      "Epoch [1/1], Step [6995/7635], Loss: 3.9679\n",
      "Epoch [1/1], Step [6996/7635], Loss: 3.9709\n",
      "Epoch [1/1], Step [6997/7635], Loss: 3.9416\n",
      "Epoch [1/1], Step [6998/7635], Loss: 3.9374\n",
      "Epoch [1/1], Step [6999/7635], Loss: 4.0335\n",
      "Epoch [1/1], Step [7000/7635], Loss: 3.8635\n",
      "Epoch [1/1], Step [7001/7635], Loss: 3.9082\n",
      "Epoch [1/1], Step [7002/7635], Loss: 3.9207\n",
      "Epoch [1/1], Step [7003/7635], Loss: 3.9738\n",
      "Epoch [1/1], Step [7004/7635], Loss: 3.9300\n",
      "Epoch [1/1], Step [7005/7635], Loss: 3.9180\n",
      "Epoch [1/1], Step [7006/7635], Loss: 3.9653\n",
      "Epoch [1/1], Step [7007/7635], Loss: 3.9599\n",
      "Epoch [1/1], Step [7008/7635], Loss: 3.9337\n",
      "Epoch [1/1], Step [7009/7635], Loss: 3.9439\n",
      "Epoch [1/1], Step [7010/7635], Loss: 3.8453\n",
      "Epoch [1/1], Step [7011/7635], Loss: 3.9789\n",
      "Epoch [1/1], Step [7012/7635], Loss: 3.9792\n",
      "Epoch [1/1], Step [7013/7635], Loss: 3.9914\n",
      "Epoch [1/1], Step [7014/7635], Loss: 3.9159\n",
      "Epoch [1/1], Step [7015/7635], Loss: 3.9470\n",
      "Epoch [1/1], Step [7016/7635], Loss: 3.8748\n",
      "Epoch [1/1], Step [7017/7635], Loss: 3.9316\n",
      "Epoch [1/1], Step [7018/7635], Loss: 3.8884\n",
      "Epoch [1/1], Step [7019/7635], Loss: 3.9705\n",
      "Epoch [1/1], Step [7020/7635], Loss: 3.9124\n",
      "Epoch [1/1], Step [7021/7635], Loss: 3.9577\n",
      "Epoch [1/1], Step [7022/7635], Loss: 3.9784\n",
      "Epoch [1/1], Step [7023/7635], Loss: 3.8824\n",
      "Epoch [1/1], Step [7024/7635], Loss: 3.9226\n",
      "Epoch [1/1], Step [7025/7635], Loss: 3.9183\n",
      "Epoch [1/1], Step [7026/7635], Loss: 3.9562\n",
      "Epoch [1/1], Step [7027/7635], Loss: 3.9040\n",
      "Epoch [1/1], Step [7028/7635], Loss: 3.9188\n",
      "Epoch [1/1], Step [7029/7635], Loss: 4.0512\n",
      "Epoch [1/1], Step [7030/7635], Loss: 3.9286\n",
      "Epoch [1/1], Step [7031/7635], Loss: 3.9145\n",
      "Epoch [1/1], Step [7032/7635], Loss: 3.9594\n",
      "Epoch [1/1], Step [7033/7635], Loss: 3.8782\n",
      "Epoch [1/1], Step [7034/7635], Loss: 3.9020\n",
      "Epoch [1/1], Step [7035/7635], Loss: 4.0374\n",
      "Epoch [1/1], Step [7036/7635], Loss: 3.8788\n",
      "Epoch [1/1], Step [7037/7635], Loss: 3.9105\n",
      "Epoch [1/1], Step [7038/7635], Loss: 3.9251\n",
      "Epoch [1/1], Step [7039/7635], Loss: 3.9878\n",
      "Epoch [1/1], Step [7040/7635], Loss: 3.9228\n",
      "Epoch [1/1], Step [7041/7635], Loss: 3.9521\n",
      "Epoch [1/1], Step [7042/7635], Loss: 3.9424\n",
      "Epoch [1/1], Step [7043/7635], Loss: 3.9696\n",
      "Epoch [1/1], Step [7044/7635], Loss: 3.9279\n",
      "Epoch [1/1], Step [7045/7635], Loss: 3.9336\n",
      "Epoch [1/1], Step [7046/7635], Loss: 3.9129\n",
      "Epoch [1/1], Step [7047/7635], Loss: 3.8940\n",
      "Epoch [1/1], Step [7048/7635], Loss: 3.8777\n",
      "Epoch [1/1], Step [7049/7635], Loss: 3.8901\n",
      "Epoch [1/1], Step [7050/7635], Loss: 3.9883\n",
      "Epoch [1/1], Step [7051/7635], Loss: 3.9414\n",
      "Epoch [1/1], Step [7052/7635], Loss: 3.9732\n",
      "Epoch [1/1], Step [7053/7635], Loss: 3.8604\n",
      "Epoch [1/1], Step [7054/7635], Loss: 3.9330\n",
      "Epoch [1/1], Step [7055/7635], Loss: 4.0158\n",
      "Epoch [1/1], Step [7056/7635], Loss: 3.9092\n",
      "Epoch [1/1], Step [7057/7635], Loss: 3.9677\n",
      "Epoch [1/1], Step [7058/7635], Loss: 3.9454\n",
      "Epoch [1/1], Step [7059/7635], Loss: 3.9818\n",
      "Epoch [1/1], Step [7060/7635], Loss: 3.9075\n",
      "Epoch [1/1], Step [7061/7635], Loss: 3.9589\n",
      "Epoch [1/1], Step [7062/7635], Loss: 3.8879\n",
      "Epoch [1/1], Step [7063/7635], Loss: 3.9685\n",
      "Epoch [1/1], Step [7064/7635], Loss: 3.9984\n",
      "Epoch [1/1], Step [7065/7635], Loss: 3.8977\n",
      "Epoch [1/1], Step [7066/7635], Loss: 4.0360\n",
      "Epoch [1/1], Step [7067/7635], Loss: 4.0011\n",
      "Epoch [1/1], Step [7068/7635], Loss: 3.9155\n",
      "Epoch [1/1], Step [7069/7635], Loss: 3.9796\n",
      "Epoch [1/1], Step [7070/7635], Loss: 3.9086\n",
      "Epoch [1/1], Step [7071/7635], Loss: 3.9634\n",
      "Epoch [1/1], Step [7072/7635], Loss: 3.9593\n",
      "Epoch [1/1], Step [7073/7635], Loss: 3.9834\n",
      "Epoch [1/1], Step [7074/7635], Loss: 3.9640\n",
      "Epoch [1/1], Step [7075/7635], Loss: 3.9096\n",
      "Epoch [1/1], Step [7076/7635], Loss: 3.9437\n",
      "Epoch [1/1], Step [7077/7635], Loss: 3.9239\n",
      "Epoch [1/1], Step [7078/7635], Loss: 3.9263\n",
      "Epoch [1/1], Step [7079/7635], Loss: 3.9071\n",
      "Epoch [1/1], Step [7080/7635], Loss: 3.8974\n",
      "Epoch [1/1], Step [7081/7635], Loss: 4.0011\n",
      "Epoch [1/1], Step [7082/7635], Loss: 3.8735\n",
      "Epoch [1/1], Step [7083/7635], Loss: 3.9398\n",
      "Epoch [1/1], Step [7084/7635], Loss: 3.9213\n",
      "Epoch [1/1], Step [7085/7635], Loss: 3.9163\n",
      "Epoch [1/1], Step [7086/7635], Loss: 3.8755\n",
      "Epoch [1/1], Step [7087/7635], Loss: 3.9533\n",
      "Epoch [1/1], Step [7088/7635], Loss: 3.9080\n",
      "Epoch [1/1], Step [7089/7635], Loss: 3.9587\n",
      "Epoch [1/1], Step [7090/7635], Loss: 3.9937\n",
      "Epoch [1/1], Step [7091/7635], Loss: 3.9180\n",
      "Epoch [1/1], Step [7092/7635], Loss: 4.0005\n",
      "Epoch [1/1], Step [7093/7635], Loss: 3.9073\n",
      "Epoch [1/1], Step [7094/7635], Loss: 3.9391\n",
      "Epoch [1/1], Step [7095/7635], Loss: 3.9993\n",
      "Epoch [1/1], Step [7096/7635], Loss: 3.9775\n",
      "Epoch [1/1], Step [7097/7635], Loss: 3.9754\n",
      "Epoch [1/1], Step [7098/7635], Loss: 3.9440\n",
      "Epoch [1/1], Step [7099/7635], Loss: 3.9896\n",
      "Epoch [1/1], Step [7100/7635], Loss: 3.9079\n",
      "Epoch [1/1], Step [7101/7635], Loss: 3.9768\n",
      "Epoch [1/1], Step [7102/7635], Loss: 4.0360\n",
      "Epoch [1/1], Step [7103/7635], Loss: 3.9905\n",
      "Epoch [1/1], Step [7104/7635], Loss: 3.9683\n",
      "Epoch [1/1], Step [7105/7635], Loss: 3.8753\n",
      "Epoch [1/1], Step [7106/7635], Loss: 3.8872\n",
      "Epoch [1/1], Step [7107/7635], Loss: 3.9216\n",
      "Epoch [1/1], Step [7108/7635], Loss: 3.9647\n",
      "Epoch [1/1], Step [7109/7635], Loss: 4.0169\n",
      "Epoch [1/1], Step [7110/7635], Loss: 3.9434\n",
      "Epoch [1/1], Step [7111/7635], Loss: 3.9591\n",
      "Epoch [1/1], Step [7112/7635], Loss: 3.8909\n",
      "Epoch [1/1], Step [7113/7635], Loss: 3.9714\n",
      "Epoch [1/1], Step [7114/7635], Loss: 3.9076\n",
      "Epoch [1/1], Step [7115/7635], Loss: 3.9458\n",
      "Epoch [1/1], Step [7116/7635], Loss: 3.9240\n",
      "Epoch [1/1], Step [7117/7635], Loss: 3.8687\n",
      "Epoch [1/1], Step [7118/7635], Loss: 3.9513\n",
      "Epoch [1/1], Step [7119/7635], Loss: 3.8968\n",
      "Epoch [1/1], Step [7120/7635], Loss: 3.9299\n",
      "Epoch [1/1], Step [7121/7635], Loss: 3.9786\n",
      "Epoch [1/1], Step [7122/7635], Loss: 3.9082\n",
      "Epoch [1/1], Step [7123/7635], Loss: 3.9201\n",
      "Epoch [1/1], Step [7124/7635], Loss: 4.0281\n",
      "Epoch [1/1], Step [7125/7635], Loss: 3.9475\n",
      "Epoch [1/1], Step [7126/7635], Loss: 3.8905\n",
      "Epoch [1/1], Step [7127/7635], Loss: 3.9447\n",
      "Epoch [1/1], Step [7128/7635], Loss: 3.9522\n",
      "Epoch [1/1], Step [7129/7635], Loss: 3.9761\n",
      "Epoch [1/1], Step [7130/7635], Loss: 3.8984\n",
      "Epoch [1/1], Step [7131/7635], Loss: 4.0304\n",
      "Epoch [1/1], Step [7132/7635], Loss: 3.9429\n",
      "Epoch [1/1], Step [7133/7635], Loss: 3.9359\n",
      "Epoch [1/1], Step [7134/7635], Loss: 3.9214\n",
      "Epoch [1/1], Step [7135/7635], Loss: 3.8509\n",
      "Epoch [1/1], Step [7136/7635], Loss: 3.9776\n",
      "Epoch [1/1], Step [7137/7635], Loss: 3.9263\n",
      "Epoch [1/1], Step [7138/7635], Loss: 3.9477\n",
      "Epoch [1/1], Step [7139/7635], Loss: 3.9328\n",
      "Epoch [1/1], Step [7140/7635], Loss: 3.9657\n",
      "Epoch [1/1], Step [7141/7635], Loss: 3.9949\n",
      "Epoch [1/1], Step [7142/7635], Loss: 3.9346\n",
      "Epoch [1/1], Step [7143/7635], Loss: 3.9903\n",
      "Epoch [1/1], Step [7144/7635], Loss: 3.9315\n",
      "Epoch [1/1], Step [7145/7635], Loss: 3.9095\n",
      "Epoch [1/1], Step [7146/7635], Loss: 3.9586\n",
      "Epoch [1/1], Step [7147/7635], Loss: 3.9160\n",
      "Epoch [1/1], Step [7148/7635], Loss: 3.9655\n",
      "Epoch [1/1], Step [7149/7635], Loss: 3.9923\n",
      "Epoch [1/1], Step [7150/7635], Loss: 3.9266\n",
      "Epoch [1/1], Step [7151/7635], Loss: 3.9549\n",
      "Epoch [1/1], Step [7152/7635], Loss: 3.9700\n",
      "Epoch [1/1], Step [7153/7635], Loss: 3.8701\n",
      "Epoch [1/1], Step [7154/7635], Loss: 3.9486\n",
      "Epoch [1/1], Step [7155/7635], Loss: 3.9080\n",
      "Epoch [1/1], Step [7156/7635], Loss: 3.9446\n",
      "Epoch [1/1], Step [7157/7635], Loss: 4.0376\n",
      "Epoch [1/1], Step [7158/7635], Loss: 3.9356\n",
      "Epoch [1/1], Step [7159/7635], Loss: 3.9365\n",
      "Epoch [1/1], Step [7160/7635], Loss: 3.9066\n",
      "Epoch [1/1], Step [7161/7635], Loss: 3.9776\n",
      "Epoch [1/1], Step [7162/7635], Loss: 3.9250\n",
      "Epoch [1/1], Step [7163/7635], Loss: 3.9586\n",
      "Epoch [1/1], Step [7164/7635], Loss: 3.9898\n",
      "Epoch [1/1], Step [7165/7635], Loss: 4.0184\n",
      "Epoch [1/1], Step [7166/7635], Loss: 3.9175\n",
      "Epoch [1/1], Step [7167/7635], Loss: 3.9898\n",
      "Epoch [1/1], Step [7168/7635], Loss: 3.9475\n",
      "Epoch [1/1], Step [7169/7635], Loss: 3.8726\n",
      "Epoch [1/1], Step [7170/7635], Loss: 3.8997\n",
      "Epoch [1/1], Step [7171/7635], Loss: 4.0185\n",
      "Epoch [1/1], Step [7172/7635], Loss: 3.9901\n",
      "Epoch [1/1], Step [7173/7635], Loss: 3.9538\n",
      "Epoch [1/1], Step [7174/7635], Loss: 3.8976\n",
      "Epoch [1/1], Step [7175/7635], Loss: 3.8992\n",
      "Epoch [1/1], Step [7176/7635], Loss: 3.8556\n",
      "Epoch [1/1], Step [7177/7635], Loss: 3.9827\n",
      "Epoch [1/1], Step [7178/7635], Loss: 3.9493\n",
      "Epoch [1/1], Step [7179/7635], Loss: 3.9468\n",
      "Epoch [1/1], Step [7180/7635], Loss: 3.9130\n",
      "Epoch [1/1], Step [7181/7635], Loss: 3.9372\n",
      "Epoch [1/1], Step [7182/7635], Loss: 3.9899\n",
      "Epoch [1/1], Step [7183/7635], Loss: 3.9115\n",
      "Epoch [1/1], Step [7184/7635], Loss: 3.9295\n",
      "Epoch [1/1], Step [7185/7635], Loss: 4.0080\n",
      "Epoch [1/1], Step [7186/7635], Loss: 3.9377\n",
      "Epoch [1/1], Step [7187/7635], Loss: 3.9645\n",
      "Epoch [1/1], Step [7188/7635], Loss: 3.9335\n",
      "Epoch [1/1], Step [7189/7635], Loss: 3.8930\n",
      "Epoch [1/1], Step [7190/7635], Loss: 3.8950\n",
      "Epoch [1/1], Step [7191/7635], Loss: 3.9525\n",
      "Epoch [1/1], Step [7192/7635], Loss: 3.9849\n",
      "Epoch [1/1], Step [7193/7635], Loss: 3.9595\n",
      "Epoch [1/1], Step [7194/7635], Loss: 3.9278\n",
      "Epoch [1/1], Step [7195/7635], Loss: 3.9374\n",
      "Epoch [1/1], Step [7196/7635], Loss: 3.9838\n",
      "Epoch [1/1], Step [7197/7635], Loss: 3.9240\n",
      "Epoch [1/1], Step [7198/7635], Loss: 3.9396\n",
      "Epoch [1/1], Step [7199/7635], Loss: 3.9467\n",
      "Epoch [1/1], Step [7200/7635], Loss: 3.9336\n",
      "Epoch [1/1], Step [7201/7635], Loss: 3.8986\n",
      "Epoch [1/1], Step [7202/7635], Loss: 3.9179\n",
      "Epoch [1/1], Step [7203/7635], Loss: 3.8861\n",
      "Epoch [1/1], Step [7204/7635], Loss: 3.8953\n",
      "Epoch [1/1], Step [7205/7635], Loss: 3.9385\n",
      "Epoch [1/1], Step [7206/7635], Loss: 3.9450\n",
      "Epoch [1/1], Step [7207/7635], Loss: 3.9111\n",
      "Epoch [1/1], Step [7208/7635], Loss: 3.8865\n",
      "Epoch [1/1], Step [7209/7635], Loss: 3.9118\n",
      "Epoch [1/1], Step [7210/7635], Loss: 3.9591\n",
      "Epoch [1/1], Step [7211/7635], Loss: 3.9704\n",
      "Epoch [1/1], Step [7212/7635], Loss: 3.8525\n",
      "Epoch [1/1], Step [7213/7635], Loss: 3.9315\n",
      "Epoch [1/1], Step [7214/7635], Loss: 3.8743\n",
      "Epoch [1/1], Step [7215/7635], Loss: 3.9249\n",
      "Epoch [1/1], Step [7216/7635], Loss: 3.9376\n",
      "Epoch [1/1], Step [7217/7635], Loss: 3.9519\n",
      "Epoch [1/1], Step [7218/7635], Loss: 3.9231\n",
      "Epoch [1/1], Step [7219/7635], Loss: 3.9675\n",
      "Epoch [1/1], Step [7220/7635], Loss: 3.9953\n",
      "Epoch [1/1], Step [7221/7635], Loss: 4.0126\n",
      "Epoch [1/1], Step [7222/7635], Loss: 3.9843\n",
      "Epoch [1/1], Step [7223/7635], Loss: 3.9457\n",
      "Epoch [1/1], Step [7224/7635], Loss: 3.9191\n",
      "Epoch [1/1], Step [7225/7635], Loss: 3.9022\n",
      "Epoch [1/1], Step [7226/7635], Loss: 3.9493\n",
      "Epoch [1/1], Step [7227/7635], Loss: 3.9748\n",
      "Epoch [1/1], Step [7228/7635], Loss: 3.9167\n",
      "Epoch [1/1], Step [7229/7635], Loss: 3.9463\n",
      "Epoch [1/1], Step [7230/7635], Loss: 3.9564\n",
      "Epoch [1/1], Step [7231/7635], Loss: 3.9618\n",
      "Epoch [1/1], Step [7232/7635], Loss: 3.9100\n",
      "Epoch [1/1], Step [7233/7635], Loss: 3.9531\n",
      "Epoch [1/1], Step [7234/7635], Loss: 3.9089\n",
      "Epoch [1/1], Step [7235/7635], Loss: 3.9627\n",
      "Epoch [1/1], Step [7236/7635], Loss: 3.9337\n",
      "Epoch [1/1], Step [7237/7635], Loss: 3.9531\n",
      "Epoch [1/1], Step [7238/7635], Loss: 3.8569\n",
      "Epoch [1/1], Step [7239/7635], Loss: 3.9002\n",
      "Epoch [1/1], Step [7240/7635], Loss: 3.9071\n",
      "Epoch [1/1], Step [7241/7635], Loss: 3.9447\n",
      "Epoch [1/1], Step [7242/7635], Loss: 3.9779\n",
      "Epoch [1/1], Step [7243/7635], Loss: 3.9441\n",
      "Epoch [1/1], Step [7244/7635], Loss: 3.8912\n",
      "Epoch [1/1], Step [7245/7635], Loss: 3.9092\n",
      "Epoch [1/1], Step [7246/7635], Loss: 3.8943\n",
      "Epoch [1/1], Step [7247/7635], Loss: 3.9980\n",
      "Epoch [1/1], Step [7248/7635], Loss: 3.8844\n",
      "Epoch [1/1], Step [7249/7635], Loss: 3.9679\n",
      "Epoch [1/1], Step [7250/7635], Loss: 3.9969\n",
      "Epoch [1/1], Step [7251/7635], Loss: 3.8446\n",
      "Epoch [1/1], Step [7252/7635], Loss: 3.8866\n",
      "Epoch [1/1], Step [7253/7635], Loss: 3.9597\n",
      "Epoch [1/1], Step [7254/7635], Loss: 3.9686\n",
      "Epoch [1/1], Step [7255/7635], Loss: 3.8799\n",
      "Epoch [1/1], Step [7256/7635], Loss: 3.9531\n",
      "Epoch [1/1], Step [7257/7635], Loss: 3.8979\n",
      "Epoch [1/1], Step [7258/7635], Loss: 3.9766\n",
      "Epoch [1/1], Step [7259/7635], Loss: 3.9430\n",
      "Epoch [1/1], Step [7260/7635], Loss: 3.9841\n",
      "Epoch [1/1], Step [7261/7635], Loss: 3.9214\n",
      "Epoch [1/1], Step [7262/7635], Loss: 3.9433\n",
      "Epoch [1/1], Step [7263/7635], Loss: 3.9454\n",
      "Epoch [1/1], Step [7264/7635], Loss: 3.9356\n",
      "Epoch [1/1], Step [7265/7635], Loss: 3.8936\n",
      "Epoch [1/1], Step [7266/7635], Loss: 3.9470\n",
      "Epoch [1/1], Step [7267/7635], Loss: 3.9296\n",
      "Epoch [1/1], Step [7268/7635], Loss: 3.8564\n",
      "Epoch [1/1], Step [7269/7635], Loss: 3.9974\n",
      "Epoch [1/1], Step [7270/7635], Loss: 4.0176\n",
      "Epoch [1/1], Step [7271/7635], Loss: 3.9769\n",
      "Epoch [1/1], Step [7272/7635], Loss: 3.9878\n",
      "Epoch [1/1], Step [7273/7635], Loss: 3.9510\n",
      "Epoch [1/1], Step [7274/7635], Loss: 3.9815\n",
      "Epoch [1/1], Step [7275/7635], Loss: 3.9337\n",
      "Epoch [1/1], Step [7276/7635], Loss: 3.9841\n",
      "Epoch [1/1], Step [7277/7635], Loss: 3.9610\n",
      "Epoch [1/1], Step [7278/7635], Loss: 3.9625\n",
      "Epoch [1/1], Step [7279/7635], Loss: 3.8998\n",
      "Epoch [1/1], Step [7280/7635], Loss: 4.0292\n",
      "Epoch [1/1], Step [7281/7635], Loss: 3.9072\n",
      "Epoch [1/1], Step [7282/7635], Loss: 3.9689\n",
      "Epoch [1/1], Step [7283/7635], Loss: 3.9761\n",
      "Epoch [1/1], Step [7284/7635], Loss: 3.9717\n",
      "Epoch [1/1], Step [7285/7635], Loss: 3.9786\n",
      "Epoch [1/1], Step [7286/7635], Loss: 3.9669\n",
      "Epoch [1/1], Step [7287/7635], Loss: 3.9311\n",
      "Epoch [1/1], Step [7288/7635], Loss: 3.9207\n",
      "Epoch [1/1], Step [7289/7635], Loss: 3.9409\n",
      "Epoch [1/1], Step [7290/7635], Loss: 3.8826\n",
      "Epoch [1/1], Step [7291/7635], Loss: 3.9810\n",
      "Epoch [1/1], Step [7292/7635], Loss: 4.0003\n",
      "Epoch [1/1], Step [7293/7635], Loss: 3.9390\n",
      "Epoch [1/1], Step [7294/7635], Loss: 3.9420\n",
      "Epoch [1/1], Step [7295/7635], Loss: 3.9524\n",
      "Epoch [1/1], Step [7296/7635], Loss: 3.9364\n",
      "Epoch [1/1], Step [7297/7635], Loss: 3.8882\n",
      "Epoch [1/1], Step [7298/7635], Loss: 3.9795\n",
      "Epoch [1/1], Step [7299/7635], Loss: 3.8494\n",
      "Epoch [1/1], Step [7300/7635], Loss: 3.9383\n",
      "Epoch [1/1], Step [7301/7635], Loss: 3.9752\n",
      "Epoch [1/1], Step [7302/7635], Loss: 3.9036\n",
      "Epoch [1/1], Step [7303/7635], Loss: 3.9236\n",
      "Epoch [1/1], Step [7304/7635], Loss: 3.8831\n",
      "Epoch [1/1], Step [7305/7635], Loss: 3.8980\n",
      "Epoch [1/1], Step [7306/7635], Loss: 3.9695\n",
      "Epoch [1/1], Step [7307/7635], Loss: 3.9491\n",
      "Epoch [1/1], Step [7308/7635], Loss: 3.9512\n",
      "Epoch [1/1], Step [7309/7635], Loss: 3.8881\n",
      "Epoch [1/1], Step [7310/7635], Loss: 3.9290\n",
      "Epoch [1/1], Step [7311/7635], Loss: 3.8962\n",
      "Epoch [1/1], Step [7312/7635], Loss: 3.9795\n",
      "Epoch [1/1], Step [7313/7635], Loss: 4.0292\n",
      "Epoch [1/1], Step [7314/7635], Loss: 3.9369\n",
      "Epoch [1/1], Step [7315/7635], Loss: 3.8742\n",
      "Epoch [1/1], Step [7316/7635], Loss: 3.9018\n",
      "Epoch [1/1], Step [7317/7635], Loss: 3.8737\n",
      "Epoch [1/1], Step [7318/7635], Loss: 3.8821\n",
      "Epoch [1/1], Step [7319/7635], Loss: 3.8912\n",
      "Epoch [1/1], Step [7320/7635], Loss: 3.9412\n",
      "Epoch [1/1], Step [7321/7635], Loss: 3.9095\n",
      "Epoch [1/1], Step [7322/7635], Loss: 3.9107\n",
      "Epoch [1/1], Step [7323/7635], Loss: 3.9473\n",
      "Epoch [1/1], Step [7324/7635], Loss: 3.9555\n",
      "Epoch [1/1], Step [7325/7635], Loss: 4.0089\n",
      "Epoch [1/1], Step [7326/7635], Loss: 3.9359\n",
      "Epoch [1/1], Step [7327/7635], Loss: 3.8971\n",
      "Epoch [1/1], Step [7328/7635], Loss: 3.9081\n",
      "Epoch [1/1], Step [7329/7635], Loss: 3.9614\n",
      "Epoch [1/1], Step [7330/7635], Loss: 3.9040\n",
      "Epoch [1/1], Step [7331/7635], Loss: 3.9377\n",
      "Epoch [1/1], Step [7332/7635], Loss: 3.9064\n",
      "Epoch [1/1], Step [7333/7635], Loss: 3.8653\n",
      "Epoch [1/1], Step [7334/7635], Loss: 3.9543\n",
      "Epoch [1/1], Step [7335/7635], Loss: 4.0157\n",
      "Epoch [1/1], Step [7336/7635], Loss: 3.9817\n",
      "Epoch [1/1], Step [7337/7635], Loss: 3.9229\n",
      "Epoch [1/1], Step [7338/7635], Loss: 3.8604\n",
      "Epoch [1/1], Step [7339/7635], Loss: 4.0106\n",
      "Epoch [1/1], Step [7340/7635], Loss: 3.8258\n",
      "Epoch [1/1], Step [7341/7635], Loss: 3.8643\n",
      "Epoch [1/1], Step [7342/7635], Loss: 3.9073\n",
      "Epoch [1/1], Step [7343/7635], Loss: 3.9355\n",
      "Epoch [1/1], Step [7344/7635], Loss: 3.9527\n",
      "Epoch [1/1], Step [7345/7635], Loss: 3.9007\n",
      "Epoch [1/1], Step [7346/7635], Loss: 3.8838\n",
      "Epoch [1/1], Step [7347/7635], Loss: 3.9202\n",
      "Epoch [1/1], Step [7348/7635], Loss: 3.9242\n",
      "Epoch [1/1], Step [7349/7635], Loss: 4.0250\n",
      "Epoch [1/1], Step [7350/7635], Loss: 3.9394\n",
      "Epoch [1/1], Step [7351/7635], Loss: 3.9453\n",
      "Epoch [1/1], Step [7352/7635], Loss: 3.8907\n",
      "Epoch [1/1], Step [7353/7635], Loss: 3.9278\n",
      "Epoch [1/1], Step [7354/7635], Loss: 3.9820\n",
      "Epoch [1/1], Step [7355/7635], Loss: 3.8825\n",
      "Epoch [1/1], Step [7356/7635], Loss: 3.9585\n",
      "Epoch [1/1], Step [7357/7635], Loss: 3.8957\n",
      "Epoch [1/1], Step [7358/7635], Loss: 3.9018\n",
      "Epoch [1/1], Step [7359/7635], Loss: 3.8494\n",
      "Epoch [1/1], Step [7360/7635], Loss: 3.9066\n",
      "Epoch [1/1], Step [7361/7635], Loss: 3.9180\n",
      "Epoch [1/1], Step [7362/7635], Loss: 3.9450\n",
      "Epoch [1/1], Step [7363/7635], Loss: 3.9342\n",
      "Epoch [1/1], Step [7364/7635], Loss: 3.8293\n",
      "Epoch [1/1], Step [7365/7635], Loss: 3.9199\n",
      "Epoch [1/1], Step [7366/7635], Loss: 3.9882\n",
      "Epoch [1/1], Step [7367/7635], Loss: 3.9935\n",
      "Epoch [1/1], Step [7368/7635], Loss: 3.8907\n",
      "Epoch [1/1], Step [7369/7635], Loss: 3.9347\n",
      "Epoch [1/1], Step [7370/7635], Loss: 3.9231\n",
      "Epoch [1/1], Step [7371/7635], Loss: 3.9225\n",
      "Epoch [1/1], Step [7372/7635], Loss: 3.9883\n",
      "Epoch [1/1], Step [7373/7635], Loss: 3.9704\n",
      "Epoch [1/1], Step [7374/7635], Loss: 3.9680\n",
      "Epoch [1/1], Step [7375/7635], Loss: 3.9708\n",
      "Epoch [1/1], Step [7376/7635], Loss: 3.9294\n",
      "Epoch [1/1], Step [7377/7635], Loss: 3.8408\n",
      "Epoch [1/1], Step [7378/7635], Loss: 3.9505\n",
      "Epoch [1/1], Step [7379/7635], Loss: 3.9296\n",
      "Epoch [1/1], Step [7380/7635], Loss: 3.8498\n",
      "Epoch [1/1], Step [7381/7635], Loss: 3.9360\n",
      "Epoch [1/1], Step [7382/7635], Loss: 3.9074\n",
      "Epoch [1/1], Step [7383/7635], Loss: 3.9567\n",
      "Epoch [1/1], Step [7384/7635], Loss: 3.8307\n",
      "Epoch [1/1], Step [7385/7635], Loss: 3.9589\n",
      "Epoch [1/1], Step [7386/7635], Loss: 3.9150\n",
      "Epoch [1/1], Step [7387/7635], Loss: 3.9416\n",
      "Epoch [1/1], Step [7388/7635], Loss: 3.9246\n",
      "Epoch [1/1], Step [7389/7635], Loss: 3.9367\n",
      "Epoch [1/1], Step [7390/7635], Loss: 3.9136\n",
      "Epoch [1/1], Step [7391/7635], Loss: 3.9194\n",
      "Epoch [1/1], Step [7392/7635], Loss: 4.0010\n",
      "Epoch [1/1], Step [7393/7635], Loss: 3.9611\n",
      "Epoch [1/1], Step [7394/7635], Loss: 3.9816\n",
      "Epoch [1/1], Step [7395/7635], Loss: 3.9533\n",
      "Epoch [1/1], Step [7396/7635], Loss: 3.9645\n",
      "Epoch [1/1], Step [7397/7635], Loss: 3.9304\n",
      "Epoch [1/1], Step [7398/7635], Loss: 3.9862\n",
      "Epoch [1/1], Step [7399/7635], Loss: 3.9613\n",
      "Epoch [1/1], Step [7400/7635], Loss: 3.8671\n",
      "Epoch [1/1], Step [7401/7635], Loss: 3.9504\n",
      "Epoch [1/1], Step [7402/7635], Loss: 3.9145\n",
      "Epoch [1/1], Step [7403/7635], Loss: 3.9753\n",
      "Epoch [1/1], Step [7404/7635], Loss: 3.8916\n",
      "Epoch [1/1], Step [7405/7635], Loss: 3.9791\n",
      "Epoch [1/1], Step [7406/7635], Loss: 3.9513\n",
      "Epoch [1/1], Step [7407/7635], Loss: 3.9390\n",
      "Epoch [1/1], Step [7408/7635], Loss: 3.9288\n",
      "Epoch [1/1], Step [7409/7635], Loss: 3.9323\n",
      "Epoch [1/1], Step [7410/7635], Loss: 3.9774\n",
      "Epoch [1/1], Step [7411/7635], Loss: 3.9021\n",
      "Epoch [1/1], Step [7412/7635], Loss: 3.9052\n",
      "Epoch [1/1], Step [7413/7635], Loss: 3.9267\n",
      "Epoch [1/1], Step [7414/7635], Loss: 3.9323\n",
      "Epoch [1/1], Step [7415/7635], Loss: 3.9706\n",
      "Epoch [1/1], Step [7416/7635], Loss: 3.9665\n",
      "Epoch [1/1], Step [7417/7635], Loss: 4.0422\n",
      "Epoch [1/1], Step [7418/7635], Loss: 3.9301\n",
      "Epoch [1/1], Step [7419/7635], Loss: 3.9205\n",
      "Epoch [1/1], Step [7420/7635], Loss: 3.9046\n",
      "Epoch [1/1], Step [7421/7635], Loss: 3.9290\n",
      "Epoch [1/1], Step [7422/7635], Loss: 3.9342\n",
      "Epoch [1/1], Step [7423/7635], Loss: 3.9349\n",
      "Epoch [1/1], Step [7424/7635], Loss: 3.8917\n",
      "Epoch [1/1], Step [7425/7635], Loss: 3.9315\n",
      "Epoch [1/1], Step [7426/7635], Loss: 4.0057\n",
      "Epoch [1/1], Step [7427/7635], Loss: 3.9259\n",
      "Epoch [1/1], Step [7428/7635], Loss: 3.9132\n",
      "Epoch [1/1], Step [7429/7635], Loss: 3.8568\n",
      "Epoch [1/1], Step [7430/7635], Loss: 3.9783\n",
      "Epoch [1/1], Step [7431/7635], Loss: 3.9296\n",
      "Epoch [1/1], Step [7432/7635], Loss: 3.9698\n",
      "Epoch [1/1], Step [7433/7635], Loss: 3.9477\n",
      "Epoch [1/1], Step [7434/7635], Loss: 3.9316\n",
      "Epoch [1/1], Step [7435/7635], Loss: 3.9243\n",
      "Epoch [1/1], Step [7436/7635], Loss: 3.9195\n",
      "Epoch [1/1], Step [7437/7635], Loss: 3.9483\n",
      "Epoch [1/1], Step [7438/7635], Loss: 3.9438\n",
      "Epoch [1/1], Step [7439/7635], Loss: 3.9307\n",
      "Epoch [1/1], Step [7440/7635], Loss: 3.9163\n",
      "Epoch [1/1], Step [7441/7635], Loss: 3.8965\n",
      "Epoch [1/1], Step [7442/7635], Loss: 3.9407\n",
      "Epoch [1/1], Step [7443/7635], Loss: 3.8787\n",
      "Epoch [1/1], Step [7444/7635], Loss: 3.8681\n",
      "Epoch [1/1], Step [7445/7635], Loss: 3.8710\n",
      "Epoch [1/1], Step [7446/7635], Loss: 3.9335\n",
      "Epoch [1/1], Step [7447/7635], Loss: 3.9609\n",
      "Epoch [1/1], Step [7448/7635], Loss: 3.8757\n",
      "Epoch [1/1], Step [7449/7635], Loss: 3.9261\n",
      "Epoch [1/1], Step [7450/7635], Loss: 3.9582\n",
      "Epoch [1/1], Step [7451/7635], Loss: 4.0127\n",
      "Epoch [1/1], Step [7452/7635], Loss: 3.8829\n",
      "Epoch [1/1], Step [7453/7635], Loss: 4.0042\n",
      "Epoch [1/1], Step [7454/7635], Loss: 3.9256\n",
      "Epoch [1/1], Step [7455/7635], Loss: 3.8670\n",
      "Epoch [1/1], Step [7456/7635], Loss: 3.9635\n",
      "Epoch [1/1], Step [7457/7635], Loss: 3.9157\n",
      "Epoch [1/1], Step [7458/7635], Loss: 4.0082\n",
      "Epoch [1/1], Step [7459/7635], Loss: 3.9925\n",
      "Epoch [1/1], Step [7460/7635], Loss: 3.9470\n",
      "Epoch [1/1], Step [7461/7635], Loss: 3.9694\n",
      "Epoch [1/1], Step [7462/7635], Loss: 3.9155\n",
      "Epoch [1/1], Step [7463/7635], Loss: 3.9612\n",
      "Epoch [1/1], Step [7464/7635], Loss: 3.9222\n",
      "Epoch [1/1], Step [7465/7635], Loss: 3.9353\n",
      "Epoch [1/1], Step [7466/7635], Loss: 3.9849\n",
      "Epoch [1/1], Step [7467/7635], Loss: 3.9493\n",
      "Epoch [1/1], Step [7468/7635], Loss: 3.9061\n",
      "Epoch [1/1], Step [7469/7635], Loss: 3.9426\n",
      "Epoch [1/1], Step [7470/7635], Loss: 3.9037\n",
      "Epoch [1/1], Step [7471/7635], Loss: 3.9573\n",
      "Epoch [1/1], Step [7472/7635], Loss: 3.9456\n",
      "Epoch [1/1], Step [7473/7635], Loss: 3.8904\n",
      "Epoch [1/1], Step [7474/7635], Loss: 3.9308\n",
      "Epoch [1/1], Step [7475/7635], Loss: 3.8910\n",
      "Epoch [1/1], Step [7476/7635], Loss: 3.9540\n",
      "Epoch [1/1], Step [7477/7635], Loss: 3.9388\n",
      "Epoch [1/1], Step [7478/7635], Loss: 3.9420\n",
      "Epoch [1/1], Step [7479/7635], Loss: 3.8691\n",
      "Epoch [1/1], Step [7480/7635], Loss: 3.9328\n",
      "Epoch [1/1], Step [7481/7635], Loss: 3.8957\n",
      "Epoch [1/1], Step [7482/7635], Loss: 3.9645\n",
      "Epoch [1/1], Step [7483/7635], Loss: 3.9055\n",
      "Epoch [1/1], Step [7484/7635], Loss: 3.8810\n",
      "Epoch [1/1], Step [7485/7635], Loss: 3.8957\n",
      "Epoch [1/1], Step [7486/7635], Loss: 3.9175\n",
      "Epoch [1/1], Step [7487/7635], Loss: 4.0064\n",
      "Epoch [1/1], Step [7488/7635], Loss: 3.9278\n",
      "Epoch [1/1], Step [7489/7635], Loss: 3.9537\n",
      "Epoch [1/1], Step [7490/7635], Loss: 3.9338\n",
      "Epoch [1/1], Step [7491/7635], Loss: 3.9223\n",
      "Epoch [1/1], Step [7492/7635], Loss: 3.8868\n",
      "Epoch [1/1], Step [7493/7635], Loss: 3.8833\n",
      "Epoch [1/1], Step [7494/7635], Loss: 3.8904\n",
      "Epoch [1/1], Step [7495/7635], Loss: 3.9167\n",
      "Epoch [1/1], Step [7496/7635], Loss: 3.9457\n",
      "Epoch [1/1], Step [7497/7635], Loss: 3.9851\n",
      "Epoch [1/1], Step [7498/7635], Loss: 3.9362\n",
      "Epoch [1/1], Step [7499/7635], Loss: 3.8491\n",
      "Epoch [1/1], Step [7500/7635], Loss: 3.8756\n",
      "Epoch [1/1], Step [7501/7635], Loss: 3.9103\n",
      "Epoch [1/1], Step [7502/7635], Loss: 3.9241\n",
      "Epoch [1/1], Step [7503/7635], Loss: 3.9011\n",
      "Epoch [1/1], Step [7504/7635], Loss: 3.9535\n",
      "Epoch [1/1], Step [7505/7635], Loss: 3.9360\n",
      "Epoch [1/1], Step [7506/7635], Loss: 3.9202\n",
      "Epoch [1/1], Step [7507/7635], Loss: 3.9329\n",
      "Epoch [1/1], Step [7508/7635], Loss: 3.9062\n",
      "Epoch [1/1], Step [7509/7635], Loss: 3.9795\n",
      "Epoch [1/1], Step [7510/7635], Loss: 3.9229\n",
      "Epoch [1/1], Step [7511/7635], Loss: 3.8695\n",
      "Epoch [1/1], Step [7512/7635], Loss: 3.9228\n",
      "Epoch [1/1], Step [7513/7635], Loss: 3.8936\n",
      "Epoch [1/1], Step [7514/7635], Loss: 3.9529\n",
      "Epoch [1/1], Step [7515/7635], Loss: 3.8913\n",
      "Epoch [1/1], Step [7516/7635], Loss: 3.8721\n",
      "Epoch [1/1], Step [7517/7635], Loss: 3.8966\n",
      "Epoch [1/1], Step [7518/7635], Loss: 3.9153\n",
      "Epoch [1/1], Step [7519/7635], Loss: 3.9546\n",
      "Epoch [1/1], Step [7520/7635], Loss: 3.9353\n",
      "Epoch [1/1], Step [7521/7635], Loss: 3.9651\n",
      "Epoch [1/1], Step [7522/7635], Loss: 3.9513\n",
      "Epoch [1/1], Step [7523/7635], Loss: 3.9085\n",
      "Epoch [1/1], Step [7524/7635], Loss: 3.9635\n",
      "Epoch [1/1], Step [7525/7635], Loss: 3.8727\n",
      "Epoch [1/1], Step [7526/7635], Loss: 3.9084\n",
      "Epoch [1/1], Step [7527/7635], Loss: 3.9169\n",
      "Epoch [1/1], Step [7528/7635], Loss: 3.9708\n",
      "Epoch [1/1], Step [7529/7635], Loss: 3.9615\n",
      "Epoch [1/1], Step [7530/7635], Loss: 3.8659\n",
      "Epoch [1/1], Step [7531/7635], Loss: 3.9397\n",
      "Epoch [1/1], Step [7532/7635], Loss: 3.9282\n",
      "Epoch [1/1], Step [7533/7635], Loss: 3.9913\n",
      "Epoch [1/1], Step [7534/7635], Loss: 4.0118\n",
      "Epoch [1/1], Step [7535/7635], Loss: 3.9498\n",
      "Epoch [1/1], Step [7536/7635], Loss: 4.0145\n",
      "Epoch [1/1], Step [7537/7635], Loss: 3.9131\n",
      "Epoch [1/1], Step [7538/7635], Loss: 3.9667\n",
      "Epoch [1/1], Step [7539/7635], Loss: 3.8762\n",
      "Epoch [1/1], Step [7540/7635], Loss: 3.9055\n",
      "Epoch [1/1], Step [7541/7635], Loss: 3.9618\n",
      "Epoch [1/1], Step [7542/7635], Loss: 3.9145\n",
      "Epoch [1/1], Step [7543/7635], Loss: 3.9206\n",
      "Epoch [1/1], Step [7544/7635], Loss: 3.9545\n",
      "Epoch [1/1], Step [7545/7635], Loss: 3.8792\n",
      "Epoch [1/1], Step [7546/7635], Loss: 3.8784\n",
      "Epoch [1/1], Step [7547/7635], Loss: 3.9457\n",
      "Epoch [1/1], Step [7548/7635], Loss: 4.0036\n",
      "Epoch [1/1], Step [7549/7635], Loss: 3.9369\n",
      "Epoch [1/1], Step [7550/7635], Loss: 3.9556\n",
      "Epoch [1/1], Step [7551/7635], Loss: 3.9452\n",
      "Epoch [1/1], Step [7552/7635], Loss: 3.9378\n",
      "Epoch [1/1], Step [7553/7635], Loss: 3.8849\n",
      "Epoch [1/1], Step [7554/7635], Loss: 4.0163\n",
      "Epoch [1/1], Step [7555/7635], Loss: 3.9029\n",
      "Epoch [1/1], Step [7556/7635], Loss: 3.9333\n",
      "Epoch [1/1], Step [7557/7635], Loss: 3.9813\n",
      "Epoch [1/1], Step [7558/7635], Loss: 3.9253\n",
      "Epoch [1/1], Step [7559/7635], Loss: 3.9054\n",
      "Epoch [1/1], Step [7560/7635], Loss: 3.9215\n",
      "Epoch [1/1], Step [7561/7635], Loss: 3.9505\n",
      "Epoch [1/1], Step [7562/7635], Loss: 4.0214\n",
      "Epoch [1/1], Step [7563/7635], Loss: 3.9291\n",
      "Epoch [1/1], Step [7564/7635], Loss: 3.9423\n",
      "Epoch [1/1], Step [7565/7635], Loss: 3.9564\n",
      "Epoch [1/1], Step [7566/7635], Loss: 3.8810\n",
      "Epoch [1/1], Step [7567/7635], Loss: 3.9239\n",
      "Epoch [1/1], Step [7568/7635], Loss: 3.9434\n",
      "Epoch [1/1], Step [7569/7635], Loss: 3.9025\n",
      "Epoch [1/1], Step [7570/7635], Loss: 3.9519\n",
      "Epoch [1/1], Step [7571/7635], Loss: 3.9094\n",
      "Epoch [1/1], Step [7572/7635], Loss: 3.9130\n",
      "Epoch [1/1], Step [7573/7635], Loss: 3.9372\n",
      "Epoch [1/1], Step [7574/7635], Loss: 3.9056\n",
      "Epoch [1/1], Step [7575/7635], Loss: 3.9237\n",
      "Epoch [1/1], Step [7576/7635], Loss: 3.9803\n",
      "Epoch [1/1], Step [7577/7635], Loss: 3.8757\n",
      "Epoch [1/1], Step [7578/7635], Loss: 3.9511\n",
      "Epoch [1/1], Step [7579/7635], Loss: 3.9534\n",
      "Epoch [1/1], Step [7580/7635], Loss: 3.8824\n",
      "Epoch [1/1], Step [7581/7635], Loss: 3.9232\n",
      "Epoch [1/1], Step [7582/7635], Loss: 3.9402\n",
      "Epoch [1/1], Step [7583/7635], Loss: 3.9043\n",
      "Epoch [1/1], Step [7584/7635], Loss: 3.9837\n",
      "Epoch [1/1], Step [7585/7635], Loss: 3.9605\n",
      "Epoch [1/1], Step [7586/7635], Loss: 3.9586\n",
      "Epoch [1/1], Step [7587/7635], Loss: 3.9173\n",
      "Epoch [1/1], Step [7588/7635], Loss: 3.9184\n",
      "Epoch [1/1], Step [7589/7635], Loss: 3.9096\n",
      "Epoch [1/1], Step [7590/7635], Loss: 3.9175\n",
      "Epoch [1/1], Step [7591/7635], Loss: 3.9871\n",
      "Epoch [1/1], Step [7592/7635], Loss: 3.9791\n",
      "Epoch [1/1], Step [7593/7635], Loss: 3.9144\n",
      "Epoch [1/1], Step [7594/7635], Loss: 3.9172\n",
      "Epoch [1/1], Step [7595/7635], Loss: 3.9845\n",
      "Epoch [1/1], Step [7596/7635], Loss: 3.9181\n",
      "Epoch [1/1], Step [7597/7635], Loss: 3.8650\n",
      "Epoch [1/1], Step [7598/7635], Loss: 3.9191\n",
      "Epoch [1/1], Step [7599/7635], Loss: 3.9997\n",
      "Epoch [1/1], Step [7600/7635], Loss: 3.8995\n",
      "Epoch [1/1], Step [7601/7635], Loss: 4.0612\n",
      "Epoch [1/1], Step [7602/7635], Loss: 3.9206\n",
      "Epoch [1/1], Step [7603/7635], Loss: 3.9203\n",
      "Epoch [1/1], Step [7604/7635], Loss: 3.9124\n",
      "Epoch [1/1], Step [7605/7635], Loss: 3.8648\n",
      "Epoch [1/1], Step [7606/7635], Loss: 3.9280\n",
      "Epoch [1/1], Step [7607/7635], Loss: 3.9363\n",
      "Epoch [1/1], Step [7608/7635], Loss: 3.9343\n",
      "Epoch [1/1], Step [7609/7635], Loss: 4.0067\n",
      "Epoch [1/1], Step [7610/7635], Loss: 3.9359\n",
      "Epoch [1/1], Step [7611/7635], Loss: 3.9607\n",
      "Epoch [1/1], Step [7612/7635], Loss: 3.9373\n",
      "Epoch [1/1], Step [7613/7635], Loss: 3.8436\n",
      "Epoch [1/1], Step [7614/7635], Loss: 3.9311\n",
      "Epoch [1/1], Step [7615/7635], Loss: 3.8789\n",
      "Epoch [1/1], Step [7616/7635], Loss: 3.9050\n",
      "Epoch [1/1], Step [7617/7635], Loss: 3.8704\n",
      "Epoch [1/1], Step [7618/7635], Loss: 3.8758\n",
      "Epoch [1/1], Step [7619/7635], Loss: 3.8289\n",
      "Epoch [1/1], Step [7620/7635], Loss: 3.8844\n",
      "Epoch [1/1], Step [7621/7635], Loss: 3.9250\n",
      "Epoch [1/1], Step [7622/7635], Loss: 3.9277\n",
      "Epoch [1/1], Step [7623/7635], Loss: 3.9539\n",
      "Epoch [1/1], Step [7624/7635], Loss: 3.9590\n",
      "Epoch [1/1], Step [7625/7635], Loss: 3.8904\n",
      "Epoch [1/1], Step [7626/7635], Loss: 3.9588\n",
      "Epoch [1/1], Step [7627/7635], Loss: 3.9083\n",
      "Epoch [1/1], Step [7628/7635], Loss: 3.9504\n",
      "Epoch [1/1], Step [7629/7635], Loss: 3.9290\n",
      "Epoch [1/1], Step [7630/7635], Loss: 3.9982\n",
      "Epoch [1/1], Step [7631/7635], Loss: 3.9788\n",
      "Epoch [1/1], Step [7632/7635], Loss: 3.8309\n",
      "Epoch [1/1], Step [7633/7635], Loss: 3.8783\n",
      "Epoch [1/1], Step [7634/7635], Loss: 3.9276\n",
      "Epoch [1/1] Average Loss: 4.2703, Perplexity: 71.54\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,train_run_label)\n",
    "#return (all_losses,train_losses,perplexities)\n",
    "from src.train import train\n",
    "(all_losses,train_losses,perplexities) = train(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,\n",
    "                                               train_run_label,vocab_size,device,print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c60d7d-b4a0-4bae-9219-b08120f3976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To read old results \n",
    "#read_list_from_file(label,path_to_save_folder)\n",
    "from src.helper import read_list_from_file\n",
    "#train_losses=read_list_from_file(\"onlyLossesUntil3593\",path_to_save_folder)\n",
    "#perplexities= read_list_from_file(\"normal_model_perplexities\",path_to_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a0686aa-a320-4e62-8f2b-45f1cd520813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.plot import plot_two\n",
    "plot_two(all_losses,\"Loss For all Steps\",train_losses,\"la2\",axLabel1=(\"Steps\",\"Loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa1dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf0af7df",
   "metadata": {},
   "source": [
    "### 3.4. Evaluating the model\n",
    "\n",
    "Now, we'll compute the perplexity of our simplest model on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ff9410d-e23f-4036-bbb3-8f280a42b55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of base model: 50.65\n"
     ]
    }
   ],
   "source": [
    "from src.train import evaluate\n",
    "perplexity_simple = evaluate(model, dev_dataloader,criterion,device,vocab_size)\n",
    "print(f\"Perplexity of base model: {perplexity_simple:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124e2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cc0d402",
   "metadata": {},
   "source": [
    "## 4. Training with dropout\n",
    "\n",
    "To prevent overfitting and improve generalization, we'll test dropout as a regularization strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5a2df",
   "metadata": {},
   "source": [
    "### 4.1. Adding dropout\n",
    "\n",
    "We'll modify our model to include a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6bdc5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import RegularizedLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182d5e3",
   "metadata": {},
   "source": [
    "### 4.2. Retraining the model with dropout\n",
    "\n",
    "We'll re-initialize the model and optimizer, then retrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef5c899f-0994-4913-98c0-aab357b12f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [0/7635], Loss: 11.2415\n",
      "Epoch [1/1], Step [1/7635], Loss: 11.2124\n",
      "Epoch [1/1], Step [2/7635], Loss: 11.1550\n",
      "Epoch [1/1], Step [3/7635], Loss: 11.1095\n",
      "Epoch [1/1], Step [4/7635], Loss: 11.0850\n",
      "Epoch [1/1], Step [5/7635], Loss: 11.0378\n",
      "Epoch [1/1], Step [6/7635], Loss: 11.0372\n",
      "Epoch [1/1], Step [7/7635], Loss: 10.9615\n",
      "Epoch [1/1], Step [8/7635], Loss: 10.8956\n",
      "Epoch [1/1], Step [9/7635], Loss: 10.8735\n",
      "Epoch [1/1], Step [10/7635], Loss: 10.7948\n",
      "Epoch [1/1], Step [11/7635], Loss: 10.7650\n",
      "Epoch [1/1], Step [12/7635], Loss: 10.7319\n",
      "Epoch [1/1], Step [13/7635], Loss: 10.7051\n",
      "Epoch [1/1], Step [14/7635], Loss: 10.6483\n",
      "Epoch [1/1], Step [15/7635], Loss: 10.5948\n",
      "Epoch [1/1], Step [16/7635], Loss: 10.5629\n",
      "Epoch [1/1], Step [17/7635], Loss: 10.5020\n",
      "Epoch [1/1], Step [18/7635], Loss: 10.4640\n",
      "Epoch [1/1], Step [19/7635], Loss: 10.4379\n",
      "Epoch [1/1], Step [20/7635], Loss: 10.4157\n",
      "Epoch [1/1], Step [21/7635], Loss: 10.3223\n",
      "Epoch [1/1], Step [22/7635], Loss: 10.3253\n",
      "Epoch [1/1], Step [23/7635], Loss: 10.2734\n",
      "Epoch [1/1], Step [24/7635], Loss: 10.2394\n",
      "Epoch [1/1], Step [25/7635], Loss: 10.1746\n",
      "Epoch [1/1], Step [26/7635], Loss: 10.1414\n",
      "Epoch [1/1], Step [27/7635], Loss: 10.1228\n",
      "Epoch [1/1], Step [28/7635], Loss: 10.0339\n",
      "Epoch [1/1], Step [29/7635], Loss: 10.0085\n",
      "Epoch [1/1], Step [30/7635], Loss: 9.9621\n",
      "Epoch [1/1], Step [31/7635], Loss: 9.9092\n",
      "Epoch [1/1], Step [32/7635], Loss: 9.8621\n",
      "Epoch [1/1], Step [33/7635], Loss: 9.7972\n",
      "Epoch [1/1], Step [34/7635], Loss: 9.8274\n",
      "Epoch [1/1], Step [35/7635], Loss: 9.7975\n",
      "Epoch [1/1], Step [36/7635], Loss: 9.7174\n",
      "Epoch [1/1], Step [37/7635], Loss: 9.6338\n",
      "Epoch [1/1], Step [38/7635], Loss: 9.5899\n",
      "Epoch [1/1], Step [39/7635], Loss: 9.5181\n",
      "Epoch [1/1], Step [40/7635], Loss: 9.4602\n",
      "Epoch [1/1], Step [41/7635], Loss: 9.4698\n",
      "Epoch [1/1], Step [42/7635], Loss: 9.3762\n",
      "Epoch [1/1], Step [43/7635], Loss: 9.3821\n",
      "Epoch [1/1], Step [44/7635], Loss: 9.3703\n",
      "Epoch [1/1], Step [45/7635], Loss: 9.2812\n",
      "Epoch [1/1], Step [46/7635], Loss: 9.1675\n",
      "Epoch [1/1], Step [47/7635], Loss: 9.2161\n",
      "Epoch [1/1], Step [48/7635], Loss: 9.1631\n",
      "Epoch [1/1], Step [49/7635], Loss: 9.0935\n",
      "Epoch [1/1], Step [50/7635], Loss: 9.0251\n",
      "Epoch [1/1], Step [51/7635], Loss: 8.9868\n",
      "Epoch [1/1], Step [52/7635], Loss: 9.0024\n",
      "Epoch [1/1], Step [53/7635], Loss: 8.8945\n",
      "Epoch [1/1], Step [54/7635], Loss: 8.8701\n",
      "Epoch [1/1], Step [55/7635], Loss: 8.7632\n",
      "Epoch [1/1], Step [56/7635], Loss: 8.7571\n",
      "Epoch [1/1], Step [57/7635], Loss: 8.7168\n",
      "Epoch [1/1], Step [58/7635], Loss: 8.6526\n",
      "Epoch [1/1], Step [59/7635], Loss: 8.5972\n",
      "Epoch [1/1], Step [60/7635], Loss: 8.5734\n",
      "Epoch [1/1], Step [61/7635], Loss: 8.5217\n",
      "Epoch [1/1], Step [62/7635], Loss: 8.4507\n",
      "Epoch [1/1], Step [63/7635], Loss: 8.4687\n",
      "Epoch [1/1], Step [64/7635], Loss: 8.3898\n",
      "Epoch [1/1], Step [65/7635], Loss: 8.3220\n",
      "Epoch [1/1], Step [66/7635], Loss: 8.2577\n",
      "Epoch [1/1], Step [67/7635], Loss: 8.2445\n",
      "Epoch [1/1], Step [68/7635], Loss: 8.1714\n",
      "Epoch [1/1], Step [69/7635], Loss: 8.1609\n",
      "Epoch [1/1], Step [70/7635], Loss: 8.0991\n",
      "Epoch [1/1], Step [71/7635], Loss: 8.1231\n",
      "Epoch [1/1], Step [72/7635], Loss: 8.0692\n",
      "Epoch [1/1], Step [73/7635], Loss: 8.0057\n",
      "Epoch [1/1], Step [74/7635], Loss: 7.9618\n",
      "Epoch [1/1], Step [75/7635], Loss: 7.9096\n",
      "Epoch [1/1], Step [76/7635], Loss: 7.9156\n",
      "Epoch [1/1], Step [77/7635], Loss: 7.8219\n",
      "Epoch [1/1], Step [78/7635], Loss: 7.8556\n",
      "Epoch [1/1], Step [79/7635], Loss: 7.7815\n",
      "Epoch [1/1], Step [80/7635], Loss: 7.6901\n",
      "Epoch [1/1], Step [81/7635], Loss: 7.6745\n",
      "Epoch [1/1], Step [82/7635], Loss: 7.5878\n",
      "Epoch [1/1], Step [83/7635], Loss: 7.5533\n",
      "Epoch [1/1], Step [84/7635], Loss: 7.5997\n",
      "Epoch [1/1], Step [85/7635], Loss: 7.5137\n",
      "Epoch [1/1], Step [86/7635], Loss: 7.4496\n",
      "Epoch [1/1], Step [87/7635], Loss: 7.5609\n",
      "Epoch [1/1], Step [88/7635], Loss: 7.4208\n",
      "Epoch [1/1], Step [89/7635], Loss: 7.3722\n",
      "Epoch [1/1], Step [90/7635], Loss: 7.3699\n",
      "Epoch [1/1], Step [91/7635], Loss: 7.3087\n",
      "Epoch [1/1], Step [92/7635], Loss: 7.3185\n",
      "Epoch [1/1], Step [93/7635], Loss: 7.2980\n",
      "Epoch [1/1], Step [94/7635], Loss: 7.2323\n",
      "Epoch [1/1], Step [95/7635], Loss: 7.1313\n",
      "Epoch [1/1], Step [96/7635], Loss: 7.2184\n",
      "Epoch [1/1], Step [97/7635], Loss: 7.1867\n",
      "Epoch [1/1], Step [98/7635], Loss: 7.1862\n",
      "Epoch [1/1], Step [99/7635], Loss: 7.0793\n",
      "Epoch [1/1], Step [100/7635], Loss: 7.0053\n",
      "Epoch [1/1], Step [101/7635], Loss: 7.0261\n",
      "Epoch [1/1], Step [102/7635], Loss: 7.0435\n",
      "Epoch [1/1], Step [103/7635], Loss: 7.0306\n",
      "Epoch [1/1], Step [104/7635], Loss: 6.9437\n",
      "Epoch [1/1], Step [105/7635], Loss: 6.8989\n",
      "Epoch [1/1], Step [106/7635], Loss: 6.8115\n",
      "Epoch [1/1], Step [107/7635], Loss: 6.8311\n",
      "Epoch [1/1], Step [108/7635], Loss: 6.8540\n",
      "Epoch [1/1], Step [109/7635], Loss: 6.9102\n",
      "Epoch [1/1], Step [110/7635], Loss: 6.7781\n",
      "Epoch [1/1], Step [111/7635], Loss: 6.8409\n",
      "Epoch [1/1], Step [112/7635], Loss: 6.7519\n",
      "Epoch [1/1], Step [113/7635], Loss: 6.7502\n",
      "Epoch [1/1], Step [114/7635], Loss: 6.7538\n",
      "Epoch [1/1], Step [115/7635], Loss: 6.7936\n",
      "Epoch [1/1], Step [116/7635], Loss: 6.6333\n",
      "Epoch [1/1], Step [117/7635], Loss: 6.7176\n",
      "Epoch [1/1], Step [118/7635], Loss: 6.7424\n",
      "Epoch [1/1], Step [119/7635], Loss: 6.6300\n",
      "Epoch [1/1], Step [120/7635], Loss: 6.6146\n",
      "Epoch [1/1], Step [121/7635], Loss: 6.5447\n",
      "Epoch [1/1], Step [122/7635], Loss: 6.6607\n",
      "Epoch [1/1], Step [123/7635], Loss: 6.6123\n",
      "Epoch [1/1], Step [124/7635], Loss: 6.5802\n",
      "Epoch [1/1], Step [125/7635], Loss: 6.5922\n",
      "Epoch [1/1], Step [126/7635], Loss: 6.5223\n",
      "Epoch [1/1], Step [127/7635], Loss: 6.5443\n",
      "Epoch [1/1], Step [128/7635], Loss: 6.5546\n",
      "Epoch [1/1], Step [129/7635], Loss: 6.5477\n",
      "Epoch [1/1], Step [130/7635], Loss: 6.4572\n",
      "Epoch [1/1], Step [131/7635], Loss: 6.4913\n",
      "Epoch [1/1], Step [132/7635], Loss: 6.4223\n",
      "Epoch [1/1], Step [133/7635], Loss: 6.4811\n",
      "Epoch [1/1], Step [134/7635], Loss: 6.3885\n",
      "Epoch [1/1], Step [135/7635], Loss: 6.4177\n",
      "Epoch [1/1], Step [136/7635], Loss: 6.3903\n",
      "Epoch [1/1], Step [137/7635], Loss: 6.4265\n",
      "Epoch [1/1], Step [138/7635], Loss: 6.4228\n",
      "Epoch [1/1], Step [139/7635], Loss: 6.3905\n",
      "Epoch [1/1], Step [140/7635], Loss: 6.3227\n",
      "Epoch [1/1], Step [141/7635], Loss: 6.3279\n",
      "Epoch [1/1], Step [142/7635], Loss: 6.3486\n",
      "Epoch [1/1], Step [143/7635], Loss: 6.3285\n",
      "Epoch [1/1], Step [144/7635], Loss: 6.2825\n",
      "Epoch [1/1], Step [145/7635], Loss: 6.3326\n",
      "Epoch [1/1], Step [146/7635], Loss: 6.3237\n",
      "Epoch [1/1], Step [147/7635], Loss: 6.4332\n",
      "Epoch [1/1], Step [148/7635], Loss: 6.2952\n",
      "Epoch [1/1], Step [149/7635], Loss: 6.2897\n",
      "Epoch [1/1], Step [150/7635], Loss: 6.3414\n",
      "Epoch [1/1], Step [151/7635], Loss: 6.2158\n",
      "Epoch [1/1], Step [152/7635], Loss: 6.3050\n",
      "Epoch [1/1], Step [153/7635], Loss: 6.2421\n",
      "Epoch [1/1], Step [154/7635], Loss: 6.1922\n",
      "Epoch [1/1], Step [155/7635], Loss: 6.2199\n",
      "Epoch [1/1], Step [156/7635], Loss: 6.2682\n",
      "Epoch [1/1], Step [157/7635], Loss: 6.2097\n",
      "Epoch [1/1], Step [158/7635], Loss: 6.2459\n",
      "Epoch [1/1], Step [159/7635], Loss: 6.1708\n",
      "Epoch [1/1], Step [160/7635], Loss: 6.1745\n",
      "Epoch [1/1], Step [161/7635], Loss: 6.1630\n",
      "Epoch [1/1], Step [162/7635], Loss: 6.1467\n",
      "Epoch [1/1], Step [163/7635], Loss: 6.1537\n",
      "Epoch [1/1], Step [164/7635], Loss: 6.1371\n",
      "Epoch [1/1], Step [165/7635], Loss: 6.0845\n",
      "Epoch [1/1], Step [166/7635], Loss: 6.1610\n",
      "Epoch [1/1], Step [167/7635], Loss: 6.1009\n",
      "Epoch [1/1], Step [168/7635], Loss: 6.1540\n",
      "Epoch [1/1], Step [169/7635], Loss: 6.1336\n",
      "Epoch [1/1], Step [170/7635], Loss: 6.1491\n",
      "Epoch [1/1], Step [171/7635], Loss: 6.0886\n",
      "Epoch [1/1], Step [172/7635], Loss: 6.0204\n",
      "Epoch [1/1], Step [173/7635], Loss: 6.1194\n",
      "Epoch [1/1], Step [174/7635], Loss: 6.0403\n",
      "Epoch [1/1], Step [175/7635], Loss: 6.0543\n",
      "Epoch [1/1], Step [176/7635], Loss: 6.0296\n",
      "Epoch [1/1], Step [177/7635], Loss: 6.0037\n",
      "Epoch [1/1], Step [178/7635], Loss: 6.0077\n",
      "Epoch [1/1], Step [179/7635], Loss: 6.1414\n",
      "Epoch [1/1], Step [180/7635], Loss: 6.0062\n",
      "Epoch [1/1], Step [181/7635], Loss: 6.0297\n",
      "Epoch [1/1], Step [182/7635], Loss: 5.9788\n",
      "Epoch [1/1], Step [183/7635], Loss: 5.9584\n",
      "Epoch [1/1], Step [184/7635], Loss: 6.1006\n",
      "Epoch [1/1], Step [185/7635], Loss: 6.0042\n",
      "Epoch [1/1], Step [186/7635], Loss: 5.9142\n",
      "Epoch [1/1], Step [187/7635], Loss: 6.0204\n",
      "Epoch [1/1], Step [188/7635], Loss: 6.0391\n",
      "Epoch [1/1], Step [189/7635], Loss: 5.8411\n",
      "Epoch [1/1], Step [190/7635], Loss: 6.0163\n",
      "Epoch [1/1], Step [191/7635], Loss: 5.9628\n",
      "Epoch [1/1], Step [192/7635], Loss: 6.0298\n",
      "Epoch [1/1], Step [193/7635], Loss: 6.0024\n",
      "Epoch [1/1], Step [194/7635], Loss: 5.9437\n",
      "Epoch [1/1], Step [195/7635], Loss: 5.8543\n",
      "Epoch [1/1], Step [196/7635], Loss: 5.9724\n",
      "Epoch [1/1], Step [197/7635], Loss: 5.9827\n",
      "Epoch [1/1], Step [198/7635], Loss: 5.8737\n",
      "Epoch [1/1], Step [199/7635], Loss: 5.8103\n",
      "Epoch [1/1], Step [200/7635], Loss: 5.9260\n",
      "Epoch [1/1], Step [201/7635], Loss: 6.0930\n",
      "Epoch [1/1], Step [202/7635], Loss: 5.9092\n",
      "Epoch [1/1], Step [203/7635], Loss: 5.8766\n",
      "Epoch [1/1], Step [204/7635], Loss: 5.8797\n",
      "Epoch [1/1], Step [205/7635], Loss: 5.8681\n",
      "Epoch [1/1], Step [206/7635], Loss: 5.8627\n",
      "Epoch [1/1], Step [207/7635], Loss: 5.9146\n",
      "Epoch [1/1], Step [208/7635], Loss: 5.9911\n",
      "Epoch [1/1], Step [209/7635], Loss: 5.8916\n",
      "Epoch [1/1], Step [210/7635], Loss: 5.9309\n",
      "Epoch [1/1], Step [211/7635], Loss: 5.9009\n",
      "Epoch [1/1], Step [212/7635], Loss: 5.8778\n",
      "Epoch [1/1], Step [213/7635], Loss: 5.8842\n",
      "Epoch [1/1], Step [214/7635], Loss: 5.8529\n",
      "Epoch [1/1], Step [215/7635], Loss: 5.8433\n",
      "Epoch [1/1], Step [216/7635], Loss: 5.8614\n",
      "Epoch [1/1], Step [217/7635], Loss: 5.8066\n",
      "Epoch [1/1], Step [218/7635], Loss: 5.7490\n",
      "Epoch [1/1], Step [219/7635], Loss: 5.8034\n",
      "Epoch [1/1], Step [220/7635], Loss: 5.8365\n",
      "Epoch [1/1], Step [221/7635], Loss: 5.9425\n",
      "Epoch [1/1], Step [222/7635], Loss: 5.7458\n",
      "Epoch [1/1], Step [223/7635], Loss: 5.8658\n",
      "Epoch [1/1], Step [224/7635], Loss: 5.9440\n",
      "Epoch [1/1], Step [225/7635], Loss: 5.8291\n",
      "Epoch [1/1], Step [226/7635], Loss: 5.7475\n",
      "Epoch [1/1], Step [227/7635], Loss: 5.8004\n",
      "Epoch [1/1], Step [228/7635], Loss: 5.7983\n",
      "Epoch [1/1], Step [229/7635], Loss: 5.9001\n",
      "Epoch [1/1], Step [230/7635], Loss: 5.7268\n",
      "Epoch [1/1], Step [231/7635], Loss: 5.7120\n",
      "Epoch [1/1], Step [232/7635], Loss: 5.8182\n",
      "Epoch [1/1], Step [233/7635], Loss: 5.7372\n",
      "Epoch [1/1], Step [234/7635], Loss: 5.7407\n",
      "Epoch [1/1], Step [235/7635], Loss: 5.7488\n",
      "Epoch [1/1], Step [236/7635], Loss: 5.7442\n",
      "Epoch [1/1], Step [237/7635], Loss: 5.7674\n",
      "Epoch [1/1], Step [238/7635], Loss: 5.8445\n",
      "Epoch [1/1], Step [239/7635], Loss: 5.7791\n",
      "Epoch [1/1], Step [240/7635], Loss: 5.8845\n",
      "Epoch [1/1], Step [241/7635], Loss: 5.6513\n",
      "Epoch [1/1], Step [242/7635], Loss: 5.8668\n",
      "Epoch [1/1], Step [243/7635], Loss: 5.7012\n",
      "Epoch [1/1], Step [244/7635], Loss: 5.8054\n",
      "Epoch [1/1], Step [245/7635], Loss: 5.8288\n",
      "Epoch [1/1], Step [246/7635], Loss: 5.6964\n",
      "Epoch [1/1], Step [247/7635], Loss: 5.8244\n",
      "Epoch [1/1], Step [248/7635], Loss: 5.8404\n",
      "Epoch [1/1], Step [249/7635], Loss: 5.7033\n",
      "Epoch [1/1], Step [250/7635], Loss: 5.8123\n",
      "Epoch [1/1], Step [251/7635], Loss: 5.6593\n",
      "Epoch [1/1], Step [252/7635], Loss: 5.8161\n",
      "Epoch [1/1], Step [253/7635], Loss: 5.7315\n",
      "Epoch [1/1], Step [254/7635], Loss: 5.7458\n",
      "Epoch [1/1], Step [255/7635], Loss: 5.6597\n",
      "Epoch [1/1], Step [256/7635], Loss: 5.6848\n",
      "Epoch [1/1], Step [257/7635], Loss: 5.6036\n",
      "Epoch [1/1], Step [258/7635], Loss: 5.6667\n",
      "Epoch [1/1], Step [259/7635], Loss: 5.7318\n",
      "Epoch [1/1], Step [260/7635], Loss: 5.7184\n",
      "Epoch [1/1], Step [261/7635], Loss: 5.6961\n",
      "Epoch [1/1], Step [262/7635], Loss: 5.6940\n",
      "Epoch [1/1], Step [263/7635], Loss: 5.7315\n",
      "Epoch [1/1], Step [264/7635], Loss: 5.6138\n",
      "Epoch [1/1], Step [265/7635], Loss: 5.6894\n",
      "Epoch [1/1], Step [266/7635], Loss: 5.6017\n",
      "Epoch [1/1], Step [267/7635], Loss: 5.8220\n",
      "Epoch [1/1], Step [268/7635], Loss: 5.6896\n",
      "Epoch [1/1], Step [269/7635], Loss: 5.6515\n",
      "Epoch [1/1], Step [270/7635], Loss: 5.6272\n",
      "Epoch [1/1], Step [271/7635], Loss: 5.8035\n",
      "Epoch [1/1], Step [272/7635], Loss: 5.6692\n",
      "Epoch [1/1], Step [273/7635], Loss: 5.7413\n",
      "Epoch [1/1], Step [274/7635], Loss: 5.6670\n",
      "Epoch [1/1], Step [275/7635], Loss: 5.6030\n",
      "Epoch [1/1], Step [276/7635], Loss: 5.6506\n",
      "Epoch [1/1], Step [277/7635], Loss: 5.6804\n",
      "Epoch [1/1], Step [278/7635], Loss: 5.5419\n",
      "Epoch [1/1], Step [279/7635], Loss: 5.6859\n",
      "Epoch [1/1], Step [280/7635], Loss: 5.5859\n",
      "Epoch [1/1], Step [281/7635], Loss: 5.7214\n",
      "Epoch [1/1], Step [282/7635], Loss: 5.6738\n",
      "Epoch [1/1], Step [283/7635], Loss: 5.5740\n",
      "Epoch [1/1], Step [284/7635], Loss: 5.6238\n",
      "Epoch [1/1], Step [285/7635], Loss: 5.6200\n",
      "Epoch [1/1], Step [286/7635], Loss: 5.5275\n",
      "Epoch [1/1], Step [287/7635], Loss: 5.7314\n",
      "Epoch [1/1], Step [288/7635], Loss: 5.6595\n",
      "Epoch [1/1], Step [289/7635], Loss: 5.5861\n",
      "Epoch [1/1], Step [290/7635], Loss: 5.5803\n",
      "Epoch [1/1], Step [291/7635], Loss: 5.5603\n",
      "Epoch [1/1], Step [292/7635], Loss: 5.4700\n",
      "Epoch [1/1], Step [293/7635], Loss: 5.6855\n",
      "Epoch [1/1], Step [294/7635], Loss: 5.6411\n",
      "Epoch [1/1], Step [295/7635], Loss: 5.5216\n",
      "Epoch [1/1], Step [296/7635], Loss: 5.6199\n",
      "Epoch [1/1], Step [297/7635], Loss: 5.6285\n",
      "Epoch [1/1], Step [298/7635], Loss: 5.6270\n",
      "Epoch [1/1], Step [299/7635], Loss: 5.6488\n",
      "Epoch [1/1], Step [300/7635], Loss: 5.5349\n",
      "Epoch [1/1], Step [301/7635], Loss: 5.5372\n",
      "Epoch [1/1], Step [302/7635], Loss: 5.6470\n",
      "Epoch [1/1], Step [303/7635], Loss: 5.5900\n",
      "Epoch [1/1], Step [304/7635], Loss: 5.5478\n",
      "Epoch [1/1], Step [305/7635], Loss: 5.5514\n",
      "Epoch [1/1], Step [306/7635], Loss: 5.5527\n",
      "Epoch [1/1], Step [307/7635], Loss: 5.5650\n",
      "Epoch [1/1], Step [308/7635], Loss: 5.5983\n",
      "Epoch [1/1], Step [309/7635], Loss: 5.4963\n",
      "Epoch [1/1], Step [310/7635], Loss: 5.6350\n",
      "Epoch [1/1], Step [311/7635], Loss: 5.5870\n",
      "Epoch [1/1], Step [312/7635], Loss: 5.5157\n",
      "Epoch [1/1], Step [313/7635], Loss: 5.6172\n",
      "Epoch [1/1], Step [314/7635], Loss: 5.5340\n",
      "Epoch [1/1], Step [315/7635], Loss: 5.5204\n",
      "Epoch [1/1], Step [316/7635], Loss: 5.6861\n",
      "Epoch [1/1], Step [317/7635], Loss: 5.6079\n",
      "Epoch [1/1], Step [318/7635], Loss: 5.5087\n",
      "Epoch [1/1], Step [319/7635], Loss: 5.4720\n",
      "Epoch [1/1], Step [320/7635], Loss: 5.5257\n",
      "Epoch [1/1], Step [321/7635], Loss: 5.6340\n",
      "Epoch [1/1], Step [322/7635], Loss: 5.5325\n",
      "Epoch [1/1], Step [323/7635], Loss: 5.4707\n",
      "Epoch [1/1], Step [324/7635], Loss: 5.6040\n",
      "Epoch [1/1], Step [325/7635], Loss: 5.5418\n",
      "Epoch [1/1], Step [326/7635], Loss: 5.5831\n",
      "Epoch [1/1], Step [327/7635], Loss: 5.4029\n",
      "Epoch [1/1], Step [328/7635], Loss: 5.4986\n",
      "Epoch [1/1], Step [329/7635], Loss: 5.4632\n",
      "Epoch [1/1], Step [330/7635], Loss: 5.4746\n",
      "Epoch [1/1], Step [331/7635], Loss: 5.6131\n",
      "Epoch [1/1], Step [332/7635], Loss: 5.5513\n",
      "Epoch [1/1], Step [333/7635], Loss: 5.4824\n",
      "Epoch [1/1], Step [334/7635], Loss: 5.4482\n",
      "Epoch [1/1], Step [335/7635], Loss: 5.5430\n",
      "Epoch [1/1], Step [336/7635], Loss: 5.4593\n",
      "Epoch [1/1], Step [337/7635], Loss: 5.5238\n",
      "Epoch [1/1], Step [338/7635], Loss: 5.4737\n",
      "Epoch [1/1], Step [339/7635], Loss: 5.5172\n",
      "Epoch [1/1], Step [340/7635], Loss: 5.4729\n",
      "Epoch [1/1], Step [341/7635], Loss: 5.4586\n",
      "Epoch [1/1], Step [342/7635], Loss: 5.4738\n",
      "Epoch [1/1], Step [343/7635], Loss: 5.4825\n",
      "Epoch [1/1], Step [344/7635], Loss: 5.4466\n",
      "Epoch [1/1], Step [345/7635], Loss: 5.4837\n",
      "Epoch [1/1], Step [346/7635], Loss: 5.4301\n",
      "Epoch [1/1], Step [347/7635], Loss: 5.5340\n",
      "Epoch [1/1], Step [348/7635], Loss: 5.4758\n",
      "Epoch [1/1], Step [349/7635], Loss: 5.4263\n",
      "Epoch [1/1], Step [350/7635], Loss: 5.4607\n",
      "Epoch [1/1], Step [351/7635], Loss: 5.5573\n",
      "Epoch [1/1], Step [352/7635], Loss: 5.3706\n",
      "Epoch [1/1], Step [353/7635], Loss: 5.4379\n",
      "Epoch [1/1], Step [354/7635], Loss: 5.4619\n",
      "Epoch [1/1], Step [355/7635], Loss: 5.5635\n",
      "Epoch [1/1], Step [356/7635], Loss: 5.4591\n",
      "Epoch [1/1], Step [357/7635], Loss: 5.6811\n",
      "Epoch [1/1], Step [358/7635], Loss: 5.4471\n",
      "Epoch [1/1], Step [359/7635], Loss: 5.4071\n",
      "Epoch [1/1], Step [360/7635], Loss: 5.4390\n",
      "Epoch [1/1], Step [361/7635], Loss: 5.4186\n",
      "Epoch [1/1], Step [362/7635], Loss: 5.5658\n",
      "Epoch [1/1], Step [363/7635], Loss: 5.3924\n",
      "Epoch [1/1], Step [364/7635], Loss: 5.4279\n",
      "Epoch [1/1], Step [365/7635], Loss: 5.4445\n",
      "Epoch [1/1], Step [366/7635], Loss: 5.5465\n",
      "Epoch [1/1], Step [367/7635], Loss: 5.5182\n",
      "Epoch [1/1], Step [368/7635], Loss: 5.4819\n",
      "Epoch [1/1], Step [369/7635], Loss: 5.4137\n",
      "Epoch [1/1], Step [370/7635], Loss: 5.4295\n",
      "Epoch [1/1], Step [371/7635], Loss: 5.4337\n",
      "Epoch [1/1], Step [372/7635], Loss: 5.4549\n",
      "Epoch [1/1], Step [373/7635], Loss: 5.4154\n",
      "Epoch [1/1], Step [374/7635], Loss: 5.4425\n",
      "Epoch [1/1], Step [375/7635], Loss: 5.3490\n",
      "Epoch [1/1], Step [376/7635], Loss: 5.4030\n",
      "Epoch [1/1], Step [377/7635], Loss: 5.3933\n",
      "Epoch [1/1], Step [378/7635], Loss: 5.3895\n",
      "Epoch [1/1], Step [379/7635], Loss: 5.3092\n",
      "Epoch [1/1], Step [380/7635], Loss: 5.5059\n",
      "Epoch [1/1], Step [381/7635], Loss: 5.3607\n",
      "Epoch [1/1], Step [382/7635], Loss: 5.3536\n",
      "Epoch [1/1], Step [383/7635], Loss: 5.4433\n",
      "Epoch [1/1], Step [384/7635], Loss: 5.4146\n",
      "Epoch [1/1], Step [385/7635], Loss: 5.4202\n",
      "Epoch [1/1], Step [386/7635], Loss: 5.4898\n",
      "Epoch [1/1], Step [387/7635], Loss: 5.3236\n",
      "Epoch [1/1], Step [388/7635], Loss: 5.3060\n",
      "Epoch [1/1], Step [389/7635], Loss: 5.4428\n",
      "Epoch [1/1], Step [390/7635], Loss: 5.3150\n",
      "Epoch [1/1], Step [391/7635], Loss: 5.4134\n",
      "Epoch [1/1], Step [392/7635], Loss: 5.5346\n",
      "Epoch [1/1], Step [393/7635], Loss: 5.3333\n",
      "Epoch [1/1], Step [394/7635], Loss: 5.4000\n",
      "Epoch [1/1], Step [395/7635], Loss: 5.5306\n",
      "Epoch [1/1], Step [396/7635], Loss: 5.4359\n",
      "Epoch [1/1], Step [397/7635], Loss: 5.2846\n",
      "Epoch [1/1], Step [398/7635], Loss: 5.3659\n",
      "Epoch [1/1], Step [399/7635], Loss: 5.3647\n",
      "Epoch [1/1], Step [400/7635], Loss: 5.4126\n",
      "Epoch [1/1], Step [401/7635], Loss: 5.4035\n",
      "Epoch [1/1], Step [402/7635], Loss: 5.4105\n",
      "Epoch [1/1], Step [403/7635], Loss: 5.4374\n",
      "Epoch [1/1], Step [404/7635], Loss: 5.3489\n",
      "Epoch [1/1], Step [405/7635], Loss: 5.3229\n",
      "Epoch [1/1], Step [406/7635], Loss: 5.3462\n",
      "Epoch [1/1], Step [407/7635], Loss: 5.5042\n",
      "Epoch [1/1], Step [408/7635], Loss: 5.3797\n",
      "Epoch [1/1], Step [409/7635], Loss: 5.3266\n",
      "Epoch [1/1], Step [410/7635], Loss: 5.4604\n",
      "Epoch [1/1], Step [411/7635], Loss: 5.4861\n",
      "Epoch [1/1], Step [412/7635], Loss: 5.4543\n",
      "Epoch [1/1], Step [413/7635], Loss: 5.3199\n",
      "Epoch [1/1], Step [414/7635], Loss: 5.3023\n",
      "Epoch [1/1], Step [415/7635], Loss: 5.3500\n",
      "Epoch [1/1], Step [416/7635], Loss: 5.2843\n",
      "Epoch [1/1], Step [417/7635], Loss: 5.3024\n",
      "Epoch [1/1], Step [418/7635], Loss: 5.3922\n",
      "Epoch [1/1], Step [419/7635], Loss: 5.3762\n",
      "Epoch [1/1], Step [420/7635], Loss: 5.3717\n",
      "Epoch [1/1], Step [421/7635], Loss: 5.2789\n",
      "Epoch [1/1], Step [422/7635], Loss: 5.4180\n",
      "Epoch [1/1], Step [423/7635], Loss: 5.2524\n",
      "Epoch [1/1], Step [424/7635], Loss: 5.2979\n",
      "Epoch [1/1], Step [425/7635], Loss: 5.4427\n",
      "Epoch [1/1], Step [426/7635], Loss: 5.3908\n",
      "Epoch [1/1], Step [427/7635], Loss: 5.3920\n",
      "Epoch [1/1], Step [428/7635], Loss: 5.3066\n",
      "Epoch [1/1], Step [429/7635], Loss: 5.3263\n",
      "Epoch [1/1], Step [430/7635], Loss: 5.2797\n",
      "Epoch [1/1], Step [431/7635], Loss: 5.4603\n",
      "Epoch [1/1], Step [432/7635], Loss: 5.2829\n",
      "Epoch [1/1], Step [433/7635], Loss: 5.2451\n",
      "Epoch [1/1], Step [434/7635], Loss: 5.3385\n",
      "Epoch [1/1], Step [435/7635], Loss: 5.3707\n",
      "Epoch [1/1], Step [436/7635], Loss: 5.3079\n",
      "Epoch [1/1], Step [437/7635], Loss: 5.3280\n",
      "Epoch [1/1], Step [438/7635], Loss: 5.2412\n",
      "Epoch [1/1], Step [439/7635], Loss: 5.4317\n",
      "Epoch [1/1], Step [440/7635], Loss: 5.3736\n",
      "Epoch [1/1], Step [441/7635], Loss: 5.3272\n",
      "Epoch [1/1], Step [442/7635], Loss: 5.3638\n",
      "Epoch [1/1], Step [443/7635], Loss: 5.2488\n",
      "Epoch [1/1], Step [444/7635], Loss: 5.3633\n",
      "Epoch [1/1], Step [445/7635], Loss: 5.3037\n",
      "Epoch [1/1], Step [446/7635], Loss: 5.4178\n",
      "Epoch [1/1], Step [447/7635], Loss: 5.3307\n",
      "Epoch [1/1], Step [448/7635], Loss: 5.3550\n",
      "Epoch [1/1], Step [449/7635], Loss: 5.2817\n",
      "Epoch [1/1], Step [450/7635], Loss: 5.3761\n",
      "Epoch [1/1], Step [451/7635], Loss: 5.2804\n",
      "Epoch [1/1], Step [452/7635], Loss: 5.3776\n",
      "Epoch [1/1], Step [453/7635], Loss: 5.3859\n",
      "Epoch [1/1], Step [454/7635], Loss: 5.2937\n",
      "Epoch [1/1], Step [455/7635], Loss: 5.2808\n",
      "Epoch [1/1], Step [456/7635], Loss: 5.3360\n",
      "Epoch [1/1], Step [457/7635], Loss: 5.2904\n",
      "Epoch [1/1], Step [458/7635], Loss: 5.1955\n",
      "Epoch [1/1], Step [459/7635], Loss: 5.2432\n",
      "Epoch [1/1], Step [460/7635], Loss: 5.2788\n",
      "Epoch [1/1], Step [461/7635], Loss: 5.3633\n",
      "Epoch [1/1], Step [462/7635], Loss: 5.2479\n",
      "Epoch [1/1], Step [463/7635], Loss: 5.4152\n",
      "Epoch [1/1], Step [464/7635], Loss: 5.3578\n",
      "Epoch [1/1], Step [465/7635], Loss: 5.2323\n",
      "Epoch [1/1], Step [466/7635], Loss: 5.2606\n",
      "Epoch [1/1], Step [467/7635], Loss: 5.2591\n",
      "Epoch [1/1], Step [468/7635], Loss: 5.2667\n",
      "Epoch [1/1], Step [469/7635], Loss: 5.3239\n",
      "Epoch [1/1], Step [470/7635], Loss: 5.3373\n",
      "Epoch [1/1], Step [471/7635], Loss: 5.2644\n",
      "Epoch [1/1], Step [472/7635], Loss: 5.3611\n",
      "Epoch [1/1], Step [473/7635], Loss: 5.3659\n",
      "Epoch [1/1], Step [474/7635], Loss: 5.3419\n",
      "Epoch [1/1], Step [475/7635], Loss: 5.2407\n",
      "Epoch [1/1], Step [476/7635], Loss: 5.2153\n",
      "Epoch [1/1], Step [477/7635], Loss: 5.2281\n",
      "Epoch [1/1], Step [478/7635], Loss: 5.2827\n",
      "Epoch [1/1], Step [479/7635], Loss: 5.2547\n",
      "Epoch [1/1], Step [480/7635], Loss: 5.2599\n",
      "Epoch [1/1], Step [481/7635], Loss: 5.2098\n",
      "Epoch [1/1], Step [482/7635], Loss: 5.1815\n",
      "Epoch [1/1], Step [483/7635], Loss: 5.2925\n",
      "Epoch [1/1], Step [484/7635], Loss: 5.1492\n",
      "Epoch [1/1], Step [485/7635], Loss: 5.2207\n",
      "Epoch [1/1], Step [486/7635], Loss: 5.2014\n",
      "Epoch [1/1], Step [487/7635], Loss: 5.2776\n",
      "Epoch [1/1], Step [488/7635], Loss: 5.1062\n",
      "Epoch [1/1], Step [489/7635], Loss: 5.2081\n",
      "Epoch [1/1], Step [490/7635], Loss: 5.3090\n",
      "Epoch [1/1], Step [491/7635], Loss: 5.3124\n",
      "Epoch [1/1], Step [492/7635], Loss: 5.1513\n",
      "Epoch [1/1], Step [493/7635], Loss: 5.1872\n",
      "Epoch [1/1], Step [494/7635], Loss: 5.1830\n",
      "Epoch [1/1], Step [495/7635], Loss: 5.3633\n",
      "Epoch [1/1], Step [496/7635], Loss: 5.2821\n",
      "Epoch [1/1], Step [497/7635], Loss: 5.3108\n",
      "Epoch [1/1], Step [498/7635], Loss: 5.3353\n",
      "Epoch [1/1], Step [499/7635], Loss: 5.2149\n",
      "Epoch [1/1], Step [500/7635], Loss: 5.1807\n",
      "Epoch [1/1], Step [501/7635], Loss: 5.2431\n",
      "Epoch [1/1], Step [502/7635], Loss: 5.1186\n",
      "Epoch [1/1], Step [503/7635], Loss: 5.2295\n",
      "Epoch [1/1], Step [504/7635], Loss: 5.0960\n",
      "Epoch [1/1], Step [505/7635], Loss: 5.2716\n",
      "Epoch [1/1], Step [506/7635], Loss: 5.1955\n",
      "Epoch [1/1], Step [507/7635], Loss: 5.2954\n",
      "Epoch [1/1], Step [508/7635], Loss: 5.2118\n",
      "Epoch [1/1], Step [509/7635], Loss: 5.1771\n",
      "Epoch [1/1], Step [510/7635], Loss: 5.1712\n",
      "Epoch [1/1], Step [511/7635], Loss: 5.2423\n",
      "Epoch [1/1], Step [512/7635], Loss: 5.3047\n",
      "Epoch [1/1], Step [513/7635], Loss: 5.1920\n",
      "Epoch [1/1], Step [514/7635], Loss: 5.2539\n",
      "Epoch [1/1], Step [515/7635], Loss: 5.0587\n",
      "Epoch [1/1], Step [516/7635], Loss: 5.1262\n",
      "Epoch [1/1], Step [517/7635], Loss: 5.2265\n",
      "Epoch [1/1], Step [518/7635], Loss: 5.2666\n",
      "Epoch [1/1], Step [519/7635], Loss: 5.1488\n",
      "Epoch [1/1], Step [520/7635], Loss: 5.1449\n",
      "Epoch [1/1], Step [521/7635], Loss: 5.3841\n",
      "Epoch [1/1], Step [522/7635], Loss: 5.0867\n",
      "Epoch [1/1], Step [523/7635], Loss: 5.1511\n",
      "Epoch [1/1], Step [524/7635], Loss: 5.2788\n",
      "Epoch [1/1], Step [525/7635], Loss: 5.1712\n",
      "Epoch [1/1], Step [526/7635], Loss: 5.2447\n",
      "Epoch [1/1], Step [527/7635], Loss: 5.2452\n",
      "Epoch [1/1], Step [528/7635], Loss: 5.0497\n",
      "Epoch [1/1], Step [529/7635], Loss: 5.2331\n",
      "Epoch [1/1], Step [530/7635], Loss: 5.2536\n",
      "Epoch [1/1], Step [531/7635], Loss: 5.1672\n",
      "Epoch [1/1], Step [532/7635], Loss: 5.2345\n",
      "Epoch [1/1], Step [533/7635], Loss: 5.1833\n",
      "Epoch [1/1], Step [534/7635], Loss: 5.1977\n",
      "Epoch [1/1], Step [535/7635], Loss: 5.2948\n",
      "Epoch [1/1], Step [536/7635], Loss: 5.2085\n",
      "Epoch [1/1], Step [537/7635], Loss: 5.1927\n",
      "Epoch [1/1], Step [538/7635], Loss: 5.1126\n",
      "Epoch [1/1], Step [539/7635], Loss: 5.0962\n",
      "Epoch [1/1], Step [540/7635], Loss: 5.1626\n",
      "Epoch [1/1], Step [541/7635], Loss: 5.1774\n",
      "Epoch [1/1], Step [542/7635], Loss: 5.2316\n",
      "Epoch [1/1], Step [543/7635], Loss: 5.1921\n",
      "Epoch [1/1], Step [544/7635], Loss: 4.9335\n",
      "Epoch [1/1], Step [545/7635], Loss: 5.2074\n",
      "Epoch [1/1], Step [546/7635], Loss: 5.1276\n",
      "Epoch [1/1], Step [547/7635], Loss: 5.2351\n",
      "Epoch [1/1], Step [548/7635], Loss: 5.2359\n",
      "Epoch [1/1], Step [549/7635], Loss: 5.0974\n",
      "Epoch [1/1], Step [550/7635], Loss: 5.1429\n",
      "Epoch [1/1], Step [551/7635], Loss: 5.2342\n",
      "Epoch [1/1], Step [552/7635], Loss: 5.1559\n",
      "Epoch [1/1], Step [553/7635], Loss: 5.1940\n",
      "Epoch [1/1], Step [554/7635], Loss: 5.1129\n",
      "Epoch [1/1], Step [555/7635], Loss: 5.2508\n",
      "Epoch [1/1], Step [556/7635], Loss: 5.1295\n",
      "Epoch [1/1], Step [557/7635], Loss: 5.1727\n",
      "Epoch [1/1], Step [558/7635], Loss: 5.1198\n",
      "Epoch [1/1], Step [559/7635], Loss: 5.0957\n",
      "Epoch [1/1], Step [560/7635], Loss: 5.1097\n",
      "Epoch [1/1], Step [561/7635], Loss: 5.1318\n",
      "Epoch [1/1], Step [562/7635], Loss: 5.2736\n",
      "Epoch [1/1], Step [563/7635], Loss: 5.0498\n",
      "Epoch [1/1], Step [564/7635], Loss: 5.0601\n",
      "Epoch [1/1], Step [565/7635], Loss: 5.1145\n",
      "Epoch [1/1], Step [566/7635], Loss: 5.1913\n",
      "Epoch [1/1], Step [567/7635], Loss: 5.0954\n",
      "Epoch [1/1], Step [568/7635], Loss: 5.1221\n",
      "Epoch [1/1], Step [569/7635], Loss: 5.2058\n",
      "Epoch [1/1], Step [570/7635], Loss: 5.1418\n",
      "Epoch [1/1], Step [571/7635], Loss: 5.1311\n",
      "Epoch [1/1], Step [572/7635], Loss: 5.1864\n",
      "Epoch [1/1], Step [573/7635], Loss: 5.1243\n",
      "Epoch [1/1], Step [574/7635], Loss: 5.0967\n",
      "Epoch [1/1], Step [575/7635], Loss: 5.1110\n",
      "Epoch [1/1], Step [576/7635], Loss: 5.1735\n",
      "Epoch [1/1], Step [577/7635], Loss: 5.1257\n",
      "Epoch [1/1], Step [578/7635], Loss: 5.1873\n",
      "Epoch [1/1], Step [579/7635], Loss: 5.0837\n",
      "Epoch [1/1], Step [580/7635], Loss: 5.0862\n",
      "Epoch [1/1], Step [581/7635], Loss: 5.2083\n",
      "Epoch [1/1], Step [582/7635], Loss: 5.1211\n",
      "Epoch [1/1], Step [583/7635], Loss: 5.1426\n",
      "Epoch [1/1], Step [584/7635], Loss: 5.0388\n",
      "Epoch [1/1], Step [585/7635], Loss: 5.1770\n",
      "Epoch [1/1], Step [586/7635], Loss: 5.2038\n",
      "Epoch [1/1], Step [587/7635], Loss: 5.1980\n",
      "Epoch [1/1], Step [588/7635], Loss: 5.0847\n",
      "Epoch [1/1], Step [589/7635], Loss: 5.1926\n",
      "Epoch [1/1], Step [590/7635], Loss: 5.0847\n",
      "Epoch [1/1], Step [591/7635], Loss: 5.1029\n",
      "Epoch [1/1], Step [592/7635], Loss: 5.1636\n",
      "Epoch [1/1], Step [593/7635], Loss: 5.1989\n",
      "Epoch [1/1], Step [594/7635], Loss: 5.1027\n",
      "Epoch [1/1], Step [595/7635], Loss: 5.1665\n",
      "Epoch [1/1], Step [596/7635], Loss: 5.1599\n",
      "Epoch [1/1], Step [597/7635], Loss: 5.2007\n",
      "Epoch [1/1], Step [598/7635], Loss: 5.0947\n",
      "Epoch [1/1], Step [599/7635], Loss: 5.0716\n",
      "Epoch [1/1], Step [600/7635], Loss: 5.3665\n",
      "Epoch [1/1], Step [601/7635], Loss: 5.1129\n",
      "Epoch [1/1], Step [602/7635], Loss: 5.0970\n",
      "Epoch [1/1], Step [603/7635], Loss: 5.0307\n",
      "Epoch [1/1], Step [604/7635], Loss: 5.0456\n",
      "Epoch [1/1], Step [605/7635], Loss: 5.0707\n",
      "Epoch [1/1], Step [606/7635], Loss: 5.0922\n",
      "Epoch [1/1], Step [607/7635], Loss: 4.9962\n",
      "Epoch [1/1], Step [608/7635], Loss: 5.1115\n",
      "Epoch [1/1], Step [609/7635], Loss: 5.0917\n",
      "Epoch [1/1], Step [610/7635], Loss: 5.0631\n",
      "Epoch [1/1], Step [611/7635], Loss: 5.1111\n",
      "Epoch [1/1], Step [612/7635], Loss: 5.0130\n",
      "Epoch [1/1], Step [613/7635], Loss: 5.0503\n",
      "Epoch [1/1], Step [614/7635], Loss: 4.9885\n",
      "Epoch [1/1], Step [615/7635], Loss: 5.0240\n",
      "Epoch [1/1], Step [616/7635], Loss: 5.1077\n",
      "Epoch [1/1], Step [617/7635], Loss: 5.0472\n",
      "Epoch [1/1], Step [618/7635], Loss: 5.1501\n",
      "Epoch [1/1], Step [619/7635], Loss: 5.2192\n",
      "Epoch [1/1], Step [620/7635], Loss: 5.0186\n",
      "Epoch [1/1], Step [621/7635], Loss: 5.1389\n",
      "Epoch [1/1], Step [622/7635], Loss: 5.0958\n",
      "Epoch [1/1], Step [623/7635], Loss: 5.0646\n",
      "Epoch [1/1], Step [624/7635], Loss: 5.1466\n",
      "Epoch [1/1], Step [625/7635], Loss: 5.0071\n",
      "Epoch [1/1], Step [626/7635], Loss: 5.0511\n",
      "Epoch [1/1], Step [627/7635], Loss: 5.0483\n",
      "Epoch [1/1], Step [628/7635], Loss: 5.0474\n",
      "Epoch [1/1], Step [629/7635], Loss: 4.9631\n",
      "Epoch [1/1], Step [630/7635], Loss: 5.1230\n",
      "Epoch [1/1], Step [631/7635], Loss: 5.0500\n",
      "Epoch [1/1], Step [632/7635], Loss: 5.0828\n",
      "Epoch [1/1], Step [633/7635], Loss: 5.1631\n",
      "Epoch [1/1], Step [634/7635], Loss: 5.0639\n",
      "Epoch [1/1], Step [635/7635], Loss: 5.2521\n",
      "Epoch [1/1], Step [636/7635], Loss: 5.1134\n",
      "Epoch [1/1], Step [637/7635], Loss: 5.0651\n",
      "Epoch [1/1], Step [638/7635], Loss: 5.0211\n",
      "Epoch [1/1], Step [639/7635], Loss: 5.0280\n",
      "Epoch [1/1], Step [640/7635], Loss: 5.0773\n",
      "Epoch [1/1], Step [641/7635], Loss: 5.0034\n",
      "Epoch [1/1], Step [642/7635], Loss: 4.9882\n",
      "Epoch [1/1], Step [643/7635], Loss: 5.0895\n",
      "Epoch [1/1], Step [644/7635], Loss: 5.0361\n",
      "Epoch [1/1], Step [645/7635], Loss: 5.0653\n",
      "Epoch [1/1], Step [646/7635], Loss: 5.1223\n",
      "Epoch [1/1], Step [647/7635], Loss: 5.0523\n",
      "Epoch [1/1], Step [648/7635], Loss: 5.1162\n",
      "Epoch [1/1], Step [649/7635], Loss: 5.0878\n",
      "Epoch [1/1], Step [650/7635], Loss: 5.0948\n",
      "Epoch [1/1], Step [651/7635], Loss: 5.0983\n",
      "Epoch [1/1], Step [652/7635], Loss: 5.1484\n",
      "Epoch [1/1], Step [653/7635], Loss: 5.0061\n",
      "Epoch [1/1], Step [654/7635], Loss: 5.0426\n",
      "Epoch [1/1], Step [655/7635], Loss: 5.1103\n",
      "Epoch [1/1], Step [656/7635], Loss: 4.9545\n",
      "Epoch [1/1], Step [657/7635], Loss: 5.0066\n",
      "Epoch [1/1], Step [658/7635], Loss: 5.0169\n",
      "Epoch [1/1], Step [659/7635], Loss: 5.1134\n",
      "Epoch [1/1], Step [660/7635], Loss: 5.0981\n",
      "Epoch [1/1], Step [661/7635], Loss: 4.9785\n",
      "Epoch [1/1], Step [662/7635], Loss: 5.1237\n",
      "Epoch [1/1], Step [663/7635], Loss: 5.1082\n",
      "Epoch [1/1], Step [664/7635], Loss: 5.0624\n",
      "Epoch [1/1], Step [665/7635], Loss: 5.0789\n",
      "Epoch [1/1], Step [666/7635], Loss: 5.0230\n",
      "Epoch [1/1], Step [667/7635], Loss: 4.9717\n",
      "Epoch [1/1], Step [668/7635], Loss: 5.0919\n",
      "Epoch [1/1], Step [669/7635], Loss: 4.9845\n",
      "Epoch [1/1], Step [670/7635], Loss: 5.0854\n",
      "Epoch [1/1], Step [671/7635], Loss: 4.9324\n",
      "Epoch [1/1], Step [672/7635], Loss: 5.0210\n",
      "Epoch [1/1], Step [673/7635], Loss: 5.0554\n",
      "Epoch [1/1], Step [674/7635], Loss: 5.1559\n",
      "Epoch [1/1], Step [675/7635], Loss: 5.0244\n",
      "Epoch [1/1], Step [676/7635], Loss: 5.0462\n",
      "Epoch [1/1], Step [677/7635], Loss: 5.0104\n",
      "Epoch [1/1], Step [678/7635], Loss: 4.9640\n",
      "Epoch [1/1], Step [679/7635], Loss: 5.0670\n",
      "Epoch [1/1], Step [680/7635], Loss: 4.9322\n",
      "Epoch [1/1], Step [681/7635], Loss: 5.0302\n",
      "Epoch [1/1], Step [682/7635], Loss: 4.9328\n",
      "Epoch [1/1], Step [683/7635], Loss: 4.9953\n",
      "Epoch [1/1], Step [684/7635], Loss: 4.9445\n",
      "Epoch [1/1], Step [685/7635], Loss: 5.0594\n",
      "Epoch [1/1], Step [686/7635], Loss: 5.0627\n",
      "Epoch [1/1], Step [687/7635], Loss: 5.0636\n",
      "Epoch [1/1], Step [688/7635], Loss: 5.0310\n",
      "Epoch [1/1], Step [689/7635], Loss: 5.0251\n",
      "Epoch [1/1], Step [690/7635], Loss: 5.0330\n",
      "Epoch [1/1], Step [691/7635], Loss: 4.9663\n",
      "Epoch [1/1], Step [692/7635], Loss: 5.1038\n",
      "Epoch [1/1], Step [693/7635], Loss: 5.0125\n",
      "Epoch [1/1], Step [694/7635], Loss: 5.0148\n",
      "Epoch [1/1], Step [695/7635], Loss: 4.9959\n",
      "Epoch [1/1], Step [696/7635], Loss: 5.1749\n",
      "Epoch [1/1], Step [697/7635], Loss: 4.9187\n",
      "Epoch [1/1], Step [698/7635], Loss: 5.0551\n",
      "Epoch [1/1], Step [699/7635], Loss: 5.0082\n",
      "Epoch [1/1], Step [700/7635], Loss: 4.9334\n",
      "Epoch [1/1], Step [701/7635], Loss: 4.9680\n",
      "Epoch [1/1], Step [702/7635], Loss: 5.0376\n",
      "Epoch [1/1], Step [703/7635], Loss: 5.0497\n",
      "Epoch [1/1], Step [704/7635], Loss: 5.0519\n",
      "Epoch [1/1], Step [705/7635], Loss: 4.9780\n",
      "Epoch [1/1], Step [706/7635], Loss: 4.9905\n",
      "Epoch [1/1], Step [707/7635], Loss: 4.9591\n",
      "Epoch [1/1], Step [708/7635], Loss: 5.0854\n",
      "Epoch [1/1], Step [709/7635], Loss: 4.9496\n",
      "Epoch [1/1], Step [710/7635], Loss: 5.1534\n",
      "Epoch [1/1], Step [711/7635], Loss: 4.9729\n",
      "Epoch [1/1], Step [712/7635], Loss: 4.9874\n",
      "Epoch [1/1], Step [713/7635], Loss: 4.9561\n",
      "Epoch [1/1], Step [714/7635], Loss: 5.0693\n",
      "Epoch [1/1], Step [715/7635], Loss: 5.0591\n",
      "Epoch [1/1], Step [716/7635], Loss: 5.0316\n",
      "Epoch [1/1], Step [717/7635], Loss: 5.0026\n",
      "Epoch [1/1], Step [718/7635], Loss: 4.9404\n",
      "Epoch [1/1], Step [719/7635], Loss: 5.0343\n",
      "Epoch [1/1], Step [720/7635], Loss: 5.0716\n",
      "Epoch [1/1], Step [721/7635], Loss: 4.8805\n",
      "Epoch [1/1], Step [722/7635], Loss: 5.0151\n",
      "Epoch [1/1], Step [723/7635], Loss: 5.0100\n",
      "Epoch [1/1], Step [724/7635], Loss: 5.1711\n",
      "Epoch [1/1], Step [725/7635], Loss: 4.9664\n",
      "Epoch [1/1], Step [726/7635], Loss: 5.0405\n",
      "Epoch [1/1], Step [727/7635], Loss: 5.0717\n",
      "Epoch [1/1], Step [728/7635], Loss: 5.0354\n",
      "Epoch [1/1], Step [729/7635], Loss: 4.9649\n",
      "Epoch [1/1], Step [730/7635], Loss: 4.9602\n",
      "Epoch [1/1], Step [731/7635], Loss: 5.1411\n",
      "Epoch [1/1], Step [732/7635], Loss: 4.8986\n",
      "Epoch [1/1], Step [733/7635], Loss: 4.8744\n",
      "Epoch [1/1], Step [734/7635], Loss: 4.9852\n",
      "Epoch [1/1], Step [735/7635], Loss: 5.0176\n",
      "Epoch [1/1], Step [736/7635], Loss: 4.9466\n",
      "Epoch [1/1], Step [737/7635], Loss: 5.0858\n",
      "Epoch [1/1], Step [738/7635], Loss: 4.9746\n",
      "Epoch [1/1], Step [739/7635], Loss: 4.9885\n",
      "Epoch [1/1], Step [740/7635], Loss: 4.8408\n",
      "Epoch [1/1], Step [741/7635], Loss: 4.9467\n",
      "Epoch [1/1], Step [742/7635], Loss: 4.9581\n",
      "Epoch [1/1], Step [743/7635], Loss: 4.9176\n",
      "Epoch [1/1], Step [744/7635], Loss: 4.9569\n",
      "Epoch [1/1], Step [745/7635], Loss: 4.9921\n",
      "Epoch [1/1], Step [746/7635], Loss: 4.8912\n",
      "Epoch [1/1], Step [747/7635], Loss: 4.9831\n",
      "Epoch [1/1], Step [748/7635], Loss: 5.0404\n",
      "Epoch [1/1], Step [749/7635], Loss: 5.1067\n",
      "Epoch [1/1], Step [750/7635], Loss: 4.9869\n",
      "Epoch [1/1], Step [751/7635], Loss: 4.9416\n",
      "Epoch [1/1], Step [752/7635], Loss: 5.0592\n",
      "Epoch [1/1], Step [753/7635], Loss: 5.0511\n",
      "Epoch [1/1], Step [754/7635], Loss: 4.9787\n",
      "Epoch [1/1], Step [755/7635], Loss: 5.0475\n",
      "Epoch [1/1], Step [756/7635], Loss: 4.9521\n",
      "Epoch [1/1], Step [757/7635], Loss: 5.0995\n",
      "Epoch [1/1], Step [758/7635], Loss: 4.9834\n",
      "Epoch [1/1], Step [759/7635], Loss: 4.9542\n",
      "Epoch [1/1], Step [760/7635], Loss: 4.8983\n",
      "Epoch [1/1], Step [761/7635], Loss: 5.0123\n",
      "Epoch [1/1], Step [762/7635], Loss: 4.9222\n",
      "Epoch [1/1], Step [763/7635], Loss: 5.0308\n",
      "Epoch [1/1], Step [764/7635], Loss: 4.9474\n",
      "Epoch [1/1], Step [765/7635], Loss: 5.0214\n",
      "Epoch [1/1], Step [766/7635], Loss: 4.9747\n",
      "Epoch [1/1], Step [767/7635], Loss: 4.9522\n",
      "Epoch [1/1], Step [768/7635], Loss: 4.8949\n",
      "Epoch [1/1], Step [769/7635], Loss: 4.8689\n",
      "Epoch [1/1], Step [770/7635], Loss: 4.9640\n",
      "Epoch [1/1], Step [771/7635], Loss: 4.9660\n",
      "Epoch [1/1], Step [772/7635], Loss: 4.9197\n",
      "Epoch [1/1], Step [773/7635], Loss: 5.0072\n",
      "Epoch [1/1], Step [774/7635], Loss: 4.9104\n",
      "Epoch [1/1], Step [775/7635], Loss: 5.0056\n",
      "Epoch [1/1], Step [776/7635], Loss: 4.9019\n",
      "Epoch [1/1], Step [777/7635], Loss: 4.9001\n",
      "Epoch [1/1], Step [778/7635], Loss: 4.9722\n",
      "Epoch [1/1], Step [779/7635], Loss: 5.1241\n",
      "Epoch [1/1], Step [780/7635], Loss: 4.9775\n",
      "Epoch [1/1], Step [781/7635], Loss: 4.8723\n",
      "Epoch [1/1], Step [782/7635], Loss: 4.9991\n",
      "Epoch [1/1], Step [783/7635], Loss: 4.9582\n",
      "Epoch [1/1], Step [784/7635], Loss: 5.0069\n",
      "Epoch [1/1], Step [785/7635], Loss: 4.9607\n",
      "Epoch [1/1], Step [786/7635], Loss: 4.9006\n",
      "Epoch [1/1], Step [787/7635], Loss: 4.9422\n",
      "Epoch [1/1], Step [788/7635], Loss: 4.9005\n",
      "Epoch [1/1], Step [789/7635], Loss: 4.9659\n",
      "Epoch [1/1], Step [790/7635], Loss: 4.8266\n",
      "Epoch [1/1], Step [791/7635], Loss: 5.0045\n",
      "Epoch [1/1], Step [792/7635], Loss: 4.9910\n",
      "Epoch [1/1], Step [793/7635], Loss: 5.0463\n",
      "Epoch [1/1], Step [794/7635], Loss: 4.9463\n",
      "Epoch [1/1], Step [795/7635], Loss: 5.0513\n",
      "Epoch [1/1], Step [796/7635], Loss: 4.9662\n",
      "Epoch [1/1], Step [797/7635], Loss: 4.7801\n",
      "Epoch [1/1], Step [798/7635], Loss: 4.9699\n",
      "Epoch [1/1], Step [799/7635], Loss: 4.9807\n",
      "Epoch [1/1], Step [800/7635], Loss: 4.9058\n",
      "Epoch [1/1], Step [801/7635], Loss: 4.9281\n",
      "Epoch [1/1], Step [802/7635], Loss: 5.0949\n",
      "Epoch [1/1], Step [803/7635], Loss: 5.0023\n",
      "Epoch [1/1], Step [804/7635], Loss: 4.8944\n",
      "Epoch [1/1], Step [805/7635], Loss: 4.8631\n",
      "Epoch [1/1], Step [806/7635], Loss: 4.9945\n",
      "Epoch [1/1], Step [807/7635], Loss: 4.8809\n",
      "Epoch [1/1], Step [808/7635], Loss: 4.9519\n",
      "Epoch [1/1], Step [809/7635], Loss: 4.9411\n",
      "Epoch [1/1], Step [810/7635], Loss: 4.9134\n",
      "Epoch [1/1], Step [811/7635], Loss: 4.9031\n",
      "Epoch [1/1], Step [812/7635], Loss: 4.8727\n",
      "Epoch [1/1], Step [813/7635], Loss: 4.9239\n",
      "Epoch [1/1], Step [814/7635], Loss: 4.9316\n",
      "Epoch [1/1], Step [815/7635], Loss: 4.9780\n",
      "Epoch [1/1], Step [816/7635], Loss: 4.8683\n",
      "Epoch [1/1], Step [817/7635], Loss: 4.8434\n",
      "Epoch [1/1], Step [818/7635], Loss: 5.1370\n",
      "Epoch [1/1], Step [819/7635], Loss: 4.8130\n",
      "Epoch [1/1], Step [820/7635], Loss: 4.8975\n",
      "Epoch [1/1], Step [821/7635], Loss: 4.8426\n",
      "Epoch [1/1], Step [822/7635], Loss: 4.8996\n",
      "Epoch [1/1], Step [823/7635], Loss: 4.8417\n",
      "Epoch [1/1], Step [824/7635], Loss: 4.8949\n",
      "Epoch [1/1], Step [825/7635], Loss: 4.9609\n",
      "Epoch [1/1], Step [826/7635], Loss: 4.8032\n",
      "Epoch [1/1], Step [827/7635], Loss: 4.8806\n",
      "Epoch [1/1], Step [828/7635], Loss: 4.9449\n",
      "Epoch [1/1], Step [829/7635], Loss: 4.9233\n",
      "Epoch [1/1], Step [830/7635], Loss: 4.9112\n",
      "Epoch [1/1], Step [831/7635], Loss: 4.9562\n",
      "Epoch [1/1], Step [832/7635], Loss: 4.8218\n",
      "Epoch [1/1], Step [833/7635], Loss: 4.9637\n",
      "Epoch [1/1], Step [834/7635], Loss: 4.9536\n",
      "Epoch [1/1], Step [835/7635], Loss: 4.9460\n",
      "Epoch [1/1], Step [836/7635], Loss: 4.9016\n",
      "Epoch [1/1], Step [837/7635], Loss: 4.9304\n",
      "Epoch [1/1], Step [838/7635], Loss: 4.8735\n",
      "Epoch [1/1], Step [839/7635], Loss: 4.8317\n",
      "Epoch [1/1], Step [840/7635], Loss: 4.9700\n",
      "Epoch [1/1], Step [841/7635], Loss: 4.9717\n",
      "Epoch [1/1], Step [842/7635], Loss: 4.8733\n",
      "Epoch [1/1], Step [843/7635], Loss: 4.9515\n",
      "Epoch [1/1], Step [844/7635], Loss: 4.9881\n",
      "Epoch [1/1], Step [845/7635], Loss: 4.8053\n",
      "Epoch [1/1], Step [846/7635], Loss: 4.9135\n",
      "Epoch [1/1], Step [847/7635], Loss: 4.9869\n",
      "Epoch [1/1], Step [848/7635], Loss: 4.9190\n",
      "Epoch [1/1], Step [849/7635], Loss: 4.9900\n",
      "Epoch [1/1], Step [850/7635], Loss: 4.8817\n",
      "Epoch [1/1], Step [851/7635], Loss: 4.9110\n",
      "Epoch [1/1], Step [852/7635], Loss: 4.8371\n",
      "Epoch [1/1], Step [853/7635], Loss: 4.9304\n",
      "Epoch [1/1], Step [854/7635], Loss: 4.8891\n",
      "Epoch [1/1], Step [855/7635], Loss: 4.8896\n",
      "Epoch [1/1], Step [856/7635], Loss: 4.8931\n",
      "Epoch [1/1], Step [857/7635], Loss: 4.9374\n",
      "Epoch [1/1], Step [858/7635], Loss: 4.8794\n",
      "Epoch [1/1], Step [859/7635], Loss: 4.9020\n",
      "Epoch [1/1], Step [860/7635], Loss: 4.9641\n",
      "Epoch [1/1], Step [861/7635], Loss: 4.9115\n",
      "Epoch [1/1], Step [862/7635], Loss: 4.8757\n",
      "Epoch [1/1], Step [863/7635], Loss: 4.8413\n",
      "Epoch [1/1], Step [864/7635], Loss: 4.9443\n",
      "Epoch [1/1], Step [865/7635], Loss: 4.8971\n",
      "Epoch [1/1], Step [866/7635], Loss: 4.8532\n",
      "Epoch [1/1], Step [867/7635], Loss: 4.9086\n",
      "Epoch [1/1], Step [868/7635], Loss: 4.8698\n",
      "Epoch [1/1], Step [869/7635], Loss: 4.8425\n",
      "Epoch [1/1], Step [870/7635], Loss: 4.8917\n",
      "Epoch [1/1], Step [871/7635], Loss: 4.7981\n",
      "Epoch [1/1], Step [872/7635], Loss: 4.9151\n",
      "Epoch [1/1], Step [873/7635], Loss: 4.9671\n",
      "Epoch [1/1], Step [874/7635], Loss: 4.8515\n",
      "Epoch [1/1], Step [875/7635], Loss: 4.8295\n",
      "Epoch [1/1], Step [876/7635], Loss: 4.7613\n",
      "Epoch [1/1], Step [877/7635], Loss: 4.9217\n",
      "Epoch [1/1], Step [878/7635], Loss: 4.7570\n",
      "Epoch [1/1], Step [879/7635], Loss: 4.8626\n",
      "Epoch [1/1], Step [880/7635], Loss: 4.9061\n",
      "Epoch [1/1], Step [881/7635], Loss: 4.8584\n",
      "Epoch [1/1], Step [882/7635], Loss: 4.8504\n",
      "Epoch [1/1], Step [883/7635], Loss: 4.9634\n",
      "Epoch [1/1], Step [884/7635], Loss: 4.8377\n",
      "Epoch [1/1], Step [885/7635], Loss: 4.9536\n",
      "Epoch [1/1], Step [886/7635], Loss: 4.9087\n",
      "Epoch [1/1], Step [887/7635], Loss: 4.7544\n",
      "Epoch [1/1], Step [888/7635], Loss: 4.9696\n",
      "Epoch [1/1], Step [889/7635], Loss: 4.9095\n",
      "Epoch [1/1], Step [890/7635], Loss: 4.8282\n",
      "Epoch [1/1], Step [891/7635], Loss: 4.8284\n",
      "Epoch [1/1], Step [892/7635], Loss: 4.9174\n",
      "Epoch [1/1], Step [893/7635], Loss: 4.9240\n",
      "Epoch [1/1], Step [894/7635], Loss: 4.8739\n",
      "Epoch [1/1], Step [895/7635], Loss: 4.8775\n",
      "Epoch [1/1], Step [896/7635], Loss: 4.8790\n",
      "Epoch [1/1], Step [897/7635], Loss: 4.8495\n",
      "Epoch [1/1], Step [898/7635], Loss: 4.8105\n",
      "Epoch [1/1], Step [899/7635], Loss: 4.8700\n",
      "Epoch [1/1], Step [900/7635], Loss: 4.9262\n",
      "Epoch [1/1], Step [901/7635], Loss: 4.8362\n",
      "Epoch [1/1], Step [902/7635], Loss: 5.0026\n",
      "Epoch [1/1], Step [903/7635], Loss: 4.9214\n",
      "Epoch [1/1], Step [904/7635], Loss: 4.8373\n",
      "Epoch [1/1], Step [905/7635], Loss: 4.9132\n",
      "Epoch [1/1], Step [906/7635], Loss: 4.8731\n",
      "Epoch [1/1], Step [907/7635], Loss: 4.8626\n",
      "Epoch [1/1], Step [908/7635], Loss: 4.9075\n",
      "Epoch [1/1], Step [909/7635], Loss: 4.9411\n",
      "Epoch [1/1], Step [910/7635], Loss: 4.9178\n",
      "Epoch [1/1], Step [911/7635], Loss: 4.8743\n",
      "Epoch [1/1], Step [912/7635], Loss: 4.8512\n",
      "Epoch [1/1], Step [913/7635], Loss: 4.9473\n",
      "Epoch [1/1], Step [914/7635], Loss: 4.8199\n",
      "Epoch [1/1], Step [915/7635], Loss: 4.8695\n",
      "Epoch [1/1], Step [916/7635], Loss: 4.8434\n",
      "Epoch [1/1], Step [917/7635], Loss: 4.8448\n",
      "Epoch [1/1], Step [918/7635], Loss: 4.8932\n",
      "Epoch [1/1], Step [919/7635], Loss: 4.8806\n",
      "Epoch [1/1], Step [920/7635], Loss: 4.6750\n",
      "Epoch [1/1], Step [921/7635], Loss: 4.7744\n",
      "Epoch [1/1], Step [922/7635], Loss: 4.7913\n",
      "Epoch [1/1], Step [923/7635], Loss: 4.9050\n",
      "Epoch [1/1], Step [924/7635], Loss: 4.8504\n",
      "Epoch [1/1], Step [925/7635], Loss: 4.9450\n",
      "Epoch [1/1], Step [926/7635], Loss: 4.8770\n",
      "Epoch [1/1], Step [927/7635], Loss: 4.8086\n",
      "Epoch [1/1], Step [928/7635], Loss: 4.8059\n",
      "Epoch [1/1], Step [929/7635], Loss: 4.9118\n",
      "Epoch [1/1], Step [930/7635], Loss: 4.9696\n",
      "Epoch [1/1], Step [931/7635], Loss: 4.8403\n",
      "Epoch [1/1], Step [932/7635], Loss: 4.9214\n",
      "Epoch [1/1], Step [933/7635], Loss: 4.7991\n",
      "Epoch [1/1], Step [934/7635], Loss: 4.7946\n",
      "Epoch [1/1], Step [935/7635], Loss: 4.8844\n",
      "Epoch [1/1], Step [936/7635], Loss: 4.9177\n",
      "Epoch [1/1], Step [937/7635], Loss: 4.8646\n",
      "Epoch [1/1], Step [938/7635], Loss: 4.8823\n",
      "Epoch [1/1], Step [939/7635], Loss: 4.7638\n",
      "Epoch [1/1], Step [940/7635], Loss: 4.8740\n",
      "Epoch [1/1], Step [941/7635], Loss: 4.9284\n",
      "Epoch [1/1], Step [942/7635], Loss: 4.8572\n",
      "Epoch [1/1], Step [943/7635], Loss: 4.8184\n",
      "Epoch [1/1], Step [944/7635], Loss: 4.8556\n",
      "Epoch [1/1], Step [945/7635], Loss: 4.9987\n",
      "Epoch [1/1], Step [946/7635], Loss: 4.7557\n",
      "Epoch [1/1], Step [947/7635], Loss: 4.7618\n",
      "Epoch [1/1], Step [948/7635], Loss: 4.8513\n",
      "Epoch [1/1], Step [949/7635], Loss: 4.8956\n",
      "Epoch [1/1], Step [950/7635], Loss: 4.7925\n",
      "Epoch [1/1], Step [951/7635], Loss: 4.8216\n",
      "Epoch [1/1], Step [952/7635], Loss: 4.8946\n",
      "Epoch [1/1], Step [953/7635], Loss: 4.8376\n",
      "Epoch [1/1], Step [954/7635], Loss: 4.8888\n",
      "Epoch [1/1], Step [955/7635], Loss: 4.8760\n",
      "Epoch [1/1], Step [956/7635], Loss: 4.8933\n",
      "Epoch [1/1], Step [957/7635], Loss: 4.8844\n",
      "Epoch [1/1], Step [958/7635], Loss: 4.7521\n",
      "Epoch [1/1], Step [959/7635], Loss: 4.7908\n",
      "Epoch [1/1], Step [960/7635], Loss: 4.8062\n",
      "Epoch [1/1], Step [961/7635], Loss: 4.8134\n",
      "Epoch [1/1], Step [962/7635], Loss: 4.8067\n",
      "Epoch [1/1], Step [963/7635], Loss: 4.7623\n",
      "Epoch [1/1], Step [964/7635], Loss: 4.8298\n",
      "Epoch [1/1], Step [965/7635], Loss: 4.8521\n",
      "Epoch [1/1], Step [966/7635], Loss: 4.8708\n",
      "Epoch [1/1], Step [967/7635], Loss: 4.9154\n",
      "Epoch [1/1], Step [968/7635], Loss: 4.8865\n",
      "Epoch [1/1], Step [969/7635], Loss: 4.8697\n",
      "Epoch [1/1], Step [970/7635], Loss: 4.8035\n",
      "Epoch [1/1], Step [971/7635], Loss: 4.8279\n",
      "Epoch [1/1], Step [972/7635], Loss: 4.8061\n",
      "Epoch [1/1], Step [973/7635], Loss: 4.6748\n",
      "Epoch [1/1], Step [974/7635], Loss: 4.8414\n",
      "Epoch [1/1], Step [975/7635], Loss: 4.9117\n",
      "Epoch [1/1], Step [976/7635], Loss: 4.8023\n",
      "Epoch [1/1], Step [977/7635], Loss: 4.8505\n",
      "Epoch [1/1], Step [978/7635], Loss: 4.7872\n",
      "Epoch [1/1], Step [979/7635], Loss: 4.8617\n",
      "Epoch [1/1], Step [980/7635], Loss: 4.7785\n",
      "Epoch [1/1], Step [981/7635], Loss: 4.9819\n",
      "Epoch [1/1], Step [982/7635], Loss: 4.8024\n",
      "Epoch [1/1], Step [983/7635], Loss: 4.8553\n",
      "Epoch [1/1], Step [984/7635], Loss: 4.8456\n",
      "Epoch [1/1], Step [985/7635], Loss: 4.8327\n",
      "Epoch [1/1], Step [986/7635], Loss: 4.7089\n",
      "Epoch [1/1], Step [987/7635], Loss: 4.8521\n",
      "Epoch [1/1], Step [988/7635], Loss: 4.7355\n",
      "Epoch [1/1], Step [989/7635], Loss: 4.7348\n",
      "Epoch [1/1], Step [990/7635], Loss: 4.8341\n",
      "Epoch [1/1], Step [991/7635], Loss: 4.8365\n",
      "Epoch [1/1], Step [992/7635], Loss: 4.8823\n",
      "Epoch [1/1], Step [993/7635], Loss: 4.8620\n",
      "Epoch [1/1], Step [994/7635], Loss: 4.8346\n",
      "Epoch [1/1], Step [995/7635], Loss: 4.8602\n",
      "Epoch [1/1], Step [996/7635], Loss: 4.7646\n",
      "Epoch [1/1], Step [997/7635], Loss: 4.7855\n",
      "Epoch [1/1], Step [998/7635], Loss: 4.8506\n",
      "Epoch [1/1], Step [999/7635], Loss: 4.7567\n",
      "Epoch [1/1], Step [1000/7635], Loss: 4.7727\n",
      "Epoch [1/1], Step [1001/7635], Loss: 4.7757\n",
      "Epoch [1/1], Step [1002/7635], Loss: 4.7994\n",
      "Epoch [1/1], Step [1003/7635], Loss: 4.8086\n",
      "Epoch [1/1], Step [1004/7635], Loss: 4.8331\n",
      "Epoch [1/1], Step [1005/7635], Loss: 4.7394\n",
      "Epoch [1/1], Step [1006/7635], Loss: 4.6706\n",
      "Epoch [1/1], Step [1007/7635], Loss: 4.8770\n",
      "Epoch [1/1], Step [1008/7635], Loss: 4.8206\n",
      "Epoch [1/1], Step [1009/7635], Loss: 4.7953\n",
      "Epoch [1/1], Step [1010/7635], Loss: 4.7629\n",
      "Epoch [1/1], Step [1011/7635], Loss: 4.8845\n",
      "Epoch [1/1], Step [1012/7635], Loss: 4.8741\n",
      "Epoch [1/1], Step [1013/7635], Loss: 4.8586\n",
      "Epoch [1/1], Step [1014/7635], Loss: 4.7468\n",
      "Epoch [1/1], Step [1015/7635], Loss: 4.7982\n",
      "Epoch [1/1], Step [1016/7635], Loss: 4.8603\n",
      "Epoch [1/1], Step [1017/7635], Loss: 4.8488\n",
      "Epoch [1/1], Step [1018/7635], Loss: 4.7703\n",
      "Epoch [1/1], Step [1019/7635], Loss: 4.7732\n",
      "Epoch [1/1], Step [1020/7635], Loss: 4.7296\n",
      "Epoch [1/1], Step [1021/7635], Loss: 4.8102\n",
      "Epoch [1/1], Step [1022/7635], Loss: 4.9254\n",
      "Epoch [1/1], Step [1023/7635], Loss: 4.8755\n",
      "Epoch [1/1], Step [1024/7635], Loss: 4.7302\n",
      "Epoch [1/1], Step [1025/7635], Loss: 4.8098\n",
      "Epoch [1/1], Step [1026/7635], Loss: 4.8318\n",
      "Epoch [1/1], Step [1027/7635], Loss: 4.7311\n",
      "Epoch [1/1], Step [1028/7635], Loss: 4.8072\n",
      "Epoch [1/1], Step [1029/7635], Loss: 4.7590\n",
      "Epoch [1/1], Step [1030/7635], Loss: 4.7338\n",
      "Epoch [1/1], Step [1031/7635], Loss: 4.7866\n",
      "Epoch [1/1], Step [1032/7635], Loss: 4.7728\n",
      "Epoch [1/1], Step [1033/7635], Loss: 4.8129\n",
      "Epoch [1/1], Step [1034/7635], Loss: 4.7875\n",
      "Epoch [1/1], Step [1035/7635], Loss: 4.7467\n",
      "Epoch [1/1], Step [1036/7635], Loss: 4.7869\n",
      "Epoch [1/1], Step [1037/7635], Loss: 4.7914\n",
      "Epoch [1/1], Step [1038/7635], Loss: 4.7797\n",
      "Epoch [1/1], Step [1039/7635], Loss: 4.8193\n",
      "Epoch [1/1], Step [1040/7635], Loss: 4.8412\n",
      "Epoch [1/1], Step [1041/7635], Loss: 4.7913\n",
      "Epoch [1/1], Step [1042/7635], Loss: 4.8033\n",
      "Epoch [1/1], Step [1043/7635], Loss: 4.6763\n",
      "Epoch [1/1], Step [1044/7635], Loss: 4.7856\n",
      "Epoch [1/1], Step [1045/7635], Loss: 4.7731\n",
      "Epoch [1/1], Step [1046/7635], Loss: 4.6936\n",
      "Epoch [1/1], Step [1047/7635], Loss: 4.7572\n",
      "Epoch [1/1], Step [1048/7635], Loss: 4.8262\n",
      "Epoch [1/1], Step [1049/7635], Loss: 4.8333\n",
      "Epoch [1/1], Step [1050/7635], Loss: 4.8153\n",
      "Epoch [1/1], Step [1051/7635], Loss: 4.8178\n",
      "Epoch [1/1], Step [1052/7635], Loss: 4.7113\n",
      "Epoch [1/1], Step [1053/7635], Loss: 4.8196\n",
      "Epoch [1/1], Step [1054/7635], Loss: 4.7725\n",
      "Epoch [1/1], Step [1055/7635], Loss: 4.7652\n",
      "Epoch [1/1], Step [1056/7635], Loss: 4.7626\n",
      "Epoch [1/1], Step [1057/7635], Loss: 4.7810\n",
      "Epoch [1/1], Step [1058/7635], Loss: 4.7669\n",
      "Epoch [1/1], Step [1059/7635], Loss: 4.7698\n",
      "Epoch [1/1], Step [1060/7635], Loss: 4.7746\n",
      "Epoch [1/1], Step [1061/7635], Loss: 4.8436\n",
      "Epoch [1/1], Step [1062/7635], Loss: 4.7888\n",
      "Epoch [1/1], Step [1063/7635], Loss: 4.8202\n",
      "Epoch [1/1], Step [1064/7635], Loss: 4.7909\n",
      "Epoch [1/1], Step [1065/7635], Loss: 4.8276\n",
      "Epoch [1/1], Step [1066/7635], Loss: 4.7448\n",
      "Epoch [1/1], Step [1067/7635], Loss: 4.7504\n",
      "Epoch [1/1], Step [1068/7635], Loss: 4.8034\n",
      "Epoch [1/1], Step [1069/7635], Loss: 4.6675\n",
      "Epoch [1/1], Step [1070/7635], Loss: 4.7155\n",
      "Epoch [1/1], Step [1071/7635], Loss: 4.7895\n",
      "Epoch [1/1], Step [1072/7635], Loss: 4.8492\n",
      "Epoch [1/1], Step [1073/7635], Loss: 4.7405\n",
      "Epoch [1/1], Step [1074/7635], Loss: 4.7808\n",
      "Epoch [1/1], Step [1075/7635], Loss: 4.7284\n",
      "Epoch [1/1], Step [1076/7635], Loss: 4.8856\n",
      "Epoch [1/1], Step [1077/7635], Loss: 4.7959\n",
      "Epoch [1/1], Step [1078/7635], Loss: 4.8779\n",
      "Epoch [1/1], Step [1079/7635], Loss: 4.7272\n",
      "Epoch [1/1], Step [1080/7635], Loss: 4.7567\n",
      "Epoch [1/1], Step [1081/7635], Loss: 4.7720\n",
      "Epoch [1/1], Step [1082/7635], Loss: 4.7909\n",
      "Epoch [1/1], Step [1083/7635], Loss: 4.7290\n",
      "Epoch [1/1], Step [1084/7635], Loss: 4.7906\n",
      "Epoch [1/1], Step [1085/7635], Loss: 4.6922\n",
      "Epoch [1/1], Step [1086/7635], Loss: 4.7662\n",
      "Epoch [1/1], Step [1087/7635], Loss: 4.7050\n",
      "Epoch [1/1], Step [1088/7635], Loss: 4.7361\n",
      "Epoch [1/1], Step [1089/7635], Loss: 4.7057\n",
      "Epoch [1/1], Step [1090/7635], Loss: 4.8505\n",
      "Epoch [1/1], Step [1091/7635], Loss: 4.8348\n",
      "Epoch [1/1], Step [1092/7635], Loss: 4.7321\n",
      "Epoch [1/1], Step [1093/7635], Loss: 4.7533\n",
      "Epoch [1/1], Step [1094/7635], Loss: 4.7435\n",
      "Epoch [1/1], Step [1095/7635], Loss: 4.7837\n",
      "Epoch [1/1], Step [1096/7635], Loss: 4.8394\n",
      "Epoch [1/1], Step [1097/7635], Loss: 4.7963\n",
      "Epoch [1/1], Step [1098/7635], Loss: 4.7536\n",
      "Epoch [1/1], Step [1099/7635], Loss: 4.7823\n",
      "Epoch [1/1], Step [1100/7635], Loss: 4.8072\n",
      "Epoch [1/1], Step [1101/7635], Loss: 4.7884\n",
      "Epoch [1/1], Step [1102/7635], Loss: 4.8515\n",
      "Epoch [1/1], Step [1103/7635], Loss: 4.7440\n",
      "Epoch [1/1], Step [1104/7635], Loss: 4.7174\n",
      "Epoch [1/1], Step [1105/7635], Loss: 4.7980\n",
      "Epoch [1/1], Step [1106/7635], Loss: 4.7808\n",
      "Epoch [1/1], Step [1107/7635], Loss: 4.7291\n",
      "Epoch [1/1], Step [1108/7635], Loss: 4.7572\n",
      "Epoch [1/1], Step [1109/7635], Loss: 4.7845\n",
      "Epoch [1/1], Step [1110/7635], Loss: 4.8075\n",
      "Epoch [1/1], Step [1111/7635], Loss: 4.6847\n",
      "Epoch [1/1], Step [1112/7635], Loss: 4.6865\n",
      "Epoch [1/1], Step [1113/7635], Loss: 4.7039\n",
      "Epoch [1/1], Step [1114/7635], Loss: 4.7220\n",
      "Epoch [1/1], Step [1115/7635], Loss: 4.8750\n",
      "Epoch [1/1], Step [1116/7635], Loss: 4.7250\n",
      "Epoch [1/1], Step [1117/7635], Loss: 4.6850\n",
      "Epoch [1/1], Step [1118/7635], Loss: 4.6064\n",
      "Epoch [1/1], Step [1119/7635], Loss: 4.7682\n",
      "Epoch [1/1], Step [1120/7635], Loss: 4.7444\n",
      "Epoch [1/1], Step [1121/7635], Loss: 4.7140\n",
      "Epoch [1/1], Step [1122/7635], Loss: 4.7470\n",
      "Epoch [1/1], Step [1123/7635], Loss: 4.7077\n",
      "Epoch [1/1], Step [1124/7635], Loss: 4.8085\n",
      "Epoch [1/1], Step [1125/7635], Loss: 4.7367\n",
      "Epoch [1/1], Step [1126/7635], Loss: 4.6067\n",
      "Epoch [1/1], Step [1127/7635], Loss: 4.7604\n",
      "Epoch [1/1], Step [1128/7635], Loss: 4.6979\n",
      "Epoch [1/1], Step [1129/7635], Loss: 4.7244\n",
      "Epoch [1/1], Step [1130/7635], Loss: 4.7526\n",
      "Epoch [1/1], Step [1131/7635], Loss: 4.7839\n",
      "Epoch [1/1], Step [1132/7635], Loss: 4.8543\n",
      "Epoch [1/1], Step [1133/7635], Loss: 4.8588\n",
      "Epoch [1/1], Step [1134/7635], Loss: 4.7495\n",
      "Epoch [1/1], Step [1135/7635], Loss: 4.7001\n",
      "Epoch [1/1], Step [1136/7635], Loss: 4.8130\n",
      "Epoch [1/1], Step [1137/7635], Loss: 4.7182\n",
      "Epoch [1/1], Step [1138/7635], Loss: 4.7092\n",
      "Epoch [1/1], Step [1139/7635], Loss: 4.8097\n",
      "Epoch [1/1], Step [1140/7635], Loss: 4.8274\n",
      "Epoch [1/1], Step [1141/7635], Loss: 4.7639\n",
      "Epoch [1/1], Step [1142/7635], Loss: 4.6894\n",
      "Epoch [1/1], Step [1143/7635], Loss: 4.7232\n",
      "Epoch [1/1], Step [1144/7635], Loss: 4.7255\n",
      "Epoch [1/1], Step [1145/7635], Loss: 4.8000\n",
      "Epoch [1/1], Step [1146/7635], Loss: 4.7911\n",
      "Epoch [1/1], Step [1147/7635], Loss: 4.6601\n",
      "Epoch [1/1], Step [1148/7635], Loss: 4.7852\n",
      "Epoch [1/1], Step [1149/7635], Loss: 4.8650\n",
      "Epoch [1/1], Step [1150/7635], Loss: 4.6940\n",
      "Epoch [1/1], Step [1151/7635], Loss: 4.6954\n",
      "Epoch [1/1], Step [1152/7635], Loss: 4.7992\n",
      "Epoch [1/1], Step [1153/7635], Loss: 4.5953\n",
      "Epoch [1/1], Step [1154/7635], Loss: 4.6942\n",
      "Epoch [1/1], Step [1155/7635], Loss: 4.8256\n",
      "Epoch [1/1], Step [1156/7635], Loss: 4.7390\n",
      "Epoch [1/1], Step [1157/7635], Loss: 4.7109\n",
      "Epoch [1/1], Step [1158/7635], Loss: 4.7079\n",
      "Epoch [1/1], Step [1159/7635], Loss: 4.7132\n",
      "Epoch [1/1], Step [1160/7635], Loss: 4.7379\n",
      "Epoch [1/1], Step [1161/7635], Loss: 4.7016\n",
      "Epoch [1/1], Step [1162/7635], Loss: 4.7862\n",
      "Epoch [1/1], Step [1163/7635], Loss: 4.7197\n",
      "Epoch [1/1], Step [1164/7635], Loss: 4.7641\n",
      "Epoch [1/1], Step [1165/7635], Loss: 4.7591\n",
      "Epoch [1/1], Step [1166/7635], Loss: 4.7072\n",
      "Epoch [1/1], Step [1167/7635], Loss: 4.6168\n",
      "Epoch [1/1], Step [1168/7635], Loss: 4.7506\n",
      "Epoch [1/1], Step [1169/7635], Loss: 4.8056\n",
      "Epoch [1/1], Step [1170/7635], Loss: 4.7009\n",
      "Epoch [1/1], Step [1171/7635], Loss: 4.7605\n",
      "Epoch [1/1], Step [1172/7635], Loss: 4.7443\n",
      "Epoch [1/1], Step [1173/7635], Loss: 4.7875\n",
      "Epoch [1/1], Step [1174/7635], Loss: 4.7668\n",
      "Epoch [1/1], Step [1175/7635], Loss: 4.7230\n",
      "Epoch [1/1], Step [1176/7635], Loss: 4.7375\n",
      "Epoch [1/1], Step [1177/7635], Loss: 4.7535\n",
      "Epoch [1/1], Step [1178/7635], Loss: 4.6884\n",
      "Epoch [1/1], Step [1179/7635], Loss: 4.7485\n",
      "Epoch [1/1], Step [1180/7635], Loss: 4.6745\n",
      "Epoch [1/1], Step [1181/7635], Loss: 4.6763\n",
      "Epoch [1/1], Step [1182/7635], Loss: 4.6452\n",
      "Epoch [1/1], Step [1183/7635], Loss: 4.7153\n",
      "Epoch [1/1], Step [1184/7635], Loss: 4.6907\n",
      "Epoch [1/1], Step [1185/7635], Loss: 4.7148\n",
      "Epoch [1/1], Step [1186/7635], Loss: 4.6528\n",
      "Epoch [1/1], Step [1187/7635], Loss: 4.7089\n",
      "Epoch [1/1], Step [1188/7635], Loss: 4.7657\n",
      "Epoch [1/1], Step [1189/7635], Loss: 4.6455\n",
      "Epoch [1/1], Step [1190/7635], Loss: 4.6252\n",
      "Epoch [1/1], Step [1191/7635], Loss: 4.5831\n",
      "Epoch [1/1], Step [1192/7635], Loss: 4.7575\n",
      "Epoch [1/1], Step [1193/7635], Loss: 4.6905\n",
      "Epoch [1/1], Step [1194/7635], Loss: 4.6962\n",
      "Epoch [1/1], Step [1195/7635], Loss: 4.8093\n",
      "Epoch [1/1], Step [1196/7635], Loss: 4.7188\n",
      "Epoch [1/1], Step [1197/7635], Loss: 4.7848\n",
      "Epoch [1/1], Step [1198/7635], Loss: 4.7335\n",
      "Epoch [1/1], Step [1199/7635], Loss: 4.6111\n",
      "Epoch [1/1], Step [1200/7635], Loss: 4.7626\n",
      "Epoch [1/1], Step [1201/7635], Loss: 4.8094\n",
      "Epoch [1/1], Step [1202/7635], Loss: 4.7768\n",
      "Epoch [1/1], Step [1203/7635], Loss: 4.6970\n",
      "Epoch [1/1], Step [1204/7635], Loss: 4.6339\n",
      "Epoch [1/1], Step [1205/7635], Loss: 4.6235\n",
      "Epoch [1/1], Step [1206/7635], Loss: 4.5570\n",
      "Epoch [1/1], Step [1207/7635], Loss: 4.6723\n",
      "Epoch [1/1], Step [1208/7635], Loss: 4.8021\n",
      "Epoch [1/1], Step [1209/7635], Loss: 4.6151\n",
      "Epoch [1/1], Step [1210/7635], Loss: 4.7618\n",
      "Epoch [1/1], Step [1211/7635], Loss: 4.7326\n",
      "Epoch [1/1], Step [1212/7635], Loss: 4.7031\n",
      "Epoch [1/1], Step [1213/7635], Loss: 4.6595\n",
      "Epoch [1/1], Step [1214/7635], Loss: 4.7003\n",
      "Epoch [1/1], Step [1215/7635], Loss: 4.7150\n",
      "Epoch [1/1], Step [1216/7635], Loss: 4.7111\n",
      "Epoch [1/1], Step [1217/7635], Loss: 4.7668\n",
      "Epoch [1/1], Step [1218/7635], Loss: 4.7691\n",
      "Epoch [1/1], Step [1219/7635], Loss: 4.7282\n",
      "Epoch [1/1], Step [1220/7635], Loss: 4.7968\n",
      "Epoch [1/1], Step [1221/7635], Loss: 4.6415\n",
      "Epoch [1/1], Step [1222/7635], Loss: 4.7868\n",
      "Epoch [1/1], Step [1223/7635], Loss: 4.6854\n",
      "Epoch [1/1], Step [1224/7635], Loss: 4.6543\n",
      "Epoch [1/1], Step [1225/7635], Loss: 4.7504\n",
      "Epoch [1/1], Step [1226/7635], Loss: 4.5913\n",
      "Epoch [1/1], Step [1227/7635], Loss: 4.7464\n",
      "Epoch [1/1], Step [1228/7635], Loss: 4.6874\n",
      "Epoch [1/1], Step [1229/7635], Loss: 4.6490\n",
      "Epoch [1/1], Step [1230/7635], Loss: 4.6303\n",
      "Epoch [1/1], Step [1231/7635], Loss: 4.6954\n",
      "Epoch [1/1], Step [1232/7635], Loss: 4.6066\n",
      "Epoch [1/1], Step [1233/7635], Loss: 4.5927\n",
      "Epoch [1/1], Step [1234/7635], Loss: 4.6919\n",
      "Epoch [1/1], Step [1235/7635], Loss: 4.6781\n",
      "Epoch [1/1], Step [1236/7635], Loss: 4.6760\n",
      "Epoch [1/1], Step [1237/7635], Loss: 4.7172\n",
      "Epoch [1/1], Step [1238/7635], Loss: 4.6895\n",
      "Epoch [1/1], Step [1239/7635], Loss: 4.7026\n",
      "Epoch [1/1], Step [1240/7635], Loss: 4.8321\n",
      "Epoch [1/1], Step [1241/7635], Loss: 4.7871\n",
      "Epoch [1/1], Step [1242/7635], Loss: 4.7393\n",
      "Epoch [1/1], Step [1243/7635], Loss: 4.6794\n",
      "Epoch [1/1], Step [1244/7635], Loss: 4.7268\n",
      "Epoch [1/1], Step [1245/7635], Loss: 4.7481\n",
      "Epoch [1/1], Step [1246/7635], Loss: 4.5796\n",
      "Epoch [1/1], Step [1247/7635], Loss: 4.7201\n",
      "Epoch [1/1], Step [1248/7635], Loss: 4.6330\n",
      "Epoch [1/1], Step [1249/7635], Loss: 4.6970\n",
      "Epoch [1/1], Step [1250/7635], Loss: 4.6720\n",
      "Epoch [1/1], Step [1251/7635], Loss: 4.7257\n",
      "Epoch [1/1], Step [1252/7635], Loss: 4.7456\n",
      "Epoch [1/1], Step [1253/7635], Loss: 4.7493\n",
      "Epoch [1/1], Step [1254/7635], Loss: 4.7565\n",
      "Epoch [1/1], Step [1255/7635], Loss: 4.6559\n",
      "Epoch [1/1], Step [1256/7635], Loss: 4.7023\n",
      "Epoch [1/1], Step [1257/7635], Loss: 4.6040\n",
      "Epoch [1/1], Step [1258/7635], Loss: 4.6436\n",
      "Epoch [1/1], Step [1259/7635], Loss: 4.6841\n",
      "Epoch [1/1], Step [1260/7635], Loss: 4.6423\n",
      "Epoch [1/1], Step [1261/7635], Loss: 4.7397\n",
      "Epoch [1/1], Step [1262/7635], Loss: 4.7132\n",
      "Epoch [1/1], Step [1263/7635], Loss: 4.6343\n",
      "Epoch [1/1], Step [1264/7635], Loss: 4.7875\n",
      "Epoch [1/1], Step [1265/7635], Loss: 4.5837\n",
      "Epoch [1/1], Step [1266/7635], Loss: 4.7014\n",
      "Epoch [1/1], Step [1267/7635], Loss: 4.7515\n",
      "Epoch [1/1], Step [1268/7635], Loss: 4.6900\n",
      "Epoch [1/1], Step [1269/7635], Loss: 4.5850\n",
      "Epoch [1/1], Step [1270/7635], Loss: 4.6523\n",
      "Epoch [1/1], Step [1271/7635], Loss: 4.7415\n",
      "Epoch [1/1], Step [1272/7635], Loss: 4.6401\n",
      "Epoch [1/1], Step [1273/7635], Loss: 4.6808\n",
      "Epoch [1/1], Step [1274/7635], Loss: 4.7318\n",
      "Epoch [1/1], Step [1275/7635], Loss: 4.7093\n",
      "Epoch [1/1], Step [1276/7635], Loss: 4.7644\n",
      "Epoch [1/1], Step [1277/7635], Loss: 4.7300\n",
      "Epoch [1/1], Step [1278/7635], Loss: 4.7061\n",
      "Epoch [1/1], Step [1279/7635], Loss: 4.7355\n",
      "Epoch [1/1], Step [1280/7635], Loss: 4.7394\n",
      "Epoch [1/1], Step [1281/7635], Loss: 4.6138\n",
      "Epoch [1/1], Step [1282/7635], Loss: 4.7030\n",
      "Epoch [1/1], Step [1283/7635], Loss: 4.6877\n",
      "Epoch [1/1], Step [1284/7635], Loss: 4.6496\n",
      "Epoch [1/1], Step [1285/7635], Loss: 4.6679\n",
      "Epoch [1/1], Step [1286/7635], Loss: 4.6802\n",
      "Epoch [1/1], Step [1287/7635], Loss: 4.7105\n",
      "Epoch [1/1], Step [1288/7635], Loss: 4.5429\n",
      "Epoch [1/1], Step [1289/7635], Loss: 4.6047\n",
      "Epoch [1/1], Step [1290/7635], Loss: 4.6372\n",
      "Epoch [1/1], Step [1291/7635], Loss: 4.6230\n",
      "Epoch [1/1], Step [1292/7635], Loss: 4.7922\n",
      "Epoch [1/1], Step [1293/7635], Loss: 4.7412\n",
      "Epoch [1/1], Step [1294/7635], Loss: 4.7658\n",
      "Epoch [1/1], Step [1295/7635], Loss: 4.7358\n",
      "Epoch [1/1], Step [1296/7635], Loss: 4.6516\n",
      "Epoch [1/1], Step [1297/7635], Loss: 4.5724\n",
      "Epoch [1/1], Step [1298/7635], Loss: 4.6042\n",
      "Epoch [1/1], Step [1299/7635], Loss: 4.7155\n",
      "Epoch [1/1], Step [1300/7635], Loss: 4.6384\n",
      "Epoch [1/1], Step [1301/7635], Loss: 4.7234\n",
      "Epoch [1/1], Step [1302/7635], Loss: 4.6561\n",
      "Epoch [1/1], Step [1303/7635], Loss: 4.6600\n",
      "Epoch [1/1], Step [1304/7635], Loss: 4.6063\n",
      "Epoch [1/1], Step [1305/7635], Loss: 4.8039\n",
      "Epoch [1/1], Step [1306/7635], Loss: 4.7213\n",
      "Epoch [1/1], Step [1307/7635], Loss: 4.7536\n",
      "Epoch [1/1], Step [1308/7635], Loss: 4.5959\n",
      "Epoch [1/1], Step [1309/7635], Loss: 4.7318\n",
      "Epoch [1/1], Step [1310/7635], Loss: 4.6715\n",
      "Epoch [1/1], Step [1311/7635], Loss: 4.6685\n",
      "Epoch [1/1], Step [1312/7635], Loss: 4.6437\n",
      "Epoch [1/1], Step [1313/7635], Loss: 4.6707\n",
      "Epoch [1/1], Step [1314/7635], Loss: 4.6343\n",
      "Epoch [1/1], Step [1315/7635], Loss: 4.7256\n",
      "Epoch [1/1], Step [1316/7635], Loss: 4.6873\n",
      "Epoch [1/1], Step [1317/7635], Loss: 4.6794\n",
      "Epoch [1/1], Step [1318/7635], Loss: 4.7538\n",
      "Epoch [1/1], Step [1319/7635], Loss: 4.7017\n",
      "Epoch [1/1], Step [1320/7635], Loss: 4.7057\n",
      "Epoch [1/1], Step [1321/7635], Loss: 4.6624\n",
      "Epoch [1/1], Step [1322/7635], Loss: 4.6405\n",
      "Epoch [1/1], Step [1323/7635], Loss: 4.7152\n",
      "Epoch [1/1], Step [1324/7635], Loss: 4.6794\n",
      "Epoch [1/1], Step [1325/7635], Loss: 4.5424\n",
      "Epoch [1/1], Step [1326/7635], Loss: 4.6466\n",
      "Epoch [1/1], Step [1327/7635], Loss: 4.6136\n",
      "Epoch [1/1], Step [1328/7635], Loss: 4.6881\n",
      "Epoch [1/1], Step [1329/7635], Loss: 4.7179\n",
      "Epoch [1/1], Step [1330/7635], Loss: 4.6924\n",
      "Epoch [1/1], Step [1331/7635], Loss: 4.6986\n",
      "Epoch [1/1], Step [1332/7635], Loss: 4.6293\n",
      "Epoch [1/1], Step [1333/7635], Loss: 4.7317\n",
      "Epoch [1/1], Step [1334/7635], Loss: 4.7350\n",
      "Epoch [1/1], Step [1335/7635], Loss: 4.6738\n",
      "Epoch [1/1], Step [1336/7635], Loss: 4.6564\n",
      "Epoch [1/1], Step [1337/7635], Loss: 4.6354\n",
      "Epoch [1/1], Step [1338/7635], Loss: 4.6581\n",
      "Epoch [1/1], Step [1339/7635], Loss: 4.6969\n",
      "Epoch [1/1], Step [1340/7635], Loss: 4.6061\n",
      "Epoch [1/1], Step [1341/7635], Loss: 4.6427\n",
      "Epoch [1/1], Step [1342/7635], Loss: 4.7593\n",
      "Epoch [1/1], Step [1343/7635], Loss: 4.6282\n",
      "Epoch [1/1], Step [1344/7635], Loss: 4.6704\n",
      "Epoch [1/1], Step [1345/7635], Loss: 4.7472\n",
      "Epoch [1/1], Step [1346/7635], Loss: 4.7541\n",
      "Epoch [1/1], Step [1347/7635], Loss: 4.5361\n",
      "Epoch [1/1], Step [1348/7635], Loss: 4.6060\n",
      "Epoch [1/1], Step [1349/7635], Loss: 4.7329\n",
      "Epoch [1/1], Step [1350/7635], Loss: 4.6412\n",
      "Epoch [1/1], Step [1351/7635], Loss: 4.6624\n",
      "Epoch [1/1], Step [1352/7635], Loss: 4.6609\n",
      "Epoch [1/1], Step [1353/7635], Loss: 4.6388\n",
      "Epoch [1/1], Step [1354/7635], Loss: 4.6269\n",
      "Epoch [1/1], Step [1355/7635], Loss: 4.7557\n",
      "Epoch [1/1], Step [1356/7635], Loss: 4.6552\n",
      "Epoch [1/1], Step [1357/7635], Loss: 4.6465\n",
      "Epoch [1/1], Step [1358/7635], Loss: 4.6360\n",
      "Epoch [1/1], Step [1359/7635], Loss: 4.7049\n",
      "Epoch [1/1], Step [1360/7635], Loss: 4.5607\n",
      "Epoch [1/1], Step [1361/7635], Loss: 4.6814\n",
      "Epoch [1/1], Step [1362/7635], Loss: 4.5903\n",
      "Epoch [1/1], Step [1363/7635], Loss: 4.6558\n",
      "Epoch [1/1], Step [1364/7635], Loss: 4.6759\n",
      "Epoch [1/1], Step [1365/7635], Loss: 4.6950\n",
      "Epoch [1/1], Step [1366/7635], Loss: 4.7181\n",
      "Epoch [1/1], Step [1367/7635], Loss: 4.7387\n",
      "Epoch [1/1], Step [1368/7635], Loss: 4.6601\n",
      "Epoch [1/1], Step [1369/7635], Loss: 4.6458\n",
      "Epoch [1/1], Step [1370/7635], Loss: 4.5991\n",
      "Epoch [1/1], Step [1371/7635], Loss: 4.6513\n",
      "Epoch [1/1], Step [1372/7635], Loss: 4.6299\n",
      "Epoch [1/1], Step [1373/7635], Loss: 4.6261\n",
      "Epoch [1/1], Step [1374/7635], Loss: 4.7099\n",
      "Epoch [1/1], Step [1375/7635], Loss: 4.7173\n",
      "Epoch [1/1], Step [1376/7635], Loss: 4.6220\n",
      "Epoch [1/1], Step [1377/7635], Loss: 4.6178\n",
      "Epoch [1/1], Step [1378/7635], Loss: 4.6592\n",
      "Epoch [1/1], Step [1379/7635], Loss: 4.5423\n",
      "Epoch [1/1], Step [1380/7635], Loss: 4.5513\n",
      "Epoch [1/1], Step [1381/7635], Loss: 4.7440\n",
      "Epoch [1/1], Step [1382/7635], Loss: 4.6056\n",
      "Epoch [1/1], Step [1383/7635], Loss: 4.6382\n",
      "Epoch [1/1], Step [1384/7635], Loss: 4.5649\n",
      "Epoch [1/1], Step [1385/7635], Loss: 4.6739\n",
      "Epoch [1/1], Step [1386/7635], Loss: 4.5661\n",
      "Epoch [1/1], Step [1387/7635], Loss: 4.6441\n",
      "Epoch [1/1], Step [1388/7635], Loss: 4.5539\n",
      "Epoch [1/1], Step [1389/7635], Loss: 4.6199\n",
      "Epoch [1/1], Step [1390/7635], Loss: 4.6063\n",
      "Epoch [1/1], Step [1391/7635], Loss: 4.6098\n",
      "Epoch [1/1], Step [1392/7635], Loss: 4.5961\n",
      "Epoch [1/1], Step [1393/7635], Loss: 4.4803\n",
      "Epoch [1/1], Step [1394/7635], Loss: 4.6245\n",
      "Epoch [1/1], Step [1395/7635], Loss: 4.7029\n",
      "Epoch [1/1], Step [1396/7635], Loss: 4.6852\n",
      "Epoch [1/1], Step [1397/7635], Loss: 4.6473\n",
      "Epoch [1/1], Step [1398/7635], Loss: 4.7030\n",
      "Epoch [1/1], Step [1399/7635], Loss: 4.5913\n",
      "Epoch [1/1], Step [1400/7635], Loss: 4.6065\n",
      "Epoch [1/1], Step [1401/7635], Loss: 4.6392\n",
      "Epoch [1/1], Step [1402/7635], Loss: 4.6086\n",
      "Epoch [1/1], Step [1403/7635], Loss: 4.5266\n",
      "Epoch [1/1], Step [1404/7635], Loss: 4.5662\n",
      "Epoch [1/1], Step [1405/7635], Loss: 4.6064\n",
      "Epoch [1/1], Step [1406/7635], Loss: 4.6352\n",
      "Epoch [1/1], Step [1407/7635], Loss: 4.6223\n",
      "Epoch [1/1], Step [1408/7635], Loss: 4.6255\n",
      "Epoch [1/1], Step [1409/7635], Loss: 4.6470\n",
      "Epoch [1/1], Step [1410/7635], Loss: 4.5656\n",
      "Epoch [1/1], Step [1411/7635], Loss: 4.6579\n",
      "Epoch [1/1], Step [1412/7635], Loss: 4.6700\n",
      "Epoch [1/1], Step [1413/7635], Loss: 4.5748\n",
      "Epoch [1/1], Step [1414/7635], Loss: 4.6644\n",
      "Epoch [1/1], Step [1415/7635], Loss: 4.6320\n",
      "Epoch [1/1], Step [1416/7635], Loss: 4.6499\n",
      "Epoch [1/1], Step [1417/7635], Loss: 4.6029\n",
      "Epoch [1/1], Step [1418/7635], Loss: 4.5657\n",
      "Epoch [1/1], Step [1419/7635], Loss: 4.7063\n",
      "Epoch [1/1], Step [1420/7635], Loss: 4.5747\n",
      "Epoch [1/1], Step [1421/7635], Loss: 4.6985\n",
      "Epoch [1/1], Step [1422/7635], Loss: 4.6436\n",
      "Epoch [1/1], Step [1423/7635], Loss: 4.6010\n",
      "Epoch [1/1], Step [1424/7635], Loss: 4.6105\n",
      "Epoch [1/1], Step [1425/7635], Loss: 4.6300\n",
      "Epoch [1/1], Step [1426/7635], Loss: 4.5759\n",
      "Epoch [1/1], Step [1427/7635], Loss: 4.6264\n",
      "Epoch [1/1], Step [1428/7635], Loss: 4.6543\n",
      "Epoch [1/1], Step [1429/7635], Loss: 4.6541\n",
      "Epoch [1/1], Step [1430/7635], Loss: 4.6234\n",
      "Epoch [1/1], Step [1431/7635], Loss: 4.6123\n",
      "Epoch [1/1], Step [1432/7635], Loss: 4.6004\n",
      "Epoch [1/1], Step [1433/7635], Loss: 4.6998\n",
      "Epoch [1/1], Step [1434/7635], Loss: 4.6063\n",
      "Epoch [1/1], Step [1435/7635], Loss: 4.6632\n",
      "Epoch [1/1], Step [1436/7635], Loss: 4.7476\n",
      "Epoch [1/1], Step [1437/7635], Loss: 4.6121\n",
      "Epoch [1/1], Step [1438/7635], Loss: 4.6209\n",
      "Epoch [1/1], Step [1439/7635], Loss: 4.6394\n",
      "Epoch [1/1], Step [1440/7635], Loss: 4.6552\n",
      "Epoch [1/1], Step [1441/7635], Loss: 4.6271\n",
      "Epoch [1/1], Step [1442/7635], Loss: 4.6248\n",
      "Epoch [1/1], Step [1443/7635], Loss: 4.6070\n",
      "Epoch [1/1], Step [1444/7635], Loss: 4.6458\n",
      "Epoch [1/1], Step [1445/7635], Loss: 4.6479\n",
      "Epoch [1/1], Step [1446/7635], Loss: 4.6250\n",
      "Epoch [1/1], Step [1447/7635], Loss: 4.6950\n",
      "Epoch [1/1], Step [1448/7635], Loss: 4.6151\n",
      "Epoch [1/1], Step [1449/7635], Loss: 4.6376\n",
      "Epoch [1/1], Step [1450/7635], Loss: 4.6880\n",
      "Epoch [1/1], Step [1451/7635], Loss: 4.7368\n",
      "Epoch [1/1], Step [1452/7635], Loss: 4.5751\n",
      "Epoch [1/1], Step [1453/7635], Loss: 4.6272\n",
      "Epoch [1/1], Step [1454/7635], Loss: 4.6816\n",
      "Epoch [1/1], Step [1455/7635], Loss: 4.6709\n",
      "Epoch [1/1], Step [1456/7635], Loss: 4.5482\n",
      "Epoch [1/1], Step [1457/7635], Loss: 4.5729\n",
      "Epoch [1/1], Step [1458/7635], Loss: 4.5646\n",
      "Epoch [1/1], Step [1459/7635], Loss: 4.7100\n",
      "Epoch [1/1], Step [1460/7635], Loss: 4.6090\n",
      "Epoch [1/1], Step [1461/7635], Loss: 4.6474\n",
      "Epoch [1/1], Step [1462/7635], Loss: 4.5620\n",
      "Epoch [1/1], Step [1463/7635], Loss: 4.6614\n",
      "Epoch [1/1], Step [1464/7635], Loss: 4.6487\n",
      "Epoch [1/1], Step [1465/7635], Loss: 4.5301\n",
      "Epoch [1/1], Step [1466/7635], Loss: 4.5940\n",
      "Epoch [1/1], Step [1467/7635], Loss: 4.5516\n",
      "Epoch [1/1], Step [1468/7635], Loss: 4.6250\n",
      "Epoch [1/1], Step [1469/7635], Loss: 4.5389\n",
      "Epoch [1/1], Step [1470/7635], Loss: 4.6173\n",
      "Epoch [1/1], Step [1471/7635], Loss: 4.6278\n",
      "Epoch [1/1], Step [1472/7635], Loss: 4.6211\n",
      "Epoch [1/1], Step [1473/7635], Loss: 4.5465\n",
      "Epoch [1/1], Step [1474/7635], Loss: 4.5897\n",
      "Epoch [1/1], Step [1475/7635], Loss: 4.6147\n",
      "Epoch [1/1], Step [1476/7635], Loss: 4.5122\n",
      "Epoch [1/1], Step [1477/7635], Loss: 4.5869\n",
      "Epoch [1/1], Step [1478/7635], Loss: 4.6311\n",
      "Epoch [1/1], Step [1479/7635], Loss: 4.6684\n",
      "Epoch [1/1], Step [1480/7635], Loss: 4.5823\n",
      "Epoch [1/1], Step [1481/7635], Loss: 4.5820\n",
      "Epoch [1/1], Step [1482/7635], Loss: 4.6419\n",
      "Epoch [1/1], Step [1483/7635], Loss: 4.6197\n",
      "Epoch [1/1], Step [1484/7635], Loss: 4.6175\n",
      "Epoch [1/1], Step [1485/7635], Loss: 4.5338\n",
      "Epoch [1/1], Step [1486/7635], Loss: 4.6502\n",
      "Epoch [1/1], Step [1487/7635], Loss: 4.7314\n",
      "Epoch [1/1], Step [1488/7635], Loss: 4.6477\n",
      "Epoch [1/1], Step [1489/7635], Loss: 4.5920\n",
      "Epoch [1/1], Step [1490/7635], Loss: 4.5503\n",
      "Epoch [1/1], Step [1491/7635], Loss: 4.5251\n",
      "Epoch [1/1], Step [1492/7635], Loss: 4.5446\n",
      "Epoch [1/1], Step [1493/7635], Loss: 4.6313\n",
      "Epoch [1/1], Step [1494/7635], Loss: 4.6821\n",
      "Epoch [1/1], Step [1495/7635], Loss: 4.6527\n",
      "Epoch [1/1], Step [1496/7635], Loss: 4.5947\n",
      "Epoch [1/1], Step [1497/7635], Loss: 4.6421\n",
      "Epoch [1/1], Step [1498/7635], Loss: 4.6862\n",
      "Epoch [1/1], Step [1499/7635], Loss: 4.6185\n",
      "Epoch [1/1], Step [1500/7635], Loss: 4.5805\n",
      "Epoch [1/1], Step [1501/7635], Loss: 4.5933\n",
      "Epoch [1/1], Step [1502/7635], Loss: 4.6235\n",
      "Epoch [1/1], Step [1503/7635], Loss: 4.5033\n",
      "Epoch [1/1], Step [1504/7635], Loss: 4.6349\n",
      "Epoch [1/1], Step [1505/7635], Loss: 4.5627\n",
      "Epoch [1/1], Step [1506/7635], Loss: 4.6511\n",
      "Epoch [1/1], Step [1507/7635], Loss: 4.6098\n",
      "Epoch [1/1], Step [1508/7635], Loss: 4.6235\n",
      "Epoch [1/1], Step [1509/7635], Loss: 4.6659\n",
      "Epoch [1/1], Step [1510/7635], Loss: 4.7211\n",
      "Epoch [1/1], Step [1511/7635], Loss: 4.8211\n",
      "Epoch [1/1], Step [1512/7635], Loss: 4.6837\n",
      "Epoch [1/1], Step [1513/7635], Loss: 4.6723\n",
      "Epoch [1/1], Step [1514/7635], Loss: 4.6524\n",
      "Epoch [1/1], Step [1515/7635], Loss: 4.5109\n",
      "Epoch [1/1], Step [1516/7635], Loss: 4.6811\n",
      "Epoch [1/1], Step [1517/7635], Loss: 4.6025\n",
      "Epoch [1/1], Step [1518/7635], Loss: 4.6249\n",
      "Epoch [1/1], Step [1519/7635], Loss: 4.5249\n",
      "Epoch [1/1], Step [1520/7635], Loss: 4.6127\n",
      "Epoch [1/1], Step [1521/7635], Loss: 4.5671\n",
      "Epoch [1/1], Step [1522/7635], Loss: 4.6100\n",
      "Epoch [1/1], Step [1523/7635], Loss: 4.5822\n",
      "Epoch [1/1], Step [1524/7635], Loss: 4.5431\n",
      "Epoch [1/1], Step [1525/7635], Loss: 4.6260\n",
      "Epoch [1/1], Step [1526/7635], Loss: 4.6450\n",
      "Epoch [1/1], Step [1527/7635], Loss: 4.5123\n",
      "Epoch [1/1], Step [1528/7635], Loss: 4.6256\n",
      "Epoch [1/1], Step [1529/7635], Loss: 4.5322\n",
      "Epoch [1/1], Step [1530/7635], Loss: 4.4751\n",
      "Epoch [1/1], Step [1531/7635], Loss: 4.5958\n",
      "Epoch [1/1], Step [1532/7635], Loss: 4.7068\n",
      "Epoch [1/1], Step [1533/7635], Loss: 4.6248\n",
      "Epoch [1/1], Step [1534/7635], Loss: 4.5962\n",
      "Epoch [1/1], Step [1535/7635], Loss: 4.5450\n",
      "Epoch [1/1], Step [1536/7635], Loss: 4.6407\n",
      "Epoch [1/1], Step [1537/7635], Loss: 4.6051\n",
      "Epoch [1/1], Step [1538/7635], Loss: 4.5418\n",
      "Epoch [1/1], Step [1539/7635], Loss: 4.6256\n",
      "Epoch [1/1], Step [1540/7635], Loss: 4.5162\n",
      "Epoch [1/1], Step [1541/7635], Loss: 4.5475\n",
      "Epoch [1/1], Step [1542/7635], Loss: 4.5415\n",
      "Epoch [1/1], Step [1543/7635], Loss: 4.6196\n",
      "Epoch [1/1], Step [1544/7635], Loss: 4.5545\n",
      "Epoch [1/1], Step [1545/7635], Loss: 4.6529\n",
      "Epoch [1/1], Step [1546/7635], Loss: 4.7114\n",
      "Epoch [1/1], Step [1547/7635], Loss: 4.5788\n",
      "Epoch [1/1], Step [1548/7635], Loss: 4.5146\n",
      "Epoch [1/1], Step [1549/7635], Loss: 4.5546\n",
      "Epoch [1/1], Step [1550/7635], Loss: 4.5842\n",
      "Epoch [1/1], Step [1551/7635], Loss: 4.5684\n",
      "Epoch [1/1], Step [1552/7635], Loss: 4.5173\n",
      "Epoch [1/1], Step [1553/7635], Loss: 4.5480\n",
      "Epoch [1/1], Step [1554/7635], Loss: 4.5619\n",
      "Epoch [1/1], Step [1555/7635], Loss: 4.5698\n",
      "Epoch [1/1], Step [1556/7635], Loss: 4.8025\n",
      "Epoch [1/1], Step [1557/7635], Loss: 4.6444\n",
      "Epoch [1/1], Step [1558/7635], Loss: 4.5945\n",
      "Epoch [1/1], Step [1559/7635], Loss: 4.6318\n",
      "Epoch [1/1], Step [1560/7635], Loss: 4.6922\n",
      "Epoch [1/1], Step [1561/7635], Loss: 4.6197\n",
      "Epoch [1/1], Step [1562/7635], Loss: 4.5917\n",
      "Epoch [1/1], Step [1563/7635], Loss: 4.5584\n",
      "Epoch [1/1], Step [1564/7635], Loss: 4.5482\n",
      "Epoch [1/1], Step [1565/7635], Loss: 4.5890\n",
      "Epoch [1/1], Step [1566/7635], Loss: 4.4957\n",
      "Epoch [1/1], Step [1567/7635], Loss: 4.5925\n",
      "Epoch [1/1], Step [1568/7635], Loss: 4.6297\n",
      "Epoch [1/1], Step [1569/7635], Loss: 4.5821\n",
      "Epoch [1/1], Step [1570/7635], Loss: 4.6927\n",
      "Epoch [1/1], Step [1571/7635], Loss: 4.5290\n",
      "Epoch [1/1], Step [1572/7635], Loss: 4.6464\n",
      "Epoch [1/1], Step [1573/7635], Loss: 4.6106\n",
      "Epoch [1/1], Step [1574/7635], Loss: 4.5328\n",
      "Epoch [1/1], Step [1575/7635], Loss: 4.5437\n",
      "Epoch [1/1], Step [1576/7635], Loss: 4.5445\n",
      "Epoch [1/1], Step [1577/7635], Loss: 4.5756\n",
      "Epoch [1/1], Step [1578/7635], Loss: 4.5876\n",
      "Epoch [1/1], Step [1579/7635], Loss: 4.4349\n",
      "Epoch [1/1], Step [1580/7635], Loss: 4.6352\n",
      "Epoch [1/1], Step [1581/7635], Loss: 4.5618\n",
      "Epoch [1/1], Step [1582/7635], Loss: 4.6778\n",
      "Epoch [1/1], Step [1583/7635], Loss: 4.5635\n",
      "Epoch [1/1], Step [1584/7635], Loss: 4.5708\n",
      "Epoch [1/1], Step [1585/7635], Loss: 4.6333\n",
      "Epoch [1/1], Step [1586/7635], Loss: 4.6118\n",
      "Epoch [1/1], Step [1587/7635], Loss: 4.6083\n",
      "Epoch [1/1], Step [1588/7635], Loss: 4.5416\n",
      "Epoch [1/1], Step [1589/7635], Loss: 4.5938\n",
      "Epoch [1/1], Step [1590/7635], Loss: 4.5165\n",
      "Epoch [1/1], Step [1591/7635], Loss: 4.6543\n",
      "Epoch [1/1], Step [1592/7635], Loss: 4.5551\n",
      "Epoch [1/1], Step [1593/7635], Loss: 4.5476\n",
      "Epoch [1/1], Step [1594/7635], Loss: 4.5850\n",
      "Epoch [1/1], Step [1595/7635], Loss: 4.5611\n",
      "Epoch [1/1], Step [1596/7635], Loss: 4.5228\n",
      "Epoch [1/1], Step [1597/7635], Loss: 4.4895\n",
      "Epoch [1/1], Step [1598/7635], Loss: 4.5154\n",
      "Epoch [1/1], Step [1599/7635], Loss: 4.5107\n",
      "Epoch [1/1], Step [1600/7635], Loss: 4.6887\n",
      "Epoch [1/1], Step [1601/7635], Loss: 4.5254\n",
      "Epoch [1/1], Step [1602/7635], Loss: 4.6446\n",
      "Epoch [1/1], Step [1603/7635], Loss: 4.5887\n",
      "Epoch [1/1], Step [1604/7635], Loss: 4.5460\n",
      "Epoch [1/1], Step [1605/7635], Loss: 4.5568\n",
      "Epoch [1/1], Step [1606/7635], Loss: 4.6253\n",
      "Epoch [1/1], Step [1607/7635], Loss: 4.5360\n",
      "Epoch [1/1], Step [1608/7635], Loss: 4.5788\n",
      "Epoch [1/1], Step [1609/7635], Loss: 4.6472\n",
      "Epoch [1/1], Step [1610/7635], Loss: 4.4913\n",
      "Epoch [1/1], Step [1611/7635], Loss: 4.5576\n",
      "Epoch [1/1], Step [1612/7635], Loss: 4.5129\n",
      "Epoch [1/1], Step [1613/7635], Loss: 4.6203\n",
      "Epoch [1/1], Step [1614/7635], Loss: 4.6126\n",
      "Epoch [1/1], Step [1615/7635], Loss: 4.6638\n",
      "Epoch [1/1], Step [1616/7635], Loss: 4.6464\n",
      "Epoch [1/1], Step [1617/7635], Loss: 4.5616\n",
      "Epoch [1/1], Step [1618/7635], Loss: 4.6389\n",
      "Epoch [1/1], Step [1619/7635], Loss: 4.6244\n",
      "Epoch [1/1], Step [1620/7635], Loss: 4.5446\n",
      "Epoch [1/1], Step [1621/7635], Loss: 4.5538\n",
      "Epoch [1/1], Step [1622/7635], Loss: 4.6467\n",
      "Epoch [1/1], Step [1623/7635], Loss: 4.5256\n",
      "Epoch [1/1], Step [1624/7635], Loss: 4.5701\n",
      "Epoch [1/1], Step [1625/7635], Loss: 4.6477\n",
      "Epoch [1/1], Step [1626/7635], Loss: 4.5526\n",
      "Epoch [1/1], Step [1627/7635], Loss: 4.5603\n",
      "Epoch [1/1], Step [1628/7635], Loss: 4.5283\n",
      "Epoch [1/1], Step [1629/7635], Loss: 4.6046\n",
      "Epoch [1/1], Step [1630/7635], Loss: 4.4552\n",
      "Epoch [1/1], Step [1631/7635], Loss: 4.5354\n",
      "Epoch [1/1], Step [1632/7635], Loss: 4.5149\n",
      "Epoch [1/1], Step [1633/7635], Loss: 4.5631\n",
      "Epoch [1/1], Step [1634/7635], Loss: 4.5258\n",
      "Epoch [1/1], Step [1635/7635], Loss: 4.6292\n",
      "Epoch [1/1], Step [1636/7635], Loss: 4.6178\n",
      "Epoch [1/1], Step [1637/7635], Loss: 4.5702\n",
      "Epoch [1/1], Step [1638/7635], Loss: 4.5960\n",
      "Epoch [1/1], Step [1639/7635], Loss: 4.5865\n",
      "Epoch [1/1], Step [1640/7635], Loss: 4.6065\n",
      "Epoch [1/1], Step [1641/7635], Loss: 4.5644\n",
      "Epoch [1/1], Step [1642/7635], Loss: 4.6732\n",
      "Epoch [1/1], Step [1643/7635], Loss: 4.6019\n",
      "Epoch [1/1], Step [1644/7635], Loss: 4.5890\n",
      "Epoch [1/1], Step [1645/7635], Loss: 4.6153\n",
      "Epoch [1/1], Step [1646/7635], Loss: 4.5582\n",
      "Epoch [1/1], Step [1647/7635], Loss: 4.5942\n",
      "Epoch [1/1], Step [1648/7635], Loss: 4.5698\n",
      "Epoch [1/1], Step [1649/7635], Loss: 4.6146\n",
      "Epoch [1/1], Step [1650/7635], Loss: 4.6522\n",
      "Epoch [1/1], Step [1651/7635], Loss: 4.5191\n",
      "Epoch [1/1], Step [1652/7635], Loss: 4.5893\n",
      "Epoch [1/1], Step [1653/7635], Loss: 4.5656\n",
      "Epoch [1/1], Step [1654/7635], Loss: 4.5923\n",
      "Epoch [1/1], Step [1655/7635], Loss: 4.5515\n",
      "Epoch [1/1], Step [1656/7635], Loss: 4.4866\n",
      "Epoch [1/1], Step [1657/7635], Loss: 4.6434\n",
      "Epoch [1/1], Step [1658/7635], Loss: 4.6733\n",
      "Epoch [1/1], Step [1659/7635], Loss: 4.5540\n",
      "Epoch [1/1], Step [1660/7635], Loss: 4.5790\n",
      "Epoch [1/1], Step [1661/7635], Loss: 4.6158\n",
      "Epoch [1/1], Step [1662/7635], Loss: 4.6549\n",
      "Epoch [1/1], Step [1663/7635], Loss: 4.5712\n",
      "Epoch [1/1], Step [1664/7635], Loss: 4.5103\n",
      "Epoch [1/1], Step [1665/7635], Loss: 4.7192\n",
      "Epoch [1/1], Step [1666/7635], Loss: 4.5773\n",
      "Epoch [1/1], Step [1667/7635], Loss: 4.5954\n",
      "Epoch [1/1], Step [1668/7635], Loss: 4.5868\n",
      "Epoch [1/1], Step [1669/7635], Loss: 4.5472\n",
      "Epoch [1/1], Step [1670/7635], Loss: 4.5951\n",
      "Epoch [1/1], Step [1671/7635], Loss: 4.5844\n",
      "Epoch [1/1], Step [1672/7635], Loss: 4.6265\n",
      "Epoch [1/1], Step [1673/7635], Loss: 4.6402\n",
      "Epoch [1/1], Step [1674/7635], Loss: 4.4998\n",
      "Epoch [1/1], Step [1675/7635], Loss: 4.5670\n",
      "Epoch [1/1], Step [1676/7635], Loss: 4.5755\n",
      "Epoch [1/1], Step [1677/7635], Loss: 4.5154\n",
      "Epoch [1/1], Step [1678/7635], Loss: 4.6287\n",
      "Epoch [1/1], Step [1679/7635], Loss: 4.5836\n",
      "Epoch [1/1], Step [1680/7635], Loss: 4.5894\n",
      "Epoch [1/1], Step [1681/7635], Loss: 4.5226\n",
      "Epoch [1/1], Step [1682/7635], Loss: 4.6156\n",
      "Epoch [1/1], Step [1683/7635], Loss: 4.5430\n",
      "Epoch [1/1], Step [1684/7635], Loss: 4.5319\n",
      "Epoch [1/1], Step [1685/7635], Loss: 4.5513\n",
      "Epoch [1/1], Step [1686/7635], Loss: 4.5607\n",
      "Epoch [1/1], Step [1687/7635], Loss: 4.5579\n",
      "Epoch [1/1], Step [1688/7635], Loss: 4.5015\n",
      "Epoch [1/1], Step [1689/7635], Loss: 4.6014\n",
      "Epoch [1/1], Step [1690/7635], Loss: 4.5628\n",
      "Epoch [1/1], Step [1691/7635], Loss: 4.4484\n",
      "Epoch [1/1], Step [1692/7635], Loss: 4.5574\n",
      "Epoch [1/1], Step [1693/7635], Loss: 4.5390\n",
      "Epoch [1/1], Step [1694/7635], Loss: 4.5893\n",
      "Epoch [1/1], Step [1695/7635], Loss: 4.5520\n",
      "Epoch [1/1], Step [1696/7635], Loss: 4.4811\n",
      "Epoch [1/1], Step [1697/7635], Loss: 4.6303\n",
      "Epoch [1/1], Step [1698/7635], Loss: 4.5466\n",
      "Epoch [1/1], Step [1699/7635], Loss: 4.5960\n",
      "Epoch [1/1], Step [1700/7635], Loss: 4.5396\n",
      "Epoch [1/1], Step [1701/7635], Loss: 4.5854\n",
      "Epoch [1/1], Step [1702/7635], Loss: 4.4794\n",
      "Epoch [1/1], Step [1703/7635], Loss: 4.4724\n",
      "Epoch [1/1], Step [1704/7635], Loss: 4.5376\n",
      "Epoch [1/1], Step [1705/7635], Loss: 4.5326\n",
      "Epoch [1/1], Step [1706/7635], Loss: 4.5945\n",
      "Epoch [1/1], Step [1707/7635], Loss: 4.6050\n",
      "Epoch [1/1], Step [1708/7635], Loss: 4.5593\n",
      "Epoch [1/1], Step [1709/7635], Loss: 4.4919\n",
      "Epoch [1/1], Step [1710/7635], Loss: 4.5545\n",
      "Epoch [1/1], Step [1711/7635], Loss: 4.5141\n",
      "Epoch [1/1], Step [1712/7635], Loss: 4.5375\n",
      "Epoch [1/1], Step [1713/7635], Loss: 4.5823\n",
      "Epoch [1/1], Step [1714/7635], Loss: 4.6026\n",
      "Epoch [1/1], Step [1715/7635], Loss: 4.5938\n",
      "Epoch [1/1], Step [1716/7635], Loss: 4.5598\n",
      "Epoch [1/1], Step [1717/7635], Loss: 4.5922\n",
      "Epoch [1/1], Step [1718/7635], Loss: 4.4565\n",
      "Epoch [1/1], Step [1719/7635], Loss: 4.5241\n",
      "Epoch [1/1], Step [1720/7635], Loss: 4.5676\n",
      "Epoch [1/1], Step [1721/7635], Loss: 4.5132\n",
      "Epoch [1/1], Step [1722/7635], Loss: 4.4639\n",
      "Epoch [1/1], Step [1723/7635], Loss: 4.4942\n",
      "Epoch [1/1], Step [1724/7635], Loss: 4.5857\n",
      "Epoch [1/1], Step [1725/7635], Loss: 4.5229\n",
      "Epoch [1/1], Step [1726/7635], Loss: 4.5579\n",
      "Epoch [1/1], Step [1727/7635], Loss: 4.5808\n",
      "Epoch [1/1], Step [1728/7635], Loss: 4.5764\n",
      "Epoch [1/1], Step [1729/7635], Loss: 4.5840\n",
      "Epoch [1/1], Step [1730/7635], Loss: 4.5532\n",
      "Epoch [1/1], Step [1731/7635], Loss: 4.5777\n",
      "Epoch [1/1], Step [1732/7635], Loss: 4.5432\n",
      "Epoch [1/1], Step [1733/7635], Loss: 4.5011\n",
      "Epoch [1/1], Step [1734/7635], Loss: 4.5693\n",
      "Epoch [1/1], Step [1735/7635], Loss: 4.6103\n",
      "Epoch [1/1], Step [1736/7635], Loss: 4.5638\n",
      "Epoch [1/1], Step [1737/7635], Loss: 4.5491\n",
      "Epoch [1/1], Step [1738/7635], Loss: 4.6284\n",
      "Epoch [1/1], Step [1739/7635], Loss: 4.5395\n",
      "Epoch [1/1], Step [1740/7635], Loss: 4.4714\n",
      "Epoch [1/1], Step [1741/7635], Loss: 4.4953\n",
      "Epoch [1/1], Step [1742/7635], Loss: 4.5501\n",
      "Epoch [1/1], Step [1743/7635], Loss: 4.6608\n",
      "Epoch [1/1], Step [1744/7635], Loss: 4.5596\n",
      "Epoch [1/1], Step [1745/7635], Loss: 4.5267\n",
      "Epoch [1/1], Step [1746/7635], Loss: 4.5148\n",
      "Epoch [1/1], Step [1747/7635], Loss: 4.4876\n",
      "Epoch [1/1], Step [1748/7635], Loss: 4.5939\n",
      "Epoch [1/1], Step [1749/7635], Loss: 4.5184\n",
      "Epoch [1/1], Step [1750/7635], Loss: 4.4973\n",
      "Epoch [1/1], Step [1751/7635], Loss: 4.5551\n",
      "Epoch [1/1], Step [1752/7635], Loss: 4.5253\n",
      "Epoch [1/1], Step [1753/7635], Loss: 4.4595\n",
      "Epoch [1/1], Step [1754/7635], Loss: 4.4894\n",
      "Epoch [1/1], Step [1755/7635], Loss: 4.5310\n",
      "Epoch [1/1], Step [1756/7635], Loss: 4.5218\n",
      "Epoch [1/1], Step [1757/7635], Loss: 4.5063\n",
      "Epoch [1/1], Step [1758/7635], Loss: 4.5509\n",
      "Epoch [1/1], Step [1759/7635], Loss: 4.4984\n",
      "Epoch [1/1], Step [1760/7635], Loss: 4.4405\n",
      "Epoch [1/1], Step [1761/7635], Loss: 4.5539\n",
      "Epoch [1/1], Step [1762/7635], Loss: 4.6337\n",
      "Epoch [1/1], Step [1763/7635], Loss: 4.6889\n",
      "Epoch [1/1], Step [1764/7635], Loss: 4.5604\n",
      "Epoch [1/1], Step [1765/7635], Loss: 4.5169\n",
      "Epoch [1/1], Step [1766/7635], Loss: 4.5735\n",
      "Epoch [1/1], Step [1767/7635], Loss: 4.4702\n",
      "Epoch [1/1], Step [1768/7635], Loss: 4.6078\n",
      "Epoch [1/1], Step [1769/7635], Loss: 4.5726\n",
      "Epoch [1/1], Step [1770/7635], Loss: 4.5147\n",
      "Epoch [1/1], Step [1771/7635], Loss: 4.4530\n",
      "Epoch [1/1], Step [1772/7635], Loss: 4.4731\n",
      "Epoch [1/1], Step [1773/7635], Loss: 4.5761\n",
      "Epoch [1/1], Step [1774/7635], Loss: 4.6235\n",
      "Epoch [1/1], Step [1775/7635], Loss: 4.5377\n",
      "Epoch [1/1], Step [1776/7635], Loss: 4.4527\n",
      "Epoch [1/1], Step [1777/7635], Loss: 4.4475\n",
      "Epoch [1/1], Step [1778/7635], Loss: 4.5414\n",
      "Epoch [1/1], Step [1779/7635], Loss: 4.4415\n",
      "Epoch [1/1], Step [1780/7635], Loss: 4.5045\n",
      "Epoch [1/1], Step [1781/7635], Loss: 4.5105\n",
      "Epoch [1/1], Step [1782/7635], Loss: 4.5123\n",
      "Epoch [1/1], Step [1783/7635], Loss: 4.5685\n",
      "Epoch [1/1], Step [1784/7635], Loss: 4.4802\n",
      "Epoch [1/1], Step [1785/7635], Loss: 4.5390\n",
      "Epoch [1/1], Step [1786/7635], Loss: 4.5980\n",
      "Epoch [1/1], Step [1787/7635], Loss: 4.4931\n",
      "Epoch [1/1], Step [1788/7635], Loss: 4.4328\n",
      "Epoch [1/1], Step [1789/7635], Loss: 4.4172\n",
      "Epoch [1/1], Step [1790/7635], Loss: 4.5482\n",
      "Epoch [1/1], Step [1791/7635], Loss: 4.5284\n",
      "Epoch [1/1], Step [1792/7635], Loss: 4.5803\n",
      "Epoch [1/1], Step [1793/7635], Loss: 4.4920\n",
      "Epoch [1/1], Step [1794/7635], Loss: 4.5244\n",
      "Epoch [1/1], Step [1795/7635], Loss: 4.5821\n",
      "Epoch [1/1], Step [1796/7635], Loss: 4.4260\n",
      "Epoch [1/1], Step [1797/7635], Loss: 4.5120\n",
      "Epoch [1/1], Step [1798/7635], Loss: 4.5680\n",
      "Epoch [1/1], Step [1799/7635], Loss: 4.4439\n",
      "Epoch [1/1], Step [1800/7635], Loss: 4.4859\n",
      "Epoch [1/1], Step [1801/7635], Loss: 4.5406\n",
      "Epoch [1/1], Step [1802/7635], Loss: 4.5363\n",
      "Epoch [1/1], Step [1803/7635], Loss: 4.5619\n",
      "Epoch [1/1], Step [1804/7635], Loss: 4.5525\n",
      "Epoch [1/1], Step [1805/7635], Loss: 4.6274\n",
      "Epoch [1/1], Step [1806/7635], Loss: 4.5117\n",
      "Epoch [1/1], Step [1807/7635], Loss: 4.5179\n",
      "Epoch [1/1], Step [1808/7635], Loss: 4.5687\n",
      "Epoch [1/1], Step [1809/7635], Loss: 4.5686\n",
      "Epoch [1/1], Step [1810/7635], Loss: 4.5344\n",
      "Epoch [1/1], Step [1811/7635], Loss: 4.5413\n",
      "Epoch [1/1], Step [1812/7635], Loss: 4.5210\n",
      "Epoch [1/1], Step [1813/7635], Loss: 4.4937\n",
      "Epoch [1/1], Step [1814/7635], Loss: 4.5332\n",
      "Epoch [1/1], Step [1815/7635], Loss: 4.4888\n",
      "Epoch [1/1], Step [1816/7635], Loss: 4.5511\n",
      "Epoch [1/1], Step [1817/7635], Loss: 4.6110\n",
      "Epoch [1/1], Step [1818/7635], Loss: 4.5274\n",
      "Epoch [1/1], Step [1819/7635], Loss: 4.6057\n",
      "Epoch [1/1], Step [1820/7635], Loss: 4.5614\n",
      "Epoch [1/1], Step [1821/7635], Loss: 4.5118\n",
      "Epoch [1/1], Step [1822/7635], Loss: 4.5465\n",
      "Epoch [1/1], Step [1823/7635], Loss: 4.5989\n",
      "Epoch [1/1], Step [1824/7635], Loss: 4.4904\n",
      "Epoch [1/1], Step [1825/7635], Loss: 4.4835\n",
      "Epoch [1/1], Step [1826/7635], Loss: 4.4882\n",
      "Epoch [1/1], Step [1827/7635], Loss: 4.4885\n",
      "Epoch [1/1], Step [1828/7635], Loss: 4.5131\n",
      "Epoch [1/1], Step [1829/7635], Loss: 4.4886\n",
      "Epoch [1/1], Step [1830/7635], Loss: 4.5683\n",
      "Epoch [1/1], Step [1831/7635], Loss: 4.5966\n",
      "Epoch [1/1], Step [1832/7635], Loss: 4.5815\n",
      "Epoch [1/1], Step [1833/7635], Loss: 4.5928\n",
      "Epoch [1/1], Step [1834/7635], Loss: 4.4743\n",
      "Epoch [1/1], Step [1835/7635], Loss: 4.4847\n",
      "Epoch [1/1], Step [1836/7635], Loss: 4.4689\n",
      "Epoch [1/1], Step [1837/7635], Loss: 4.5536\n",
      "Epoch [1/1], Step [1838/7635], Loss: 4.4664\n",
      "Epoch [1/1], Step [1839/7635], Loss: 4.5601\n",
      "Epoch [1/1], Step [1840/7635], Loss: 4.5037\n",
      "Epoch [1/1], Step [1841/7635], Loss: 4.6012\n",
      "Epoch [1/1], Step [1842/7635], Loss: 4.4651\n",
      "Epoch [1/1], Step [1843/7635], Loss: 4.5140\n",
      "Epoch [1/1], Step [1844/7635], Loss: 4.5294\n",
      "Epoch [1/1], Step [1845/7635], Loss: 4.4936\n",
      "Epoch [1/1], Step [1846/7635], Loss: 4.5016\n",
      "Epoch [1/1], Step [1847/7635], Loss: 4.5564\n",
      "Epoch [1/1], Step [1848/7635], Loss: 4.5549\n",
      "Epoch [1/1], Step [1849/7635], Loss: 4.5393\n",
      "Epoch [1/1], Step [1850/7635], Loss: 4.6505\n",
      "Epoch [1/1], Step [1851/7635], Loss: 4.5890\n",
      "Epoch [1/1], Step [1852/7635], Loss: 4.4435\n",
      "Epoch [1/1], Step [1853/7635], Loss: 4.5326\n",
      "Epoch [1/1], Step [1854/7635], Loss: 4.4812\n",
      "Epoch [1/1], Step [1855/7635], Loss: 4.6084\n",
      "Epoch [1/1], Step [1856/7635], Loss: 4.4801\n",
      "Epoch [1/1], Step [1857/7635], Loss: 4.5653\n",
      "Epoch [1/1], Step [1858/7635], Loss: 4.5722\n",
      "Epoch [1/1], Step [1859/7635], Loss: 4.4406\n",
      "Epoch [1/1], Step [1860/7635], Loss: 4.5512\n",
      "Epoch [1/1], Step [1861/7635], Loss: 4.4827\n",
      "Epoch [1/1], Step [1862/7635], Loss: 4.5747\n",
      "Epoch [1/1], Step [1863/7635], Loss: 4.5487\n",
      "Epoch [1/1], Step [1864/7635], Loss: 4.5446\n",
      "Epoch [1/1], Step [1865/7635], Loss: 4.5979\n",
      "Epoch [1/1], Step [1866/7635], Loss: 4.4829\n",
      "Epoch [1/1], Step [1867/7635], Loss: 4.5897\n",
      "Epoch [1/1], Step [1868/7635], Loss: 4.5008\n",
      "Epoch [1/1], Step [1869/7635], Loss: 4.5223\n",
      "Epoch [1/1], Step [1870/7635], Loss: 4.5796\n",
      "Epoch [1/1], Step [1871/7635], Loss: 4.4626\n",
      "Epoch [1/1], Step [1872/7635], Loss: 4.6187\n",
      "Epoch [1/1], Step [1873/7635], Loss: 4.5047\n",
      "Epoch [1/1], Step [1874/7635], Loss: 4.5238\n",
      "Epoch [1/1], Step [1875/7635], Loss: 4.5221\n",
      "Epoch [1/1], Step [1876/7635], Loss: 4.5336\n",
      "Epoch [1/1], Step [1877/7635], Loss: 4.5303\n",
      "Epoch [1/1], Step [1878/7635], Loss: 4.5502\n",
      "Epoch [1/1], Step [1879/7635], Loss: 4.5220\n",
      "Epoch [1/1], Step [1880/7635], Loss: 4.4788\n",
      "Epoch [1/1], Step [1881/7635], Loss: 4.4964\n",
      "Epoch [1/1], Step [1882/7635], Loss: 4.3558\n",
      "Epoch [1/1], Step [1883/7635], Loss: 4.5686\n",
      "Epoch [1/1], Step [1884/7635], Loss: 4.5817\n",
      "Epoch [1/1], Step [1885/7635], Loss: 4.5317\n",
      "Epoch [1/1], Step [1886/7635], Loss: 4.4302\n",
      "Epoch [1/1], Step [1887/7635], Loss: 4.4326\n",
      "Epoch [1/1], Step [1888/7635], Loss: 4.5539\n",
      "Epoch [1/1], Step [1889/7635], Loss: 4.6054\n",
      "Epoch [1/1], Step [1890/7635], Loss: 4.4049\n",
      "Epoch [1/1], Step [1891/7635], Loss: 4.5567\n",
      "Epoch [1/1], Step [1892/7635], Loss: 4.4314\n",
      "Epoch [1/1], Step [1893/7635], Loss: 4.4606\n",
      "Epoch [1/1], Step [1894/7635], Loss: 4.5247\n",
      "Epoch [1/1], Step [1895/7635], Loss: 4.4993\n",
      "Epoch [1/1], Step [1896/7635], Loss: 4.5293\n",
      "Epoch [1/1], Step [1897/7635], Loss: 4.5349\n",
      "Epoch [1/1], Step [1898/7635], Loss: 4.4547\n",
      "Epoch [1/1], Step [1899/7635], Loss: 4.5281\n",
      "Epoch [1/1], Step [1900/7635], Loss: 4.4583\n",
      "Epoch [1/1], Step [1901/7635], Loss: 4.5285\n",
      "Epoch [1/1], Step [1902/7635], Loss: 4.5537\n",
      "Epoch [1/1], Step [1903/7635], Loss: 4.5396\n",
      "Epoch [1/1], Step [1904/7635], Loss: 4.5780\n",
      "Epoch [1/1], Step [1905/7635], Loss: 4.5685\n",
      "Epoch [1/1], Step [1906/7635], Loss: 4.5455\n",
      "Epoch [1/1], Step [1907/7635], Loss: 4.4851\n",
      "Epoch [1/1], Step [1908/7635], Loss: 4.4897\n",
      "Epoch [1/1], Step [1909/7635], Loss: 4.5159\n",
      "Epoch [1/1], Step [1910/7635], Loss: 4.5568\n",
      "Epoch [1/1], Step [1911/7635], Loss: 4.4494\n",
      "Epoch [1/1], Step [1912/7635], Loss: 4.6122\n",
      "Epoch [1/1], Step [1913/7635], Loss: 4.5168\n",
      "Epoch [1/1], Step [1914/7635], Loss: 4.4486\n",
      "Epoch [1/1], Step [1915/7635], Loss: 4.5369\n",
      "Epoch [1/1], Step [1916/7635], Loss: 4.5148\n",
      "Epoch [1/1], Step [1917/7635], Loss: 4.5713\n",
      "Epoch [1/1], Step [1918/7635], Loss: 4.5287\n",
      "Epoch [1/1], Step [1919/7635], Loss: 4.5596\n",
      "Epoch [1/1], Step [1920/7635], Loss: 4.5760\n",
      "Epoch [1/1], Step [1921/7635], Loss: 4.5032\n",
      "Epoch [1/1], Step [1922/7635], Loss: 4.4467\n",
      "Epoch [1/1], Step [1923/7635], Loss: 4.6030\n",
      "Epoch [1/1], Step [1924/7635], Loss: 4.4885\n",
      "Epoch [1/1], Step [1925/7635], Loss: 4.6206\n",
      "Epoch [1/1], Step [1926/7635], Loss: 4.4724\n",
      "Epoch [1/1], Step [1927/7635], Loss: 4.5116\n",
      "Epoch [1/1], Step [1928/7635], Loss: 4.5180\n",
      "Epoch [1/1], Step [1929/7635], Loss: 4.4973\n",
      "Epoch [1/1], Step [1930/7635], Loss: 4.4092\n",
      "Epoch [1/1], Step [1931/7635], Loss: 4.4440\n",
      "Epoch [1/1], Step [1932/7635], Loss: 4.4711\n",
      "Epoch [1/1], Step [1933/7635], Loss: 4.4666\n",
      "Epoch [1/1], Step [1934/7635], Loss: 4.5395\n",
      "Epoch [1/1], Step [1935/7635], Loss: 4.4106\n",
      "Epoch [1/1], Step [1936/7635], Loss: 4.4920\n",
      "Epoch [1/1], Step [1937/7635], Loss: 4.5222\n",
      "Epoch [1/1], Step [1938/7635], Loss: 4.4454\n",
      "Epoch [1/1], Step [1939/7635], Loss: 4.4933\n",
      "Epoch [1/1], Step [1940/7635], Loss: 4.4601\n",
      "Epoch [1/1], Step [1941/7635], Loss: 4.4521\n",
      "Epoch [1/1], Step [1942/7635], Loss: 4.5962\n",
      "Epoch [1/1], Step [1943/7635], Loss: 4.4885\n",
      "Epoch [1/1], Step [1944/7635], Loss: 4.5289\n",
      "Epoch [1/1], Step [1945/7635], Loss: 4.5264\n",
      "Epoch [1/1], Step [1946/7635], Loss: 4.4762\n",
      "Epoch [1/1], Step [1947/7635], Loss: 4.4311\n",
      "Epoch [1/1], Step [1948/7635], Loss: 4.4951\n",
      "Epoch [1/1], Step [1949/7635], Loss: 4.3760\n",
      "Epoch [1/1], Step [1950/7635], Loss: 4.4972\n",
      "Epoch [1/1], Step [1951/7635], Loss: 4.3890\n",
      "Epoch [1/1], Step [1952/7635], Loss: 4.4639\n",
      "Epoch [1/1], Step [1953/7635], Loss: 4.5530\n",
      "Epoch [1/1], Step [1954/7635], Loss: 4.4763\n",
      "Epoch [1/1], Step [1955/7635], Loss: 4.5252\n",
      "Epoch [1/1], Step [1956/7635], Loss: 4.5040\n",
      "Epoch [1/1], Step [1957/7635], Loss: 4.5040\n",
      "Epoch [1/1], Step [1958/7635], Loss: 4.4714\n",
      "Epoch [1/1], Step [1959/7635], Loss: 4.4582\n",
      "Epoch [1/1], Step [1960/7635], Loss: 4.4408\n",
      "Epoch [1/1], Step [1961/7635], Loss: 4.4353\n",
      "Epoch [1/1], Step [1962/7635], Loss: 4.4963\n",
      "Epoch [1/1], Step [1963/7635], Loss: 4.5024\n",
      "Epoch [1/1], Step [1964/7635], Loss: 4.4591\n",
      "Epoch [1/1], Step [1965/7635], Loss: 4.4748\n",
      "Epoch [1/1], Step [1966/7635], Loss: 4.5138\n",
      "Epoch [1/1], Step [1967/7635], Loss: 4.5086\n",
      "Epoch [1/1], Step [1968/7635], Loss: 4.4530\n",
      "Epoch [1/1], Step [1969/7635], Loss: 4.5493\n",
      "Epoch [1/1], Step [1970/7635], Loss: 4.4635\n",
      "Epoch [1/1], Step [1971/7635], Loss: 4.4510\n",
      "Epoch [1/1], Step [1972/7635], Loss: 4.4977\n",
      "Epoch [1/1], Step [1973/7635], Loss: 4.5477\n",
      "Epoch [1/1], Step [1974/7635], Loss: 4.4715\n",
      "Epoch [1/1], Step [1975/7635], Loss: 4.4884\n",
      "Epoch [1/1], Step [1976/7635], Loss: 4.4842\n",
      "Epoch [1/1], Step [1977/7635], Loss: 4.4928\n",
      "Epoch [1/1], Step [1978/7635], Loss: 4.5393\n",
      "Epoch [1/1], Step [1979/7635], Loss: 4.4527\n",
      "Epoch [1/1], Step [1980/7635], Loss: 4.5909\n",
      "Epoch [1/1], Step [1981/7635], Loss: 4.4449\n",
      "Epoch [1/1], Step [1982/7635], Loss: 4.4929\n",
      "Epoch [1/1], Step [1983/7635], Loss: 4.4006\n",
      "Epoch [1/1], Step [1984/7635], Loss: 4.5311\n",
      "Epoch [1/1], Step [1985/7635], Loss: 4.4784\n",
      "Epoch [1/1], Step [1986/7635], Loss: 4.4315\n",
      "Epoch [1/1], Step [1987/7635], Loss: 4.5300\n",
      "Epoch [1/1], Step [1988/7635], Loss: 4.4173\n",
      "Epoch [1/1], Step [1989/7635], Loss: 4.5482\n",
      "Epoch [1/1], Step [1990/7635], Loss: 4.5899\n",
      "Epoch [1/1], Step [1991/7635], Loss: 4.5425\n",
      "Epoch [1/1], Step [1992/7635], Loss: 4.5680\n",
      "Epoch [1/1], Step [1993/7635], Loss: 4.4693\n",
      "Epoch [1/1], Step [1994/7635], Loss: 4.5641\n",
      "Epoch [1/1], Step [1995/7635], Loss: 4.4078\n",
      "Epoch [1/1], Step [1996/7635], Loss: 4.4656\n",
      "Epoch [1/1], Step [1997/7635], Loss: 4.4444\n",
      "Epoch [1/1], Step [1998/7635], Loss: 4.4580\n",
      "Epoch [1/1], Step [1999/7635], Loss: 4.3707\n",
      "Epoch [1/1], Step [2000/7635], Loss: 4.4733\n",
      "Epoch [1/1], Step [2001/7635], Loss: 4.5042\n",
      "Epoch [1/1], Step [2002/7635], Loss: 4.4458\n",
      "Epoch [1/1], Step [2003/7635], Loss: 4.4581\n",
      "Epoch [1/1], Step [2004/7635], Loss: 4.4173\n",
      "Epoch [1/1], Step [2005/7635], Loss: 4.4096\n",
      "Epoch [1/1], Step [2006/7635], Loss: 4.3766\n",
      "Epoch [1/1], Step [2007/7635], Loss: 4.4540\n",
      "Epoch [1/1], Step [2008/7635], Loss: 4.5058\n",
      "Epoch [1/1], Step [2009/7635], Loss: 4.6271\n",
      "Epoch [1/1], Step [2010/7635], Loss: 4.4379\n",
      "Epoch [1/1], Step [2011/7635], Loss: 4.5322\n",
      "Epoch [1/1], Step [2012/7635], Loss: 4.5457\n",
      "Epoch [1/1], Step [2013/7635], Loss: 4.5197\n",
      "Epoch [1/1], Step [2014/7635], Loss: 4.4644\n",
      "Epoch [1/1], Step [2015/7635], Loss: 4.5429\n",
      "Epoch [1/1], Step [2016/7635], Loss: 4.4582\n",
      "Epoch [1/1], Step [2017/7635], Loss: 4.5558\n",
      "Epoch [1/1], Step [2018/7635], Loss: 4.3668\n",
      "Epoch [1/1], Step [2019/7635], Loss: 4.5191\n",
      "Epoch [1/1], Step [2020/7635], Loss: 4.5024\n",
      "Epoch [1/1], Step [2021/7635], Loss: 4.4627\n",
      "Epoch [1/1], Step [2022/7635], Loss: 4.4447\n",
      "Epoch [1/1], Step [2023/7635], Loss: 4.5472\n",
      "Epoch [1/1], Step [2024/7635], Loss: 4.3555\n",
      "Epoch [1/1], Step [2025/7635], Loss: 4.5225\n",
      "Epoch [1/1], Step [2026/7635], Loss: 4.5034\n",
      "Epoch [1/1], Step [2027/7635], Loss: 4.3353\n",
      "Epoch [1/1], Step [2028/7635], Loss: 4.4681\n",
      "Epoch [1/1], Step [2029/7635], Loss: 4.4496\n",
      "Epoch [1/1], Step [2030/7635], Loss: 4.4898\n",
      "Epoch [1/1], Step [2031/7635], Loss: 4.4957\n",
      "Epoch [1/1], Step [2032/7635], Loss: 4.5182\n",
      "Epoch [1/1], Step [2033/7635], Loss: 4.5120\n",
      "Epoch [1/1], Step [2034/7635], Loss: 4.5621\n",
      "Epoch [1/1], Step [2035/7635], Loss: 4.5180\n",
      "Epoch [1/1], Step [2036/7635], Loss: 4.5709\n",
      "Epoch [1/1], Step [2037/7635], Loss: 4.4852\n",
      "Epoch [1/1], Step [2038/7635], Loss: 4.5358\n",
      "Epoch [1/1], Step [2039/7635], Loss: 4.4904\n",
      "Epoch [1/1], Step [2040/7635], Loss: 4.4870\n",
      "Epoch [1/1], Step [2041/7635], Loss: 4.4941\n",
      "Epoch [1/1], Step [2042/7635], Loss: 4.4531\n",
      "Epoch [1/1], Step [2043/7635], Loss: 4.4352\n",
      "Epoch [1/1], Step [2044/7635], Loss: 4.4521\n",
      "Epoch [1/1], Step [2045/7635], Loss: 4.5370\n",
      "Epoch [1/1], Step [2046/7635], Loss: 4.4218\n",
      "Epoch [1/1], Step [2047/7635], Loss: 4.4672\n",
      "Epoch [1/1], Step [2048/7635], Loss: 4.4840\n",
      "Epoch [1/1], Step [2049/7635], Loss: 4.4058\n",
      "Epoch [1/1], Step [2050/7635], Loss: 4.4566\n",
      "Epoch [1/1], Step [2051/7635], Loss: 4.5573\n",
      "Epoch [1/1], Step [2052/7635], Loss: 4.4816\n",
      "Epoch [1/1], Step [2053/7635], Loss: 4.4285\n",
      "Epoch [1/1], Step [2054/7635], Loss: 4.5499\n",
      "Epoch [1/1], Step [2055/7635], Loss: 4.3864\n",
      "Epoch [1/1], Step [2056/7635], Loss: 4.5036\n",
      "Epoch [1/1], Step [2057/7635], Loss: 4.3502\n",
      "Epoch [1/1], Step [2058/7635], Loss: 4.4210\n",
      "Epoch [1/1], Step [2059/7635], Loss: 4.4648\n",
      "Epoch [1/1], Step [2060/7635], Loss: 4.3963\n",
      "Epoch [1/1], Step [2061/7635], Loss: 4.4333\n",
      "Epoch [1/1], Step [2062/7635], Loss: 4.5943\n",
      "Epoch [1/1], Step [2063/7635], Loss: 4.5680\n",
      "Epoch [1/1], Step [2064/7635], Loss: 4.4651\n",
      "Epoch [1/1], Step [2065/7635], Loss: 4.4824\n",
      "Epoch [1/1], Step [2066/7635], Loss: 4.6716\n",
      "Epoch [1/1], Step [2067/7635], Loss: 4.5196\n",
      "Epoch [1/1], Step [2068/7635], Loss: 4.5228\n",
      "Epoch [1/1], Step [2069/7635], Loss: 4.5329\n",
      "Epoch [1/1], Step [2070/7635], Loss: 4.4837\n",
      "Epoch [1/1], Step [2071/7635], Loss: 4.4522\n",
      "Epoch [1/1], Step [2072/7635], Loss: 4.4768\n",
      "Epoch [1/1], Step [2073/7635], Loss: 4.4546\n",
      "Epoch [1/1], Step [2074/7635], Loss: 4.5936\n",
      "Epoch [1/1], Step [2075/7635], Loss: 4.4257\n",
      "Epoch [1/1], Step [2076/7635], Loss: 4.5442\n",
      "Epoch [1/1], Step [2077/7635], Loss: 4.4771\n",
      "Epoch [1/1], Step [2078/7635], Loss: 4.4916\n",
      "Epoch [1/1], Step [2079/7635], Loss: 4.4382\n",
      "Epoch [1/1], Step [2080/7635], Loss: 4.4401\n",
      "Epoch [1/1], Step [2081/7635], Loss: 4.4730\n",
      "Epoch [1/1], Step [2082/7635], Loss: 4.4634\n",
      "Epoch [1/1], Step [2083/7635], Loss: 4.4054\n",
      "Epoch [1/1], Step [2084/7635], Loss: 4.4460\n",
      "Epoch [1/1], Step [2085/7635], Loss: 4.4443\n",
      "Epoch [1/1], Step [2086/7635], Loss: 4.4591\n",
      "Epoch [1/1], Step [2087/7635], Loss: 4.5608\n",
      "Epoch [1/1], Step [2088/7635], Loss: 4.4621\n",
      "Epoch [1/1], Step [2089/7635], Loss: 4.3655\n",
      "Epoch [1/1], Step [2090/7635], Loss: 4.4471\n",
      "Epoch [1/1], Step [2091/7635], Loss: 4.4361\n",
      "Epoch [1/1], Step [2092/7635], Loss: 4.4040\n",
      "Epoch [1/1], Step [2093/7635], Loss: 4.4633\n",
      "Epoch [1/1], Step [2094/7635], Loss: 4.5338\n",
      "Epoch [1/1], Step [2095/7635], Loss: 4.3800\n",
      "Epoch [1/1], Step [2096/7635], Loss: 4.4940\n",
      "Epoch [1/1], Step [2097/7635], Loss: 4.4346\n",
      "Epoch [1/1], Step [2098/7635], Loss: 4.4396\n",
      "Epoch [1/1], Step [2099/7635], Loss: 4.5038\n",
      "Epoch [1/1], Step [2100/7635], Loss: 4.5291\n",
      "Epoch [1/1], Step [2101/7635], Loss: 4.3059\n",
      "Epoch [1/1], Step [2102/7635], Loss: 4.4375\n",
      "Epoch [1/1], Step [2103/7635], Loss: 4.5157\n",
      "Epoch [1/1], Step [2104/7635], Loss: 4.4432\n",
      "Epoch [1/1], Step [2105/7635], Loss: 4.4607\n",
      "Epoch [1/1], Step [2106/7635], Loss: 4.4150\n",
      "Epoch [1/1], Step [2107/7635], Loss: 4.5349\n",
      "Epoch [1/1], Step [2108/7635], Loss: 4.4652\n",
      "Epoch [1/1], Step [2109/7635], Loss: 4.3771\n",
      "Epoch [1/1], Step [2110/7635], Loss: 4.4968\n",
      "Epoch [1/1], Step [2111/7635], Loss: 4.4515\n",
      "Epoch [1/1], Step [2112/7635], Loss: 4.4850\n",
      "Epoch [1/1], Step [2113/7635], Loss: 4.4057\n",
      "Epoch [1/1], Step [2114/7635], Loss: 4.4549\n",
      "Epoch [1/1], Step [2115/7635], Loss: 4.4067\n",
      "Epoch [1/1], Step [2116/7635], Loss: 4.4112\n",
      "Epoch [1/1], Step [2117/7635], Loss: 4.4320\n",
      "Epoch [1/1], Step [2118/7635], Loss: 4.5279\n",
      "Epoch [1/1], Step [2119/7635], Loss: 4.5239\n",
      "Epoch [1/1], Step [2120/7635], Loss: 4.4949\n",
      "Epoch [1/1], Step [2121/7635], Loss: 4.4430\n",
      "Epoch [1/1], Step [2122/7635], Loss: 4.5842\n",
      "Epoch [1/1], Step [2123/7635], Loss: 4.5195\n",
      "Epoch [1/1], Step [2124/7635], Loss: 4.4845\n",
      "Epoch [1/1], Step [2125/7635], Loss: 4.3993\n",
      "Epoch [1/1], Step [2126/7635], Loss: 4.4427\n",
      "Epoch [1/1], Step [2127/7635], Loss: 4.5415\n",
      "Epoch [1/1], Step [2128/7635], Loss: 4.4139\n",
      "Epoch [1/1], Step [2129/7635], Loss: 4.4230\n",
      "Epoch [1/1], Step [2130/7635], Loss: 4.4396\n",
      "Epoch [1/1], Step [2131/7635], Loss: 4.4325\n",
      "Epoch [1/1], Step [2132/7635], Loss: 4.5467\n",
      "Epoch [1/1], Step [2133/7635], Loss: 4.5331\n",
      "Epoch [1/1], Step [2134/7635], Loss: 4.4476\n",
      "Epoch [1/1], Step [2135/7635], Loss: 4.4574\n",
      "Epoch [1/1], Step [2136/7635], Loss: 4.4790\n",
      "Epoch [1/1], Step [2137/7635], Loss: 4.4896\n",
      "Epoch [1/1], Step [2138/7635], Loss: 4.3779\n",
      "Epoch [1/1], Step [2139/7635], Loss: 4.3993\n",
      "Epoch [1/1], Step [2140/7635], Loss: 4.4729\n",
      "Epoch [1/1], Step [2141/7635], Loss: 4.4947\n",
      "Epoch [1/1], Step [2142/7635], Loss: 4.5040\n",
      "Epoch [1/1], Step [2143/7635], Loss: 4.4433\n",
      "Epoch [1/1], Step [2144/7635], Loss: 4.4961\n",
      "Epoch [1/1], Step [2145/7635], Loss: 4.5417\n",
      "Epoch [1/1], Step [2146/7635], Loss: 4.3946\n",
      "Epoch [1/1], Step [2147/7635], Loss: 4.4959\n",
      "Epoch [1/1], Step [2148/7635], Loss: 4.4626\n",
      "Epoch [1/1], Step [2149/7635], Loss: 4.4534\n",
      "Epoch [1/1], Step [2150/7635], Loss: 4.4329\n",
      "Epoch [1/1], Step [2151/7635], Loss: 4.4856\n",
      "Epoch [1/1], Step [2152/7635], Loss: 4.4156\n",
      "Epoch [1/1], Step [2153/7635], Loss: 4.4895\n",
      "Epoch [1/1], Step [2154/7635], Loss: 4.3968\n",
      "Epoch [1/1], Step [2155/7635], Loss: 4.5044\n",
      "Epoch [1/1], Step [2156/7635], Loss: 4.5477\n",
      "Epoch [1/1], Step [2157/7635], Loss: 4.3936\n",
      "Epoch [1/1], Step [2158/7635], Loss: 4.4636\n",
      "Epoch [1/1], Step [2159/7635], Loss: 4.5739\n",
      "Epoch [1/1], Step [2160/7635], Loss: 4.4260\n",
      "Epoch [1/1], Step [2161/7635], Loss: 4.5533\n",
      "Epoch [1/1], Step [2162/7635], Loss: 4.5697\n",
      "Epoch [1/1], Step [2163/7635], Loss: 4.5162\n",
      "Epoch [1/1], Step [2164/7635], Loss: 4.4110\n",
      "Epoch [1/1], Step [2165/7635], Loss: 4.4923\n",
      "Epoch [1/1], Step [2166/7635], Loss: 4.5427\n",
      "Epoch [1/1], Step [2167/7635], Loss: 4.4745\n",
      "Epoch [1/1], Step [2168/7635], Loss: 4.4043\n",
      "Epoch [1/1], Step [2169/7635], Loss: 4.4452\n",
      "Epoch [1/1], Step [2170/7635], Loss: 4.3564\n",
      "Epoch [1/1], Step [2171/7635], Loss: 4.4550\n",
      "Epoch [1/1], Step [2172/7635], Loss: 4.4619\n",
      "Epoch [1/1], Step [2173/7635], Loss: 4.3986\n",
      "Epoch [1/1], Step [2174/7635], Loss: 4.4762\n",
      "Epoch [1/1], Step [2175/7635], Loss: 4.3961\n",
      "Epoch [1/1], Step [2176/7635], Loss: 4.4157\n",
      "Epoch [1/1], Step [2177/7635], Loss: 4.5179\n",
      "Epoch [1/1], Step [2178/7635], Loss: 4.4713\n",
      "Epoch [1/1], Step [2179/7635], Loss: 4.4070\n",
      "Epoch [1/1], Step [2180/7635], Loss: 4.4347\n",
      "Epoch [1/1], Step [2181/7635], Loss: 4.4912\n",
      "Epoch [1/1], Step [2182/7635], Loss: 4.4573\n",
      "Epoch [1/1], Step [2183/7635], Loss: 4.4272\n",
      "Epoch [1/1], Step [2184/7635], Loss: 4.3668\n",
      "Epoch [1/1], Step [2185/7635], Loss: 4.5139\n",
      "Epoch [1/1], Step [2186/7635], Loss: 4.4906\n",
      "Epoch [1/1], Step [2187/7635], Loss: 4.5034\n",
      "Epoch [1/1], Step [2188/7635], Loss: 4.4903\n",
      "Epoch [1/1], Step [2189/7635], Loss: 4.4275\n",
      "Epoch [1/1], Step [2190/7635], Loss: 4.4151\n",
      "Epoch [1/1], Step [2191/7635], Loss: 4.4432\n",
      "Epoch [1/1], Step [2192/7635], Loss: 4.5119\n",
      "Epoch [1/1], Step [2193/7635], Loss: 4.3500\n",
      "Epoch [1/1], Step [2194/7635], Loss: 4.4956\n",
      "Epoch [1/1], Step [2195/7635], Loss: 4.4467\n",
      "Epoch [1/1], Step [2196/7635], Loss: 4.4064\n",
      "Epoch [1/1], Step [2197/7635], Loss: 4.5307\n",
      "Epoch [1/1], Step [2198/7635], Loss: 4.4892\n",
      "Epoch [1/1], Step [2199/7635], Loss: 4.3822\n",
      "Epoch [1/1], Step [2200/7635], Loss: 4.5475\n",
      "Epoch [1/1], Step [2201/7635], Loss: 4.4440\n",
      "Epoch [1/1], Step [2202/7635], Loss: 4.4243\n",
      "Epoch [1/1], Step [2203/7635], Loss: 4.3847\n",
      "Epoch [1/1], Step [2204/7635], Loss: 4.4186\n",
      "Epoch [1/1], Step [2205/7635], Loss: 4.4142\n",
      "Epoch [1/1], Step [2206/7635], Loss: 4.4310\n",
      "Epoch [1/1], Step [2207/7635], Loss: 4.4576\n",
      "Epoch [1/1], Step [2208/7635], Loss: 4.4934\n",
      "Epoch [1/1], Step [2209/7635], Loss: 4.4644\n",
      "Epoch [1/1], Step [2210/7635], Loss: 4.4389\n",
      "Epoch [1/1], Step [2211/7635], Loss: 4.4940\n",
      "Epoch [1/1], Step [2212/7635], Loss: 4.5110\n",
      "Epoch [1/1], Step [2213/7635], Loss: 4.4538\n",
      "Epoch [1/1], Step [2214/7635], Loss: 4.4730\n",
      "Epoch [1/1], Step [2215/7635], Loss: 4.4094\n",
      "Epoch [1/1], Step [2216/7635], Loss: 4.4431\n",
      "Epoch [1/1], Step [2217/7635], Loss: 4.4784\n",
      "Epoch [1/1], Step [2218/7635], Loss: 4.4443\n",
      "Epoch [1/1], Step [2219/7635], Loss: 4.4825\n",
      "Epoch [1/1], Step [2220/7635], Loss: 4.4264\n",
      "Epoch [1/1], Step [2221/7635], Loss: 4.3777\n",
      "Epoch [1/1], Step [2222/7635], Loss: 4.4727\n",
      "Epoch [1/1], Step [2223/7635], Loss: 4.4087\n",
      "Epoch [1/1], Step [2224/7635], Loss: 4.4332\n",
      "Epoch [1/1], Step [2225/7635], Loss: 4.4480\n",
      "Epoch [1/1], Step [2226/7635], Loss: 4.4710\n",
      "Epoch [1/1], Step [2227/7635], Loss: 4.5061\n",
      "Epoch [1/1], Step [2228/7635], Loss: 4.4749\n",
      "Epoch [1/1], Step [2229/7635], Loss: 4.4490\n",
      "Epoch [1/1], Step [2230/7635], Loss: 4.4462\n",
      "Epoch [1/1], Step [2231/7635], Loss: 4.4680\n",
      "Epoch [1/1], Step [2232/7635], Loss: 4.4427\n",
      "Epoch [1/1], Step [2233/7635], Loss: 4.4796\n",
      "Epoch [1/1], Step [2234/7635], Loss: 4.5477\n",
      "Epoch [1/1], Step [2235/7635], Loss: 4.5223\n",
      "Epoch [1/1], Step [2236/7635], Loss: 4.4490\n",
      "Epoch [1/1], Step [2237/7635], Loss: 4.4174\n",
      "Epoch [1/1], Step [2238/7635], Loss: 4.3752\n",
      "Epoch [1/1], Step [2239/7635], Loss: 4.3877\n",
      "Epoch [1/1], Step [2240/7635], Loss: 4.4357\n",
      "Epoch [1/1], Step [2241/7635], Loss: 4.4351\n",
      "Epoch [1/1], Step [2242/7635], Loss: 4.3791\n",
      "Epoch [1/1], Step [2243/7635], Loss: 4.5161\n",
      "Epoch [1/1], Step [2244/7635], Loss: 4.4998\n",
      "Epoch [1/1], Step [2245/7635], Loss: 4.4644\n",
      "Epoch [1/1], Step [2246/7635], Loss: 4.3597\n",
      "Epoch [1/1], Step [2247/7635], Loss: 4.4629\n",
      "Epoch [1/1], Step [2248/7635], Loss: 4.5292\n",
      "Epoch [1/1], Step [2249/7635], Loss: 4.4384\n",
      "Epoch [1/1], Step [2250/7635], Loss: 4.4070\n",
      "Epoch [1/1], Step [2251/7635], Loss: 4.4453\n",
      "Epoch [1/1], Step [2252/7635], Loss: 4.3913\n",
      "Epoch [1/1], Step [2253/7635], Loss: 4.4357\n",
      "Epoch [1/1], Step [2254/7635], Loss: 4.4367\n",
      "Epoch [1/1], Step [2255/7635], Loss: 4.5077\n",
      "Epoch [1/1], Step [2256/7635], Loss: 4.4635\n",
      "Epoch [1/1], Step [2257/7635], Loss: 4.4389\n",
      "Epoch [1/1], Step [2258/7635], Loss: 4.3518\n",
      "Epoch [1/1], Step [2259/7635], Loss: 4.4469\n",
      "Epoch [1/1], Step [2260/7635], Loss: 4.4427\n",
      "Epoch [1/1], Step [2261/7635], Loss: 4.4373\n",
      "Epoch [1/1], Step [2262/7635], Loss: 4.4713\n",
      "Epoch [1/1], Step [2263/7635], Loss: 4.4092\n",
      "Epoch [1/1], Step [2264/7635], Loss: 4.3795\n",
      "Epoch [1/1], Step [2265/7635], Loss: 4.3982\n",
      "Epoch [1/1], Step [2266/7635], Loss: 4.4927\n",
      "Epoch [1/1], Step [2267/7635], Loss: 4.4031\n",
      "Epoch [1/1], Step [2268/7635], Loss: 4.4592\n",
      "Epoch [1/1], Step [2269/7635], Loss: 4.5364\n",
      "Epoch [1/1], Step [2270/7635], Loss: 4.3267\n",
      "Epoch [1/1], Step [2271/7635], Loss: 4.4584\n",
      "Epoch [1/1], Step [2272/7635], Loss: 4.5182\n",
      "Epoch [1/1], Step [2273/7635], Loss: 4.3508\n",
      "Epoch [1/1], Step [2274/7635], Loss: 4.3970\n",
      "Epoch [1/1], Step [2275/7635], Loss: 4.3966\n",
      "Epoch [1/1], Step [2276/7635], Loss: 4.4294\n",
      "Epoch [1/1], Step [2277/7635], Loss: 4.4985\n",
      "Epoch [1/1], Step [2278/7635], Loss: 4.4614\n",
      "Epoch [1/1], Step [2279/7635], Loss: 4.3882\n",
      "Epoch [1/1], Step [2280/7635], Loss: 4.4529\n",
      "Epoch [1/1], Step [2281/7635], Loss: 4.3678\n",
      "Epoch [1/1], Step [2282/7635], Loss: 4.4858\n",
      "Epoch [1/1], Step [2283/7635], Loss: 4.4279\n",
      "Epoch [1/1], Step [2284/7635], Loss: 4.4157\n",
      "Epoch [1/1], Step [2285/7635], Loss: 4.4158\n",
      "Epoch [1/1], Step [2286/7635], Loss: 4.4518\n",
      "Epoch [1/1], Step [2287/7635], Loss: 4.4824\n",
      "Epoch [1/1], Step [2288/7635], Loss: 4.5162\n",
      "Epoch [1/1], Step [2289/7635], Loss: 4.5571\n",
      "Epoch [1/1], Step [2290/7635], Loss: 4.4149\n",
      "Epoch [1/1], Step [2291/7635], Loss: 4.3124\n",
      "Epoch [1/1], Step [2292/7635], Loss: 4.4754\n",
      "Epoch [1/1], Step [2293/7635], Loss: 4.4369\n",
      "Epoch [1/1], Step [2294/7635], Loss: 4.3818\n",
      "Epoch [1/1], Step [2295/7635], Loss: 4.4423\n",
      "Epoch [1/1], Step [2296/7635], Loss: 4.4424\n",
      "Epoch [1/1], Step [2297/7635], Loss: 4.3662\n",
      "Epoch [1/1], Step [2298/7635], Loss: 4.3984\n",
      "Epoch [1/1], Step [2299/7635], Loss: 4.3317\n",
      "Epoch [1/1], Step [2300/7635], Loss: 4.4437\n",
      "Epoch [1/1], Step [2301/7635], Loss: 4.4432\n",
      "Epoch [1/1], Step [2302/7635], Loss: 4.4595\n",
      "Epoch [1/1], Step [2303/7635], Loss: 4.5163\n",
      "Epoch [1/1], Step [2304/7635], Loss: 4.4324\n",
      "Epoch [1/1], Step [2305/7635], Loss: 4.4780\n",
      "Epoch [1/1], Step [2306/7635], Loss: 4.3170\n",
      "Epoch [1/1], Step [2307/7635], Loss: 4.4645\n",
      "Epoch [1/1], Step [2308/7635], Loss: 4.3436\n",
      "Epoch [1/1], Step [2309/7635], Loss: 4.5126\n",
      "Epoch [1/1], Step [2310/7635], Loss: 4.4898\n",
      "Epoch [1/1], Step [2311/7635], Loss: 4.4590\n",
      "Epoch [1/1], Step [2312/7635], Loss: 4.4511\n",
      "Epoch [1/1], Step [2313/7635], Loss: 4.4127\n",
      "Epoch [1/1], Step [2314/7635], Loss: 4.4517\n",
      "Epoch [1/1], Step [2315/7635], Loss: 4.3823\n",
      "Epoch [1/1], Step [2316/7635], Loss: 4.4809\n",
      "Epoch [1/1], Step [2317/7635], Loss: 4.4248\n",
      "Epoch [1/1], Step [2318/7635], Loss: 4.4056\n",
      "Epoch [1/1], Step [2319/7635], Loss: 4.4227\n",
      "Epoch [1/1], Step [2320/7635], Loss: 4.4779\n",
      "Epoch [1/1], Step [2321/7635], Loss: 4.4192\n",
      "Epoch [1/1], Step [2322/7635], Loss: 4.4641\n",
      "Epoch [1/1], Step [2323/7635], Loss: 4.3771\n",
      "Epoch [1/1], Step [2324/7635], Loss: 4.4250\n",
      "Epoch [1/1], Step [2325/7635], Loss: 4.5056\n",
      "Epoch [1/1], Step [2326/7635], Loss: 4.4889\n",
      "Epoch [1/1], Step [2327/7635], Loss: 4.4151\n",
      "Epoch [1/1], Step [2328/7635], Loss: 4.4353\n",
      "Epoch [1/1], Step [2329/7635], Loss: 4.4393\n",
      "Epoch [1/1], Step [2330/7635], Loss: 4.4172\n",
      "Epoch [1/1], Step [2331/7635], Loss: 4.3430\n",
      "Epoch [1/1], Step [2332/7635], Loss: 4.3953\n",
      "Epoch [1/1], Step [2333/7635], Loss: 4.4252\n",
      "Epoch [1/1], Step [2334/7635], Loss: 4.4533\n",
      "Epoch [1/1], Step [2335/7635], Loss: 4.4475\n",
      "Epoch [1/1], Step [2336/7635], Loss: 4.4770\n",
      "Epoch [1/1], Step [2337/7635], Loss: 4.5027\n",
      "Epoch [1/1], Step [2338/7635], Loss: 4.3765\n",
      "Epoch [1/1], Step [2339/7635], Loss: 4.3760\n",
      "Epoch [1/1], Step [2340/7635], Loss: 4.5073\n",
      "Epoch [1/1], Step [2341/7635], Loss: 4.4939\n",
      "Epoch [1/1], Step [2342/7635], Loss: 4.5337\n",
      "Epoch [1/1], Step [2343/7635], Loss: 4.4598\n",
      "Epoch [1/1], Step [2344/7635], Loss: 4.4065\n",
      "Epoch [1/1], Step [2345/7635], Loss: 4.4124\n",
      "Epoch [1/1], Step [2346/7635], Loss: 4.4529\n",
      "Epoch [1/1], Step [2347/7635], Loss: 4.4003\n",
      "Epoch [1/1], Step [2348/7635], Loss: 4.3711\n",
      "Epoch [1/1], Step [2349/7635], Loss: 4.3683\n",
      "Epoch [1/1], Step [2350/7635], Loss: 4.4060\n",
      "Epoch [1/1], Step [2351/7635], Loss: 4.3062\n",
      "Epoch [1/1], Step [2352/7635], Loss: 4.4436\n",
      "Epoch [1/1], Step [2353/7635], Loss: 4.4428\n",
      "Epoch [1/1], Step [2354/7635], Loss: 4.3501\n",
      "Epoch [1/1], Step [2355/7635], Loss: 4.3798\n",
      "Epoch [1/1], Step [2356/7635], Loss: 4.5009\n",
      "Epoch [1/1], Step [2357/7635], Loss: 4.4345\n",
      "Epoch [1/1], Step [2358/7635], Loss: 4.4346\n",
      "Epoch [1/1], Step [2359/7635], Loss: 4.4421\n",
      "Epoch [1/1], Step [2360/7635], Loss: 4.3548\n",
      "Epoch [1/1], Step [2361/7635], Loss: 4.4620\n",
      "Epoch [1/1], Step [2362/7635], Loss: 4.4144\n",
      "Epoch [1/1], Step [2363/7635], Loss: 4.4399\n",
      "Epoch [1/1], Step [2364/7635], Loss: 4.4258\n",
      "Epoch [1/1], Step [2365/7635], Loss: 4.3686\n",
      "Epoch [1/1], Step [2366/7635], Loss: 4.4455\n",
      "Epoch [1/1], Step [2367/7635], Loss: 4.3817\n",
      "Epoch [1/1], Step [2368/7635], Loss: 4.4090\n",
      "Epoch [1/1], Step [2369/7635], Loss: 4.4227\n",
      "Epoch [1/1], Step [2370/7635], Loss: 4.3855\n",
      "Epoch [1/1], Step [2371/7635], Loss: 4.3750\n",
      "Epoch [1/1], Step [2372/7635], Loss: 4.4312\n",
      "Epoch [1/1], Step [2373/7635], Loss: 4.4256\n",
      "Epoch [1/1], Step [2374/7635], Loss: 4.4578\n",
      "Epoch [1/1], Step [2375/7635], Loss: 4.4595\n",
      "Epoch [1/1], Step [2376/7635], Loss: 4.3645\n",
      "Epoch [1/1], Step [2377/7635], Loss: 4.4318\n",
      "Epoch [1/1], Step [2378/7635], Loss: 4.4910\n",
      "Epoch [1/1], Step [2379/7635], Loss: 4.4217\n",
      "Epoch [1/1], Step [2380/7635], Loss: 4.4682\n",
      "Epoch [1/1], Step [2381/7635], Loss: 4.3679\n",
      "Epoch [1/1], Step [2382/7635], Loss: 4.3990\n",
      "Epoch [1/1], Step [2383/7635], Loss: 4.3713\n",
      "Epoch [1/1], Step [2384/7635], Loss: 4.2462\n",
      "Epoch [1/1], Step [2385/7635], Loss: 4.3897\n",
      "Epoch [1/1], Step [2386/7635], Loss: 4.4401\n",
      "Epoch [1/1], Step [2387/7635], Loss: 4.4053\n",
      "Epoch [1/1], Step [2388/7635], Loss: 4.4335\n",
      "Epoch [1/1], Step [2389/7635], Loss: 4.4563\n",
      "Epoch [1/1], Step [2390/7635], Loss: 4.3653\n",
      "Epoch [1/1], Step [2391/7635], Loss: 4.4266\n",
      "Epoch [1/1], Step [2392/7635], Loss: 4.4880\n",
      "Epoch [1/1], Step [2393/7635], Loss: 4.4165\n",
      "Epoch [1/1], Step [2394/7635], Loss: 4.3771\n",
      "Epoch [1/1], Step [2395/7635], Loss: 4.4062\n",
      "Epoch [1/1], Step [2396/7635], Loss: 4.4588\n",
      "Epoch [1/1], Step [2397/7635], Loss: 4.4291\n",
      "Epoch [1/1], Step [2398/7635], Loss: 4.4450\n",
      "Epoch [1/1], Step [2399/7635], Loss: 4.3232\n",
      "Epoch [1/1], Step [2400/7635], Loss: 4.4071\n",
      "Epoch [1/1], Step [2401/7635], Loss: 4.4017\n",
      "Epoch [1/1], Step [2402/7635], Loss: 4.4569\n",
      "Epoch [1/1], Step [2403/7635], Loss: 4.3833\n",
      "Epoch [1/1], Step [2404/7635], Loss: 4.4344\n",
      "Epoch [1/1], Step [2405/7635], Loss: 4.4085\n",
      "Epoch [1/1], Step [2406/7635], Loss: 4.4213\n",
      "Epoch [1/1], Step [2407/7635], Loss: 4.3683\n",
      "Epoch [1/1], Step [2408/7635], Loss: 4.3229\n",
      "Epoch [1/1], Step [2409/7635], Loss: 4.4733\n",
      "Epoch [1/1], Step [2410/7635], Loss: 4.5042\n",
      "Epoch [1/1], Step [2411/7635], Loss: 4.4355\n",
      "Epoch [1/1], Step [2412/7635], Loss: 4.4810\n",
      "Epoch [1/1], Step [2413/7635], Loss: 4.3711\n",
      "Epoch [1/1], Step [2414/7635], Loss: 4.4362\n",
      "Epoch [1/1], Step [2415/7635], Loss: 4.4889\n",
      "Epoch [1/1], Step [2416/7635], Loss: 4.4796\n",
      "Epoch [1/1], Step [2417/7635], Loss: 4.3893\n",
      "Epoch [1/1], Step [2418/7635], Loss: 4.4065\n",
      "Epoch [1/1], Step [2419/7635], Loss: 4.3433\n",
      "Epoch [1/1], Step [2420/7635], Loss: 4.5131\n",
      "Epoch [1/1], Step [2421/7635], Loss: 4.3789\n",
      "Epoch [1/1], Step [2422/7635], Loss: 4.3852\n",
      "Epoch [1/1], Step [2423/7635], Loss: 4.3837\n",
      "Epoch [1/1], Step [2424/7635], Loss: 4.4783\n",
      "Epoch [1/1], Step [2425/7635], Loss: 4.3849\n",
      "Epoch [1/1], Step [2426/7635], Loss: 4.3956\n",
      "Epoch [1/1], Step [2427/7635], Loss: 4.4536\n",
      "Epoch [1/1], Step [2428/7635], Loss: 4.3587\n",
      "Epoch [1/1], Step [2429/7635], Loss: 4.3766\n",
      "Epoch [1/1], Step [2430/7635], Loss: 4.4418\n",
      "Epoch [1/1], Step [2431/7635], Loss: 4.4565\n",
      "Epoch [1/1], Step [2432/7635], Loss: 4.3750\n",
      "Epoch [1/1], Step [2433/7635], Loss: 4.4186\n",
      "Epoch [1/1], Step [2434/7635], Loss: 4.3531\n",
      "Epoch [1/1], Step [2435/7635], Loss: 4.4058\n",
      "Epoch [1/1], Step [2436/7635], Loss: 4.3392\n",
      "Epoch [1/1], Step [2437/7635], Loss: 4.3271\n",
      "Epoch [1/1], Step [2438/7635], Loss: 4.4223\n",
      "Epoch [1/1], Step [2439/7635], Loss: 4.4214\n",
      "Epoch [1/1], Step [2440/7635], Loss: 4.4353\n",
      "Epoch [1/1], Step [2441/7635], Loss: 4.3616\n",
      "Epoch [1/1], Step [2442/7635], Loss: 4.4516\n",
      "Epoch [1/1], Step [2443/7635], Loss: 4.4815\n",
      "Epoch [1/1], Step [2444/7635], Loss: 4.3762\n",
      "Epoch [1/1], Step [2445/7635], Loss: 4.3391\n",
      "Epoch [1/1], Step [2446/7635], Loss: 4.2666\n",
      "Epoch [1/1], Step [2447/7635], Loss: 4.4326\n",
      "Epoch [1/1], Step [2448/7635], Loss: 4.4264\n",
      "Epoch [1/1], Step [2449/7635], Loss: 4.3796\n",
      "Epoch [1/1], Step [2450/7635], Loss: 4.3739\n",
      "Epoch [1/1], Step [2451/7635], Loss: 4.3235\n",
      "Epoch [1/1], Step [2452/7635], Loss: 4.4270\n",
      "Epoch [1/1], Step [2453/7635], Loss: 4.4516\n",
      "Epoch [1/1], Step [2454/7635], Loss: 4.3818\n",
      "Epoch [1/1], Step [2455/7635], Loss: 4.4648\n",
      "Epoch [1/1], Step [2456/7635], Loss: 4.3917\n",
      "Epoch [1/1], Step [2457/7635], Loss: 4.4087\n",
      "Epoch [1/1], Step [2458/7635], Loss: 4.4432\n",
      "Epoch [1/1], Step [2459/7635], Loss: 4.3536\n",
      "Epoch [1/1], Step [2460/7635], Loss: 4.3359\n",
      "Epoch [1/1], Step [2461/7635], Loss: 4.4397\n",
      "Epoch [1/1], Step [2462/7635], Loss: 4.5233\n",
      "Epoch [1/1], Step [2463/7635], Loss: 4.4455\n",
      "Epoch [1/1], Step [2464/7635], Loss: 4.3584\n",
      "Epoch [1/1], Step [2465/7635], Loss: 4.3880\n",
      "Epoch [1/1], Step [2466/7635], Loss: 4.4022\n",
      "Epoch [1/1], Step [2467/7635], Loss: 4.4710\n",
      "Epoch [1/1], Step [2468/7635], Loss: 4.3630\n",
      "Epoch [1/1], Step [2469/7635], Loss: 4.4725\n",
      "Epoch [1/1], Step [2470/7635], Loss: 4.4155\n",
      "Epoch [1/1], Step [2471/7635], Loss: 4.4919\n",
      "Epoch [1/1], Step [2472/7635], Loss: 4.3912\n",
      "Epoch [1/1], Step [2473/7635], Loss: 4.3842\n",
      "Epoch [1/1], Step [2474/7635], Loss: 4.3260\n",
      "Epoch [1/1], Step [2475/7635], Loss: 4.3998\n",
      "Epoch [1/1], Step [2476/7635], Loss: 4.4485\n",
      "Epoch [1/1], Step [2477/7635], Loss: 4.5001\n",
      "Epoch [1/1], Step [2478/7635], Loss: 4.3798\n",
      "Epoch [1/1], Step [2479/7635], Loss: 4.5254\n",
      "Epoch [1/1], Step [2480/7635], Loss: 4.4038\n",
      "Epoch [1/1], Step [2481/7635], Loss: 4.3568\n",
      "Epoch [1/1], Step [2482/7635], Loss: 4.3865\n",
      "Epoch [1/1], Step [2483/7635], Loss: 4.4314\n",
      "Epoch [1/1], Step [2484/7635], Loss: 4.3440\n",
      "Epoch [1/1], Step [2485/7635], Loss: 4.3178\n",
      "Epoch [1/1], Step [2486/7635], Loss: 4.4907\n",
      "Epoch [1/1], Step [2487/7635], Loss: 4.3966\n",
      "Epoch [1/1], Step [2488/7635], Loss: 4.3528\n",
      "Epoch [1/1], Step [2489/7635], Loss: 4.4068\n",
      "Epoch [1/1], Step [2490/7635], Loss: 4.4119\n",
      "Epoch [1/1], Step [2491/7635], Loss: 4.4123\n",
      "Epoch [1/1], Step [2492/7635], Loss: 4.3283\n",
      "Epoch [1/1], Step [2493/7635], Loss: 4.3184\n",
      "Epoch [1/1], Step [2494/7635], Loss: 4.4210\n",
      "Epoch [1/1], Step [2495/7635], Loss: 4.3656\n",
      "Epoch [1/1], Step [2496/7635], Loss: 4.4702\n",
      "Epoch [1/1], Step [2497/7635], Loss: 4.3503\n",
      "Epoch [1/1], Step [2498/7635], Loss: 4.3683\n",
      "Epoch [1/1], Step [2499/7635], Loss: 4.3589\n",
      "Epoch [1/1], Step [2500/7635], Loss: 4.4766\n",
      "Epoch [1/1], Step [2501/7635], Loss: 4.3893\n",
      "Epoch [1/1], Step [2502/7635], Loss: 4.4790\n",
      "Epoch [1/1], Step [2503/7635], Loss: 4.3921\n",
      "Epoch [1/1], Step [2504/7635], Loss: 4.4243\n",
      "Epoch [1/1], Step [2505/7635], Loss: 4.3452\n",
      "Epoch [1/1], Step [2506/7635], Loss: 4.4243\n",
      "Epoch [1/1], Step [2507/7635], Loss: 4.3640\n",
      "Epoch [1/1], Step [2508/7635], Loss: 4.4195\n",
      "Epoch [1/1], Step [2509/7635], Loss: 4.3816\n",
      "Epoch [1/1], Step [2510/7635], Loss: 4.3424\n",
      "Epoch [1/1], Step [2511/7635], Loss: 4.3443\n",
      "Epoch [1/1], Step [2512/7635], Loss: 4.3946\n",
      "Epoch [1/1], Step [2513/7635], Loss: 4.4604\n",
      "Epoch [1/1], Step [2514/7635], Loss: 4.4417\n",
      "Epoch [1/1], Step [2515/7635], Loss: 4.3130\n",
      "Epoch [1/1], Step [2516/7635], Loss: 4.3626\n",
      "Epoch [1/1], Step [2517/7635], Loss: 4.3721\n",
      "Epoch [1/1], Step [2518/7635], Loss: 4.4364\n",
      "Epoch [1/1], Step [2519/7635], Loss: 4.3084\n",
      "Epoch [1/1], Step [2520/7635], Loss: 4.5599\n",
      "Epoch [1/1], Step [2521/7635], Loss: 4.4101\n",
      "Epoch [1/1], Step [2522/7635], Loss: 4.4396\n",
      "Epoch [1/1], Step [2523/7635], Loss: 4.3558\n",
      "Epoch [1/1], Step [2524/7635], Loss: 4.4577\n",
      "Epoch [1/1], Step [2525/7635], Loss: 4.3813\n",
      "Epoch [1/1], Step [2526/7635], Loss: 4.4182\n",
      "Epoch [1/1], Step [2527/7635], Loss: 4.4605\n",
      "Epoch [1/1], Step [2528/7635], Loss: 4.4797\n",
      "Epoch [1/1], Step [2529/7635], Loss: 4.3549\n",
      "Epoch [1/1], Step [2530/7635], Loss: 4.3804\n",
      "Epoch [1/1], Step [2531/7635], Loss: 4.4011\n",
      "Epoch [1/1], Step [2532/7635], Loss: 4.4809\n",
      "Epoch [1/1], Step [2533/7635], Loss: 4.3543\n",
      "Epoch [1/1], Step [2534/7635], Loss: 4.2770\n",
      "Epoch [1/1], Step [2535/7635], Loss: 4.4418\n",
      "Epoch [1/1], Step [2536/7635], Loss: 4.4999\n",
      "Epoch [1/1], Step [2537/7635], Loss: 4.3990\n",
      "Epoch [1/1], Step [2538/7635], Loss: 4.3128\n",
      "Epoch [1/1], Step [2539/7635], Loss: 4.4534\n",
      "Epoch [1/1], Step [2540/7635], Loss: 4.4545\n",
      "Epoch [1/1], Step [2541/7635], Loss: 4.3843\n",
      "Epoch [1/1], Step [2542/7635], Loss: 4.3437\n",
      "Epoch [1/1], Step [2543/7635], Loss: 4.2722\n",
      "Epoch [1/1], Step [2544/7635], Loss: 4.3785\n",
      "Epoch [1/1], Step [2545/7635], Loss: 4.4059\n",
      "Epoch [1/1], Step [2546/7635], Loss: 4.3345\n",
      "Epoch [1/1], Step [2547/7635], Loss: 4.4142\n",
      "Epoch [1/1], Step [2548/7635], Loss: 4.3644\n",
      "Epoch [1/1], Step [2549/7635], Loss: 4.4177\n",
      "Epoch [1/1], Step [2550/7635], Loss: 4.3535\n",
      "Epoch [1/1], Step [2551/7635], Loss: 4.3683\n",
      "Epoch [1/1], Step [2552/7635], Loss: 4.3581\n",
      "Epoch [1/1], Step [2553/7635], Loss: 4.3947\n",
      "Epoch [1/1], Step [2554/7635], Loss: 4.4420\n",
      "Epoch [1/1], Step [2555/7635], Loss: 4.4059\n",
      "Epoch [1/1], Step [2556/7635], Loss: 4.3925\n",
      "Epoch [1/1], Step [2557/7635], Loss: 4.4458\n",
      "Epoch [1/1], Step [2558/7635], Loss: 4.3014\n",
      "Epoch [1/1], Step [2559/7635], Loss: 4.4176\n",
      "Epoch [1/1], Step [2560/7635], Loss: 4.3743\n",
      "Epoch [1/1], Step [2561/7635], Loss: 4.3494\n",
      "Epoch [1/1], Step [2562/7635], Loss: 4.3761\n",
      "Epoch [1/1], Step [2563/7635], Loss: 4.4074\n",
      "Epoch [1/1], Step [2564/7635], Loss: 4.4047\n",
      "Epoch [1/1], Step [2565/7635], Loss: 4.4561\n",
      "Epoch [1/1], Step [2566/7635], Loss: 4.4459\n",
      "Epoch [1/1], Step [2567/7635], Loss: 4.3516\n",
      "Epoch [1/1], Step [2568/7635], Loss: 4.4340\n",
      "Epoch [1/1], Step [2569/7635], Loss: 4.4175\n",
      "Epoch [1/1], Step [2570/7635], Loss: 4.5013\n",
      "Epoch [1/1], Step [2571/7635], Loss: 4.3911\n",
      "Epoch [1/1], Step [2572/7635], Loss: 4.4031\n",
      "Epoch [1/1], Step [2573/7635], Loss: 4.3833\n",
      "Epoch [1/1], Step [2574/7635], Loss: 4.3482\n",
      "Epoch [1/1], Step [2575/7635], Loss: 4.5049\n",
      "Epoch [1/1], Step [2576/7635], Loss: 4.3772\n",
      "Epoch [1/1], Step [2577/7635], Loss: 4.3520\n",
      "Epoch [1/1], Step [2578/7635], Loss: 4.3922\n",
      "Epoch [1/1], Step [2579/7635], Loss: 4.4602\n",
      "Epoch [1/1], Step [2580/7635], Loss: 4.3405\n",
      "Epoch [1/1], Step [2581/7635], Loss: 4.4169\n",
      "Epoch [1/1], Step [2582/7635], Loss: 4.4059\n",
      "Epoch [1/1], Step [2583/7635], Loss: 4.3496\n",
      "Epoch [1/1], Step [2584/7635], Loss: 4.4578\n",
      "Epoch [1/1], Step [2585/7635], Loss: 4.3474\n",
      "Epoch [1/1], Step [2586/7635], Loss: 4.3491\n",
      "Epoch [1/1], Step [2587/7635], Loss: 4.3109\n",
      "Epoch [1/1], Step [2588/7635], Loss: 4.4015\n",
      "Epoch [1/1], Step [2589/7635], Loss: 4.3723\n",
      "Epoch [1/1], Step [2590/7635], Loss: 4.3027\n",
      "Epoch [1/1], Step [2591/7635], Loss: 4.3845\n",
      "Epoch [1/1], Step [2592/7635], Loss: 4.4328\n",
      "Epoch [1/1], Step [2593/7635], Loss: 4.3368\n",
      "Epoch [1/1], Step [2594/7635], Loss: 4.3690\n",
      "Epoch [1/1], Step [2595/7635], Loss: 4.3683\n",
      "Epoch [1/1], Step [2596/7635], Loss: 4.3685\n",
      "Epoch [1/1], Step [2597/7635], Loss: 4.3337\n",
      "Epoch [1/1], Step [2598/7635], Loss: 4.4311\n",
      "Epoch [1/1], Step [2599/7635], Loss: 4.3101\n",
      "Epoch [1/1], Step [2600/7635], Loss: 4.5170\n",
      "Epoch [1/1], Step [2601/7635], Loss: 4.4162\n",
      "Epoch [1/1], Step [2602/7635], Loss: 4.4467\n",
      "Epoch [1/1], Step [2603/7635], Loss: 4.4719\n",
      "Epoch [1/1], Step [2604/7635], Loss: 4.4795\n",
      "Epoch [1/1], Step [2605/7635], Loss: 4.3763\n",
      "Epoch [1/1], Step [2606/7635], Loss: 4.4173\n",
      "Epoch [1/1], Step [2607/7635], Loss: 4.4422\n",
      "Epoch [1/1], Step [2608/7635], Loss: 4.4386\n",
      "Epoch [1/1], Step [2609/7635], Loss: 4.4153\n",
      "Epoch [1/1], Step [2610/7635], Loss: 4.3753\n",
      "Epoch [1/1], Step [2611/7635], Loss: 4.4136\n",
      "Epoch [1/1], Step [2612/7635], Loss: 4.3720\n",
      "Epoch [1/1], Step [2613/7635], Loss: 4.3547\n",
      "Epoch [1/1], Step [2614/7635], Loss: 4.4926\n",
      "Epoch [1/1], Step [2615/7635], Loss: 4.3953\n",
      "Epoch [1/1], Step [2616/7635], Loss: 4.3457\n",
      "Epoch [1/1], Step [2617/7635], Loss: 4.3679\n",
      "Epoch [1/1], Step [2618/7635], Loss: 4.4060\n",
      "Epoch [1/1], Step [2619/7635], Loss: 4.3819\n",
      "Epoch [1/1], Step [2620/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [2621/7635], Loss: 4.3731\n",
      "Epoch [1/1], Step [2622/7635], Loss: 4.4079\n",
      "Epoch [1/1], Step [2623/7635], Loss: 4.3964\n",
      "Epoch [1/1], Step [2624/7635], Loss: 4.4068\n",
      "Epoch [1/1], Step [2625/7635], Loss: 4.3365\n",
      "Epoch [1/1], Step [2626/7635], Loss: 4.3769\n",
      "Epoch [1/1], Step [2627/7635], Loss: 4.2980\n",
      "Epoch [1/1], Step [2628/7635], Loss: 4.3214\n",
      "Epoch [1/1], Step [2629/7635], Loss: 4.3431\n",
      "Epoch [1/1], Step [2630/7635], Loss: 4.3599\n",
      "Epoch [1/1], Step [2631/7635], Loss: 4.3072\n",
      "Epoch [1/1], Step [2632/7635], Loss: 4.4351\n",
      "Epoch [1/1], Step [2633/7635], Loss: 4.4518\n",
      "Epoch [1/1], Step [2634/7635], Loss: 4.4138\n",
      "Epoch [1/1], Step [2635/7635], Loss: 4.4260\n",
      "Epoch [1/1], Step [2636/7635], Loss: 4.2748\n",
      "Epoch [1/1], Step [2637/7635], Loss: 4.3525\n",
      "Epoch [1/1], Step [2638/7635], Loss: 4.3955\n",
      "Epoch [1/1], Step [2639/7635], Loss: 4.3623\n",
      "Epoch [1/1], Step [2640/7635], Loss: 4.4063\n",
      "Epoch [1/1], Step [2641/7635], Loss: 4.3588\n",
      "Epoch [1/1], Step [2642/7635], Loss: 4.4556\n",
      "Epoch [1/1], Step [2643/7635], Loss: 4.3836\n",
      "Epoch [1/1], Step [2644/7635], Loss: 4.3049\n",
      "Epoch [1/1], Step [2645/7635], Loss: 4.4908\n",
      "Epoch [1/1], Step [2646/7635], Loss: 4.4035\n",
      "Epoch [1/1], Step [2647/7635], Loss: 4.3715\n",
      "Epoch [1/1], Step [2648/7635], Loss: 4.3031\n",
      "Epoch [1/1], Step [2649/7635], Loss: 4.4123\n",
      "Epoch [1/1], Step [2650/7635], Loss: 4.2940\n",
      "Epoch [1/1], Step [2651/7635], Loss: 4.3440\n",
      "Epoch [1/1], Step [2652/7635], Loss: 4.3635\n",
      "Epoch [1/1], Step [2653/7635], Loss: 4.2244\n",
      "Epoch [1/1], Step [2654/7635], Loss: 4.4407\n",
      "Epoch [1/1], Step [2655/7635], Loss: 4.3946\n",
      "Epoch [1/1], Step [2656/7635], Loss: 4.4324\n",
      "Epoch [1/1], Step [2657/7635], Loss: 4.3055\n",
      "Epoch [1/1], Step [2658/7635], Loss: 4.4019\n",
      "Epoch [1/1], Step [2659/7635], Loss: 4.4022\n",
      "Epoch [1/1], Step [2660/7635], Loss: 4.4090\n",
      "Epoch [1/1], Step [2661/7635], Loss: 4.3682\n",
      "Epoch [1/1], Step [2662/7635], Loss: 4.3956\n",
      "Epoch [1/1], Step [2663/7635], Loss: 4.3703\n",
      "Epoch [1/1], Step [2664/7635], Loss: 4.4348\n",
      "Epoch [1/1], Step [2665/7635], Loss: 4.3872\n",
      "Epoch [1/1], Step [2666/7635], Loss: 4.3765\n",
      "Epoch [1/1], Step [2667/7635], Loss: 4.3578\n",
      "Epoch [1/1], Step [2668/7635], Loss: 4.3139\n",
      "Epoch [1/1], Step [2669/7635], Loss: 4.3601\n",
      "Epoch [1/1], Step [2670/7635], Loss: 4.4146\n",
      "Epoch [1/1], Step [2671/7635], Loss: 4.3676\n",
      "Epoch [1/1], Step [2672/7635], Loss: 4.2995\n",
      "Epoch [1/1], Step [2673/7635], Loss: 4.3622\n",
      "Epoch [1/1], Step [2674/7635], Loss: 4.3935\n",
      "Epoch [1/1], Step [2675/7635], Loss: 4.3662\n",
      "Epoch [1/1], Step [2676/7635], Loss: 4.3201\n",
      "Epoch [1/1], Step [2677/7635], Loss: 4.3516\n",
      "Epoch [1/1], Step [2678/7635], Loss: 4.4414\n",
      "Epoch [1/1], Step [2679/7635], Loss: 4.4007\n",
      "Epoch [1/1], Step [2680/7635], Loss: 4.4332\n",
      "Epoch [1/1], Step [2681/7635], Loss: 4.4167\n",
      "Epoch [1/1], Step [2682/7635], Loss: 4.4054\n",
      "Epoch [1/1], Step [2683/7635], Loss: 4.4180\n",
      "Epoch [1/1], Step [2684/7635], Loss: 4.3078\n",
      "Epoch [1/1], Step [2685/7635], Loss: 4.3271\n",
      "Epoch [1/1], Step [2686/7635], Loss: 4.2965\n",
      "Epoch [1/1], Step [2687/7635], Loss: 4.3259\n",
      "Epoch [1/1], Step [2688/7635], Loss: 4.3294\n",
      "Epoch [1/1], Step [2689/7635], Loss: 4.3623\n",
      "Epoch [1/1], Step [2690/7635], Loss: 4.3626\n",
      "Epoch [1/1], Step [2691/7635], Loss: 4.3739\n",
      "Epoch [1/1], Step [2692/7635], Loss: 4.3031\n",
      "Epoch [1/1], Step [2693/7635], Loss: 4.2450\n",
      "Epoch [1/1], Step [2694/7635], Loss: 4.4442\n",
      "Epoch [1/1], Step [2695/7635], Loss: 4.3925\n",
      "Epoch [1/1], Step [2696/7635], Loss: 4.3053\n",
      "Epoch [1/1], Step [2697/7635], Loss: 4.3886\n",
      "Epoch [1/1], Step [2698/7635], Loss: 4.3582\n",
      "Epoch [1/1], Step [2699/7635], Loss: 4.3274\n",
      "Epoch [1/1], Step [2700/7635], Loss: 4.4347\n",
      "Epoch [1/1], Step [2701/7635], Loss: 4.3345\n",
      "Epoch [1/1], Step [2702/7635], Loss: 4.4782\n",
      "Epoch [1/1], Step [2703/7635], Loss: 4.4603\n",
      "Epoch [1/1], Step [2704/7635], Loss: 4.4565\n",
      "Epoch [1/1], Step [2705/7635], Loss: 4.3688\n",
      "Epoch [1/1], Step [2706/7635], Loss: 4.3863\n",
      "Epoch [1/1], Step [2707/7635], Loss: 4.4169\n",
      "Epoch [1/1], Step [2708/7635], Loss: 4.3931\n",
      "Epoch [1/1], Step [2709/7635], Loss: 4.3532\n",
      "Epoch [1/1], Step [2710/7635], Loss: 4.3823\n",
      "Epoch [1/1], Step [2711/7635], Loss: 4.3907\n",
      "Epoch [1/1], Step [2712/7635], Loss: 4.2755\n",
      "Epoch [1/1], Step [2713/7635], Loss: 4.3427\n",
      "Epoch [1/1], Step [2714/7635], Loss: 4.4480\n",
      "Epoch [1/1], Step [2715/7635], Loss: 4.3516\n",
      "Epoch [1/1], Step [2716/7635], Loss: 4.4007\n",
      "Epoch [1/1], Step [2717/7635], Loss: 4.3462\n",
      "Epoch [1/1], Step [2718/7635], Loss: 4.3842\n",
      "Epoch [1/1], Step [2719/7635], Loss: 4.3817\n",
      "Epoch [1/1], Step [2720/7635], Loss: 4.3580\n",
      "Epoch [1/1], Step [2721/7635], Loss: 4.3157\n",
      "Epoch [1/1], Step [2722/7635], Loss: 4.3920\n",
      "Epoch [1/1], Step [2723/7635], Loss: 4.4139\n",
      "Epoch [1/1], Step [2724/7635], Loss: 4.3693\n",
      "Epoch [1/1], Step [2725/7635], Loss: 4.4151\n",
      "Epoch [1/1], Step [2726/7635], Loss: 4.2871\n",
      "Epoch [1/1], Step [2727/7635], Loss: 4.3051\n",
      "Epoch [1/1], Step [2728/7635], Loss: 4.5033\n",
      "Epoch [1/1], Step [2729/7635], Loss: 4.3638\n",
      "Epoch [1/1], Step [2730/7635], Loss: 4.3945\n",
      "Epoch [1/1], Step [2731/7635], Loss: 4.3232\n",
      "Epoch [1/1], Step [2732/7635], Loss: 4.3406\n",
      "Epoch [1/1], Step [2733/7635], Loss: 4.3344\n",
      "Epoch [1/1], Step [2734/7635], Loss: 4.4048\n",
      "Epoch [1/1], Step [2735/7635], Loss: 4.4121\n",
      "Epoch [1/1], Step [2736/7635], Loss: 4.4432\n",
      "Epoch [1/1], Step [2737/7635], Loss: 4.4958\n",
      "Epoch [1/1], Step [2738/7635], Loss: 4.4104\n",
      "Epoch [1/1], Step [2739/7635], Loss: 4.3220\n",
      "Epoch [1/1], Step [2740/7635], Loss: 4.4299\n",
      "Epoch [1/1], Step [2741/7635], Loss: 4.4158\n",
      "Epoch [1/1], Step [2742/7635], Loss: 4.3975\n",
      "Epoch [1/1], Step [2743/7635], Loss: 4.3292\n",
      "Epoch [1/1], Step [2744/7635], Loss: 4.4438\n",
      "Epoch [1/1], Step [2745/7635], Loss: 4.4241\n",
      "Epoch [1/1], Step [2746/7635], Loss: 4.4199\n",
      "Epoch [1/1], Step [2747/7635], Loss: 4.4439\n",
      "Epoch [1/1], Step [2748/7635], Loss: 4.4003\n",
      "Epoch [1/1], Step [2749/7635], Loss: 4.3233\n",
      "Epoch [1/1], Step [2750/7635], Loss: 4.3388\n",
      "Epoch [1/1], Step [2751/7635], Loss: 4.3238\n",
      "Epoch [1/1], Step [2752/7635], Loss: 4.3366\n",
      "Epoch [1/1], Step [2753/7635], Loss: 4.2746\n",
      "Epoch [1/1], Step [2754/7635], Loss: 4.4221\n",
      "Epoch [1/1], Step [2755/7635], Loss: 4.3882\n",
      "Epoch [1/1], Step [2756/7635], Loss: 4.3596\n",
      "Epoch [1/1], Step [2757/7635], Loss: 4.4829\n",
      "Epoch [1/1], Step [2758/7635], Loss: 4.3803\n",
      "Epoch [1/1], Step [2759/7635], Loss: 4.3858\n",
      "Epoch [1/1], Step [2760/7635], Loss: 4.3204\n",
      "Epoch [1/1], Step [2761/7635], Loss: 4.3182\n",
      "Epoch [1/1], Step [2762/7635], Loss: 4.3435\n",
      "Epoch [1/1], Step [2763/7635], Loss: 4.4262\n",
      "Epoch [1/1], Step [2764/7635], Loss: 4.3750\n",
      "Epoch [1/1], Step [2765/7635], Loss: 4.4334\n",
      "Epoch [1/1], Step [2766/7635], Loss: 4.3644\n",
      "Epoch [1/1], Step [2767/7635], Loss: 4.3421\n",
      "Epoch [1/1], Step [2768/7635], Loss: 4.3233\n",
      "Epoch [1/1], Step [2769/7635], Loss: 4.3904\n",
      "Epoch [1/1], Step [2770/7635], Loss: 4.3379\n",
      "Epoch [1/1], Step [2771/7635], Loss: 4.3339\n",
      "Epoch [1/1], Step [2772/7635], Loss: 4.3300\n",
      "Epoch [1/1], Step [2773/7635], Loss: 4.3850\n",
      "Epoch [1/1], Step [2774/7635], Loss: 4.3524\n",
      "Epoch [1/1], Step [2775/7635], Loss: 4.3889\n",
      "Epoch [1/1], Step [2776/7635], Loss: 4.3628\n",
      "Epoch [1/1], Step [2777/7635], Loss: 4.3319\n",
      "Epoch [1/1], Step [2778/7635], Loss: 4.5001\n",
      "Epoch [1/1], Step [2779/7635], Loss: 4.3297\n",
      "Epoch [1/1], Step [2780/7635], Loss: 4.3616\n",
      "Epoch [1/1], Step [2781/7635], Loss: 4.3726\n",
      "Epoch [1/1], Step [2782/7635], Loss: 4.3087\n",
      "Epoch [1/1], Step [2783/7635], Loss: 4.3444\n",
      "Epoch [1/1], Step [2784/7635], Loss: 4.3661\n",
      "Epoch [1/1], Step [2785/7635], Loss: 4.2961\n",
      "Epoch [1/1], Step [2786/7635], Loss: 4.3593\n",
      "Epoch [1/1], Step [2787/7635], Loss: 4.3356\n",
      "Epoch [1/1], Step [2788/7635], Loss: 4.3338\n",
      "Epoch [1/1], Step [2789/7635], Loss: 4.4424\n",
      "Epoch [1/1], Step [2790/7635], Loss: 4.3489\n",
      "Epoch [1/1], Step [2791/7635], Loss: 4.4224\n",
      "Epoch [1/1], Step [2792/7635], Loss: 4.4198\n",
      "Epoch [1/1], Step [2793/7635], Loss: 4.3173\n",
      "Epoch [1/1], Step [2794/7635], Loss: 4.4026\n",
      "Epoch [1/1], Step [2795/7635], Loss: 4.3692\n",
      "Epoch [1/1], Step [2796/7635], Loss: 4.4835\n",
      "Epoch [1/1], Step [2797/7635], Loss: 4.3617\n",
      "Epoch [1/1], Step [2798/7635], Loss: 4.3043\n",
      "Epoch [1/1], Step [2799/7635], Loss: 4.3565\n",
      "Epoch [1/1], Step [2800/7635], Loss: 4.2978\n",
      "Epoch [1/1], Step [2801/7635], Loss: 4.3462\n",
      "Epoch [1/1], Step [2802/7635], Loss: 4.3842\n",
      "Epoch [1/1], Step [2803/7635], Loss: 4.3388\n",
      "Epoch [1/1], Step [2804/7635], Loss: 4.3389\n",
      "Epoch [1/1], Step [2805/7635], Loss: 4.3347\n",
      "Epoch [1/1], Step [2806/7635], Loss: 4.3086\n",
      "Epoch [1/1], Step [2807/7635], Loss: 4.2750\n",
      "Epoch [1/1], Step [2808/7635], Loss: 4.3526\n",
      "Epoch [1/1], Step [2809/7635], Loss: 4.3888\n",
      "Epoch [1/1], Step [2810/7635], Loss: 4.3481\n",
      "Epoch [1/1], Step [2811/7635], Loss: 4.3565\n",
      "Epoch [1/1], Step [2812/7635], Loss: 4.3406\n",
      "Epoch [1/1], Step [2813/7635], Loss: 4.3439\n",
      "Epoch [1/1], Step [2814/7635], Loss: 4.3237\n",
      "Epoch [1/1], Step [2815/7635], Loss: 4.3502\n",
      "Epoch [1/1], Step [2816/7635], Loss: 4.2784\n",
      "Epoch [1/1], Step [2817/7635], Loss: 4.3827\n",
      "Epoch [1/1], Step [2818/7635], Loss: 4.3452\n",
      "Epoch [1/1], Step [2819/7635], Loss: 4.4344\n",
      "Epoch [1/1], Step [2820/7635], Loss: 4.3563\n",
      "Epoch [1/1], Step [2821/7635], Loss: 4.2906\n",
      "Epoch [1/1], Step [2822/7635], Loss: 4.3568\n",
      "Epoch [1/1], Step [2823/7635], Loss: 4.4085\n",
      "Epoch [1/1], Step [2824/7635], Loss: 4.3861\n",
      "Epoch [1/1], Step [2825/7635], Loss: 4.3553\n",
      "Epoch [1/1], Step [2826/7635], Loss: 4.2949\n",
      "Epoch [1/1], Step [2827/7635], Loss: 4.3314\n",
      "Epoch [1/1], Step [2828/7635], Loss: 4.3673\n",
      "Epoch [1/1], Step [2829/7635], Loss: 4.3211\n",
      "Epoch [1/1], Step [2830/7635], Loss: 4.4808\n",
      "Epoch [1/1], Step [2831/7635], Loss: 4.3631\n",
      "Epoch [1/1], Step [2832/7635], Loss: 4.3309\n",
      "Epoch [1/1], Step [2833/7635], Loss: 4.2917\n",
      "Epoch [1/1], Step [2834/7635], Loss: 4.3793\n",
      "Epoch [1/1], Step [2835/7635], Loss: 4.3215\n",
      "Epoch [1/1], Step [2836/7635], Loss: 4.2930\n",
      "Epoch [1/1], Step [2837/7635], Loss: 4.3427\n",
      "Epoch [1/1], Step [2838/7635], Loss: 4.3600\n",
      "Epoch [1/1], Step [2839/7635], Loss: 4.4177\n",
      "Epoch [1/1], Step [2840/7635], Loss: 4.3417\n",
      "Epoch [1/1], Step [2841/7635], Loss: 4.3442\n",
      "Epoch [1/1], Step [2842/7635], Loss: 4.3844\n",
      "Epoch [1/1], Step [2843/7635], Loss: 4.3061\n",
      "Epoch [1/1], Step [2844/7635], Loss: 4.3471\n",
      "Epoch [1/1], Step [2845/7635], Loss: 4.3291\n",
      "Epoch [1/1], Step [2846/7635], Loss: 4.3098\n",
      "Epoch [1/1], Step [2847/7635], Loss: 4.3515\n",
      "Epoch [1/1], Step [2848/7635], Loss: 4.3097\n",
      "Epoch [1/1], Step [2849/7635], Loss: 4.3647\n",
      "Epoch [1/1], Step [2850/7635], Loss: 4.2868\n",
      "Epoch [1/1], Step [2851/7635], Loss: 4.3846\n",
      "Epoch [1/1], Step [2852/7635], Loss: 4.3691\n",
      "Epoch [1/1], Step [2853/7635], Loss: 4.3989\n",
      "Epoch [1/1], Step [2854/7635], Loss: 4.4459\n",
      "Epoch [1/1], Step [2855/7635], Loss: 4.4714\n",
      "Epoch [1/1], Step [2856/7635], Loss: 4.3590\n",
      "Epoch [1/1], Step [2857/7635], Loss: 4.2835\n",
      "Epoch [1/1], Step [2858/7635], Loss: 4.3086\n",
      "Epoch [1/1], Step [2859/7635], Loss: 4.3573\n",
      "Epoch [1/1], Step [2860/7635], Loss: 4.4069\n",
      "Epoch [1/1], Step [2861/7635], Loss: 4.3157\n",
      "Epoch [1/1], Step [2862/7635], Loss: 4.4236\n",
      "Epoch [1/1], Step [2863/7635], Loss: 4.2875\n",
      "Epoch [1/1], Step [2864/7635], Loss: 4.3165\n",
      "Epoch [1/1], Step [2865/7635], Loss: 4.3970\n",
      "Epoch [1/1], Step [2866/7635], Loss: 4.3901\n",
      "Epoch [1/1], Step [2867/7635], Loss: 4.3004\n",
      "Epoch [1/1], Step [2868/7635], Loss: 4.2587\n",
      "Epoch [1/1], Step [2869/7635], Loss: 4.3844\n",
      "Epoch [1/1], Step [2870/7635], Loss: 4.3054\n",
      "Epoch [1/1], Step [2871/7635], Loss: 4.4296\n",
      "Epoch [1/1], Step [2872/7635], Loss: 4.3848\n",
      "Epoch [1/1], Step [2873/7635], Loss: 4.4198\n",
      "Epoch [1/1], Step [2874/7635], Loss: 4.3230\n",
      "Epoch [1/1], Step [2875/7635], Loss: 4.3531\n",
      "Epoch [1/1], Step [2876/7635], Loss: 4.3709\n",
      "Epoch [1/1], Step [2877/7635], Loss: 4.4434\n",
      "Epoch [1/1], Step [2878/7635], Loss: 4.3924\n",
      "Epoch [1/1], Step [2879/7635], Loss: 4.3037\n",
      "Epoch [1/1], Step [2880/7635], Loss: 4.3573\n",
      "Epoch [1/1], Step [2881/7635], Loss: 4.3530\n",
      "Epoch [1/1], Step [2882/7635], Loss: 4.3027\n",
      "Epoch [1/1], Step [2883/7635], Loss: 4.4010\n",
      "Epoch [1/1], Step [2884/7635], Loss: 4.3350\n",
      "Epoch [1/1], Step [2885/7635], Loss: 4.4268\n",
      "Epoch [1/1], Step [2886/7635], Loss: 4.3704\n",
      "Epoch [1/1], Step [2887/7635], Loss: 4.2516\n",
      "Epoch [1/1], Step [2888/7635], Loss: 4.3239\n",
      "Epoch [1/1], Step [2889/7635], Loss: 4.3157\n",
      "Epoch [1/1], Step [2890/7635], Loss: 4.4027\n",
      "Epoch [1/1], Step [2891/7635], Loss: 4.2802\n",
      "Epoch [1/1], Step [2892/7635], Loss: 4.3407\n",
      "Epoch [1/1], Step [2893/7635], Loss: 4.2963\n",
      "Epoch [1/1], Step [2894/7635], Loss: 4.3537\n",
      "Epoch [1/1], Step [2895/7635], Loss: 4.3542\n",
      "Epoch [1/1], Step [2896/7635], Loss: 4.3540\n",
      "Epoch [1/1], Step [2897/7635], Loss: 4.3845\n",
      "Epoch [1/1], Step [2898/7635], Loss: 4.3848\n",
      "Epoch [1/1], Step [2899/7635], Loss: 4.4751\n",
      "Epoch [1/1], Step [2900/7635], Loss: 4.3298\n",
      "Epoch [1/1], Step [2901/7635], Loss: 4.3759\n",
      "Epoch [1/1], Step [2902/7635], Loss: 4.3460\n",
      "Epoch [1/1], Step [2903/7635], Loss: 4.4165\n",
      "Epoch [1/1], Step [2904/7635], Loss: 4.3788\n",
      "Epoch [1/1], Step [2905/7635], Loss: 4.3176\n",
      "Epoch [1/1], Step [2906/7635], Loss: 4.3077\n",
      "Epoch [1/1], Step [2907/7635], Loss: 4.3665\n",
      "Epoch [1/1], Step [2908/7635], Loss: 4.3598\n",
      "Epoch [1/1], Step [2909/7635], Loss: 4.3369\n",
      "Epoch [1/1], Step [2910/7635], Loss: 4.3307\n",
      "Epoch [1/1], Step [2911/7635], Loss: 4.3029\n",
      "Epoch [1/1], Step [2912/7635], Loss: 4.4305\n",
      "Epoch [1/1], Step [2913/7635], Loss: 4.3941\n",
      "Epoch [1/1], Step [2914/7635], Loss: 4.3841\n",
      "Epoch [1/1], Step [2915/7635], Loss: 4.2932\n",
      "Epoch [1/1], Step [2916/7635], Loss: 4.3699\n",
      "Epoch [1/1], Step [2917/7635], Loss: 4.3514\n",
      "Epoch [1/1], Step [2918/7635], Loss: 4.3187\n",
      "Epoch [1/1], Step [2919/7635], Loss: 4.3308\n",
      "Epoch [1/1], Step [2920/7635], Loss: 4.3702\n",
      "Epoch [1/1], Step [2921/7635], Loss: 4.4030\n",
      "Epoch [1/1], Step [2922/7635], Loss: 4.3145\n",
      "Epoch [1/1], Step [2923/7635], Loss: 4.4072\n",
      "Epoch [1/1], Step [2924/7635], Loss: 4.3284\n",
      "Epoch [1/1], Step [2925/7635], Loss: 4.3066\n",
      "Epoch [1/1], Step [2926/7635], Loss: 4.3550\n",
      "Epoch [1/1], Step [2927/7635], Loss: 4.3395\n",
      "Epoch [1/1], Step [2928/7635], Loss: 4.2900\n",
      "Epoch [1/1], Step [2929/7635], Loss: 4.3099\n",
      "Epoch [1/1], Step [2930/7635], Loss: 4.3862\n",
      "Epoch [1/1], Step [2931/7635], Loss: 4.3115\n",
      "Epoch [1/1], Step [2932/7635], Loss: 4.3108\n",
      "Epoch [1/1], Step [2933/7635], Loss: 4.4148\n",
      "Epoch [1/1], Step [2934/7635], Loss: 4.3439\n",
      "Epoch [1/1], Step [2935/7635], Loss: 4.3052\n",
      "Epoch [1/1], Step [2936/7635], Loss: 4.3101\n",
      "Epoch [1/1], Step [2937/7635], Loss: 4.3449\n",
      "Epoch [1/1], Step [2938/7635], Loss: 4.3961\n",
      "Epoch [1/1], Step [2939/7635], Loss: 4.4244\n",
      "Epoch [1/1], Step [2940/7635], Loss: 4.3065\n",
      "Epoch [1/1], Step [2941/7635], Loss: 4.4187\n",
      "Epoch [1/1], Step [2942/7635], Loss: 4.3573\n",
      "Epoch [1/1], Step [2943/7635], Loss: 4.3145\n",
      "Epoch [1/1], Step [2944/7635], Loss: 4.3668\n",
      "Epoch [1/1], Step [2945/7635], Loss: 4.3259\n",
      "Epoch [1/1], Step [2946/7635], Loss: 4.2447\n",
      "Epoch [1/1], Step [2947/7635], Loss: 4.3537\n",
      "Epoch [1/1], Step [2948/7635], Loss: 4.3038\n",
      "Epoch [1/1], Step [2949/7635], Loss: 4.4510\n",
      "Epoch [1/1], Step [2950/7635], Loss: 4.2841\n",
      "Epoch [1/1], Step [2951/7635], Loss: 4.3746\n",
      "Epoch [1/1], Step [2952/7635], Loss: 4.3680\n",
      "Epoch [1/1], Step [2953/7635], Loss: 4.3219\n",
      "Epoch [1/1], Step [2954/7635], Loss: 4.3356\n",
      "Epoch [1/1], Step [2955/7635], Loss: 4.2798\n",
      "Epoch [1/1], Step [2956/7635], Loss: 4.3561\n",
      "Epoch [1/1], Step [2957/7635], Loss: 4.3281\n",
      "Epoch [1/1], Step [2958/7635], Loss: 4.3607\n",
      "Epoch [1/1], Step [2959/7635], Loss: 4.2610\n",
      "Epoch [1/1], Step [2960/7635], Loss: 4.3060\n",
      "Epoch [1/1], Step [2961/7635], Loss: 4.2850\n",
      "Epoch [1/1], Step [2962/7635], Loss: 4.3452\n",
      "Epoch [1/1], Step [2963/7635], Loss: 4.3171\n",
      "Epoch [1/1], Step [2964/7635], Loss: 4.3916\n",
      "Epoch [1/1], Step [2965/7635], Loss: 4.4168\n",
      "Epoch [1/1], Step [2966/7635], Loss: 4.3449\n",
      "Epoch [1/1], Step [2967/7635], Loss: 4.3584\n",
      "Epoch [1/1], Step [2968/7635], Loss: 4.3142\n",
      "Epoch [1/1], Step [2969/7635], Loss: 4.3491\n",
      "Epoch [1/1], Step [2970/7635], Loss: 4.3537\n",
      "Epoch [1/1], Step [2971/7635], Loss: 4.4272\n",
      "Epoch [1/1], Step [2972/7635], Loss: 4.3623\n",
      "Epoch [1/1], Step [2973/7635], Loss: 4.3330\n",
      "Epoch [1/1], Step [2974/7635], Loss: 4.3278\n",
      "Epoch [1/1], Step [2975/7635], Loss: 4.3868\n",
      "Epoch [1/1], Step [2976/7635], Loss: 4.2697\n",
      "Epoch [1/1], Step [2977/7635], Loss: 4.3560\n",
      "Epoch [1/1], Step [2978/7635], Loss: 4.3396\n",
      "Epoch [1/1], Step [2979/7635], Loss: 4.2909\n",
      "Epoch [1/1], Step [2980/7635], Loss: 4.3915\n",
      "Epoch [1/1], Step [2981/7635], Loss: 4.3556\n",
      "Epoch [1/1], Step [2982/7635], Loss: 4.2893\n",
      "Epoch [1/1], Step [2983/7635], Loss: 4.3709\n",
      "Epoch [1/1], Step [2984/7635], Loss: 4.3368\n",
      "Epoch [1/1], Step [2985/7635], Loss: 4.3198\n",
      "Epoch [1/1], Step [2986/7635], Loss: 4.3322\n",
      "Epoch [1/1], Step [2987/7635], Loss: 4.3398\n",
      "Epoch [1/1], Step [2988/7635], Loss: 4.3202\n",
      "Epoch [1/1], Step [2989/7635], Loss: 4.2990\n",
      "Epoch [1/1], Step [2990/7635], Loss: 4.3907\n",
      "Epoch [1/1], Step [2991/7635], Loss: 4.3099\n",
      "Epoch [1/1], Step [2992/7635], Loss: 4.3170\n",
      "Epoch [1/1], Step [2993/7635], Loss: 4.2925\n",
      "Epoch [1/1], Step [2994/7635], Loss: 4.2834\n",
      "Epoch [1/1], Step [2995/7635], Loss: 4.3703\n",
      "Epoch [1/1], Step [2996/7635], Loss: 4.3562\n",
      "Epoch [1/1], Step [2997/7635], Loss: 4.3428\n",
      "Epoch [1/1], Step [2998/7635], Loss: 4.3903\n",
      "Epoch [1/1], Step [2999/7635], Loss: 4.3431\n",
      "Epoch [1/1], Step [3000/7635], Loss: 4.4857\n",
      "Epoch [1/1], Step [3001/7635], Loss: 4.3193\n",
      "Epoch [1/1], Step [3002/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [3003/7635], Loss: 4.2912\n",
      "Epoch [1/1], Step [3004/7635], Loss: 4.2902\n",
      "Epoch [1/1], Step [3005/7635], Loss: 4.3515\n",
      "Epoch [1/1], Step [3006/7635], Loss: 4.3405\n",
      "Epoch [1/1], Step [3007/7635], Loss: 4.3056\n",
      "Epoch [1/1], Step [3008/7635], Loss: 4.3155\n",
      "Epoch [1/1], Step [3009/7635], Loss: 4.4036\n",
      "Epoch [1/1], Step [3010/7635], Loss: 4.3211\n",
      "Epoch [1/1], Step [3011/7635], Loss: 4.2909\n",
      "Epoch [1/1], Step [3012/7635], Loss: 4.4150\n",
      "Epoch [1/1], Step [3013/7635], Loss: 4.3883\n",
      "Epoch [1/1], Step [3014/7635], Loss: 4.3329\n",
      "Epoch [1/1], Step [3015/7635], Loss: 4.2758\n",
      "Epoch [1/1], Step [3016/7635], Loss: 4.3353\n",
      "Epoch [1/1], Step [3017/7635], Loss: 4.3351\n",
      "Epoch [1/1], Step [3018/7635], Loss: 4.3296\n",
      "Epoch [1/1], Step [3019/7635], Loss: 4.2830\n",
      "Epoch [1/1], Step [3020/7635], Loss: 4.3271\n",
      "Epoch [1/1], Step [3021/7635], Loss: 4.2938\n",
      "Epoch [1/1], Step [3022/7635], Loss: 4.3252\n",
      "Epoch [1/1], Step [3023/7635], Loss: 4.3631\n",
      "Epoch [1/1], Step [3024/7635], Loss: 4.2979\n",
      "Epoch [1/1], Step [3025/7635], Loss: 4.3308\n",
      "Epoch [1/1], Step [3026/7635], Loss: 4.3331\n",
      "Epoch [1/1], Step [3027/7635], Loss: 4.3661\n",
      "Epoch [1/1], Step [3028/7635], Loss: 4.3223\n",
      "Epoch [1/1], Step [3029/7635], Loss: 4.3621\n",
      "Epoch [1/1], Step [3030/7635], Loss: 4.3973\n",
      "Epoch [1/1], Step [3031/7635], Loss: 4.2850\n",
      "Epoch [1/1], Step [3032/7635], Loss: 4.3268\n",
      "Epoch [1/1], Step [3033/7635], Loss: 4.3580\n",
      "Epoch [1/1], Step [3034/7635], Loss: 4.3319\n",
      "Epoch [1/1], Step [3035/7635], Loss: 4.2889\n",
      "Epoch [1/1], Step [3036/7635], Loss: 4.3733\n",
      "Epoch [1/1], Step [3037/7635], Loss: 4.3599\n",
      "Epoch [1/1], Step [3038/7635], Loss: 4.4060\n",
      "Epoch [1/1], Step [3039/7635], Loss: 4.3897\n",
      "Epoch [1/1], Step [3040/7635], Loss: 4.4180\n",
      "Epoch [1/1], Step [3041/7635], Loss: 4.3391\n",
      "Epoch [1/1], Step [3042/7635], Loss: 4.3693\n",
      "Epoch [1/1], Step [3043/7635], Loss: 4.3921\n",
      "Epoch [1/1], Step [3044/7635], Loss: 4.3458\n",
      "Epoch [1/1], Step [3045/7635], Loss: 4.2880\n",
      "Epoch [1/1], Step [3046/7635], Loss: 4.3458\n",
      "Epoch [1/1], Step [3047/7635], Loss: 4.3559\n",
      "Epoch [1/1], Step [3048/7635], Loss: 4.3142\n",
      "Epoch [1/1], Step [3049/7635], Loss: 4.3265\n",
      "Epoch [1/1], Step [3050/7635], Loss: 4.2672\n",
      "Epoch [1/1], Step [3051/7635], Loss: 4.3330\n",
      "Epoch [1/1], Step [3052/7635], Loss: 4.3200\n",
      "Epoch [1/1], Step [3053/7635], Loss: 4.3081\n",
      "Epoch [1/1], Step [3054/7635], Loss: 4.3926\n",
      "Epoch [1/1], Step [3055/7635], Loss: 4.2853\n",
      "Epoch [1/1], Step [3056/7635], Loss: 4.2813\n",
      "Epoch [1/1], Step [3057/7635], Loss: 4.3757\n",
      "Epoch [1/1], Step [3058/7635], Loss: 4.2788\n",
      "Epoch [1/1], Step [3059/7635], Loss: 4.2955\n",
      "Epoch [1/1], Step [3060/7635], Loss: 4.2106\n",
      "Epoch [1/1], Step [3061/7635], Loss: 4.3401\n",
      "Epoch [1/1], Step [3062/7635], Loss: 4.3729\n",
      "Epoch [1/1], Step [3063/7635], Loss: 4.3926\n",
      "Epoch [1/1], Step [3064/7635], Loss: 4.2986\n",
      "Epoch [1/1], Step [3065/7635], Loss: 4.2800\n",
      "Epoch [1/1], Step [3066/7635], Loss: 4.3179\n",
      "Epoch [1/1], Step [3067/7635], Loss: 4.3113\n",
      "Epoch [1/1], Step [3068/7635], Loss: 4.3584\n",
      "Epoch [1/1], Step [3069/7635], Loss: 4.3441\n",
      "Epoch [1/1], Step [3070/7635], Loss: 4.3103\n",
      "Epoch [1/1], Step [3071/7635], Loss: 4.3460\n",
      "Epoch [1/1], Step [3072/7635], Loss: 4.3238\n",
      "Epoch [1/1], Step [3073/7635], Loss: 4.3292\n",
      "Epoch [1/1], Step [3074/7635], Loss: 4.3266\n",
      "Epoch [1/1], Step [3075/7635], Loss: 4.3218\n",
      "Epoch [1/1], Step [3076/7635], Loss: 4.3460\n",
      "Epoch [1/1], Step [3077/7635], Loss: 4.2195\n",
      "Epoch [1/1], Step [3078/7635], Loss: 4.3555\n",
      "Epoch [1/1], Step [3079/7635], Loss: 4.2675\n",
      "Epoch [1/1], Step [3080/7635], Loss: 4.3641\n",
      "Epoch [1/1], Step [3081/7635], Loss: 4.3493\n",
      "Epoch [1/1], Step [3082/7635], Loss: 4.2861\n",
      "Epoch [1/1], Step [3083/7635], Loss: 4.3940\n",
      "Epoch [1/1], Step [3084/7635], Loss: 4.3582\n",
      "Epoch [1/1], Step [3085/7635], Loss: 4.3312\n",
      "Epoch [1/1], Step [3086/7635], Loss: 4.2424\n",
      "Epoch [1/1], Step [3087/7635], Loss: 4.3209\n",
      "Epoch [1/1], Step [3088/7635], Loss: 4.4214\n",
      "Epoch [1/1], Step [3089/7635], Loss: 4.3351\n",
      "Epoch [1/1], Step [3090/7635], Loss: 4.3266\n",
      "Epoch [1/1], Step [3091/7635], Loss: 4.3595\n",
      "Epoch [1/1], Step [3092/7635], Loss: 4.3672\n",
      "Epoch [1/1], Step [3093/7635], Loss: 4.2679\n",
      "Epoch [1/1], Step [3094/7635], Loss: 4.3266\n",
      "Epoch [1/1], Step [3095/7635], Loss: 4.3897\n",
      "Epoch [1/1], Step [3096/7635], Loss: 4.3513\n",
      "Epoch [1/1], Step [3097/7635], Loss: 4.3065\n",
      "Epoch [1/1], Step [3098/7635], Loss: 4.2695\n",
      "Epoch [1/1], Step [3099/7635], Loss: 4.3763\n",
      "Epoch [1/1], Step [3100/7635], Loss: 4.3447\n",
      "Epoch [1/1], Step [3101/7635], Loss: 4.3018\n",
      "Epoch [1/1], Step [3102/7635], Loss: 4.3751\n",
      "Epoch [1/1], Step [3103/7635], Loss: 4.3587\n",
      "Epoch [1/1], Step [3104/7635], Loss: 4.3325\n",
      "Epoch [1/1], Step [3105/7635], Loss: 4.3532\n",
      "Epoch [1/1], Step [3106/7635], Loss: 4.2742\n",
      "Epoch [1/1], Step [3107/7635], Loss: 4.3031\n",
      "Epoch [1/1], Step [3108/7635], Loss: 4.3701\n",
      "Epoch [1/1], Step [3109/7635], Loss: 4.3104\n",
      "Epoch [1/1], Step [3110/7635], Loss: 4.2669\n",
      "Epoch [1/1], Step [3111/7635], Loss: 4.2969\n",
      "Epoch [1/1], Step [3112/7635], Loss: 4.3767\n",
      "Epoch [1/1], Step [3113/7635], Loss: 4.3104\n",
      "Epoch [1/1], Step [3114/7635], Loss: 4.3377\n",
      "Epoch [1/1], Step [3115/7635], Loss: 4.3313\n",
      "Epoch [1/1], Step [3116/7635], Loss: 4.3043\n",
      "Epoch [1/1], Step [3117/7635], Loss: 4.2583\n",
      "Epoch [1/1], Step [3118/7635], Loss: 4.3563\n",
      "Epoch [1/1], Step [3119/7635], Loss: 4.3277\n",
      "Epoch [1/1], Step [3120/7635], Loss: 4.2418\n",
      "Epoch [1/1], Step [3121/7635], Loss: 4.3313\n",
      "Epoch [1/1], Step [3122/7635], Loss: 4.3964\n",
      "Epoch [1/1], Step [3123/7635], Loss: 4.3207\n",
      "Epoch [1/1], Step [3124/7635], Loss: 4.3431\n",
      "Epoch [1/1], Step [3125/7635], Loss: 4.3442\n",
      "Epoch [1/1], Step [3126/7635], Loss: 4.3424\n",
      "Epoch [1/1], Step [3127/7635], Loss: 4.2744\n",
      "Epoch [1/1], Step [3128/7635], Loss: 4.3266\n",
      "Epoch [1/1], Step [3129/7635], Loss: 4.3355\n",
      "Epoch [1/1], Step [3130/7635], Loss: 4.3567\n",
      "Epoch [1/1], Step [3131/7635], Loss: 4.2925\n",
      "Epoch [1/1], Step [3132/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [3133/7635], Loss: 4.3591\n",
      "Epoch [1/1], Step [3134/7635], Loss: 4.2686\n",
      "Epoch [1/1], Step [3135/7635], Loss: 4.2993\n",
      "Epoch [1/1], Step [3136/7635], Loss: 4.2635\n",
      "Epoch [1/1], Step [3137/7635], Loss: 4.3180\n",
      "Epoch [1/1], Step [3138/7635], Loss: 4.3166\n",
      "Epoch [1/1], Step [3139/7635], Loss: 4.3205\n",
      "Epoch [1/1], Step [3140/7635], Loss: 4.3867\n",
      "Epoch [1/1], Step [3141/7635], Loss: 4.3243\n",
      "Epoch [1/1], Step [3142/7635], Loss: 4.3376\n",
      "Epoch [1/1], Step [3143/7635], Loss: 4.2727\n",
      "Epoch [1/1], Step [3144/7635], Loss: 4.3593\n",
      "Epoch [1/1], Step [3145/7635], Loss: 4.2237\n",
      "Epoch [1/1], Step [3146/7635], Loss: 4.3485\n",
      "Epoch [1/1], Step [3147/7635], Loss: 4.3853\n",
      "Epoch [1/1], Step [3148/7635], Loss: 4.2061\n",
      "Epoch [1/1], Step [3149/7635], Loss: 4.3348\n",
      "Epoch [1/1], Step [3150/7635], Loss: 4.3399\n",
      "Epoch [1/1], Step [3151/7635], Loss: 4.2462\n",
      "Epoch [1/1], Step [3152/7635], Loss: 4.3811\n",
      "Epoch [1/1], Step [3153/7635], Loss: 4.3769\n",
      "Epoch [1/1], Step [3154/7635], Loss: 4.3750\n",
      "Epoch [1/1], Step [3155/7635], Loss: 4.3271\n",
      "Epoch [1/1], Step [3156/7635], Loss: 4.3740\n",
      "Epoch [1/1], Step [3157/7635], Loss: 4.3906\n",
      "Epoch [1/1], Step [3158/7635], Loss: 4.3764\n",
      "Epoch [1/1], Step [3159/7635], Loss: 4.2803\n",
      "Epoch [1/1], Step [3160/7635], Loss: 4.2999\n",
      "Epoch [1/1], Step [3161/7635], Loss: 4.3060\n",
      "Epoch [1/1], Step [3162/7635], Loss: 4.3014\n",
      "Epoch [1/1], Step [3163/7635], Loss: 4.3326\n",
      "Epoch [1/1], Step [3164/7635], Loss: 4.3861\n",
      "Epoch [1/1], Step [3165/7635], Loss: 4.2369\n",
      "Epoch [1/1], Step [3166/7635], Loss: 4.3841\n",
      "Epoch [1/1], Step [3167/7635], Loss: 4.3470\n",
      "Epoch [1/1], Step [3168/7635], Loss: 4.3904\n",
      "Epoch [1/1], Step [3169/7635], Loss: 4.2874\n",
      "Epoch [1/1], Step [3170/7635], Loss: 4.2791\n",
      "Epoch [1/1], Step [3171/7635], Loss: 4.2969\n",
      "Epoch [1/1], Step [3172/7635], Loss: 4.3774\n",
      "Epoch [1/1], Step [3173/7635], Loss: 4.3645\n",
      "Epoch [1/1], Step [3174/7635], Loss: 4.2344\n",
      "Epoch [1/1], Step [3175/7635], Loss: 4.2335\n",
      "Epoch [1/1], Step [3176/7635], Loss: 4.2873\n",
      "Epoch [1/1], Step [3177/7635], Loss: 4.2089\n",
      "Epoch [1/1], Step [3178/7635], Loss: 4.3304\n",
      "Epoch [1/1], Step [3179/7635], Loss: 4.4098\n",
      "Epoch [1/1], Step [3180/7635], Loss: 4.3431\n",
      "Epoch [1/1], Step [3181/7635], Loss: 4.2903\n",
      "Epoch [1/1], Step [3182/7635], Loss: 4.3272\n",
      "Epoch [1/1], Step [3183/7635], Loss: 4.3031\n",
      "Epoch [1/1], Step [3184/7635], Loss: 4.3327\n",
      "Epoch [1/1], Step [3185/7635], Loss: 4.2446\n",
      "Epoch [1/1], Step [3186/7635], Loss: 4.3907\n",
      "Epoch [1/1], Step [3187/7635], Loss: 4.2572\n",
      "Epoch [1/1], Step [3188/7635], Loss: 4.3728\n",
      "Epoch [1/1], Step [3189/7635], Loss: 4.3387\n",
      "Epoch [1/1], Step [3190/7635], Loss: 4.3532\n",
      "Epoch [1/1], Step [3191/7635], Loss: 4.3586\n",
      "Epoch [1/1], Step [3192/7635], Loss: 4.3014\n",
      "Epoch [1/1], Step [3193/7635], Loss: 4.2659\n",
      "Epoch [1/1], Step [3194/7635], Loss: 4.3780\n",
      "Epoch [1/1], Step [3195/7635], Loss: 4.3634\n",
      "Epoch [1/1], Step [3196/7635], Loss: 4.3444\n",
      "Epoch [1/1], Step [3197/7635], Loss: 4.2847\n",
      "Epoch [1/1], Step [3198/7635], Loss: 4.3419\n",
      "Epoch [1/1], Step [3199/7635], Loss: 4.2897\n",
      "Epoch [1/1], Step [3200/7635], Loss: 4.3027\n",
      "Epoch [1/1], Step [3201/7635], Loss: 4.3722\n",
      "Epoch [1/1], Step [3202/7635], Loss: 4.4386\n",
      "Epoch [1/1], Step [3203/7635], Loss: 4.2820\n",
      "Epoch [1/1], Step [3204/7635], Loss: 4.3416\n",
      "Epoch [1/1], Step [3205/7635], Loss: 4.3074\n",
      "Epoch [1/1], Step [3206/7635], Loss: 4.3510\n",
      "Epoch [1/1], Step [3207/7635], Loss: 4.2365\n",
      "Epoch [1/1], Step [3208/7635], Loss: 4.3714\n",
      "Epoch [1/1], Step [3209/7635], Loss: 4.3588\n",
      "Epoch [1/1], Step [3210/7635], Loss: 4.3717\n",
      "Epoch [1/1], Step [3211/7635], Loss: 4.3237\n",
      "Epoch [1/1], Step [3212/7635], Loss: 4.3293\n",
      "Epoch [1/1], Step [3213/7635], Loss: 4.3203\n",
      "Epoch [1/1], Step [3214/7635], Loss: 4.2046\n",
      "Epoch [1/1], Step [3215/7635], Loss: 4.2930\n",
      "Epoch [1/1], Step [3216/7635], Loss: 4.3136\n",
      "Epoch [1/1], Step [3217/7635], Loss: 4.3252\n",
      "Epoch [1/1], Step [3218/7635], Loss: 4.3363\n",
      "Epoch [1/1], Step [3219/7635], Loss: 4.2991\n",
      "Epoch [1/1], Step [3220/7635], Loss: 4.2887\n",
      "Epoch [1/1], Step [3221/7635], Loss: 4.3487\n",
      "Epoch [1/1], Step [3222/7635], Loss: 4.4204\n",
      "Epoch [1/1], Step [3223/7635], Loss: 4.3118\n",
      "Epoch [1/1], Step [3224/7635], Loss: 4.3014\n",
      "Epoch [1/1], Step [3225/7635], Loss: 4.2933\n",
      "Epoch [1/1], Step [3226/7635], Loss: 4.2721\n",
      "Epoch [1/1], Step [3227/7635], Loss: 4.2276\n",
      "Epoch [1/1], Step [3228/7635], Loss: 4.3126\n",
      "Epoch [1/1], Step [3229/7635], Loss: 4.2676\n",
      "Epoch [1/1], Step [3230/7635], Loss: 4.2987\n",
      "Epoch [1/1], Step [3231/7635], Loss: 4.3003\n",
      "Epoch [1/1], Step [3232/7635], Loss: 4.1993\n",
      "Epoch [1/1], Step [3233/7635], Loss: 4.3765\n",
      "Epoch [1/1], Step [3234/7635], Loss: 4.2728\n",
      "Epoch [1/1], Step [3235/7635], Loss: 4.3301\n",
      "Epoch [1/1], Step [3236/7635], Loss: 4.2798\n",
      "Epoch [1/1], Step [3237/7635], Loss: 4.3820\n",
      "Epoch [1/1], Step [3238/7635], Loss: 4.3201\n",
      "Epoch [1/1], Step [3239/7635], Loss: 4.3527\n",
      "Epoch [1/1], Step [3240/7635], Loss: 4.3069\n",
      "Epoch [1/1], Step [3241/7635], Loss: 4.3245\n",
      "Epoch [1/1], Step [3242/7635], Loss: 4.2749\n",
      "Epoch [1/1], Step [3243/7635], Loss: 4.3904\n",
      "Epoch [1/1], Step [3244/7635], Loss: 4.3560\n",
      "Epoch [1/1], Step [3245/7635], Loss: 4.2913\n",
      "Epoch [1/1], Step [3246/7635], Loss: 4.3286\n",
      "Epoch [1/1], Step [3247/7635], Loss: 4.3677\n",
      "Epoch [1/1], Step [3248/7635], Loss: 4.3075\n",
      "Epoch [1/1], Step [3249/7635], Loss: 4.2854\n",
      "Epoch [1/1], Step [3250/7635], Loss: 4.2162\n",
      "Epoch [1/1], Step [3251/7635], Loss: 4.2504\n",
      "Epoch [1/1], Step [3252/7635], Loss: 4.2954\n",
      "Epoch [1/1], Step [3253/7635], Loss: 4.3596\n",
      "Epoch [1/1], Step [3254/7635], Loss: 4.2615\n",
      "Epoch [1/1], Step [3255/7635], Loss: 4.3445\n",
      "Epoch [1/1], Step [3256/7635], Loss: 4.2855\n",
      "Epoch [1/1], Step [3257/7635], Loss: 4.2562\n",
      "Epoch [1/1], Step [3258/7635], Loss: 4.2702\n",
      "Epoch [1/1], Step [3259/7635], Loss: 4.2918\n",
      "Epoch [1/1], Step [3260/7635], Loss: 4.3377\n",
      "Epoch [1/1], Step [3261/7635], Loss: 4.2957\n",
      "Epoch [1/1], Step [3262/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [3263/7635], Loss: 4.2892\n",
      "Epoch [1/1], Step [3264/7635], Loss: 4.2489\n",
      "Epoch [1/1], Step [3265/7635], Loss: 4.3410\n",
      "Epoch [1/1], Step [3266/7635], Loss: 4.3002\n",
      "Epoch [1/1], Step [3267/7635], Loss: 4.3568\n",
      "Epoch [1/1], Step [3268/7635], Loss: 4.3072\n",
      "Epoch [1/1], Step [3269/7635], Loss: 4.2948\n",
      "Epoch [1/1], Step [3270/7635], Loss: 4.3074\n",
      "Epoch [1/1], Step [3271/7635], Loss: 4.2412\n",
      "Epoch [1/1], Step [3272/7635], Loss: 4.3751\n",
      "Epoch [1/1], Step [3273/7635], Loss: 4.2332\n",
      "Epoch [1/1], Step [3274/7635], Loss: 4.3214\n",
      "Epoch [1/1], Step [3275/7635], Loss: 4.2308\n",
      "Epoch [1/1], Step [3276/7635], Loss: 4.2604\n",
      "Epoch [1/1], Step [3277/7635], Loss: 4.3223\n",
      "Epoch [1/1], Step [3278/7635], Loss: 4.2887\n",
      "Epoch [1/1], Step [3279/7635], Loss: 4.3013\n",
      "Epoch [1/1], Step [3280/7635], Loss: 4.2271\n",
      "Epoch [1/1], Step [3281/7635], Loss: 4.3497\n",
      "Epoch [1/1], Step [3282/7635], Loss: 4.2962\n",
      "Epoch [1/1], Step [3283/7635], Loss: 4.2963\n",
      "Epoch [1/1], Step [3284/7635], Loss: 4.3747\n",
      "Epoch [1/1], Step [3285/7635], Loss: 4.3385\n",
      "Epoch [1/1], Step [3286/7635], Loss: 4.2519\n",
      "Epoch [1/1], Step [3287/7635], Loss: 4.2568\n",
      "Epoch [1/1], Step [3288/7635], Loss: 4.2601\n",
      "Epoch [1/1], Step [3289/7635], Loss: 4.2696\n",
      "Epoch [1/1], Step [3290/7635], Loss: 4.3008\n",
      "Epoch [1/1], Step [3291/7635], Loss: 4.3154\n",
      "Epoch [1/1], Step [3292/7635], Loss: 4.2623\n",
      "Epoch [1/1], Step [3293/7635], Loss: 4.3223\n",
      "Epoch [1/1], Step [3294/7635], Loss: 4.2922\n",
      "Epoch [1/1], Step [3295/7635], Loss: 4.3396\n",
      "Epoch [1/1], Step [3296/7635], Loss: 4.3369\n",
      "Epoch [1/1], Step [3297/7635], Loss: 4.3637\n",
      "Epoch [1/1], Step [3298/7635], Loss: 4.3891\n",
      "Epoch [1/1], Step [3299/7635], Loss: 4.2116\n",
      "Epoch [1/1], Step [3300/7635], Loss: 4.2678\n",
      "Epoch [1/1], Step [3301/7635], Loss: 4.3635\n",
      "Epoch [1/1], Step [3302/7635], Loss: 4.3031\n",
      "Epoch [1/1], Step [3303/7635], Loss: 4.3138\n",
      "Epoch [1/1], Step [3304/7635], Loss: 4.2309\n",
      "Epoch [1/1], Step [3305/7635], Loss: 4.2782\n",
      "Epoch [1/1], Step [3306/7635], Loss: 4.2791\n",
      "Epoch [1/1], Step [3307/7635], Loss: 4.2440\n",
      "Epoch [1/1], Step [3308/7635], Loss: 4.2569\n",
      "Epoch [1/1], Step [3309/7635], Loss: 4.3377\n",
      "Epoch [1/1], Step [3310/7635], Loss: 4.3434\n",
      "Epoch [1/1], Step [3311/7635], Loss: 4.2456\n",
      "Epoch [1/1], Step [3312/7635], Loss: 4.2620\n",
      "Epoch [1/1], Step [3313/7635], Loss: 4.2941\n",
      "Epoch [1/1], Step [3314/7635], Loss: 4.3435\n",
      "Epoch [1/1], Step [3315/7635], Loss: 4.2855\n",
      "Epoch [1/1], Step [3316/7635], Loss: 4.3470\n",
      "Epoch [1/1], Step [3317/7635], Loss: 4.2538\n",
      "Epoch [1/1], Step [3318/7635], Loss: 4.2273\n",
      "Epoch [1/1], Step [3319/7635], Loss: 4.2588\n",
      "Epoch [1/1], Step [3320/7635], Loss: 4.3578\n",
      "Epoch [1/1], Step [3321/7635], Loss: 4.2404\n",
      "Epoch [1/1], Step [3322/7635], Loss: 4.2755\n",
      "Epoch [1/1], Step [3323/7635], Loss: 4.2914\n",
      "Epoch [1/1], Step [3324/7635], Loss: 4.3054\n",
      "Epoch [1/1], Step [3325/7635], Loss: 4.2067\n",
      "Epoch [1/1], Step [3326/7635], Loss: 4.2067\n",
      "Epoch [1/1], Step [3327/7635], Loss: 4.3035\n",
      "Epoch [1/1], Step [3328/7635], Loss: 4.2770\n",
      "Epoch [1/1], Step [3329/7635], Loss: 4.3290\n",
      "Epoch [1/1], Step [3330/7635], Loss: 4.3661\n",
      "Epoch [1/1], Step [3331/7635], Loss: 4.2931\n",
      "Epoch [1/1], Step [3332/7635], Loss: 4.3791\n",
      "Epoch [1/1], Step [3333/7635], Loss: 4.3459\n",
      "Epoch [1/1], Step [3334/7635], Loss: 4.3198\n",
      "Epoch [1/1], Step [3335/7635], Loss: 4.3281\n",
      "Epoch [1/1], Step [3336/7635], Loss: 4.3200\n",
      "Epoch [1/1], Step [3337/7635], Loss: 4.4029\n",
      "Epoch [1/1], Step [3338/7635], Loss: 4.3117\n",
      "Epoch [1/1], Step [3339/7635], Loss: 4.3059\n",
      "Epoch [1/1], Step [3340/7635], Loss: 4.2843\n",
      "Epoch [1/1], Step [3341/7635], Loss: 4.2407\n",
      "Epoch [1/1], Step [3342/7635], Loss: 4.2435\n",
      "Epoch [1/1], Step [3343/7635], Loss: 4.2898\n",
      "Epoch [1/1], Step [3344/7635], Loss: 4.3781\n",
      "Epoch [1/1], Step [3345/7635], Loss: 4.2936\n",
      "Epoch [1/1], Step [3346/7635], Loss: 4.2467\n",
      "Epoch [1/1], Step [3347/7635], Loss: 4.3608\n",
      "Epoch [1/1], Step [3348/7635], Loss: 4.2544\n",
      "Epoch [1/1], Step [3349/7635], Loss: 4.3461\n",
      "Epoch [1/1], Step [3350/7635], Loss: 4.3138\n",
      "Epoch [1/1], Step [3351/7635], Loss: 4.4068\n",
      "Epoch [1/1], Step [3352/7635], Loss: 4.2907\n",
      "Epoch [1/1], Step [3353/7635], Loss: 4.2790\n",
      "Epoch [1/1], Step [3354/7635], Loss: 4.2564\n",
      "Epoch [1/1], Step [3355/7635], Loss: 4.4336\n",
      "Epoch [1/1], Step [3356/7635], Loss: 4.2139\n",
      "Epoch [1/1], Step [3357/7635], Loss: 4.3788\n",
      "Epoch [1/1], Step [3358/7635], Loss: 4.3544\n",
      "Epoch [1/1], Step [3359/7635], Loss: 4.3075\n",
      "Epoch [1/1], Step [3360/7635], Loss: 4.3465\n",
      "Epoch [1/1], Step [3361/7635], Loss: 4.3162\n",
      "Epoch [1/1], Step [3362/7635], Loss: 4.3089\n",
      "Epoch [1/1], Step [3363/7635], Loss: 4.3155\n",
      "Epoch [1/1], Step [3364/7635], Loss: 4.3436\n",
      "Epoch [1/1], Step [3365/7635], Loss: 4.2383\n",
      "Epoch [1/1], Step [3366/7635], Loss: 4.3111\n",
      "Epoch [1/1], Step [3367/7635], Loss: 4.3644\n",
      "Epoch [1/1], Step [3368/7635], Loss: 4.3248\n",
      "Epoch [1/1], Step [3369/7635], Loss: 4.3171\n",
      "Epoch [1/1], Step [3370/7635], Loss: 4.2989\n",
      "Epoch [1/1], Step [3371/7635], Loss: 4.3700\n",
      "Epoch [1/1], Step [3372/7635], Loss: 4.3368\n",
      "Epoch [1/1], Step [3373/7635], Loss: 4.2281\n",
      "Epoch [1/1], Step [3374/7635], Loss: 4.3560\n",
      "Epoch [1/1], Step [3375/7635], Loss: 4.2209\n",
      "Epoch [1/1], Step [3376/7635], Loss: 4.2932\n",
      "Epoch [1/1], Step [3377/7635], Loss: 4.3311\n",
      "Epoch [1/1], Step [3378/7635], Loss: 4.3194\n",
      "Epoch [1/1], Step [3379/7635], Loss: 4.2564\n",
      "Epoch [1/1], Step [3380/7635], Loss: 4.3284\n",
      "Epoch [1/1], Step [3381/7635], Loss: 4.2831\n",
      "Epoch [1/1], Step [3382/7635], Loss: 4.3084\n",
      "Epoch [1/1], Step [3383/7635], Loss: 4.3774\n",
      "Epoch [1/1], Step [3384/7635], Loss: 4.3664\n",
      "Epoch [1/1], Step [3385/7635], Loss: 4.2391\n",
      "Epoch [1/1], Step [3386/7635], Loss: 4.2531\n",
      "Epoch [1/1], Step [3387/7635], Loss: 4.3143\n",
      "Epoch [1/1], Step [3388/7635], Loss: 4.3730\n",
      "Epoch [1/1], Step [3389/7635], Loss: 4.3435\n",
      "Epoch [1/1], Step [3390/7635], Loss: 4.2846\n",
      "Epoch [1/1], Step [3391/7635], Loss: 4.3811\n",
      "Epoch [1/1], Step [3392/7635], Loss: 4.3052\n",
      "Epoch [1/1], Step [3393/7635], Loss: 4.2646\n",
      "Epoch [1/1], Step [3394/7635], Loss: 4.3046\n",
      "Epoch [1/1], Step [3395/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [3396/7635], Loss: 4.3510\n",
      "Epoch [1/1], Step [3397/7635], Loss: 4.3420\n",
      "Epoch [1/1], Step [3398/7635], Loss: 4.2861\n",
      "Epoch [1/1], Step [3399/7635], Loss: 4.2909\n",
      "Epoch [1/1], Step [3400/7635], Loss: 4.3285\n",
      "Epoch [1/1], Step [3401/7635], Loss: 4.3159\n",
      "Epoch [1/1], Step [3402/7635], Loss: 4.3589\n",
      "Epoch [1/1], Step [3403/7635], Loss: 4.3176\n",
      "Epoch [1/1], Step [3404/7635], Loss: 4.2080\n",
      "Epoch [1/1], Step [3405/7635], Loss: 4.2622\n",
      "Epoch [1/1], Step [3406/7635], Loss: 4.3351\n",
      "Epoch [1/1], Step [3407/7635], Loss: 4.2379\n",
      "Epoch [1/1], Step [3408/7635], Loss: 4.3371\n",
      "Epoch [1/1], Step [3409/7635], Loss: 4.2015\n",
      "Epoch [1/1], Step [3410/7635], Loss: 4.3514\n",
      "Epoch [1/1], Step [3411/7635], Loss: 4.2908\n",
      "Epoch [1/1], Step [3412/7635], Loss: 4.1886\n",
      "Epoch [1/1], Step [3413/7635], Loss: 4.2350\n",
      "Epoch [1/1], Step [3414/7635], Loss: 4.2890\n",
      "Epoch [1/1], Step [3415/7635], Loss: 4.2608\n",
      "Epoch [1/1], Step [3416/7635], Loss: 4.2929\n",
      "Epoch [1/1], Step [3417/7635], Loss: 4.2761\n",
      "Epoch [1/1], Step [3418/7635], Loss: 4.3901\n",
      "Epoch [1/1], Step [3419/7635], Loss: 4.2840\n",
      "Epoch [1/1], Step [3420/7635], Loss: 4.2747\n",
      "Epoch [1/1], Step [3421/7635], Loss: 4.2386\n",
      "Epoch [1/1], Step [3422/7635], Loss: 4.2079\n",
      "Epoch [1/1], Step [3423/7635], Loss: 4.3765\n",
      "Epoch [1/1], Step [3424/7635], Loss: 4.3732\n",
      "Epoch [1/1], Step [3425/7635], Loss: 4.2993\n",
      "Epoch [1/1], Step [3426/7635], Loss: 4.2904\n",
      "Epoch [1/1], Step [3427/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [3428/7635], Loss: 4.3422\n",
      "Epoch [1/1], Step [3429/7635], Loss: 4.2722\n",
      "Epoch [1/1], Step [3430/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [3431/7635], Loss: 4.3267\n",
      "Epoch [1/1], Step [3432/7635], Loss: 4.3629\n",
      "Epoch [1/1], Step [3433/7635], Loss: 4.2726\n",
      "Epoch [1/1], Step [3434/7635], Loss: 4.2477\n",
      "Epoch [1/1], Step [3435/7635], Loss: 4.3558\n",
      "Epoch [1/1], Step [3436/7635], Loss: 4.2243\n",
      "Epoch [1/1], Step [3437/7635], Loss: 4.3329\n",
      "Epoch [1/1], Step [3438/7635], Loss: 4.2510\n",
      "Epoch [1/1], Step [3439/7635], Loss: 4.2304\n",
      "Epoch [1/1], Step [3440/7635], Loss: 4.3131\n",
      "Epoch [1/1], Step [3441/7635], Loss: 4.3043\n",
      "Epoch [1/1], Step [3442/7635], Loss: 4.3021\n",
      "Epoch [1/1], Step [3443/7635], Loss: 4.3218\n",
      "Epoch [1/1], Step [3444/7635], Loss: 4.3016\n",
      "Epoch [1/1], Step [3445/7635], Loss: 4.3266\n",
      "Epoch [1/1], Step [3446/7635], Loss: 4.2252\n",
      "Epoch [1/1], Step [3447/7635], Loss: 4.3508\n",
      "Epoch [1/1], Step [3448/7635], Loss: 4.3038\n",
      "Epoch [1/1], Step [3449/7635], Loss: 4.3071\n",
      "Epoch [1/1], Step [3450/7635], Loss: 4.3091\n",
      "Epoch [1/1], Step [3451/7635], Loss: 4.3058\n",
      "Epoch [1/1], Step [3452/7635], Loss: 4.3812\n",
      "Epoch [1/1], Step [3453/7635], Loss: 4.3690\n",
      "Epoch [1/1], Step [3454/7635], Loss: 4.2388\n",
      "Epoch [1/1], Step [3455/7635], Loss: 4.3592\n",
      "Epoch [1/1], Step [3456/7635], Loss: 4.3412\n",
      "Epoch [1/1], Step [3457/7635], Loss: 4.1948\n",
      "Epoch [1/1], Step [3458/7635], Loss: 4.3489\n",
      "Epoch [1/1], Step [3459/7635], Loss: 4.3373\n",
      "Epoch [1/1], Step [3460/7635], Loss: 4.3318\n",
      "Epoch [1/1], Step [3461/7635], Loss: 4.3368\n",
      "Epoch [1/1], Step [3462/7635], Loss: 4.2305\n",
      "Epoch [1/1], Step [3463/7635], Loss: 4.2811\n",
      "Epoch [1/1], Step [3464/7635], Loss: 4.2973\n",
      "Epoch [1/1], Step [3465/7635], Loss: 4.2565\n",
      "Epoch [1/1], Step [3466/7635], Loss: 4.3033\n",
      "Epoch [1/1], Step [3467/7635], Loss: 4.2639\n",
      "Epoch [1/1], Step [3468/7635], Loss: 4.3021\n",
      "Epoch [1/1], Step [3469/7635], Loss: 4.3214\n",
      "Epoch [1/1], Step [3470/7635], Loss: 4.3422\n",
      "Epoch [1/1], Step [3471/7635], Loss: 4.2459\n",
      "Epoch [1/1], Step [3472/7635], Loss: 4.2984\n",
      "Epoch [1/1], Step [3473/7635], Loss: 4.3810\n",
      "Epoch [1/1], Step [3474/7635], Loss: 4.2604\n",
      "Epoch [1/1], Step [3475/7635], Loss: 4.2752\n",
      "Epoch [1/1], Step [3476/7635], Loss: 4.3009\n",
      "Epoch [1/1], Step [3477/7635], Loss: 4.3194\n",
      "Epoch [1/1], Step [3478/7635], Loss: 4.3117\n",
      "Epoch [1/1], Step [3479/7635], Loss: 4.2959\n",
      "Epoch [1/1], Step [3480/7635], Loss: 4.3130\n",
      "Epoch [1/1], Step [3481/7635], Loss: 4.3480\n",
      "Epoch [1/1], Step [3482/7635], Loss: 4.2617\n",
      "Epoch [1/1], Step [3483/7635], Loss: 4.3856\n",
      "Epoch [1/1], Step [3484/7635], Loss: 4.2570\n",
      "Epoch [1/1], Step [3485/7635], Loss: 4.2110\n",
      "Epoch [1/1], Step [3486/7635], Loss: 4.3041\n",
      "Epoch [1/1], Step [3487/7635], Loss: 4.2937\n",
      "Epoch [1/1], Step [3488/7635], Loss: 4.3141\n",
      "Epoch [1/1], Step [3489/7635], Loss: 4.2443\n",
      "Epoch [1/1], Step [3490/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [3491/7635], Loss: 4.3009\n",
      "Epoch [1/1], Step [3492/7635], Loss: 4.2747\n",
      "Epoch [1/1], Step [3493/7635], Loss: 4.3295\n",
      "Epoch [1/1], Step [3494/7635], Loss: 4.2208\n",
      "Epoch [1/1], Step [3495/7635], Loss: 4.2561\n",
      "Epoch [1/1], Step [3496/7635], Loss: 4.2360\n",
      "Epoch [1/1], Step [3497/7635], Loss: 4.2825\n",
      "Epoch [1/1], Step [3498/7635], Loss: 4.2592\n",
      "Epoch [1/1], Step [3499/7635], Loss: 4.2621\n",
      "Epoch [1/1], Step [3500/7635], Loss: 4.2494\n",
      "Epoch [1/1], Step [3501/7635], Loss: 4.2120\n",
      "Epoch [1/1], Step [3502/7635], Loss: 4.3214\n",
      "Epoch [1/1], Step [3503/7635], Loss: 4.2812\n",
      "Epoch [1/1], Step [3504/7635], Loss: 4.2523\n",
      "Epoch [1/1], Step [3505/7635], Loss: 4.2513\n",
      "Epoch [1/1], Step [3506/7635], Loss: 4.2754\n",
      "Epoch [1/1], Step [3507/7635], Loss: 4.2676\n",
      "Epoch [1/1], Step [3508/7635], Loss: 4.2690\n",
      "Epoch [1/1], Step [3509/7635], Loss: 4.3231\n",
      "Epoch [1/1], Step [3510/7635], Loss: 4.3054\n",
      "Epoch [1/1], Step [3511/7635], Loss: 4.3072\n",
      "Epoch [1/1], Step [3512/7635], Loss: 4.3313\n",
      "Epoch [1/1], Step [3513/7635], Loss: 4.2488\n",
      "Epoch [1/1], Step [3514/7635], Loss: 4.2594\n",
      "Epoch [1/1], Step [3515/7635], Loss: 4.2254\n",
      "Epoch [1/1], Step [3516/7635], Loss: 4.2703\n",
      "Epoch [1/1], Step [3517/7635], Loss: 4.2283\n",
      "Epoch [1/1], Step [3518/7635], Loss: 4.2476\n",
      "Epoch [1/1], Step [3519/7635], Loss: 4.3419\n",
      "Epoch [1/1], Step [3520/7635], Loss: 4.3180\n",
      "Epoch [1/1], Step [3521/7635], Loss: 4.2496\n",
      "Epoch [1/1], Step [3522/7635], Loss: 4.1696\n",
      "Epoch [1/1], Step [3523/7635], Loss: 4.2953\n",
      "Epoch [1/1], Step [3524/7635], Loss: 4.2870\n",
      "Epoch [1/1], Step [3525/7635], Loss: 4.3554\n",
      "Epoch [1/1], Step [3526/7635], Loss: 4.3434\n",
      "Epoch [1/1], Step [3527/7635], Loss: 4.3245\n",
      "Epoch [1/1], Step [3528/7635], Loss: 4.2302\n",
      "Epoch [1/1], Step [3529/7635], Loss: 4.3239\n",
      "Epoch [1/1], Step [3530/7635], Loss: 4.3520\n",
      "Epoch [1/1], Step [3531/7635], Loss: 4.3401\n",
      "Epoch [1/1], Step [3532/7635], Loss: 4.2959\n",
      "Epoch [1/1], Step [3533/7635], Loss: 4.2871\n",
      "Epoch [1/1], Step [3534/7635], Loss: 4.3079\n",
      "Epoch [1/1], Step [3535/7635], Loss: 4.3151\n",
      "Epoch [1/1], Step [3536/7635], Loss: 4.3558\n",
      "Epoch [1/1], Step [3537/7635], Loss: 4.2478\n",
      "Epoch [1/1], Step [3538/7635], Loss: 4.3436\n",
      "Epoch [1/1], Step [3539/7635], Loss: 4.2383\n",
      "Epoch [1/1], Step [3540/7635], Loss: 4.3354\n",
      "Epoch [1/1], Step [3541/7635], Loss: 4.3431\n",
      "Epoch [1/1], Step [3542/7635], Loss: 4.3094\n",
      "Epoch [1/1], Step [3543/7635], Loss: 4.2838\n",
      "Epoch [1/1], Step [3544/7635], Loss: 4.1414\n",
      "Epoch [1/1], Step [3545/7635], Loss: 4.2826\n",
      "Epoch [1/1], Step [3546/7635], Loss: 4.2891\n",
      "Epoch [1/1], Step [3547/7635], Loss: 4.3654\n",
      "Epoch [1/1], Step [3548/7635], Loss: 4.2590\n",
      "Epoch [1/1], Step [3549/7635], Loss: 4.1871\n",
      "Epoch [1/1], Step [3550/7635], Loss: 4.2288\n",
      "Epoch [1/1], Step [3551/7635], Loss: 4.2987\n",
      "Epoch [1/1], Step [3552/7635], Loss: 4.3009\n",
      "Epoch [1/1], Step [3553/7635], Loss: 4.2513\n",
      "Epoch [1/1], Step [3554/7635], Loss: 4.3712\n",
      "Epoch [1/1], Step [3555/7635], Loss: 4.2413\n",
      "Epoch [1/1], Step [3556/7635], Loss: 4.3441\n",
      "Epoch [1/1], Step [3557/7635], Loss: 4.3074\n",
      "Epoch [1/1], Step [3558/7635], Loss: 4.3922\n",
      "Epoch [1/1], Step [3559/7635], Loss: 4.2551\n",
      "Epoch [1/1], Step [3560/7635], Loss: 4.2542\n",
      "Epoch [1/1], Step [3561/7635], Loss: 4.3103\n",
      "Epoch [1/1], Step [3562/7635], Loss: 4.3528\n",
      "Epoch [1/1], Step [3563/7635], Loss: 4.2008\n",
      "Epoch [1/1], Step [3564/7635], Loss: 4.2383\n",
      "Epoch [1/1], Step [3565/7635], Loss: 4.2229\n",
      "Epoch [1/1], Step [3566/7635], Loss: 4.2617\n",
      "Epoch [1/1], Step [3567/7635], Loss: 4.2506\n",
      "Epoch [1/1], Step [3568/7635], Loss: 4.3244\n",
      "Epoch [1/1], Step [3569/7635], Loss: 4.2921\n",
      "Epoch [1/1], Step [3570/7635], Loss: 4.2671\n",
      "Epoch [1/1], Step [3571/7635], Loss: 4.3052\n",
      "Epoch [1/1], Step [3572/7635], Loss: 4.3290\n",
      "Epoch [1/1], Step [3573/7635], Loss: 4.2957\n",
      "Epoch [1/1], Step [3574/7635], Loss: 4.2496\n",
      "Epoch [1/1], Step [3575/7635], Loss: 4.1970\n",
      "Epoch [1/1], Step [3576/7635], Loss: 4.2957\n",
      "Epoch [1/1], Step [3577/7635], Loss: 4.1931\n",
      "Epoch [1/1], Step [3578/7635], Loss: 4.3479\n",
      "Epoch [1/1], Step [3579/7635], Loss: 4.2539\n",
      "Epoch [1/1], Step [3580/7635], Loss: 4.2590\n",
      "Epoch [1/1], Step [3581/7635], Loss: 4.3077\n",
      "Epoch [1/1], Step [3582/7635], Loss: 4.1968\n",
      "Epoch [1/1], Step [3583/7635], Loss: 4.2613\n",
      "Epoch [1/1], Step [3584/7635], Loss: 4.3063\n",
      "Epoch [1/1], Step [3585/7635], Loss: 4.2759\n",
      "Epoch [1/1], Step [3586/7635], Loss: 4.2432\n",
      "Epoch [1/1], Step [3587/7635], Loss: 4.3166\n",
      "Epoch [1/1], Step [3588/7635], Loss: 4.2468\n",
      "Epoch [1/1], Step [3589/7635], Loss: 4.1961\n",
      "Epoch [1/1], Step [3590/7635], Loss: 4.2919\n",
      "Epoch [1/1], Step [3591/7635], Loss: 4.3119\n",
      "Epoch [1/1], Step [3592/7635], Loss: 4.2718\n",
      "Epoch [1/1], Step [3593/7635], Loss: 4.2655\n",
      "Epoch [1/1], Step [3594/7635], Loss: 4.3009\n",
      "Epoch [1/1], Step [3595/7635], Loss: 4.3458\n",
      "Epoch [1/1], Step [3596/7635], Loss: 4.3572\n",
      "Epoch [1/1], Step [3597/7635], Loss: 4.3381\n",
      "Epoch [1/1], Step [3598/7635], Loss: 4.3561\n",
      "Epoch [1/1], Step [3599/7635], Loss: 4.3314\n",
      "Epoch [1/1], Step [3600/7635], Loss: 4.2930\n",
      "Epoch [1/1], Step [3601/7635], Loss: 4.2836\n",
      "Epoch [1/1], Step [3602/7635], Loss: 4.2960\n",
      "Epoch [1/1], Step [3603/7635], Loss: 4.3351\n",
      "Epoch [1/1], Step [3604/7635], Loss: 4.2218\n",
      "Epoch [1/1], Step [3605/7635], Loss: 4.3820\n",
      "Epoch [1/1], Step [3606/7635], Loss: 4.2632\n",
      "Epoch [1/1], Step [3607/7635], Loss: 4.3050\n",
      "Epoch [1/1], Step [3608/7635], Loss: 4.2617\n",
      "Epoch [1/1], Step [3609/7635], Loss: 4.2073\n",
      "Epoch [1/1], Step [3610/7635], Loss: 4.2693\n",
      "Epoch [1/1], Step [3611/7635], Loss: 4.3038\n",
      "Epoch [1/1], Step [3612/7635], Loss: 4.3079\n",
      "Epoch [1/1], Step [3613/7635], Loss: 4.1973\n",
      "Epoch [1/1], Step [3614/7635], Loss: 4.2781\n",
      "Epoch [1/1], Step [3615/7635], Loss: 4.2561\n",
      "Epoch [1/1], Step [3616/7635], Loss: 4.2525\n",
      "Epoch [1/1], Step [3617/7635], Loss: 4.2455\n",
      "Epoch [1/1], Step [3618/7635], Loss: 4.2694\n",
      "Epoch [1/1], Step [3619/7635], Loss: 4.2842\n",
      "Epoch [1/1], Step [3620/7635], Loss: 4.2779\n",
      "Epoch [1/1], Step [3621/7635], Loss: 4.1952\n",
      "Epoch [1/1], Step [3622/7635], Loss: 4.3116\n",
      "Epoch [1/1], Step [3623/7635], Loss: 4.2876\n",
      "Epoch [1/1], Step [3624/7635], Loss: 4.2290\n",
      "Epoch [1/1], Step [3625/7635], Loss: 4.2655\n",
      "Epoch [1/1], Step [3626/7635], Loss: 4.3030\n",
      "Epoch [1/1], Step [3627/7635], Loss: 4.1338\n",
      "Epoch [1/1], Step [3628/7635], Loss: 4.2388\n",
      "Epoch [1/1], Step [3629/7635], Loss: 4.2411\n",
      "Epoch [1/1], Step [3630/7635], Loss: 4.1968\n",
      "Epoch [1/1], Step [3631/7635], Loss: 4.2219\n",
      "Epoch [1/1], Step [3632/7635], Loss: 4.2820\n",
      "Epoch [1/1], Step [3633/7635], Loss: 4.3251\n",
      "Epoch [1/1], Step [3634/7635], Loss: 4.3685\n",
      "Epoch [1/1], Step [3635/7635], Loss: 4.2547\n",
      "Epoch [1/1], Step [3636/7635], Loss: 4.2908\n",
      "Epoch [1/1], Step [3637/7635], Loss: 4.2770\n",
      "Epoch [1/1], Step [3638/7635], Loss: 4.2352\n",
      "Epoch [1/1], Step [3639/7635], Loss: 4.3175\n",
      "Epoch [1/1], Step [3640/7635], Loss: 4.2053\n",
      "Epoch [1/1], Step [3641/7635], Loss: 4.3094\n",
      "Epoch [1/1], Step [3642/7635], Loss: 4.3048\n",
      "Epoch [1/1], Step [3643/7635], Loss: 4.2414\n",
      "Epoch [1/1], Step [3644/7635], Loss: 4.2465\n",
      "Epoch [1/1], Step [3645/7635], Loss: 4.2712\n",
      "Epoch [1/1], Step [3646/7635], Loss: 4.2554\n",
      "Epoch [1/1], Step [3647/7635], Loss: 4.2013\n",
      "Epoch [1/1], Step [3648/7635], Loss: 4.2668\n",
      "Epoch [1/1], Step [3649/7635], Loss: 4.2690\n",
      "Epoch [1/1], Step [3650/7635], Loss: 4.3550\n",
      "Epoch [1/1], Step [3651/7635], Loss: 4.1721\n",
      "Epoch [1/1], Step [3652/7635], Loss: 4.4158\n",
      "Epoch [1/1], Step [3653/7635], Loss: 4.2074\n",
      "Epoch [1/1], Step [3654/7635], Loss: 4.2858\n",
      "Epoch [1/1], Step [3655/7635], Loss: 4.2765\n",
      "Epoch [1/1], Step [3656/7635], Loss: 4.2584\n",
      "Epoch [1/1], Step [3657/7635], Loss: 4.2497\n",
      "Epoch [1/1], Step [3658/7635], Loss: 4.3088\n",
      "Epoch [1/1], Step [3659/7635], Loss: 4.2296\n",
      "Epoch [1/1], Step [3660/7635], Loss: 4.3208\n",
      "Epoch [1/1], Step [3661/7635], Loss: 4.2643\n",
      "Epoch [1/1], Step [3662/7635], Loss: 4.3047\n",
      "Epoch [1/1], Step [3663/7635], Loss: 4.2785\n",
      "Epoch [1/1], Step [3664/7635], Loss: 4.1833\n",
      "Epoch [1/1], Step [3665/7635], Loss: 4.2787\n",
      "Epoch [1/1], Step [3666/7635], Loss: 4.3442\n",
      "Epoch [1/1], Step [3667/7635], Loss: 4.3065\n",
      "Epoch [1/1], Step [3668/7635], Loss: 4.2292\n",
      "Epoch [1/1], Step [3669/7635], Loss: 4.2955\n",
      "Epoch [1/1], Step [3670/7635], Loss: 4.2827\n",
      "Epoch [1/1], Step [3671/7635], Loss: 4.2347\n",
      "Epoch [1/1], Step [3672/7635], Loss: 4.3170\n",
      "Epoch [1/1], Step [3673/7635], Loss: 4.1891\n",
      "Epoch [1/1], Step [3674/7635], Loss: 4.2770\n",
      "Epoch [1/1], Step [3675/7635], Loss: 4.2246\n",
      "Epoch [1/1], Step [3676/7635], Loss: 4.2595\n",
      "Epoch [1/1], Step [3677/7635], Loss: 4.2526\n",
      "Epoch [1/1], Step [3678/7635], Loss: 4.2766\n",
      "Epoch [1/1], Step [3679/7635], Loss: 4.3412\n",
      "Epoch [1/1], Step [3680/7635], Loss: 4.3061\n",
      "Epoch [1/1], Step [3681/7635], Loss: 4.2181\n",
      "Epoch [1/1], Step [3682/7635], Loss: 4.2527\n",
      "Epoch [1/1], Step [3683/7635], Loss: 4.2925\n",
      "Epoch [1/1], Step [3684/7635], Loss: 4.2584\n",
      "Epoch [1/1], Step [3685/7635], Loss: 4.2470\n",
      "Epoch [1/1], Step [3686/7635], Loss: 4.3201\n",
      "Epoch [1/1], Step [3687/7635], Loss: 4.2808\n",
      "Epoch [1/1], Step [3688/7635], Loss: 4.2222\n",
      "Epoch [1/1], Step [3689/7635], Loss: 4.1564\n",
      "Epoch [1/1], Step [3690/7635], Loss: 4.2874\n",
      "Epoch [1/1], Step [3691/7635], Loss: 4.3095\n",
      "Epoch [1/1], Step [3692/7635], Loss: 4.3095\n",
      "Epoch [1/1], Step [3693/7635], Loss: 4.2553\n",
      "Epoch [1/1], Step [3694/7635], Loss: 4.3059\n",
      "Epoch [1/1], Step [3695/7635], Loss: 4.2673\n",
      "Epoch [1/1], Step [3696/7635], Loss: 4.2646\n",
      "Epoch [1/1], Step [3697/7635], Loss: 4.2287\n",
      "Epoch [1/1], Step [3698/7635], Loss: 4.2625\n",
      "Epoch [1/1], Step [3699/7635], Loss: 4.2876\n",
      "Epoch [1/1], Step [3700/7635], Loss: 4.3135\n",
      "Epoch [1/1], Step [3701/7635], Loss: 4.2271\n",
      "Epoch [1/1], Step [3702/7635], Loss: 4.2524\n",
      "Epoch [1/1], Step [3703/7635], Loss: 4.2043\n",
      "Epoch [1/1], Step [3704/7635], Loss: 4.2983\n",
      "Epoch [1/1], Step [3705/7635], Loss: 4.3423\n",
      "Epoch [1/1], Step [3706/7635], Loss: 4.2250\n",
      "Epoch [1/1], Step [3707/7635], Loss: 4.2457\n",
      "Epoch [1/1], Step [3708/7635], Loss: 4.2987\n",
      "Epoch [1/1], Step [3709/7635], Loss: 4.2502\n",
      "Epoch [1/1], Step [3710/7635], Loss: 4.3367\n",
      "Epoch [1/1], Step [3711/7635], Loss: 4.3296\n",
      "Epoch [1/1], Step [3712/7635], Loss: 4.3299\n",
      "Epoch [1/1], Step [3713/7635], Loss: 4.2661\n",
      "Epoch [1/1], Step [3714/7635], Loss: 4.1828\n",
      "Epoch [1/1], Step [3715/7635], Loss: 4.3194\n",
      "Epoch [1/1], Step [3716/7635], Loss: 4.3155\n",
      "Epoch [1/1], Step [3717/7635], Loss: 4.3019\n",
      "Epoch [1/1], Step [3718/7635], Loss: 4.3216\n",
      "Epoch [1/1], Step [3719/7635], Loss: 4.3085\n",
      "Epoch [1/1], Step [3720/7635], Loss: 4.2773\n",
      "Epoch [1/1], Step [3721/7635], Loss: 4.3454\n",
      "Epoch [1/1], Step [3722/7635], Loss: 4.2281\n",
      "Epoch [1/1], Step [3723/7635], Loss: 4.2363\n",
      "Epoch [1/1], Step [3724/7635], Loss: 4.2065\n",
      "Epoch [1/1], Step [3725/7635], Loss: 4.2611\n",
      "Epoch [1/1], Step [3726/7635], Loss: 4.2443\n",
      "Epoch [1/1], Step [3727/7635], Loss: 4.2600\n",
      "Epoch [1/1], Step [3728/7635], Loss: 4.3465\n",
      "Epoch [1/1], Step [3729/7635], Loss: 4.2456\n",
      "Epoch [1/1], Step [3730/7635], Loss: 4.2573\n",
      "Epoch [1/1], Step [3731/7635], Loss: 4.1961\n",
      "Epoch [1/1], Step [3732/7635], Loss: 4.2486\n",
      "Epoch [1/1], Step [3733/7635], Loss: 4.1575\n",
      "Epoch [1/1], Step [3734/7635], Loss: 4.2741\n",
      "Epoch [1/1], Step [3735/7635], Loss: 4.2233\n",
      "Epoch [1/1], Step [3736/7635], Loss: 4.2690\n",
      "Epoch [1/1], Step [3737/7635], Loss: 4.2690\n",
      "Epoch [1/1], Step [3738/7635], Loss: 4.2742\n",
      "Epoch [1/1], Step [3739/7635], Loss: 4.2250\n",
      "Epoch [1/1], Step [3740/7635], Loss: 4.2379\n",
      "Epoch [1/1], Step [3741/7635], Loss: 4.3207\n",
      "Epoch [1/1], Step [3742/7635], Loss: 4.2453\n",
      "Epoch [1/1], Step [3743/7635], Loss: 4.2347\n",
      "Epoch [1/1], Step [3744/7635], Loss: 4.1959\n",
      "Epoch [1/1], Step [3745/7635], Loss: 4.2370\n",
      "Epoch [1/1], Step [3746/7635], Loss: 4.2255\n",
      "Epoch [1/1], Step [3747/7635], Loss: 4.2488\n",
      "Epoch [1/1], Step [3748/7635], Loss: 4.2068\n",
      "Epoch [1/1], Step [3749/7635], Loss: 4.2161\n",
      "Epoch [1/1], Step [3750/7635], Loss: 4.2946\n",
      "Epoch [1/1], Step [3751/7635], Loss: 4.2727\n",
      "Epoch [1/1], Step [3752/7635], Loss: 4.2417\n",
      "Epoch [1/1], Step [3753/7635], Loss: 4.3382\n",
      "Epoch [1/1], Step [3754/7635], Loss: 4.1496\n",
      "Epoch [1/1], Step [3755/7635], Loss: 4.2084\n",
      "Epoch [1/1], Step [3756/7635], Loss: 4.2629\n",
      "Epoch [1/1], Step [3757/7635], Loss: 4.2436\n",
      "Epoch [1/1], Step [3758/7635], Loss: 4.1852\n",
      "Epoch [1/1], Step [3759/7635], Loss: 4.2578\n",
      "Epoch [1/1], Step [3760/7635], Loss: 4.3085\n",
      "Epoch [1/1], Step [3761/7635], Loss: 4.2783\n",
      "Epoch [1/1], Step [3762/7635], Loss: 4.3441\n",
      "Epoch [1/1], Step [3763/7635], Loss: 4.2461\n",
      "Epoch [1/1], Step [3764/7635], Loss: 4.2317\n",
      "Epoch [1/1], Step [3765/7635], Loss: 4.3040\n",
      "Epoch [1/1], Step [3766/7635], Loss: 4.2888\n",
      "Epoch [1/1], Step [3767/7635], Loss: 4.3851\n",
      "Epoch [1/1], Step [3768/7635], Loss: 4.3465\n",
      "Epoch [1/1], Step [3769/7635], Loss: 4.3719\n",
      "Epoch [1/1], Step [3770/7635], Loss: 4.2519\n",
      "Epoch [1/1], Step [3771/7635], Loss: 4.2553\n",
      "Epoch [1/1], Step [3772/7635], Loss: 4.3337\n",
      "Epoch [1/1], Step [3773/7635], Loss: 4.2589\n",
      "Epoch [1/1], Step [3774/7635], Loss: 4.1974\n",
      "Epoch [1/1], Step [3775/7635], Loss: 4.2748\n",
      "Epoch [1/1], Step [3776/7635], Loss: 4.3298\n",
      "Epoch [1/1], Step [3777/7635], Loss: 4.3024\n",
      "Epoch [1/1], Step [3778/7635], Loss: 4.1668\n",
      "Epoch [1/1], Step [3779/7635], Loss: 4.3095\n",
      "Epoch [1/1], Step [3780/7635], Loss: 4.2425\n",
      "Epoch [1/1], Step [3781/7635], Loss: 4.2736\n",
      "Epoch [1/1], Step [3782/7635], Loss: 4.1965\n",
      "Epoch [1/1], Step [3783/7635], Loss: 4.2818\n",
      "Epoch [1/1], Step [3784/7635], Loss: 4.2612\n",
      "Epoch [1/1], Step [3785/7635], Loss: 4.2986\n",
      "Epoch [1/1], Step [3786/7635], Loss: 4.2621\n",
      "Epoch [1/1], Step [3787/7635], Loss: 4.1925\n",
      "Epoch [1/1], Step [3788/7635], Loss: 4.2660\n",
      "Epoch [1/1], Step [3789/7635], Loss: 4.2179\n",
      "Epoch [1/1], Step [3790/7635], Loss: 4.2382\n",
      "Epoch [1/1], Step [3791/7635], Loss: 4.2796\n",
      "Epoch [1/1], Step [3792/7635], Loss: 4.2601\n",
      "Epoch [1/1], Step [3793/7635], Loss: 4.1854\n",
      "Epoch [1/1], Step [3794/7635], Loss: 4.3915\n",
      "Epoch [1/1], Step [3795/7635], Loss: 4.1902\n",
      "Epoch [1/1], Step [3796/7635], Loss: 4.2688\n",
      "Epoch [1/1], Step [3797/7635], Loss: 4.2606\n",
      "Epoch [1/1], Step [3798/7635], Loss: 4.2897\n",
      "Epoch [1/1], Step [3799/7635], Loss: 4.3455\n",
      "Epoch [1/1], Step [3800/7635], Loss: 4.3962\n",
      "Epoch [1/1], Step [3801/7635], Loss: 4.2988\n",
      "Epoch [1/1], Step [3802/7635], Loss: 4.3012\n",
      "Epoch [1/1], Step [3803/7635], Loss: 4.2056\n",
      "Epoch [1/1], Step [3804/7635], Loss: 4.2209\n",
      "Epoch [1/1], Step [3805/7635], Loss: 4.2933\n",
      "Epoch [1/1], Step [3806/7635], Loss: 4.2686\n",
      "Epoch [1/1], Step [3807/7635], Loss: 4.3032\n",
      "Epoch [1/1], Step [3808/7635], Loss: 4.2922\n",
      "Epoch [1/1], Step [3809/7635], Loss: 4.2291\n",
      "Epoch [1/1], Step [3810/7635], Loss: 4.2862\n",
      "Epoch [1/1], Step [3811/7635], Loss: 4.1679\n",
      "Epoch [1/1], Step [3812/7635], Loss: 4.2367\n",
      "Epoch [1/1], Step [3813/7635], Loss: 4.2245\n",
      "Epoch [1/1], Step [3814/7635], Loss: 4.1943\n",
      "Epoch [1/1], Step [3815/7635], Loss: 4.3438\n",
      "Epoch [1/1], Step [3816/7635], Loss: 4.2606\n",
      "Epoch [1/1], Step [3817/7635], Loss: 4.2458\n",
      "Epoch [1/1], Step [3818/7635], Loss: 4.2533\n",
      "Epoch [1/1], Step [3819/7635], Loss: 4.2264\n",
      "Epoch [1/1], Step [3820/7635], Loss: 4.2743\n",
      "Epoch [1/1], Step [3821/7635], Loss: 4.2307\n",
      "Epoch [1/1], Step [3822/7635], Loss: 4.2531\n",
      "Epoch [1/1], Step [3823/7635], Loss: 4.2514\n",
      "Epoch [1/1], Step [3824/7635], Loss: 4.2541\n",
      "Epoch [1/1], Step [3825/7635], Loss: 4.2407\n",
      "Epoch [1/1], Step [3826/7635], Loss: 4.2511\n",
      "Epoch [1/1], Step [3827/7635], Loss: 4.2305\n",
      "Epoch [1/1], Step [3828/7635], Loss: 4.1934\n",
      "Epoch [1/1], Step [3829/7635], Loss: 4.2485\n",
      "Epoch [1/1], Step [3830/7635], Loss: 4.2659\n",
      "Epoch [1/1], Step [3831/7635], Loss: 4.2018\n",
      "Epoch [1/1], Step [3832/7635], Loss: 4.2904\n",
      "Epoch [1/1], Step [3833/7635], Loss: 4.2248\n",
      "Epoch [1/1], Step [3834/7635], Loss: 4.2317\n",
      "Epoch [1/1], Step [3835/7635], Loss: 4.2409\n",
      "Epoch [1/1], Step [3836/7635], Loss: 4.2866\n",
      "Epoch [1/1], Step [3837/7635], Loss: 4.2901\n",
      "Epoch [1/1], Step [3838/7635], Loss: 4.2342\n",
      "Epoch [1/1], Step [3839/7635], Loss: 4.2504\n",
      "Epoch [1/1], Step [3840/7635], Loss: 4.2929\n",
      "Epoch [1/1], Step [3841/7635], Loss: 4.2559\n",
      "Epoch [1/1], Step [3842/7635], Loss: 4.2517\n",
      "Epoch [1/1], Step [3843/7635], Loss: 4.2326\n",
      "Epoch [1/1], Step [3844/7635], Loss: 4.2836\n",
      "Epoch [1/1], Step [3845/7635], Loss: 4.2340\n",
      "Epoch [1/1], Step [3846/7635], Loss: 4.3506\n",
      "Epoch [1/1], Step [3847/7635], Loss: 4.3042\n",
      "Epoch [1/1], Step [3848/7635], Loss: 4.2536\n",
      "Epoch [1/1], Step [3849/7635], Loss: 4.2411\n",
      "Epoch [1/1], Step [3850/7635], Loss: 4.2634\n",
      "Epoch [1/1], Step [3851/7635], Loss: 4.2589\n",
      "Epoch [1/1], Step [3852/7635], Loss: 4.2098\n",
      "Epoch [1/1], Step [3853/7635], Loss: 4.2174\n",
      "Epoch [1/1], Step [3854/7635], Loss: 4.2608\n",
      "Epoch [1/1], Step [3855/7635], Loss: 4.2445\n",
      "Epoch [1/1], Step [3856/7635], Loss: 4.1906\n",
      "Epoch [1/1], Step [3857/7635], Loss: 4.2832\n",
      "Epoch [1/1], Step [3858/7635], Loss: 4.2303\n",
      "Epoch [1/1], Step [3859/7635], Loss: 4.2497\n",
      "Epoch [1/1], Step [3860/7635], Loss: 4.2387\n",
      "Epoch [1/1], Step [3861/7635], Loss: 4.2843\n",
      "Epoch [1/1], Step [3862/7635], Loss: 4.2255\n",
      "Epoch [1/1], Step [3863/7635], Loss: 4.2684\n",
      "Epoch [1/1], Step [3864/7635], Loss: 4.2291\n",
      "Epoch [1/1], Step [3865/7635], Loss: 4.2229\n",
      "Epoch [1/1], Step [3866/7635], Loss: 4.2793\n",
      "Epoch [1/1], Step [3867/7635], Loss: 4.1643\n",
      "Epoch [1/1], Step [3868/7635], Loss: 4.3304\n",
      "Epoch [1/1], Step [3869/7635], Loss: 4.2466\n",
      "Epoch [1/1], Step [3870/7635], Loss: 4.3160\n",
      "Epoch [1/1], Step [3871/7635], Loss: 4.2745\n",
      "Epoch [1/1], Step [3872/7635], Loss: 4.1787\n",
      "Epoch [1/1], Step [3873/7635], Loss: 4.1949\n",
      "Epoch [1/1], Step [3874/7635], Loss: 4.3198\n",
      "Epoch [1/1], Step [3875/7635], Loss: 4.2469\n",
      "Epoch [1/1], Step [3876/7635], Loss: 4.2912\n",
      "Epoch [1/1], Step [3877/7635], Loss: 4.2929\n",
      "Epoch [1/1], Step [3878/7635], Loss: 4.2343\n",
      "Epoch [1/1], Step [3879/7635], Loss: 4.3000\n",
      "Epoch [1/1], Step [3880/7635], Loss: 4.3131\n",
      "Epoch [1/1], Step [3881/7635], Loss: 4.3271\n",
      "Epoch [1/1], Step [3882/7635], Loss: 4.2184\n",
      "Epoch [1/1], Step [3883/7635], Loss: 4.1364\n",
      "Epoch [1/1], Step [3884/7635], Loss: 4.2475\n",
      "Epoch [1/1], Step [3885/7635], Loss: 4.1988\n",
      "Epoch [1/1], Step [3886/7635], Loss: 4.2397\n",
      "Epoch [1/1], Step [3887/7635], Loss: 4.2588\n",
      "Epoch [1/1], Step [3888/7635], Loss: 4.2958\n",
      "Epoch [1/1], Step [3889/7635], Loss: 4.1731\n",
      "Epoch [1/1], Step [3890/7635], Loss: 4.2932\n",
      "Epoch [1/1], Step [3891/7635], Loss: 4.2411\n",
      "Epoch [1/1], Step [3892/7635], Loss: 4.2718\n",
      "Epoch [1/1], Step [3893/7635], Loss: 4.2858\n",
      "Epoch [1/1], Step [3894/7635], Loss: 4.3186\n",
      "Epoch [1/1], Step [3895/7635], Loss: 4.1808\n",
      "Epoch [1/1], Step [3896/7635], Loss: 4.3287\n",
      "Epoch [1/1], Step [3897/7635], Loss: 4.1978\n",
      "Epoch [1/1], Step [3898/7635], Loss: 4.2405\n",
      "Epoch [1/1], Step [3899/7635], Loss: 4.2258\n",
      "Epoch [1/1], Step [3900/7635], Loss: 4.2844\n",
      "Epoch [1/1], Step [3901/7635], Loss: 4.3714\n",
      "Epoch [1/1], Step [3902/7635], Loss: 4.2589\n",
      "Epoch [1/1], Step [3903/7635], Loss: 4.1722\n",
      "Epoch [1/1], Step [3904/7635], Loss: 4.3169\n",
      "Epoch [1/1], Step [3905/7635], Loss: 4.2350\n",
      "Epoch [1/1], Step [3906/7635], Loss: 4.2925\n",
      "Epoch [1/1], Step [3907/7635], Loss: 4.2628\n",
      "Epoch [1/1], Step [3908/7635], Loss: 4.2392\n",
      "Epoch [1/1], Step [3909/7635], Loss: 4.2917\n",
      "Epoch [1/1], Step [3910/7635], Loss: 4.1938\n",
      "Epoch [1/1], Step [3911/7635], Loss: 4.2715\n",
      "Epoch [1/1], Step [3912/7635], Loss: 4.1882\n",
      "Epoch [1/1], Step [3913/7635], Loss: 4.1952\n",
      "Epoch [1/1], Step [3914/7635], Loss: 4.1894\n",
      "Epoch [1/1], Step [3915/7635], Loss: 4.2359\n",
      "Epoch [1/1], Step [3916/7635], Loss: 4.2518\n",
      "Epoch [1/1], Step [3917/7635], Loss: 4.2517\n",
      "Epoch [1/1], Step [3918/7635], Loss: 4.2234\n",
      "Epoch [1/1], Step [3919/7635], Loss: 4.2911\n",
      "Epoch [1/1], Step [3920/7635], Loss: 4.2069\n",
      "Epoch [1/1], Step [3921/7635], Loss: 4.2533\n",
      "Epoch [1/1], Step [3922/7635], Loss: 4.2778\n",
      "Epoch [1/1], Step [3923/7635], Loss: 4.2800\n",
      "Epoch [1/1], Step [3924/7635], Loss: 4.2739\n",
      "Epoch [1/1], Step [3925/7635], Loss: 4.2001\n",
      "Epoch [1/1], Step [3926/7635], Loss: 4.2679\n",
      "Epoch [1/1], Step [3927/7635], Loss: 4.2079\n",
      "Epoch [1/1], Step [3928/7635], Loss: 4.3375\n",
      "Epoch [1/1], Step [3929/7635], Loss: 4.2581\n",
      "Epoch [1/1], Step [3930/7635], Loss: 4.1644\n",
      "Epoch [1/1], Step [3931/7635], Loss: 4.1949\n",
      "Epoch [1/1], Step [3932/7635], Loss: 4.3427\n",
      "Epoch [1/1], Step [3933/7635], Loss: 4.2083\n",
      "Epoch [1/1], Step [3934/7635], Loss: 4.2701\n",
      "Epoch [1/1], Step [3935/7635], Loss: 4.2855\n",
      "Epoch [1/1], Step [3936/7635], Loss: 4.3501\n",
      "Epoch [1/1], Step [3937/7635], Loss: 4.2491\n",
      "Epoch [1/1], Step [3938/7635], Loss: 4.2649\n",
      "Epoch [1/1], Step [3939/7635], Loss: 4.1481\n",
      "Epoch [1/1], Step [3940/7635], Loss: 4.2548\n",
      "Epoch [1/1], Step [3941/7635], Loss: 4.2931\n",
      "Epoch [1/1], Step [3942/7635], Loss: 4.2608\n",
      "Epoch [1/1], Step [3943/7635], Loss: 4.2020\n",
      "Epoch [1/1], Step [3944/7635], Loss: 4.2230\n",
      "Epoch [1/1], Step [3945/7635], Loss: 4.3332\n",
      "Epoch [1/1], Step [3946/7635], Loss: 4.1964\n",
      "Epoch [1/1], Step [3947/7635], Loss: 4.2308\n",
      "Epoch [1/1], Step [3948/7635], Loss: 4.2085\n",
      "Epoch [1/1], Step [3949/7635], Loss: 4.2559\n",
      "Epoch [1/1], Step [3950/7635], Loss: 4.2805\n",
      "Epoch [1/1], Step [3951/7635], Loss: 4.2161\n",
      "Epoch [1/1], Step [3952/7635], Loss: 4.3162\n",
      "Epoch [1/1], Step [3953/7635], Loss: 4.2832\n",
      "Epoch [1/1], Step [3954/7635], Loss: 4.2703\n",
      "Epoch [1/1], Step [3955/7635], Loss: 4.2314\n",
      "Epoch [1/1], Step [3956/7635], Loss: 4.2908\n",
      "Epoch [1/1], Step [3957/7635], Loss: 4.2285\n",
      "Epoch [1/1], Step [3958/7635], Loss: 4.3129\n",
      "Epoch [1/1], Step [3959/7635], Loss: 4.2934\n",
      "Epoch [1/1], Step [3960/7635], Loss: 4.2065\n",
      "Epoch [1/1], Step [3961/7635], Loss: 4.1982\n",
      "Epoch [1/1], Step [3962/7635], Loss: 4.2539\n",
      "Epoch [1/1], Step [3963/7635], Loss: 4.1800\n",
      "Epoch [1/1], Step [3964/7635], Loss: 4.3097\n",
      "Epoch [1/1], Step [3965/7635], Loss: 4.2578\n",
      "Epoch [1/1], Step [3966/7635], Loss: 4.2795\n",
      "Epoch [1/1], Step [3967/7635], Loss: 4.1673\n",
      "Epoch [1/1], Step [3968/7635], Loss: 4.2398\n",
      "Epoch [1/1], Step [3969/7635], Loss: 4.1537\n",
      "Epoch [1/1], Step [3970/7635], Loss: 4.1865\n",
      "Epoch [1/1], Step [3971/7635], Loss: 4.1995\n",
      "Epoch [1/1], Step [3972/7635], Loss: 4.2151\n",
      "Epoch [1/1], Step [3973/7635], Loss: 4.3442\n",
      "Epoch [1/1], Step [3974/7635], Loss: 4.2755\n",
      "Epoch [1/1], Step [3975/7635], Loss: 4.2484\n",
      "Epoch [1/1], Step [3976/7635], Loss: 4.3091\n",
      "Epoch [1/1], Step [3977/7635], Loss: 4.1969\n",
      "Epoch [1/1], Step [3978/7635], Loss: 4.2121\n",
      "Epoch [1/1], Step [3979/7635], Loss: 4.2002\n",
      "Epoch [1/1], Step [3980/7635], Loss: 4.2445\n",
      "Epoch [1/1], Step [3981/7635], Loss: 4.2099\n",
      "Epoch [1/1], Step [3982/7635], Loss: 4.2270\n",
      "Epoch [1/1], Step [3983/7635], Loss: 4.2748\n",
      "Epoch [1/1], Step [3984/7635], Loss: 4.2556\n",
      "Epoch [1/1], Step [3985/7635], Loss: 4.2184\n",
      "Epoch [1/1], Step [3986/7635], Loss: 4.2603\n",
      "Epoch [1/1], Step [3987/7635], Loss: 4.2723\n",
      "Epoch [1/1], Step [3988/7635], Loss: 4.2202\n",
      "Epoch [1/1], Step [3989/7635], Loss: 4.2831\n",
      "Epoch [1/1], Step [3990/7635], Loss: 4.1674\n",
      "Epoch [1/1], Step [3991/7635], Loss: 4.2210\n",
      "Epoch [1/1], Step [3992/7635], Loss: 4.3279\n",
      "Epoch [1/1], Step [3993/7635], Loss: 4.3119\n",
      "Epoch [1/1], Step [3994/7635], Loss: 4.2347\n",
      "Epoch [1/1], Step [3995/7635], Loss: 4.2503\n",
      "Epoch [1/1], Step [3996/7635], Loss: 4.3513\n",
      "Epoch [1/1], Step [3997/7635], Loss: 4.3521\n",
      "Epoch [1/1], Step [3998/7635], Loss: 4.2631\n",
      "Epoch [1/1], Step [3999/7635], Loss: 4.2340\n",
      "Epoch [1/1], Step [4000/7635], Loss: 4.2750\n",
      "Epoch [1/1], Step [4001/7635], Loss: 4.2849\n",
      "Epoch [1/1], Step [4002/7635], Loss: 4.2906\n",
      "Epoch [1/1], Step [4003/7635], Loss: 4.3091\n",
      "Epoch [1/1], Step [4004/7635], Loss: 4.2977\n",
      "Epoch [1/1], Step [4005/7635], Loss: 4.2524\n",
      "Epoch [1/1], Step [4006/7635], Loss: 4.1973\n",
      "Epoch [1/1], Step [4007/7635], Loss: 4.2237\n",
      "Epoch [1/1], Step [4008/7635], Loss: 4.4026\n",
      "Epoch [1/1], Step [4009/7635], Loss: 4.2085\n",
      "Epoch [1/1], Step [4010/7635], Loss: 4.1779\n",
      "Epoch [1/1], Step [4011/7635], Loss: 4.1992\n",
      "Epoch [1/1], Step [4012/7635], Loss: 4.1729\n",
      "Epoch [1/1], Step [4013/7635], Loss: 4.2622\n",
      "Epoch [1/1], Step [4014/7635], Loss: 4.2906\n",
      "Epoch [1/1], Step [4015/7635], Loss: 4.1902\n",
      "Epoch [1/1], Step [4016/7635], Loss: 4.2175\n",
      "Epoch [1/1], Step [4017/7635], Loss: 4.2850\n",
      "Epoch [1/1], Step [4018/7635], Loss: 4.2241\n",
      "Epoch [1/1], Step [4019/7635], Loss: 4.2218\n",
      "Epoch [1/1], Step [4020/7635], Loss: 4.1702\n",
      "Epoch [1/1], Step [4021/7635], Loss: 4.2698\n",
      "Epoch [1/1], Step [4022/7635], Loss: 4.2480\n",
      "Epoch [1/1], Step [4023/7635], Loss: 4.3419\n",
      "Epoch [1/1], Step [4024/7635], Loss: 4.2771\n",
      "Epoch [1/1], Step [4025/7635], Loss: 4.1982\n",
      "Epoch [1/1], Step [4026/7635], Loss: 4.2823\n",
      "Epoch [1/1], Step [4027/7635], Loss: 4.2258\n",
      "Epoch [1/1], Step [4028/7635], Loss: 4.2660\n",
      "Epoch [1/1], Step [4029/7635], Loss: 4.2878\n",
      "Epoch [1/1], Step [4030/7635], Loss: 4.2482\n",
      "Epoch [1/1], Step [4031/7635], Loss: 4.1793\n",
      "Epoch [1/1], Step [4032/7635], Loss: 4.1972\n",
      "Epoch [1/1], Step [4033/7635], Loss: 4.2747\n",
      "Epoch [1/1], Step [4034/7635], Loss: 4.1990\n",
      "Epoch [1/1], Step [4035/7635], Loss: 4.2801\n",
      "Epoch [1/1], Step [4036/7635], Loss: 4.3460\n",
      "Epoch [1/1], Step [4037/7635], Loss: 4.1597\n",
      "Epoch [1/1], Step [4038/7635], Loss: 4.3210\n",
      "Epoch [1/1], Step [4039/7635], Loss: 4.2275\n",
      "Epoch [1/1], Step [4040/7635], Loss: 4.1977\n",
      "Epoch [1/1], Step [4041/7635], Loss: 4.1785\n",
      "Epoch [1/1], Step [4042/7635], Loss: 4.2473\n",
      "Epoch [1/1], Step [4043/7635], Loss: 4.2602\n",
      "Epoch [1/1], Step [4044/7635], Loss: 4.2015\n",
      "Epoch [1/1], Step [4045/7635], Loss: 4.2441\n",
      "Epoch [1/1], Step [4046/7635], Loss: 4.2744\n",
      "Epoch [1/1], Step [4047/7635], Loss: 4.2663\n",
      "Epoch [1/1], Step [4048/7635], Loss: 4.1670\n",
      "Epoch [1/1], Step [4049/7635], Loss: 4.2895\n",
      "Epoch [1/1], Step [4050/7635], Loss: 4.2412\n",
      "Epoch [1/1], Step [4051/7635], Loss: 4.2567\n",
      "Epoch [1/1], Step [4052/7635], Loss: 4.2494\n",
      "Epoch [1/1], Step [4053/7635], Loss: 4.2012\n",
      "Epoch [1/1], Step [4054/7635], Loss: 4.2844\n",
      "Epoch [1/1], Step [4055/7635], Loss: 4.2728\n",
      "Epoch [1/1], Step [4056/7635], Loss: 4.2055\n",
      "Epoch [1/1], Step [4057/7635], Loss: 4.2461\n",
      "Epoch [1/1], Step [4058/7635], Loss: 4.2674\n",
      "Epoch [1/1], Step [4059/7635], Loss: 4.2220\n",
      "Epoch [1/1], Step [4060/7635], Loss: 4.2736\n",
      "Epoch [1/1], Step [4061/7635], Loss: 4.2574\n",
      "Epoch [1/1], Step [4062/7635], Loss: 4.1899\n",
      "Epoch [1/1], Step [4063/7635], Loss: 4.2217\n",
      "Epoch [1/1], Step [4064/7635], Loss: 4.3329\n",
      "Epoch [1/1], Step [4065/7635], Loss: 4.2027\n",
      "Epoch [1/1], Step [4066/7635], Loss: 4.2180\n",
      "Epoch [1/1], Step [4067/7635], Loss: 4.2512\n",
      "Epoch [1/1], Step [4068/7635], Loss: 4.1028\n",
      "Epoch [1/1], Step [4069/7635], Loss: 4.1838\n",
      "Epoch [1/1], Step [4070/7635], Loss: 4.1885\n",
      "Epoch [1/1], Step [4071/7635], Loss: 4.2481\n",
      "Epoch [1/1], Step [4072/7635], Loss: 4.1974\n",
      "Epoch [1/1], Step [4073/7635], Loss: 4.2797\n",
      "Epoch [1/1], Step [4074/7635], Loss: 4.2886\n",
      "Epoch [1/1], Step [4075/7635], Loss: 4.2675\n",
      "Epoch [1/1], Step [4076/7635], Loss: 4.2308\n",
      "Epoch [1/1], Step [4077/7635], Loss: 4.3331\n",
      "Epoch [1/1], Step [4078/7635], Loss: 4.2330\n",
      "Epoch [1/1], Step [4079/7635], Loss: 4.3353\n",
      "Epoch [1/1], Step [4080/7635], Loss: 4.3283\n",
      "Epoch [1/1], Step [4081/7635], Loss: 4.2396\n",
      "Epoch [1/1], Step [4082/7635], Loss: 4.2402\n",
      "Epoch [1/1], Step [4083/7635], Loss: 4.2493\n",
      "Epoch [1/1], Step [4084/7635], Loss: 4.2246\n",
      "Epoch [1/1], Step [4085/7635], Loss: 4.2493\n",
      "Epoch [1/1], Step [4086/7635], Loss: 4.2163\n",
      "Epoch [1/1], Step [4087/7635], Loss: 4.2102\n",
      "Epoch [1/1], Step [4088/7635], Loss: 4.3174\n",
      "Epoch [1/1], Step [4089/7635], Loss: 4.1992\n",
      "Epoch [1/1], Step [4090/7635], Loss: 4.1940\n",
      "Epoch [1/1], Step [4091/7635], Loss: 4.2041\n",
      "Epoch [1/1], Step [4092/7635], Loss: 4.2553\n",
      "Epoch [1/1], Step [4093/7635], Loss: 4.2365\n",
      "Epoch [1/1], Step [4094/7635], Loss: 4.3113\n",
      "Epoch [1/1], Step [4095/7635], Loss: 4.2543\n",
      "Epoch [1/1], Step [4096/7635], Loss: 4.2376\n",
      "Epoch [1/1], Step [4097/7635], Loss: 4.2645\n",
      "Epoch [1/1], Step [4098/7635], Loss: 4.2138\n",
      "Epoch [1/1], Step [4099/7635], Loss: 4.1786\n",
      "Epoch [1/1], Step [4100/7635], Loss: 4.2112\n",
      "Epoch [1/1], Step [4101/7635], Loss: 4.2970\n",
      "Epoch [1/1], Step [4102/7635], Loss: 4.2045\n",
      "Epoch [1/1], Step [4103/7635], Loss: 4.1771\n",
      "Epoch [1/1], Step [4104/7635], Loss: 4.2478\n",
      "Epoch [1/1], Step [4105/7635], Loss: 4.2993\n",
      "Epoch [1/1], Step [4106/7635], Loss: 4.1659\n",
      "Epoch [1/1], Step [4107/7635], Loss: 4.1797\n",
      "Epoch [1/1], Step [4108/7635], Loss: 4.2442\n",
      "Epoch [1/1], Step [4109/7635], Loss: 4.2609\n",
      "Epoch [1/1], Step [4110/7635], Loss: 4.3252\n",
      "Epoch [1/1], Step [4111/7635], Loss: 4.2143\n",
      "Epoch [1/1], Step [4112/7635], Loss: 4.2791\n",
      "Epoch [1/1], Step [4113/7635], Loss: 4.2053\n",
      "Epoch [1/1], Step [4114/7635], Loss: 4.1802\n",
      "Epoch [1/1], Step [4115/7635], Loss: 4.2819\n",
      "Epoch [1/1], Step [4116/7635], Loss: 4.2299\n",
      "Epoch [1/1], Step [4117/7635], Loss: 4.2298\n",
      "Epoch [1/1], Step [4118/7635], Loss: 4.2116\n",
      "Epoch [1/1], Step [4119/7635], Loss: 4.1691\n",
      "Epoch [1/1], Step [4120/7635], Loss: 4.2455\n",
      "Epoch [1/1], Step [4121/7635], Loss: 4.1758\n",
      "Epoch [1/1], Step [4122/7635], Loss: 4.2805\n",
      "Epoch [1/1], Step [4123/7635], Loss: 4.2234\n",
      "Epoch [1/1], Step [4124/7635], Loss: 4.2789\n",
      "Epoch [1/1], Step [4125/7635], Loss: 4.1935\n",
      "Epoch [1/1], Step [4126/7635], Loss: 4.2580\n",
      "Epoch [1/1], Step [4127/7635], Loss: 4.2435\n",
      "Epoch [1/1], Step [4128/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [4129/7635], Loss: 4.2044\n",
      "Epoch [1/1], Step [4130/7635], Loss: 4.2702\n",
      "Epoch [1/1], Step [4131/7635], Loss: 4.1475\n",
      "Epoch [1/1], Step [4132/7635], Loss: 4.2224\n",
      "Epoch [1/1], Step [4133/7635], Loss: 4.3060\n",
      "Epoch [1/1], Step [4134/7635], Loss: 4.2084\n",
      "Epoch [1/1], Step [4135/7635], Loss: 4.3110\n",
      "Epoch [1/1], Step [4136/7635], Loss: 4.2653\n",
      "Epoch [1/1], Step [4137/7635], Loss: 4.2111\n",
      "Epoch [1/1], Step [4138/7635], Loss: 4.1932\n",
      "Epoch [1/1], Step [4139/7635], Loss: 4.1952\n",
      "Epoch [1/1], Step [4140/7635], Loss: 4.2488\n",
      "Epoch [1/1], Step [4141/7635], Loss: 4.3990\n",
      "Epoch [1/1], Step [4142/7635], Loss: 4.2569\n",
      "Epoch [1/1], Step [4143/7635], Loss: 4.2329\n",
      "Epoch [1/1], Step [4144/7635], Loss: 4.2651\n",
      "Epoch [1/1], Step [4145/7635], Loss: 4.2357\n",
      "Epoch [1/1], Step [4146/7635], Loss: 4.2719\n",
      "Epoch [1/1], Step [4147/7635], Loss: 4.2837\n",
      "Epoch [1/1], Step [4148/7635], Loss: 4.2089\n",
      "Epoch [1/1], Step [4149/7635], Loss: 4.2225\n",
      "Epoch [1/1], Step [4150/7635], Loss: 4.2002\n",
      "Epoch [1/1], Step [4151/7635], Loss: 4.2694\n",
      "Epoch [1/1], Step [4152/7635], Loss: 4.2912\n",
      "Epoch [1/1], Step [4153/7635], Loss: 4.2049\n",
      "Epoch [1/1], Step [4154/7635], Loss: 4.2596\n",
      "Epoch [1/1], Step [4155/7635], Loss: 4.1952\n",
      "Epoch [1/1], Step [4156/7635], Loss: 4.1382\n",
      "Epoch [1/1], Step [4157/7635], Loss: 4.2442\n",
      "Epoch [1/1], Step [4158/7635], Loss: 4.2725\n",
      "Epoch [1/1], Step [4159/7635], Loss: 4.1958\n",
      "Epoch [1/1], Step [4160/7635], Loss: 4.2024\n",
      "Epoch [1/1], Step [4161/7635], Loss: 4.2065\n",
      "Epoch [1/1], Step [4162/7635], Loss: 4.1931\n",
      "Epoch [1/1], Step [4163/7635], Loss: 4.2682\n",
      "Epoch [1/1], Step [4164/7635], Loss: 4.2888\n",
      "Epoch [1/1], Step [4165/7635], Loss: 4.2777\n",
      "Epoch [1/1], Step [4166/7635], Loss: 4.2165\n",
      "Epoch [1/1], Step [4167/7635], Loss: 4.2384\n",
      "Epoch [1/1], Step [4168/7635], Loss: 4.2349\n",
      "Epoch [1/1], Step [4169/7635], Loss: 4.1543\n",
      "Epoch [1/1], Step [4170/7635], Loss: 4.3169\n",
      "Epoch [1/1], Step [4171/7635], Loss: 4.2817\n",
      "Epoch [1/1], Step [4172/7635], Loss: 4.2012\n",
      "Epoch [1/1], Step [4173/7635], Loss: 4.2160\n",
      "Epoch [1/1], Step [4174/7635], Loss: 4.3066\n",
      "Epoch [1/1], Step [4175/7635], Loss: 4.2785\n",
      "Epoch [1/1], Step [4176/7635], Loss: 4.2170\n",
      "Epoch [1/1], Step [4177/7635], Loss: 4.2219\n",
      "Epoch [1/1], Step [4178/7635], Loss: 4.2016\n",
      "Epoch [1/1], Step [4179/7635], Loss: 4.2385\n",
      "Epoch [1/1], Step [4180/7635], Loss: 4.2602\n",
      "Epoch [1/1], Step [4181/7635], Loss: 4.2442\n",
      "Epoch [1/1], Step [4182/7635], Loss: 4.1672\n",
      "Epoch [1/1], Step [4183/7635], Loss: 4.2813\n",
      "Epoch [1/1], Step [4184/7635], Loss: 4.3060\n",
      "Epoch [1/1], Step [4185/7635], Loss: 4.2382\n",
      "Epoch [1/1], Step [4186/7635], Loss: 4.2891\n",
      "Epoch [1/1], Step [4187/7635], Loss: 4.1809\n",
      "Epoch [1/1], Step [4188/7635], Loss: 4.2260\n",
      "Epoch [1/1], Step [4189/7635], Loss: 4.2820\n",
      "Epoch [1/1], Step [4190/7635], Loss: 4.2356\n",
      "Epoch [1/1], Step [4191/7635], Loss: 4.2340\n",
      "Epoch [1/1], Step [4192/7635], Loss: 4.2702\n",
      "Epoch [1/1], Step [4193/7635], Loss: 4.2136\n",
      "Epoch [1/1], Step [4194/7635], Loss: 4.2263\n",
      "Epoch [1/1], Step [4195/7635], Loss: 4.3064\n",
      "Epoch [1/1], Step [4196/7635], Loss: 4.3090\n",
      "Epoch [1/1], Step [4197/7635], Loss: 4.1876\n",
      "Epoch [1/1], Step [4198/7635], Loss: 4.3265\n",
      "Epoch [1/1], Step [4199/7635], Loss: 4.2632\n",
      "Epoch [1/1], Step [4200/7635], Loss: 4.2412\n",
      "Epoch [1/1], Step [4201/7635], Loss: 4.2312\n",
      "Epoch [1/1], Step [4202/7635], Loss: 4.2627\n",
      "Epoch [1/1], Step [4203/7635], Loss: 4.2188\n",
      "Epoch [1/1], Step [4204/7635], Loss: 4.2888\n",
      "Epoch [1/1], Step [4205/7635], Loss: 4.2000\n",
      "Epoch [1/1], Step [4206/7635], Loss: 4.2716\n",
      "Epoch [1/1], Step [4207/7635], Loss: 4.1432\n",
      "Epoch [1/1], Step [4208/7635], Loss: 4.2681\n",
      "Epoch [1/1], Step [4209/7635], Loss: 4.2606\n",
      "Epoch [1/1], Step [4210/7635], Loss: 4.2481\n",
      "Epoch [1/1], Step [4211/7635], Loss: 4.1860\n",
      "Epoch [1/1], Step [4212/7635], Loss: 4.2809\n",
      "Epoch [1/1], Step [4213/7635], Loss: 4.2016\n",
      "Epoch [1/1], Step [4214/7635], Loss: 4.3227\n",
      "Epoch [1/1], Step [4215/7635], Loss: 4.1360\n",
      "Epoch [1/1], Step [4216/7635], Loss: 4.1797\n",
      "Epoch [1/1], Step [4217/7635], Loss: 4.1605\n",
      "Epoch [1/1], Step [4218/7635], Loss: 4.2696\n",
      "Epoch [1/1], Step [4219/7635], Loss: 4.2662\n",
      "Epoch [1/1], Step [4220/7635], Loss: 4.2712\n",
      "Epoch [1/1], Step [4221/7635], Loss: 4.2106\n",
      "Epoch [1/1], Step [4222/7635], Loss: 4.1648\n",
      "Epoch [1/1], Step [4223/7635], Loss: 4.2336\n",
      "Epoch [1/1], Step [4224/7635], Loss: 4.2376\n",
      "Epoch [1/1], Step [4225/7635], Loss: 4.1976\n",
      "Epoch [1/1], Step [4226/7635], Loss: 4.3126\n",
      "Epoch [1/1], Step [4227/7635], Loss: 4.2814\n",
      "Epoch [1/1], Step [4228/7635], Loss: 4.2274\n",
      "Epoch [1/1], Step [4229/7635], Loss: 4.2033\n",
      "Epoch [1/1], Step [4230/7635], Loss: 4.2744\n",
      "Epoch [1/1], Step [4231/7635], Loss: 4.2646\n",
      "Epoch [1/1], Step [4232/7635], Loss: 4.2590\n",
      "Epoch [1/1], Step [4233/7635], Loss: 4.2930\n",
      "Epoch [1/1], Step [4234/7635], Loss: 4.1575\n",
      "Epoch [1/1], Step [4235/7635], Loss: 4.1742\n",
      "Epoch [1/1], Step [4236/7635], Loss: 4.2649\n",
      "Epoch [1/1], Step [4237/7635], Loss: 4.2606\n",
      "Epoch [1/1], Step [4238/7635], Loss: 4.2491\n",
      "Epoch [1/1], Step [4239/7635], Loss: 4.2204\n",
      "Epoch [1/1], Step [4240/7635], Loss: 4.1932\n",
      "Epoch [1/1], Step [4241/7635], Loss: 4.2477\n",
      "Epoch [1/1], Step [4242/7635], Loss: 4.1781\n",
      "Epoch [1/1], Step [4243/7635], Loss: 4.2034\n",
      "Epoch [1/1], Step [4244/7635], Loss: 4.2124\n",
      "Epoch [1/1], Step [4245/7635], Loss: 4.2498\n",
      "Epoch [1/1], Step [4246/7635], Loss: 4.2354\n",
      "Epoch [1/1], Step [4247/7635], Loss: 4.2166\n",
      "Epoch [1/1], Step [4248/7635], Loss: 4.2629\n",
      "Epoch [1/1], Step [4249/7635], Loss: 4.1930\n",
      "Epoch [1/1], Step [4250/7635], Loss: 4.1810\n",
      "Epoch [1/1], Step [4251/7635], Loss: 4.2107\n",
      "Epoch [1/1], Step [4252/7635], Loss: 4.2428\n",
      "Epoch [1/1], Step [4253/7635], Loss: 4.2181\n",
      "Epoch [1/1], Step [4254/7635], Loss: 4.2335\n",
      "Epoch [1/1], Step [4255/7635], Loss: 4.2608\n",
      "Epoch [1/1], Step [4256/7635], Loss: 4.2356\n",
      "Epoch [1/1], Step [4257/7635], Loss: 4.1862\n",
      "Epoch [1/1], Step [4258/7635], Loss: 4.2320\n",
      "Epoch [1/1], Step [4259/7635], Loss: 4.2575\n",
      "Epoch [1/1], Step [4260/7635], Loss: 4.1668\n",
      "Epoch [1/1], Step [4261/7635], Loss: 4.2056\n",
      "Epoch [1/1], Step [4262/7635], Loss: 4.2099\n",
      "Epoch [1/1], Step [4263/7635], Loss: 4.2117\n",
      "Epoch [1/1], Step [4264/7635], Loss: 4.2982\n",
      "Epoch [1/1], Step [4265/7635], Loss: 4.1595\n",
      "Epoch [1/1], Step [4266/7635], Loss: 4.1815\n",
      "Epoch [1/1], Step [4267/7635], Loss: 4.1718\n",
      "Epoch [1/1], Step [4268/7635], Loss: 4.1939\n",
      "Epoch [1/1], Step [4269/7635], Loss: 4.2226\n",
      "Epoch [1/1], Step [4270/7635], Loss: 4.1794\n",
      "Epoch [1/1], Step [4271/7635], Loss: 4.2157\n",
      "Epoch [1/1], Step [4272/7635], Loss: 4.2194\n",
      "Epoch [1/1], Step [4273/7635], Loss: 4.2390\n",
      "Epoch [1/1], Step [4274/7635], Loss: 4.2774\n",
      "Epoch [1/1], Step [4275/7635], Loss: 4.1666\n",
      "Epoch [1/1], Step [4276/7635], Loss: 4.2059\n",
      "Epoch [1/1], Step [4277/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [4278/7635], Loss: 4.2754\n",
      "Epoch [1/1], Step [4279/7635], Loss: 4.3106\n",
      "Epoch [1/1], Step [4280/7635], Loss: 4.2440\n",
      "Epoch [1/1], Step [4281/7635], Loss: 4.2391\n",
      "Epoch [1/1], Step [4282/7635], Loss: 4.2229\n",
      "Epoch [1/1], Step [4283/7635], Loss: 4.2771\n",
      "Epoch [1/1], Step [4284/7635], Loss: 4.2345\n",
      "Epoch [1/1], Step [4285/7635], Loss: 4.2081\n",
      "Epoch [1/1], Step [4286/7635], Loss: 4.2339\n",
      "Epoch [1/1], Step [4287/7635], Loss: 4.1976\n",
      "Epoch [1/1], Step [4288/7635], Loss: 4.2240\n",
      "Epoch [1/1], Step [4289/7635], Loss: 4.1902\n",
      "Epoch [1/1], Step [4290/7635], Loss: 4.1467\n",
      "Epoch [1/1], Step [4291/7635], Loss: 4.1935\n",
      "Epoch [1/1], Step [4292/7635], Loss: 4.2074\n",
      "Epoch [1/1], Step [4293/7635], Loss: 4.2933\n",
      "Epoch [1/1], Step [4294/7635], Loss: 4.2442\n",
      "Epoch [1/1], Step [4295/7635], Loss: 4.1663\n",
      "Epoch [1/1], Step [4296/7635], Loss: 4.2798\n",
      "Epoch [1/1], Step [4297/7635], Loss: 4.2250\n",
      "Epoch [1/1], Step [4298/7635], Loss: 4.2675\n",
      "Epoch [1/1], Step [4299/7635], Loss: 4.1956\n",
      "Epoch [1/1], Step [4300/7635], Loss: 4.2610\n",
      "Epoch [1/1], Step [4301/7635], Loss: 4.1960\n",
      "Epoch [1/1], Step [4302/7635], Loss: 4.2280\n",
      "Epoch [1/1], Step [4303/7635], Loss: 4.1331\n",
      "Epoch [1/1], Step [4304/7635], Loss: 4.1751\n",
      "Epoch [1/1], Step [4305/7635], Loss: 4.3030\n",
      "Epoch [1/1], Step [4306/7635], Loss: 4.2524\n",
      "Epoch [1/1], Step [4307/7635], Loss: 4.2014\n",
      "Epoch [1/1], Step [4308/7635], Loss: 4.2117\n",
      "Epoch [1/1], Step [4309/7635], Loss: 4.2048\n",
      "Epoch [1/1], Step [4310/7635], Loss: 4.1842\n",
      "Epoch [1/1], Step [4311/7635], Loss: 4.2003\n",
      "Epoch [1/1], Step [4312/7635], Loss: 4.0888\n",
      "Epoch [1/1], Step [4313/7635], Loss: 4.2214\n",
      "Epoch [1/1], Step [4314/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [4315/7635], Loss: 4.2495\n",
      "Epoch [1/1], Step [4316/7635], Loss: 4.1455\n",
      "Epoch [1/1], Step [4317/7635], Loss: 4.2661\n",
      "Epoch [1/1], Step [4318/7635], Loss: 4.2297\n",
      "Epoch [1/1], Step [4319/7635], Loss: 4.2112\n",
      "Epoch [1/1], Step [4320/7635], Loss: 4.1999\n",
      "Epoch [1/1], Step [4321/7635], Loss: 4.1895\n",
      "Epoch [1/1], Step [4322/7635], Loss: 4.1812\n",
      "Epoch [1/1], Step [4323/7635], Loss: 4.1571\n",
      "Epoch [1/1], Step [4324/7635], Loss: 4.1710\n",
      "Epoch [1/1], Step [4325/7635], Loss: 4.2921\n",
      "Epoch [1/1], Step [4326/7635], Loss: 4.2697\n",
      "Epoch [1/1], Step [4327/7635], Loss: 4.1600\n",
      "Epoch [1/1], Step [4328/7635], Loss: 4.1829\n",
      "Epoch [1/1], Step [4329/7635], Loss: 4.1795\n",
      "Epoch [1/1], Step [4330/7635], Loss: 4.1656\n",
      "Epoch [1/1], Step [4331/7635], Loss: 4.2307\n",
      "Epoch [1/1], Step [4332/7635], Loss: 4.2119\n",
      "Epoch [1/1], Step [4333/7635], Loss: 4.1958\n",
      "Epoch [1/1], Step [4334/7635], Loss: 4.2557\n",
      "Epoch [1/1], Step [4335/7635], Loss: 4.1812\n",
      "Epoch [1/1], Step [4336/7635], Loss: 4.2149\n",
      "Epoch [1/1], Step [4337/7635], Loss: 4.2993\n",
      "Epoch [1/1], Step [4338/7635], Loss: 4.2104\n",
      "Epoch [1/1], Step [4339/7635], Loss: 4.1736\n",
      "Epoch [1/1], Step [4340/7635], Loss: 4.2203\n",
      "Epoch [1/1], Step [4341/7635], Loss: 4.1176\n",
      "Epoch [1/1], Step [4342/7635], Loss: 4.1553\n",
      "Epoch [1/1], Step [4343/7635], Loss: 4.1855\n",
      "Epoch [1/1], Step [4344/7635], Loss: 4.2090\n",
      "Epoch [1/1], Step [4345/7635], Loss: 4.2222\n",
      "Epoch [1/1], Step [4346/7635], Loss: 4.2935\n",
      "Epoch [1/1], Step [4347/7635], Loss: 4.2425\n",
      "Epoch [1/1], Step [4348/7635], Loss: 4.1624\n",
      "Epoch [1/1], Step [4349/7635], Loss: 4.2515\n",
      "Epoch [1/1], Step [4350/7635], Loss: 4.2560\n",
      "Epoch [1/1], Step [4351/7635], Loss: 4.1720\n",
      "Epoch [1/1], Step [4352/7635], Loss: 4.1727\n",
      "Epoch [1/1], Step [4353/7635], Loss: 4.1946\n",
      "Epoch [1/1], Step [4354/7635], Loss: 4.2656\n",
      "Epoch [1/1], Step [4355/7635], Loss: 4.2128\n",
      "Epoch [1/1], Step [4356/7635], Loss: 4.2238\n",
      "Epoch [1/1], Step [4357/7635], Loss: 4.2054\n",
      "Epoch [1/1], Step [4358/7635], Loss: 4.1660\n",
      "Epoch [1/1], Step [4359/7635], Loss: 4.2853\n",
      "Epoch [1/1], Step [4360/7635], Loss: 4.1847\n",
      "Epoch [1/1], Step [4361/7635], Loss: 4.1886\n",
      "Epoch [1/1], Step [4362/7635], Loss: 4.2078\n",
      "Epoch [1/1], Step [4363/7635], Loss: 4.1847\n",
      "Epoch [1/1], Step [4364/7635], Loss: 4.2070\n",
      "Epoch [1/1], Step [4365/7635], Loss: 4.1822\n",
      "Epoch [1/1], Step [4366/7635], Loss: 4.2388\n",
      "Epoch [1/1], Step [4367/7635], Loss: 4.1706\n",
      "Epoch [1/1], Step [4368/7635], Loss: 4.2884\n",
      "Epoch [1/1], Step [4369/7635], Loss: 4.2270\n",
      "Epoch [1/1], Step [4370/7635], Loss: 4.2208\n",
      "Epoch [1/1], Step [4371/7635], Loss: 4.2793\n",
      "Epoch [1/1], Step [4372/7635], Loss: 4.2279\n",
      "Epoch [1/1], Step [4373/7635], Loss: 4.2498\n",
      "Epoch [1/1], Step [4374/7635], Loss: 4.2668\n",
      "Epoch [1/1], Step [4375/7635], Loss: 4.2502\n",
      "Epoch [1/1], Step [4376/7635], Loss: 4.1797\n",
      "Epoch [1/1], Step [4377/7635], Loss: 4.2166\n",
      "Epoch [1/1], Step [4378/7635], Loss: 4.2624\n",
      "Epoch [1/1], Step [4379/7635], Loss: 4.1587\n",
      "Epoch [1/1], Step [4380/7635], Loss: 4.2102\n",
      "Epoch [1/1], Step [4381/7635], Loss: 4.2002\n",
      "Epoch [1/1], Step [4382/7635], Loss: 4.1685\n",
      "Epoch [1/1], Step [4383/7635], Loss: 4.2456\n",
      "Epoch [1/1], Step [4384/7635], Loss: 4.2466\n",
      "Epoch [1/1], Step [4385/7635], Loss: 4.2030\n",
      "Epoch [1/1], Step [4386/7635], Loss: 4.1703\n",
      "Epoch [1/1], Step [4387/7635], Loss: 4.1643\n",
      "Epoch [1/1], Step [4388/7635], Loss: 4.2210\n",
      "Epoch [1/1], Step [4389/7635], Loss: 4.1965\n",
      "Epoch [1/1], Step [4390/7635], Loss: 4.2601\n",
      "Epoch [1/1], Step [4391/7635], Loss: 4.2090\n",
      "Epoch [1/1], Step [4392/7635], Loss: 4.2919\n",
      "Epoch [1/1], Step [4393/7635], Loss: 4.2224\n",
      "Epoch [1/1], Step [4394/7635], Loss: 4.1735\n",
      "Epoch [1/1], Step [4395/7635], Loss: 4.1794\n",
      "Epoch [1/1], Step [4396/7635], Loss: 4.1861\n",
      "Epoch [1/1], Step [4397/7635], Loss: 4.3234\n",
      "Epoch [1/1], Step [4398/7635], Loss: 4.2413\n",
      "Epoch [1/1], Step [4399/7635], Loss: 4.2024\n",
      "Epoch [1/1], Step [4400/7635], Loss: 4.2030\n",
      "Epoch [1/1], Step [4401/7635], Loss: 4.1848\n",
      "Epoch [1/1], Step [4402/7635], Loss: 4.1850\n",
      "Epoch [1/1], Step [4403/7635], Loss: 4.1657\n",
      "Epoch [1/1], Step [4404/7635], Loss: 4.1743\n",
      "Epoch [1/1], Step [4405/7635], Loss: 4.1280\n",
      "Epoch [1/1], Step [4406/7635], Loss: 4.2002\n",
      "Epoch [1/1], Step [4407/7635], Loss: 4.1809\n",
      "Epoch [1/1], Step [4408/7635], Loss: 4.2034\n",
      "Epoch [1/1], Step [4409/7635], Loss: 4.2671\n",
      "Epoch [1/1], Step [4410/7635], Loss: 4.2162\n",
      "Epoch [1/1], Step [4411/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [4412/7635], Loss: 4.2183\n",
      "Epoch [1/1], Step [4413/7635], Loss: 4.1296\n",
      "Epoch [1/1], Step [4414/7635], Loss: 4.2915\n",
      "Epoch [1/1], Step [4415/7635], Loss: 4.2730\n",
      "Epoch [1/1], Step [4416/7635], Loss: 4.2239\n",
      "Epoch [1/1], Step [4417/7635], Loss: 4.2181\n",
      "Epoch [1/1], Step [4418/7635], Loss: 4.2069\n",
      "Epoch [1/1], Step [4419/7635], Loss: 4.1771\n",
      "Epoch [1/1], Step [4420/7635], Loss: 4.1798\n",
      "Epoch [1/1], Step [4421/7635], Loss: 4.2109\n",
      "Epoch [1/1], Step [4422/7635], Loss: 4.1929\n",
      "Epoch [1/1], Step [4423/7635], Loss: 4.1033\n",
      "Epoch [1/1], Step [4424/7635], Loss: 4.2030\n",
      "Epoch [1/1], Step [4425/7635], Loss: 4.1873\n",
      "Epoch [1/1], Step [4426/7635], Loss: 4.2131\n",
      "Epoch [1/1], Step [4427/7635], Loss: 4.2262\n",
      "Epoch [1/1], Step [4428/7635], Loss: 4.2780\n",
      "Epoch [1/1], Step [4429/7635], Loss: 4.2459\n",
      "Epoch [1/1], Step [4430/7635], Loss: 4.2288\n",
      "Epoch [1/1], Step [4431/7635], Loss: 4.1779\n",
      "Epoch [1/1], Step [4432/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [4433/7635], Loss: 4.2025\n",
      "Epoch [1/1], Step [4434/7635], Loss: 4.1744\n",
      "Epoch [1/1], Step [4435/7635], Loss: 4.2341\n",
      "Epoch [1/1], Step [4436/7635], Loss: 4.2196\n",
      "Epoch [1/1], Step [4437/7635], Loss: 4.2305\n",
      "Epoch [1/1], Step [4438/7635], Loss: 4.1379\n",
      "Epoch [1/1], Step [4439/7635], Loss: 4.2283\n",
      "Epoch [1/1], Step [4440/7635], Loss: 4.2322\n",
      "Epoch [1/1], Step [4441/7635], Loss: 4.2012\n",
      "Epoch [1/1], Step [4442/7635], Loss: 4.3145\n",
      "Epoch [1/1], Step [4443/7635], Loss: 4.1255\n",
      "Epoch [1/1], Step [4444/7635], Loss: 4.1865\n",
      "Epoch [1/1], Step [4445/7635], Loss: 4.2093\n",
      "Epoch [1/1], Step [4446/7635], Loss: 4.2700\n",
      "Epoch [1/1], Step [4447/7635], Loss: 4.1637\n",
      "Epoch [1/1], Step [4448/7635], Loss: 4.2753\n",
      "Epoch [1/1], Step [4449/7635], Loss: 4.2015\n",
      "Epoch [1/1], Step [4450/7635], Loss: 4.2708\n",
      "Epoch [1/1], Step [4451/7635], Loss: 4.1332\n",
      "Epoch [1/1], Step [4452/7635], Loss: 4.3034\n",
      "Epoch [1/1], Step [4453/7635], Loss: 4.2058\n",
      "Epoch [1/1], Step [4454/7635], Loss: 4.1508\n",
      "Epoch [1/1], Step [4455/7635], Loss: 4.1894\n",
      "Epoch [1/1], Step [4456/7635], Loss: 4.1810\n",
      "Epoch [1/1], Step [4457/7635], Loss: 4.1791\n",
      "Epoch [1/1], Step [4458/7635], Loss: 4.1671\n",
      "Epoch [1/1], Step [4459/7635], Loss: 4.1942\n",
      "Epoch [1/1], Step [4460/7635], Loss: 4.1630\n",
      "Epoch [1/1], Step [4461/7635], Loss: 4.2867\n",
      "Epoch [1/1], Step [4462/7635], Loss: 4.1499\n",
      "Epoch [1/1], Step [4463/7635], Loss: 4.1833\n",
      "Epoch [1/1], Step [4464/7635], Loss: 4.2674\n",
      "Epoch [1/1], Step [4465/7635], Loss: 4.1752\n",
      "Epoch [1/1], Step [4466/7635], Loss: 4.1840\n",
      "Epoch [1/1], Step [4467/7635], Loss: 4.2038\n",
      "Epoch [1/1], Step [4468/7635], Loss: 4.1976\n",
      "Epoch [1/1], Step [4469/7635], Loss: 4.1366\n",
      "Epoch [1/1], Step [4470/7635], Loss: 4.1569\n",
      "Epoch [1/1], Step [4471/7635], Loss: 4.2334\n",
      "Epoch [1/1], Step [4472/7635], Loss: 4.2656\n",
      "Epoch [1/1], Step [4473/7635], Loss: 4.2307\n",
      "Epoch [1/1], Step [4474/7635], Loss: 4.2515\n",
      "Epoch [1/1], Step [4475/7635], Loss: 4.2284\n",
      "Epoch [1/1], Step [4476/7635], Loss: 4.1483\n",
      "Epoch [1/1], Step [4477/7635], Loss: 4.2233\n",
      "Epoch [1/1], Step [4478/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [4479/7635], Loss: 4.2438\n",
      "Epoch [1/1], Step [4480/7635], Loss: 4.1905\n",
      "Epoch [1/1], Step [4481/7635], Loss: 4.2294\n",
      "Epoch [1/1], Step [4482/7635], Loss: 4.1575\n",
      "Epoch [1/1], Step [4483/7635], Loss: 4.2146\n",
      "Epoch [1/1], Step [4484/7635], Loss: 4.0919\n",
      "Epoch [1/1], Step [4485/7635], Loss: 4.1698\n",
      "Epoch [1/1], Step [4486/7635], Loss: 4.2717\n",
      "Epoch [1/1], Step [4487/7635], Loss: 4.2792\n",
      "Epoch [1/1], Step [4488/7635], Loss: 4.1728\n",
      "Epoch [1/1], Step [4489/7635], Loss: 4.2219\n",
      "Epoch [1/1], Step [4490/7635], Loss: 4.2065\n",
      "Epoch [1/1], Step [4491/7635], Loss: 4.2075\n",
      "Epoch [1/1], Step [4492/7635], Loss: 4.2680\n",
      "Epoch [1/1], Step [4493/7635], Loss: 4.2200\n",
      "Epoch [1/1], Step [4494/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [4495/7635], Loss: 4.2249\n",
      "Epoch [1/1], Step [4496/7635], Loss: 4.1889\n",
      "Epoch [1/1], Step [4497/7635], Loss: 4.2297\n",
      "Epoch [1/1], Step [4498/7635], Loss: 4.1887\n",
      "Epoch [1/1], Step [4499/7635], Loss: 4.2368\n",
      "Epoch [1/1], Step [4500/7635], Loss: 4.1762\n",
      "Epoch [1/1], Step [4501/7635], Loss: 4.1957\n",
      "Epoch [1/1], Step [4502/7635], Loss: 4.2447\n",
      "Epoch [1/1], Step [4503/7635], Loss: 4.2265\n",
      "Epoch [1/1], Step [4504/7635], Loss: 4.2089\n",
      "Epoch [1/1], Step [4505/7635], Loss: 4.1644\n",
      "Epoch [1/1], Step [4506/7635], Loss: 4.2416\n",
      "Epoch [1/1], Step [4507/7635], Loss: 4.2435\n",
      "Epoch [1/1], Step [4508/7635], Loss: 4.2329\n",
      "Epoch [1/1], Step [4509/7635], Loss: 4.2345\n",
      "Epoch [1/1], Step [4510/7635], Loss: 4.1608\n",
      "Epoch [1/1], Step [4511/7635], Loss: 4.2007\n",
      "Epoch [1/1], Step [4512/7635], Loss: 4.2424\n",
      "Epoch [1/1], Step [4513/7635], Loss: 4.2482\n",
      "Epoch [1/1], Step [4514/7635], Loss: 4.1816\n",
      "Epoch [1/1], Step [4515/7635], Loss: 4.2830\n",
      "Epoch [1/1], Step [4516/7635], Loss: 4.2578\n",
      "Epoch [1/1], Step [4517/7635], Loss: 4.3211\n",
      "Epoch [1/1], Step [4518/7635], Loss: 4.2031\n",
      "Epoch [1/1], Step [4519/7635], Loss: 4.2309\n",
      "Epoch [1/1], Step [4520/7635], Loss: 4.2514\n",
      "Epoch [1/1], Step [4521/7635], Loss: 4.1783\n",
      "Epoch [1/1], Step [4522/7635], Loss: 4.2367\n",
      "Epoch [1/1], Step [4523/7635], Loss: 4.2039\n",
      "Epoch [1/1], Step [4524/7635], Loss: 4.2702\n",
      "Epoch [1/1], Step [4525/7635], Loss: 4.2168\n",
      "Epoch [1/1], Step [4526/7635], Loss: 4.1770\n",
      "Epoch [1/1], Step [4527/7635], Loss: 4.2267\n",
      "Epoch [1/1], Step [4528/7635], Loss: 4.2114\n",
      "Epoch [1/1], Step [4529/7635], Loss: 4.2179\n",
      "Epoch [1/1], Step [4530/7635], Loss: 4.1779\n",
      "Epoch [1/1], Step [4531/7635], Loss: 4.2535\n",
      "Epoch [1/1], Step [4532/7635], Loss: 4.1170\n",
      "Epoch [1/1], Step [4533/7635], Loss: 4.1516\n",
      "Epoch [1/1], Step [4534/7635], Loss: 4.1260\n",
      "Epoch [1/1], Step [4535/7635], Loss: 4.2027\n",
      "Epoch [1/1], Step [4536/7635], Loss: 4.1187\n",
      "Epoch [1/1], Step [4537/7635], Loss: 4.2673\n",
      "Epoch [1/1], Step [4538/7635], Loss: 4.2321\n",
      "Epoch [1/1], Step [4539/7635], Loss: 4.2818\n",
      "Epoch [1/1], Step [4540/7635], Loss: 4.1677\n",
      "Epoch [1/1], Step [4541/7635], Loss: 4.1625\n",
      "Epoch [1/1], Step [4542/7635], Loss: 4.2234\n",
      "Epoch [1/1], Step [4543/7635], Loss: 4.1154\n",
      "Epoch [1/1], Step [4544/7635], Loss: 4.2318\n",
      "Epoch [1/1], Step [4545/7635], Loss: 4.2230\n",
      "Epoch [1/1], Step [4546/7635], Loss: 4.2609\n",
      "Epoch [1/1], Step [4547/7635], Loss: 4.2678\n",
      "Epoch [1/1], Step [4548/7635], Loss: 4.1441\n",
      "Epoch [1/1], Step [4549/7635], Loss: 4.1996\n",
      "Epoch [1/1], Step [4550/7635], Loss: 4.1866\n",
      "Epoch [1/1], Step [4551/7635], Loss: 4.2239\n",
      "Epoch [1/1], Step [4552/7635], Loss: 4.2418\n",
      "Epoch [1/1], Step [4553/7635], Loss: 4.2168\n",
      "Epoch [1/1], Step [4554/7635], Loss: 4.2268\n",
      "Epoch [1/1], Step [4555/7635], Loss: 4.1916\n",
      "Epoch [1/1], Step [4556/7635], Loss: 4.1894\n",
      "Epoch [1/1], Step [4557/7635], Loss: 4.2421\n",
      "Epoch [1/1], Step [4558/7635], Loss: 4.2680\n",
      "Epoch [1/1], Step [4559/7635], Loss: 4.2346\n",
      "Epoch [1/1], Step [4560/7635], Loss: 4.1827\n",
      "Epoch [1/1], Step [4561/7635], Loss: 4.2847\n",
      "Epoch [1/1], Step [4562/7635], Loss: 4.1807\n",
      "Epoch [1/1], Step [4563/7635], Loss: 4.1982\n",
      "Epoch [1/1], Step [4564/7635], Loss: 4.2128\n",
      "Epoch [1/1], Step [4565/7635], Loss: 4.1855\n",
      "Epoch [1/1], Step [4566/7635], Loss: 4.2367\n",
      "Epoch [1/1], Step [4567/7635], Loss: 4.1976\n",
      "Epoch [1/1], Step [4568/7635], Loss: 4.2094\n",
      "Epoch [1/1], Step [4569/7635], Loss: 4.1677\n",
      "Epoch [1/1], Step [4570/7635], Loss: 4.1938\n",
      "Epoch [1/1], Step [4571/7635], Loss: 4.1752\n",
      "Epoch [1/1], Step [4572/7635], Loss: 4.2817\n",
      "Epoch [1/1], Step [4573/7635], Loss: 4.2513\n",
      "Epoch [1/1], Step [4574/7635], Loss: 4.2368\n",
      "Epoch [1/1], Step [4575/7635], Loss: 4.2952\n",
      "Epoch [1/1], Step [4576/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [4577/7635], Loss: 4.1738\n",
      "Epoch [1/1], Step [4578/7635], Loss: 4.2126\n",
      "Epoch [1/1], Step [4579/7635], Loss: 4.1881\n",
      "Epoch [1/1], Step [4580/7635], Loss: 4.1457\n",
      "Epoch [1/1], Step [4581/7635], Loss: 4.2660\n",
      "Epoch [1/1], Step [4582/7635], Loss: 4.2493\n",
      "Epoch [1/1], Step [4583/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [4584/7635], Loss: 4.2035\n",
      "Epoch [1/1], Step [4585/7635], Loss: 4.2715\n",
      "Epoch [1/1], Step [4586/7635], Loss: 4.2233\n",
      "Epoch [1/1], Step [4587/7635], Loss: 4.2733\n",
      "Epoch [1/1], Step [4588/7635], Loss: 4.2327\n",
      "Epoch [1/1], Step [4589/7635], Loss: 4.1896\n",
      "Epoch [1/1], Step [4590/7635], Loss: 4.2557\n",
      "Epoch [1/1], Step [4591/7635], Loss: 4.2241\n",
      "Epoch [1/1], Step [4592/7635], Loss: 4.2322\n",
      "Epoch [1/1], Step [4593/7635], Loss: 4.2143\n",
      "Epoch [1/1], Step [4594/7635], Loss: 4.1014\n",
      "Epoch [1/1], Step [4595/7635], Loss: 4.1674\n",
      "Epoch [1/1], Step [4596/7635], Loss: 4.2443\n",
      "Epoch [1/1], Step [4597/7635], Loss: 4.2621\n",
      "Epoch [1/1], Step [4598/7635], Loss: 4.2012\n",
      "Epoch [1/1], Step [4599/7635], Loss: 4.1933\n",
      "Epoch [1/1], Step [4600/7635], Loss: 4.2031\n",
      "Epoch [1/1], Step [4601/7635], Loss: 4.1761\n",
      "Epoch [1/1], Step [4602/7635], Loss: 4.1879\n",
      "Epoch [1/1], Step [4603/7635], Loss: 4.1994\n",
      "Epoch [1/1], Step [4604/7635], Loss: 4.1604\n",
      "Epoch [1/1], Step [4605/7635], Loss: 4.1968\n",
      "Epoch [1/1], Step [4606/7635], Loss: 4.2267\n",
      "Epoch [1/1], Step [4607/7635], Loss: 4.2295\n",
      "Epoch [1/1], Step [4608/7635], Loss: 4.2190\n",
      "Epoch [1/1], Step [4609/7635], Loss: 4.2155\n",
      "Epoch [1/1], Step [4610/7635], Loss: 4.2043\n",
      "Epoch [1/1], Step [4611/7635], Loss: 4.1950\n",
      "Epoch [1/1], Step [4612/7635], Loss: 4.1608\n",
      "Epoch [1/1], Step [4613/7635], Loss: 4.2806\n",
      "Epoch [1/1], Step [4614/7635], Loss: 4.2233\n",
      "Epoch [1/1], Step [4615/7635], Loss: 4.1863\n",
      "Epoch [1/1], Step [4616/7635], Loss: 4.2820\n",
      "Epoch [1/1], Step [4617/7635], Loss: 4.1632\n",
      "Epoch [1/1], Step [4618/7635], Loss: 4.2028\n",
      "Epoch [1/1], Step [4619/7635], Loss: 4.1876\n",
      "Epoch [1/1], Step [4620/7635], Loss: 4.2354\n",
      "Epoch [1/1], Step [4621/7635], Loss: 4.2261\n",
      "Epoch [1/1], Step [4622/7635], Loss: 4.2010\n",
      "Epoch [1/1], Step [4623/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [4624/7635], Loss: 4.1506\n",
      "Epoch [1/1], Step [4625/7635], Loss: 4.1678\n",
      "Epoch [1/1], Step [4626/7635], Loss: 4.1482\n",
      "Epoch [1/1], Step [4627/7635], Loss: 4.2014\n",
      "Epoch [1/1], Step [4628/7635], Loss: 4.2149\n",
      "Epoch [1/1], Step [4629/7635], Loss: 4.1673\n",
      "Epoch [1/1], Step [4630/7635], Loss: 4.2424\n",
      "Epoch [1/1], Step [4631/7635], Loss: 4.1670\n",
      "Epoch [1/1], Step [4632/7635], Loss: 4.2399\n",
      "Epoch [1/1], Step [4633/7635], Loss: 4.2130\n",
      "Epoch [1/1], Step [4634/7635], Loss: 4.1711\n",
      "Epoch [1/1], Step [4635/7635], Loss: 4.2459\n",
      "Epoch [1/1], Step [4636/7635], Loss: 4.1898\n",
      "Epoch [1/1], Step [4637/7635], Loss: 4.2101\n",
      "Epoch [1/1], Step [4638/7635], Loss: 4.1782\n",
      "Epoch [1/1], Step [4639/7635], Loss: 4.1315\n",
      "Epoch [1/1], Step [4640/7635], Loss: 4.1332\n",
      "Epoch [1/1], Step [4641/7635], Loss: 4.1767\n",
      "Epoch [1/1], Step [4642/7635], Loss: 4.2015\n",
      "Epoch [1/1], Step [4643/7635], Loss: 4.2036\n",
      "Epoch [1/1], Step [4644/7635], Loss: 4.0901\n",
      "Epoch [1/1], Step [4645/7635], Loss: 4.2112\n",
      "Epoch [1/1], Step [4646/7635], Loss: 4.2014\n",
      "Epoch [1/1], Step [4647/7635], Loss: 4.1501\n",
      "Epoch [1/1], Step [4648/7635], Loss: 4.2303\n",
      "Epoch [1/1], Step [4649/7635], Loss: 4.1876\n",
      "Epoch [1/1], Step [4650/7635], Loss: 4.3276\n",
      "Epoch [1/1], Step [4651/7635], Loss: 4.2376\n",
      "Epoch [1/1], Step [4652/7635], Loss: 4.1582\n",
      "Epoch [1/1], Step [4653/7635], Loss: 4.1908\n",
      "Epoch [1/1], Step [4654/7635], Loss: 4.1710\n",
      "Epoch [1/1], Step [4655/7635], Loss: 4.1351\n",
      "Epoch [1/1], Step [4656/7635], Loss: 4.2092\n",
      "Epoch [1/1], Step [4657/7635], Loss: 4.2043\n",
      "Epoch [1/1], Step [4658/7635], Loss: 4.1662\n",
      "Epoch [1/1], Step [4659/7635], Loss: 4.2427\n",
      "Epoch [1/1], Step [4660/7635], Loss: 4.2494\n",
      "Epoch [1/1], Step [4661/7635], Loss: 4.1902\n",
      "Epoch [1/1], Step [4662/7635], Loss: 4.2315\n",
      "Epoch [1/1], Step [4663/7635], Loss: 4.2316\n",
      "Epoch [1/1], Step [4664/7635], Loss: 4.1762\n",
      "Epoch [1/1], Step [4665/7635], Loss: 4.1737\n",
      "Epoch [1/1], Step [4666/7635], Loss: 4.1778\n",
      "Epoch [1/1], Step [4667/7635], Loss: 4.2791\n",
      "Epoch [1/1], Step [4668/7635], Loss: 4.1912\n",
      "Epoch [1/1], Step [4669/7635], Loss: 4.2245\n",
      "Epoch [1/1], Step [4670/7635], Loss: 4.1532\n",
      "Epoch [1/1], Step [4671/7635], Loss: 4.1278\n",
      "Epoch [1/1], Step [4672/7635], Loss: 4.1900\n",
      "Epoch [1/1], Step [4673/7635], Loss: 4.2437\n",
      "Epoch [1/1], Step [4674/7635], Loss: 4.1946\n",
      "Epoch [1/1], Step [4675/7635], Loss: 4.1478\n",
      "Epoch [1/1], Step [4676/7635], Loss: 4.2342\n",
      "Epoch [1/1], Step [4677/7635], Loss: 4.1725\n",
      "Epoch [1/1], Step [4678/7635], Loss: 4.1768\n",
      "Epoch [1/1], Step [4679/7635], Loss: 4.1759\n",
      "Epoch [1/1], Step [4680/7635], Loss: 4.1534\n",
      "Epoch [1/1], Step [4681/7635], Loss: 4.2215\n",
      "Epoch [1/1], Step [4682/7635], Loss: 4.1645\n",
      "Epoch [1/1], Step [4683/7635], Loss: 4.2766\n",
      "Epoch [1/1], Step [4684/7635], Loss: 4.1782\n",
      "Epoch [1/1], Step [4685/7635], Loss: 4.2340\n",
      "Epoch [1/1], Step [4686/7635], Loss: 4.2385\n",
      "Epoch [1/1], Step [4687/7635], Loss: 4.2372\n",
      "Epoch [1/1], Step [4688/7635], Loss: 4.2207\n",
      "Epoch [1/1], Step [4689/7635], Loss: 4.2748\n",
      "Epoch [1/1], Step [4690/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [4691/7635], Loss: 4.1317\n",
      "Epoch [1/1], Step [4692/7635], Loss: 4.1331\n",
      "Epoch [1/1], Step [4693/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [4694/7635], Loss: 4.1698\n",
      "Epoch [1/1], Step [4695/7635], Loss: 4.2258\n",
      "Epoch [1/1], Step [4696/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [4697/7635], Loss: 4.2209\n",
      "Epoch [1/1], Step [4698/7635], Loss: 4.1160\n",
      "Epoch [1/1], Step [4699/7635], Loss: 4.2011\n",
      "Epoch [1/1], Step [4700/7635], Loss: 4.1384\n",
      "Epoch [1/1], Step [4701/7635], Loss: 4.2213\n",
      "Epoch [1/1], Step [4702/7635], Loss: 4.1796\n",
      "Epoch [1/1], Step [4703/7635], Loss: 4.2590\n",
      "Epoch [1/1], Step [4704/7635], Loss: 4.1564\n",
      "Epoch [1/1], Step [4705/7635], Loss: 4.1789\n",
      "Epoch [1/1], Step [4706/7635], Loss: 4.2132\n",
      "Epoch [1/1], Step [4707/7635], Loss: 4.2275\n",
      "Epoch [1/1], Step [4708/7635], Loss: 4.1349\n",
      "Epoch [1/1], Step [4709/7635], Loss: 4.1948\n",
      "Epoch [1/1], Step [4710/7635], Loss: 4.2204\n",
      "Epoch [1/1], Step [4711/7635], Loss: 4.1914\n",
      "Epoch [1/1], Step [4712/7635], Loss: 4.1676\n",
      "Epoch [1/1], Step [4713/7635], Loss: 4.2393\n",
      "Epoch [1/1], Step [4714/7635], Loss: 4.2107\n",
      "Epoch [1/1], Step [4715/7635], Loss: 4.1522\n",
      "Epoch [1/1], Step [4716/7635], Loss: 4.2226\n",
      "Epoch [1/1], Step [4717/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [4718/7635], Loss: 4.0976\n",
      "Epoch [1/1], Step [4719/7635], Loss: 4.1510\n",
      "Epoch [1/1], Step [4720/7635], Loss: 4.1619\n",
      "Epoch [1/1], Step [4721/7635], Loss: 4.2249\n",
      "Epoch [1/1], Step [4722/7635], Loss: 4.1675\n",
      "Epoch [1/1], Step [4723/7635], Loss: 4.1495\n",
      "Epoch [1/1], Step [4724/7635], Loss: 4.1899\n",
      "Epoch [1/1], Step [4725/7635], Loss: 4.2347\n",
      "Epoch [1/1], Step [4726/7635], Loss: 4.2417\n",
      "Epoch [1/1], Step [4727/7635], Loss: 4.1466\n",
      "Epoch [1/1], Step [4728/7635], Loss: 4.2006\n",
      "Epoch [1/1], Step [4729/7635], Loss: 4.2245\n",
      "Epoch [1/1], Step [4730/7635], Loss: 4.1769\n",
      "Epoch [1/1], Step [4731/7635], Loss: 4.1874\n",
      "Epoch [1/1], Step [4732/7635], Loss: 4.0798\n",
      "Epoch [1/1], Step [4733/7635], Loss: 4.1881\n",
      "Epoch [1/1], Step [4734/7635], Loss: 4.2079\n",
      "Epoch [1/1], Step [4735/7635], Loss: 4.1736\n",
      "Epoch [1/1], Step [4736/7635], Loss: 4.2272\n",
      "Epoch [1/1], Step [4737/7635], Loss: 4.1809\n",
      "Epoch [1/1], Step [4738/7635], Loss: 4.2090\n",
      "Epoch [1/1], Step [4739/7635], Loss: 4.1733\n",
      "Epoch [1/1], Step [4740/7635], Loss: 4.2185\n",
      "Epoch [1/1], Step [4741/7635], Loss: 4.1319\n",
      "Epoch [1/1], Step [4742/7635], Loss: 4.3179\n",
      "Epoch [1/1], Step [4743/7635], Loss: 4.2410\n",
      "Epoch [1/1], Step [4744/7635], Loss: 4.2413\n",
      "Epoch [1/1], Step [4745/7635], Loss: 4.1351\n",
      "Epoch [1/1], Step [4746/7635], Loss: 4.1917\n",
      "Epoch [1/1], Step [4747/7635], Loss: 4.1369\n",
      "Epoch [1/1], Step [4748/7635], Loss: 4.1451\n",
      "Epoch [1/1], Step [4749/7635], Loss: 4.1672\n",
      "Epoch [1/1], Step [4750/7635], Loss: 4.1600\n",
      "Epoch [1/1], Step [4751/7635], Loss: 4.2570\n",
      "Epoch [1/1], Step [4752/7635], Loss: 4.2112\n",
      "Epoch [1/1], Step [4753/7635], Loss: 4.2123\n",
      "Epoch [1/1], Step [4754/7635], Loss: 4.2571\n",
      "Epoch [1/1], Step [4755/7635], Loss: 4.1518\n",
      "Epoch [1/1], Step [4756/7635], Loss: 4.1817\n",
      "Epoch [1/1], Step [4757/7635], Loss: 4.1053\n",
      "Epoch [1/1], Step [4758/7635], Loss: 4.1813\n",
      "Epoch [1/1], Step [4759/7635], Loss: 4.1374\n",
      "Epoch [1/1], Step [4760/7635], Loss: 4.2331\n",
      "Epoch [1/1], Step [4761/7635], Loss: 4.2399\n",
      "Epoch [1/1], Step [4762/7635], Loss: 4.1837\n",
      "Epoch [1/1], Step [4763/7635], Loss: 4.2848\n",
      "Epoch [1/1], Step [4764/7635], Loss: 4.1932\n",
      "Epoch [1/1], Step [4765/7635], Loss: 4.1749\n",
      "Epoch [1/1], Step [4766/7635], Loss: 4.0935\n",
      "Epoch [1/1], Step [4767/7635], Loss: 4.2543\n",
      "Epoch [1/1], Step [4768/7635], Loss: 4.1903\n",
      "Epoch [1/1], Step [4769/7635], Loss: 4.2493\n",
      "Epoch [1/1], Step [4770/7635], Loss: 4.1912\n",
      "Epoch [1/1], Step [4771/7635], Loss: 4.2311\n",
      "Epoch [1/1], Step [4772/7635], Loss: 4.1895\n",
      "Epoch [1/1], Step [4773/7635], Loss: 4.1673\n",
      "Epoch [1/1], Step [4774/7635], Loss: 4.1250\n",
      "Epoch [1/1], Step [4775/7635], Loss: 4.2301\n",
      "Epoch [1/1], Step [4776/7635], Loss: 4.1824\n",
      "Epoch [1/1], Step [4777/7635], Loss: 4.1715\n",
      "Epoch [1/1], Step [4778/7635], Loss: 4.1377\n",
      "Epoch [1/1], Step [4779/7635], Loss: 4.2227\n",
      "Epoch [1/1], Step [4780/7635], Loss: 4.2143\n",
      "Epoch [1/1], Step [4781/7635], Loss: 4.1063\n",
      "Epoch [1/1], Step [4782/7635], Loss: 4.1997\n",
      "Epoch [1/1], Step [4783/7635], Loss: 4.2929\n",
      "Epoch [1/1], Step [4784/7635], Loss: 4.1624\n",
      "Epoch [1/1], Step [4785/7635], Loss: 4.2227\n",
      "Epoch [1/1], Step [4786/7635], Loss: 4.2676\n",
      "Epoch [1/1], Step [4787/7635], Loss: 4.2347\n",
      "Epoch [1/1], Step [4788/7635], Loss: 4.2146\n",
      "Epoch [1/1], Step [4789/7635], Loss: 4.2180\n",
      "Epoch [1/1], Step [4790/7635], Loss: 4.2192\n",
      "Epoch [1/1], Step [4791/7635], Loss: 4.2520\n",
      "Epoch [1/1], Step [4792/7635], Loss: 4.2144\n",
      "Epoch [1/1], Step [4793/7635], Loss: 4.2624\n",
      "Epoch [1/1], Step [4794/7635], Loss: 4.2844\n",
      "Epoch [1/1], Step [4795/7635], Loss: 4.2079\n",
      "Epoch [1/1], Step [4796/7635], Loss: 4.2068\n",
      "Epoch [1/1], Step [4797/7635], Loss: 4.2233\n",
      "Epoch [1/1], Step [4798/7635], Loss: 4.2509\n",
      "Epoch [1/1], Step [4799/7635], Loss: 4.1686\n",
      "Epoch [1/1], Step [4800/7635], Loss: 4.2534\n",
      "Epoch [1/1], Step [4801/7635], Loss: 4.1480\n",
      "Epoch [1/1], Step [4802/7635], Loss: 4.1498\n",
      "Epoch [1/1], Step [4803/7635], Loss: 4.2094\n",
      "Epoch [1/1], Step [4804/7635], Loss: 4.1735\n",
      "Epoch [1/1], Step [4805/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [4806/7635], Loss: 4.0600\n",
      "Epoch [1/1], Step [4807/7635], Loss: 4.1372\n",
      "Epoch [1/1], Step [4808/7635], Loss: 4.1806\n",
      "Epoch [1/1], Step [4809/7635], Loss: 4.2354\n",
      "Epoch [1/1], Step [4810/7635], Loss: 4.2384\n",
      "Epoch [1/1], Step [4811/7635], Loss: 4.2561\n",
      "Epoch [1/1], Step [4812/7635], Loss: 4.1422\n",
      "Epoch [1/1], Step [4813/7635], Loss: 4.2070\n",
      "Epoch [1/1], Step [4814/7635], Loss: 4.2502\n",
      "Epoch [1/1], Step [4815/7635], Loss: 4.2789\n",
      "Epoch [1/1], Step [4816/7635], Loss: 4.1544\n",
      "Epoch [1/1], Step [4817/7635], Loss: 4.1927\n",
      "Epoch [1/1], Step [4818/7635], Loss: 4.2176\n",
      "Epoch [1/1], Step [4819/7635], Loss: 4.2068\n",
      "Epoch [1/1], Step [4820/7635], Loss: 4.2267\n",
      "Epoch [1/1], Step [4821/7635], Loss: 4.2625\n",
      "Epoch [1/1], Step [4822/7635], Loss: 4.2326\n",
      "Epoch [1/1], Step [4823/7635], Loss: 4.2083\n",
      "Epoch [1/1], Step [4824/7635], Loss: 4.2775\n",
      "Epoch [1/1], Step [4825/7635], Loss: 4.1556\n",
      "Epoch [1/1], Step [4826/7635], Loss: 4.2783\n",
      "Epoch [1/1], Step [4827/7635], Loss: 4.2667\n",
      "Epoch [1/1], Step [4828/7635], Loss: 4.2924\n",
      "Epoch [1/1], Step [4829/7635], Loss: 4.2060\n",
      "Epoch [1/1], Step [4830/7635], Loss: 4.2155\n",
      "Epoch [1/1], Step [4831/7635], Loss: 4.1903\n",
      "Epoch [1/1], Step [4832/7635], Loss: 4.1497\n",
      "Epoch [1/1], Step [4833/7635], Loss: 4.2260\n",
      "Epoch [1/1], Step [4834/7635], Loss: 4.1815\n",
      "Epoch [1/1], Step [4835/7635], Loss: 4.1971\n",
      "Epoch [1/1], Step [4836/7635], Loss: 4.2263\n",
      "Epoch [1/1], Step [4837/7635], Loss: 4.2146\n",
      "Epoch [1/1], Step [4838/7635], Loss: 4.1883\n",
      "Epoch [1/1], Step [4839/7635], Loss: 4.1614\n",
      "Epoch [1/1], Step [4840/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [4841/7635], Loss: 4.1641\n",
      "Epoch [1/1], Step [4842/7635], Loss: 4.2297\n",
      "Epoch [1/1], Step [4843/7635], Loss: 4.2026\n",
      "Epoch [1/1], Step [4844/7635], Loss: 4.2805\n",
      "Epoch [1/1], Step [4845/7635], Loss: 4.1720\n",
      "Epoch [1/1], Step [4846/7635], Loss: 4.1343\n",
      "Epoch [1/1], Step [4847/7635], Loss: 4.2048\n",
      "Epoch [1/1], Step [4848/7635], Loss: 4.1549\n",
      "Epoch [1/1], Step [4849/7635], Loss: 4.1453\n",
      "Epoch [1/1], Step [4850/7635], Loss: 4.2507\n",
      "Epoch [1/1], Step [4851/7635], Loss: 4.2559\n",
      "Epoch [1/1], Step [4852/7635], Loss: 4.1486\n",
      "Epoch [1/1], Step [4853/7635], Loss: 4.1421\n",
      "Epoch [1/1], Step [4854/7635], Loss: 4.1738\n",
      "Epoch [1/1], Step [4855/7635], Loss: 4.2735\n",
      "Epoch [1/1], Step [4856/7635], Loss: 4.2559\n",
      "Epoch [1/1], Step [4857/7635], Loss: 4.1586\n",
      "Epoch [1/1], Step [4858/7635], Loss: 4.1969\n",
      "Epoch [1/1], Step [4859/7635], Loss: 4.2054\n",
      "Epoch [1/1], Step [4860/7635], Loss: 4.1607\n",
      "Epoch [1/1], Step [4861/7635], Loss: 4.1262\n",
      "Epoch [1/1], Step [4862/7635], Loss: 4.2111\n",
      "Epoch [1/1], Step [4863/7635], Loss: 4.1712\n",
      "Epoch [1/1], Step [4864/7635], Loss: 4.2333\n",
      "Epoch [1/1], Step [4865/7635], Loss: 4.1752\n",
      "Epoch [1/1], Step [4866/7635], Loss: 4.2120\n",
      "Epoch [1/1], Step [4867/7635], Loss: 4.2267\n",
      "Epoch [1/1], Step [4868/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [4869/7635], Loss: 4.2017\n",
      "Epoch [1/1], Step [4870/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [4871/7635], Loss: 4.2158\n",
      "Epoch [1/1], Step [4872/7635], Loss: 4.2360\n",
      "Epoch [1/1], Step [4873/7635], Loss: 4.1519\n",
      "Epoch [1/1], Step [4874/7635], Loss: 4.2008\n",
      "Epoch [1/1], Step [4875/7635], Loss: 4.0735\n",
      "Epoch [1/1], Step [4876/7635], Loss: 4.1559\n",
      "Epoch [1/1], Step [4877/7635], Loss: 4.1905\n",
      "Epoch [1/1], Step [4878/7635], Loss: 4.1603\n",
      "Epoch [1/1], Step [4879/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [4880/7635], Loss: 4.1435\n",
      "Epoch [1/1], Step [4881/7635], Loss: 4.1546\n",
      "Epoch [1/1], Step [4882/7635], Loss: 4.1590\n",
      "Epoch [1/1], Step [4883/7635], Loss: 4.2337\n",
      "Epoch [1/1], Step [4884/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [4885/7635], Loss: 4.2608\n",
      "Epoch [1/1], Step [4886/7635], Loss: 4.1803\n",
      "Epoch [1/1], Step [4887/7635], Loss: 4.2366\n",
      "Epoch [1/1], Step [4888/7635], Loss: 4.0827\n",
      "Epoch [1/1], Step [4889/7635], Loss: 4.2072\n",
      "Epoch [1/1], Step [4890/7635], Loss: 4.2526\n",
      "Epoch [1/1], Step [4891/7635], Loss: 4.1299\n",
      "Epoch [1/1], Step [4892/7635], Loss: 4.1448\n",
      "Epoch [1/1], Step [4893/7635], Loss: 4.1600\n",
      "Epoch [1/1], Step [4894/7635], Loss: 4.2727\n",
      "Epoch [1/1], Step [4895/7635], Loss: 4.1880\n",
      "Epoch [1/1], Step [4896/7635], Loss: 4.1029\n",
      "Epoch [1/1], Step [4897/7635], Loss: 4.1626\n",
      "Epoch [1/1], Step [4898/7635], Loss: 4.2427\n",
      "Epoch [1/1], Step [4899/7635], Loss: 4.2210\n",
      "Epoch [1/1], Step [4900/7635], Loss: 4.2191\n",
      "Epoch [1/1], Step [4901/7635], Loss: 4.1552\n",
      "Epoch [1/1], Step [4902/7635], Loss: 4.2060\n",
      "Epoch [1/1], Step [4903/7635], Loss: 4.2275\n",
      "Epoch [1/1], Step [4904/7635], Loss: 4.1884\n",
      "Epoch [1/1], Step [4905/7635], Loss: 4.1706\n",
      "Epoch [1/1], Step [4906/7635], Loss: 4.1678\n",
      "Epoch [1/1], Step [4907/7635], Loss: 4.1063\n",
      "Epoch [1/1], Step [4908/7635], Loss: 4.1950\n",
      "Epoch [1/1], Step [4909/7635], Loss: 4.1719\n",
      "Epoch [1/1], Step [4910/7635], Loss: 4.2021\n",
      "Epoch [1/1], Step [4911/7635], Loss: 4.2273\n",
      "Epoch [1/1], Step [4912/7635], Loss: 4.2449\n",
      "Epoch [1/1], Step [4913/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [4914/7635], Loss: 4.1721\n",
      "Epoch [1/1], Step [4915/7635], Loss: 4.1419\n",
      "Epoch [1/1], Step [4916/7635], Loss: 4.0668\n",
      "Epoch [1/1], Step [4917/7635], Loss: 4.2005\n",
      "Epoch [1/1], Step [4918/7635], Loss: 4.3266\n",
      "Epoch [1/1], Step [4919/7635], Loss: 4.1820\n",
      "Epoch [1/1], Step [4920/7635], Loss: 4.1695\n",
      "Epoch [1/1], Step [4921/7635], Loss: 4.1684\n",
      "Epoch [1/1], Step [4922/7635], Loss: 4.1376\n",
      "Epoch [1/1], Step [4923/7635], Loss: 4.1290\n",
      "Epoch [1/1], Step [4924/7635], Loss: 4.0752\n",
      "Epoch [1/1], Step [4925/7635], Loss: 4.1534\n",
      "Epoch [1/1], Step [4926/7635], Loss: 4.2188\n",
      "Epoch [1/1], Step [4927/7635], Loss: 4.1794\n",
      "Epoch [1/1], Step [4928/7635], Loss: 4.1238\n",
      "Epoch [1/1], Step [4929/7635], Loss: 4.1421\n",
      "Epoch [1/1], Step [4930/7635], Loss: 4.1455\n",
      "Epoch [1/1], Step [4931/7635], Loss: 4.1948\n",
      "Epoch [1/1], Step [4932/7635], Loss: 4.1830\n",
      "Epoch [1/1], Step [4933/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [4934/7635], Loss: 4.1865\n",
      "Epoch [1/1], Step [4935/7635], Loss: 4.1956\n",
      "Epoch [1/1], Step [4936/7635], Loss: 4.2215\n",
      "Epoch [1/1], Step [4937/7635], Loss: 4.1301\n",
      "Epoch [1/1], Step [4938/7635], Loss: 4.1873\n",
      "Epoch [1/1], Step [4939/7635], Loss: 4.2490\n",
      "Epoch [1/1], Step [4940/7635], Loss: 4.1997\n",
      "Epoch [1/1], Step [4941/7635], Loss: 4.2971\n",
      "Epoch [1/1], Step [4942/7635], Loss: 4.1816\n",
      "Epoch [1/1], Step [4943/7635], Loss: 4.2044\n",
      "Epoch [1/1], Step [4944/7635], Loss: 4.1900\n",
      "Epoch [1/1], Step [4945/7635], Loss: 4.1862\n",
      "Epoch [1/1], Step [4946/7635], Loss: 4.1786\n",
      "Epoch [1/1], Step [4947/7635], Loss: 4.2137\n",
      "Epoch [1/1], Step [4948/7635], Loss: 4.1616\n",
      "Epoch [1/1], Step [4949/7635], Loss: 4.1777\n",
      "Epoch [1/1], Step [4950/7635], Loss: 4.2425\n",
      "Epoch [1/1], Step [4951/7635], Loss: 4.1951\n",
      "Epoch [1/1], Step [4952/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [4953/7635], Loss: 4.1080\n",
      "Epoch [1/1], Step [4954/7635], Loss: 4.2052\n",
      "Epoch [1/1], Step [4955/7635], Loss: 4.1813\n",
      "Epoch [1/1], Step [4956/7635], Loss: 4.2483\n",
      "Epoch [1/1], Step [4957/7635], Loss: 4.2107\n",
      "Epoch [1/1], Step [4958/7635], Loss: 4.0827\n",
      "Epoch [1/1], Step [4959/7635], Loss: 4.2001\n",
      "Epoch [1/1], Step [4960/7635], Loss: 4.2600\n",
      "Epoch [1/1], Step [4961/7635], Loss: 4.2092\n",
      "Epoch [1/1], Step [4962/7635], Loss: 4.2451\n",
      "Epoch [1/1], Step [4963/7635], Loss: 4.2238\n",
      "Epoch [1/1], Step [4964/7635], Loss: 4.2090\n",
      "Epoch [1/1], Step [4965/7635], Loss: 4.1080\n",
      "Epoch [1/1], Step [4966/7635], Loss: 4.1588\n",
      "Epoch [1/1], Step [4967/7635], Loss: 4.2307\n",
      "Epoch [1/1], Step [4968/7635], Loss: 4.1677\n",
      "Epoch [1/1], Step [4969/7635], Loss: 4.1138\n",
      "Epoch [1/1], Step [4970/7635], Loss: 4.1918\n",
      "Epoch [1/1], Step [4971/7635], Loss: 4.1504\n",
      "Epoch [1/1], Step [4972/7635], Loss: 4.1726\n",
      "Epoch [1/1], Step [4973/7635], Loss: 4.1262\n",
      "Epoch [1/1], Step [4974/7635], Loss: 4.1894\n",
      "Epoch [1/1], Step [4975/7635], Loss: 4.1781\n",
      "Epoch [1/1], Step [4976/7635], Loss: 4.1752\n",
      "Epoch [1/1], Step [4977/7635], Loss: 4.1629\n",
      "Epoch [1/1], Step [4978/7635], Loss: 4.1337\n",
      "Epoch [1/1], Step [4979/7635], Loss: 4.1547\n",
      "Epoch [1/1], Step [4980/7635], Loss: 4.2268\n",
      "Epoch [1/1], Step [4981/7635], Loss: 4.1439\n",
      "Epoch [1/1], Step [4982/7635], Loss: 4.2472\n",
      "Epoch [1/1], Step [4983/7635], Loss: 4.1257\n",
      "Epoch [1/1], Step [4984/7635], Loss: 4.1944\n",
      "Epoch [1/1], Step [4985/7635], Loss: 4.2371\n",
      "Epoch [1/1], Step [4986/7635], Loss: 4.2338\n",
      "Epoch [1/1], Step [4987/7635], Loss: 4.1499\n",
      "Epoch [1/1], Step [4988/7635], Loss: 4.1634\n",
      "Epoch [1/1], Step [4989/7635], Loss: 4.2210\n",
      "Epoch [1/1], Step [4990/7635], Loss: 4.2346\n",
      "Epoch [1/1], Step [4991/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [4992/7635], Loss: 4.2784\n",
      "Epoch [1/1], Step [4993/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [4994/7635], Loss: 4.1560\n",
      "Epoch [1/1], Step [4995/7635], Loss: 4.1495\n",
      "Epoch [1/1], Step [4996/7635], Loss: 4.1769\n",
      "Epoch [1/1], Step [4997/7635], Loss: 4.2473\n",
      "Epoch [1/1], Step [4998/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [4999/7635], Loss: 4.1120\n",
      "Epoch [1/1], Step [5000/7635], Loss: 4.1412\n",
      "Epoch [1/1], Step [5001/7635], Loss: 4.0964\n",
      "Epoch [1/1], Step [5002/7635], Loss: 4.1673\n",
      "Epoch [1/1], Step [5003/7635], Loss: 4.1920\n",
      "Epoch [1/1], Step [5004/7635], Loss: 4.1697\n",
      "Epoch [1/1], Step [5005/7635], Loss: 4.1391\n",
      "Epoch [1/1], Step [5006/7635], Loss: 4.1836\n",
      "Epoch [1/1], Step [5007/7635], Loss: 4.1970\n",
      "Epoch [1/1], Step [5008/7635], Loss: 4.0844\n",
      "Epoch [1/1], Step [5009/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [5010/7635], Loss: 4.1416\n",
      "Epoch [1/1], Step [5011/7635], Loss: 4.1968\n",
      "Epoch [1/1], Step [5012/7635], Loss: 4.1502\n",
      "Epoch [1/1], Step [5013/7635], Loss: 4.1826\n",
      "Epoch [1/1], Step [5014/7635], Loss: 4.1472\n",
      "Epoch [1/1], Step [5015/7635], Loss: 4.2046\n",
      "Epoch [1/1], Step [5016/7635], Loss: 4.2489\n",
      "Epoch [1/1], Step [5017/7635], Loss: 4.1570\n",
      "Epoch [1/1], Step [5018/7635], Loss: 4.2388\n",
      "Epoch [1/1], Step [5019/7635], Loss: 4.2520\n",
      "Epoch [1/1], Step [5020/7635], Loss: 4.2038\n",
      "Epoch [1/1], Step [5021/7635], Loss: 4.2736\n",
      "Epoch [1/1], Step [5022/7635], Loss: 4.1935\n",
      "Epoch [1/1], Step [5023/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [5024/7635], Loss: 4.1734\n",
      "Epoch [1/1], Step [5025/7635], Loss: 4.2321\n",
      "Epoch [1/1], Step [5026/7635], Loss: 4.2641\n",
      "Epoch [1/1], Step [5027/7635], Loss: 4.1972\n",
      "Epoch [1/1], Step [5028/7635], Loss: 4.2036\n",
      "Epoch [1/1], Step [5029/7635], Loss: 4.1986\n",
      "Epoch [1/1], Step [5030/7635], Loss: 4.2526\n",
      "Epoch [1/1], Step [5031/7635], Loss: 4.2774\n",
      "Epoch [1/1], Step [5032/7635], Loss: 4.1423\n",
      "Epoch [1/1], Step [5033/7635], Loss: 4.1845\n",
      "Epoch [1/1], Step [5034/7635], Loss: 4.2410\n",
      "Epoch [1/1], Step [5035/7635], Loss: 4.1707\n",
      "Epoch [1/1], Step [5036/7635], Loss: 4.2801\n",
      "Epoch [1/1], Step [5037/7635], Loss: 4.1666\n",
      "Epoch [1/1], Step [5038/7635], Loss: 4.1747\n",
      "Epoch [1/1], Step [5039/7635], Loss: 4.1394\n",
      "Epoch [1/1], Step [5040/7635], Loss: 4.1537\n",
      "Epoch [1/1], Step [5041/7635], Loss: 4.1514\n",
      "Epoch [1/1], Step [5042/7635], Loss: 4.1739\n",
      "Epoch [1/1], Step [5043/7635], Loss: 4.2184\n",
      "Epoch [1/1], Step [5044/7635], Loss: 4.2178\n",
      "Epoch [1/1], Step [5045/7635], Loss: 4.1684\n",
      "Epoch [1/1], Step [5046/7635], Loss: 4.2313\n",
      "Epoch [1/1], Step [5047/7635], Loss: 4.1627\n",
      "Epoch [1/1], Step [5048/7635], Loss: 4.2052\n",
      "Epoch [1/1], Step [5049/7635], Loss: 4.2021\n",
      "Epoch [1/1], Step [5050/7635], Loss: 4.1482\n",
      "Epoch [1/1], Step [5051/7635], Loss: 4.1632\n",
      "Epoch [1/1], Step [5052/7635], Loss: 4.1760\n",
      "Epoch [1/1], Step [5053/7635], Loss: 4.1868\n",
      "Epoch [1/1], Step [5054/7635], Loss: 4.1758\n",
      "Epoch [1/1], Step [5055/7635], Loss: 4.2579\n",
      "Epoch [1/1], Step [5056/7635], Loss: 4.2158\n",
      "Epoch [1/1], Step [5057/7635], Loss: 4.2203\n",
      "Epoch [1/1], Step [5058/7635], Loss: 4.0968\n",
      "Epoch [1/1], Step [5059/7635], Loss: 4.1901\n",
      "Epoch [1/1], Step [5060/7635], Loss: 4.1598\n",
      "Epoch [1/1], Step [5061/7635], Loss: 4.1675\n",
      "Epoch [1/1], Step [5062/7635], Loss: 4.1215\n",
      "Epoch [1/1], Step [5063/7635], Loss: 4.2377\n",
      "Epoch [1/1], Step [5064/7635], Loss: 4.2034\n",
      "Epoch [1/1], Step [5065/7635], Loss: 4.1816\n",
      "Epoch [1/1], Step [5066/7635], Loss: 4.1944\n",
      "Epoch [1/1], Step [5067/7635], Loss: 4.1104\n",
      "Epoch [1/1], Step [5068/7635], Loss: 4.1224\n",
      "Epoch [1/1], Step [5069/7635], Loss: 4.1738\n",
      "Epoch [1/1], Step [5070/7635], Loss: 4.2846\n",
      "Epoch [1/1], Step [5071/7635], Loss: 4.1863\n",
      "Epoch [1/1], Step [5072/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [5073/7635], Loss: 4.1254\n",
      "Epoch [1/1], Step [5074/7635], Loss: 4.2149\n",
      "Epoch [1/1], Step [5075/7635], Loss: 4.1069\n",
      "Epoch [1/1], Step [5076/7635], Loss: 4.2143\n",
      "Epoch [1/1], Step [5077/7635], Loss: 4.1403\n",
      "Epoch [1/1], Step [5078/7635], Loss: 4.2356\n",
      "Epoch [1/1], Step [5079/7635], Loss: 4.1740\n",
      "Epoch [1/1], Step [5080/7635], Loss: 4.1922\n",
      "Epoch [1/1], Step [5081/7635], Loss: 4.1657\n",
      "Epoch [1/1], Step [5082/7635], Loss: 4.2864\n",
      "Epoch [1/1], Step [5083/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [5084/7635], Loss: 4.2376\n",
      "Epoch [1/1], Step [5085/7635], Loss: 4.1311\n",
      "Epoch [1/1], Step [5086/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [5087/7635], Loss: 4.2098\n",
      "Epoch [1/1], Step [5088/7635], Loss: 4.1162\n",
      "Epoch [1/1], Step [5089/7635], Loss: 4.1197\n",
      "Epoch [1/1], Step [5090/7635], Loss: 4.1486\n",
      "Epoch [1/1], Step [5091/7635], Loss: 4.1491\n",
      "Epoch [1/1], Step [5092/7635], Loss: 4.1243\n",
      "Epoch [1/1], Step [5093/7635], Loss: 4.1417\n",
      "Epoch [1/1], Step [5094/7635], Loss: 4.2314\n",
      "Epoch [1/1], Step [5095/7635], Loss: 4.2494\n",
      "Epoch [1/1], Step [5096/7635], Loss: 4.1478\n",
      "Epoch [1/1], Step [5097/7635], Loss: 4.1799\n",
      "Epoch [1/1], Step [5098/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [5099/7635], Loss: 4.1785\n",
      "Epoch [1/1], Step [5100/7635], Loss: 4.2976\n",
      "Epoch [1/1], Step [5101/7635], Loss: 4.1430\n",
      "Epoch [1/1], Step [5102/7635], Loss: 4.2215\n",
      "Epoch [1/1], Step [5103/7635], Loss: 4.1558\n",
      "Epoch [1/1], Step [5104/7635], Loss: 4.2284\n",
      "Epoch [1/1], Step [5105/7635], Loss: 4.1918\n",
      "Epoch [1/1], Step [5106/7635], Loss: 4.1543\n",
      "Epoch [1/1], Step [5107/7635], Loss: 4.2200\n",
      "Epoch [1/1], Step [5108/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [5109/7635], Loss: 4.1708\n",
      "Epoch [1/1], Step [5110/7635], Loss: 4.2092\n",
      "Epoch [1/1], Step [5111/7635], Loss: 4.2443\n",
      "Epoch [1/1], Step [5112/7635], Loss: 4.2126\n",
      "Epoch [1/1], Step [5113/7635], Loss: 4.1616\n",
      "Epoch [1/1], Step [5114/7635], Loss: 4.2193\n",
      "Epoch [1/1], Step [5115/7635], Loss: 4.2156\n",
      "Epoch [1/1], Step [5116/7635], Loss: 4.1629\n",
      "Epoch [1/1], Step [5117/7635], Loss: 4.0644\n",
      "Epoch [1/1], Step [5118/7635], Loss: 4.0636\n",
      "Epoch [1/1], Step [5119/7635], Loss: 4.2181\n",
      "Epoch [1/1], Step [5120/7635], Loss: 4.2022\n",
      "Epoch [1/1], Step [5121/7635], Loss: 4.1527\n",
      "Epoch [1/1], Step [5122/7635], Loss: 4.1714\n",
      "Epoch [1/1], Step [5123/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [5124/7635], Loss: 4.2296\n",
      "Epoch [1/1], Step [5125/7635], Loss: 4.1084\n",
      "Epoch [1/1], Step [5126/7635], Loss: 4.1950\n",
      "Epoch [1/1], Step [5127/7635], Loss: 4.2188\n",
      "Epoch [1/1], Step [5128/7635], Loss: 4.1648\n",
      "Epoch [1/1], Step [5129/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [5130/7635], Loss: 4.2223\n",
      "Epoch [1/1], Step [5131/7635], Loss: 4.1930\n",
      "Epoch [1/1], Step [5132/7635], Loss: 4.2049\n",
      "Epoch [1/1], Step [5133/7635], Loss: 4.2090\n",
      "Epoch [1/1], Step [5134/7635], Loss: 4.1696\n",
      "Epoch [1/1], Step [5135/7635], Loss: 4.1556\n",
      "Epoch [1/1], Step [5136/7635], Loss: 4.1868\n",
      "Epoch [1/1], Step [5137/7635], Loss: 4.2087\n",
      "Epoch [1/1], Step [5138/7635], Loss: 4.1177\n",
      "Epoch [1/1], Step [5139/7635], Loss: 4.1893\n",
      "Epoch [1/1], Step [5140/7635], Loss: 4.0785\n",
      "Epoch [1/1], Step [5141/7635], Loss: 4.2059\n",
      "Epoch [1/1], Step [5142/7635], Loss: 4.1830\n",
      "Epoch [1/1], Step [5143/7635], Loss: 4.2003\n",
      "Epoch [1/1], Step [5144/7635], Loss: 4.2268\n",
      "Epoch [1/1], Step [5145/7635], Loss: 4.1643\n",
      "Epoch [1/1], Step [5146/7635], Loss: 4.2413\n",
      "Epoch [1/1], Step [5147/7635], Loss: 4.2125\n",
      "Epoch [1/1], Step [5148/7635], Loss: 4.1358\n",
      "Epoch [1/1], Step [5149/7635], Loss: 4.1832\n",
      "Epoch [1/1], Step [5150/7635], Loss: 4.0959\n",
      "Epoch [1/1], Step [5151/7635], Loss: 4.1549\n",
      "Epoch [1/1], Step [5152/7635], Loss: 4.0960\n",
      "Epoch [1/1], Step [5153/7635], Loss: 4.1872\n",
      "Epoch [1/1], Step [5154/7635], Loss: 4.3028\n",
      "Epoch [1/1], Step [5155/7635], Loss: 4.1632\n",
      "Epoch [1/1], Step [5156/7635], Loss: 4.1536\n",
      "Epoch [1/1], Step [5157/7635], Loss: 4.1346\n",
      "Epoch [1/1], Step [5158/7635], Loss: 4.1702\n",
      "Epoch [1/1], Step [5159/7635], Loss: 4.1900\n",
      "Epoch [1/1], Step [5160/7635], Loss: 4.1860\n",
      "Epoch [1/1], Step [5161/7635], Loss: 4.1708\n",
      "Epoch [1/1], Step [5162/7635], Loss: 4.2151\n",
      "Epoch [1/1], Step [5163/7635], Loss: 4.1528\n",
      "Epoch [1/1], Step [5164/7635], Loss: 4.2080\n",
      "Epoch [1/1], Step [5165/7635], Loss: 4.1351\n",
      "Epoch [1/1], Step [5166/7635], Loss: 4.1693\n",
      "Epoch [1/1], Step [5167/7635], Loss: 4.1528\n",
      "Epoch [1/1], Step [5168/7635], Loss: 4.1941\n",
      "Epoch [1/1], Step [5169/7635], Loss: 4.1688\n",
      "Epoch [1/1], Step [5170/7635], Loss: 4.1581\n",
      "Epoch [1/1], Step [5171/7635], Loss: 4.1085\n",
      "Epoch [1/1], Step [5172/7635], Loss: 4.2241\n",
      "Epoch [1/1], Step [5173/7635], Loss: 4.1915\n",
      "Epoch [1/1], Step [5174/7635], Loss: 4.1825\n",
      "Epoch [1/1], Step [5175/7635], Loss: 4.0709\n",
      "Epoch [1/1], Step [5176/7635], Loss: 4.1648\n",
      "Epoch [1/1], Step [5177/7635], Loss: 4.1740\n",
      "Epoch [1/1], Step [5178/7635], Loss: 4.1754\n",
      "Epoch [1/1], Step [5179/7635], Loss: 4.1290\n",
      "Epoch [1/1], Step [5180/7635], Loss: 4.2486\n",
      "Epoch [1/1], Step [5181/7635], Loss: 4.1987\n",
      "Epoch [1/1], Step [5182/7635], Loss: 4.1578\n",
      "Epoch [1/1], Step [5183/7635], Loss: 4.1209\n",
      "Epoch [1/1], Step [5184/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [5185/7635], Loss: 4.1879\n",
      "Epoch [1/1], Step [5186/7635], Loss: 4.2077\n",
      "Epoch [1/1], Step [5187/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [5188/7635], Loss: 4.1671\n",
      "Epoch [1/1], Step [5189/7635], Loss: 4.2153\n",
      "Epoch [1/1], Step [5190/7635], Loss: 4.1519\n",
      "Epoch [1/1], Step [5191/7635], Loss: 4.1456\n",
      "Epoch [1/1], Step [5192/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [5193/7635], Loss: 4.2453\n",
      "Epoch [1/1], Step [5194/7635], Loss: 4.1086\n",
      "Epoch [1/1], Step [5195/7635], Loss: 4.1065\n",
      "Epoch [1/1], Step [5196/7635], Loss: 4.1394\n",
      "Epoch [1/1], Step [5197/7635], Loss: 4.1497\n",
      "Epoch [1/1], Step [5198/7635], Loss: 4.2206\n",
      "Epoch [1/1], Step [5199/7635], Loss: 4.1668\n",
      "Epoch [1/1], Step [5200/7635], Loss: 4.1660\n",
      "Epoch [1/1], Step [5201/7635], Loss: 4.1186\n",
      "Epoch [1/1], Step [5202/7635], Loss: 4.1849\n",
      "Epoch [1/1], Step [5203/7635], Loss: 4.2600\n",
      "Epoch [1/1], Step [5204/7635], Loss: 4.2412\n",
      "Epoch [1/1], Step [5205/7635], Loss: 4.1549\n",
      "Epoch [1/1], Step [5206/7635], Loss: 4.2248\n",
      "Epoch [1/1], Step [5207/7635], Loss: 4.1708\n",
      "Epoch [1/1], Step [5208/7635], Loss: 4.1960\n",
      "Epoch [1/1], Step [5209/7635], Loss: 4.2474\n",
      "Epoch [1/1], Step [5210/7635], Loss: 4.1096\n",
      "Epoch [1/1], Step [5211/7635], Loss: 4.1456\n",
      "Epoch [1/1], Step [5212/7635], Loss: 4.1163\n",
      "Epoch [1/1], Step [5213/7635], Loss: 4.2042\n",
      "Epoch [1/1], Step [5214/7635], Loss: 4.2200\n",
      "Epoch [1/1], Step [5215/7635], Loss: 4.1490\n",
      "Epoch [1/1], Step [5216/7635], Loss: 4.1833\n",
      "Epoch [1/1], Step [5217/7635], Loss: 4.1424\n",
      "Epoch [1/1], Step [5218/7635], Loss: 4.0994\n",
      "Epoch [1/1], Step [5219/7635], Loss: 4.1727\n",
      "Epoch [1/1], Step [5220/7635], Loss: 4.1581\n",
      "Epoch [1/1], Step [5221/7635], Loss: 4.1402\n",
      "Epoch [1/1], Step [5222/7635], Loss: 4.0694\n",
      "Epoch [1/1], Step [5223/7635], Loss: 4.1670\n",
      "Epoch [1/1], Step [5224/7635], Loss: 4.1821\n",
      "Epoch [1/1], Step [5225/7635], Loss: 4.2370\n",
      "Epoch [1/1], Step [5226/7635], Loss: 4.1862\n",
      "Epoch [1/1], Step [5227/7635], Loss: 4.1896\n",
      "Epoch [1/1], Step [5228/7635], Loss: 4.2061\n",
      "Epoch [1/1], Step [5229/7635], Loss: 4.1528\n",
      "Epoch [1/1], Step [5230/7635], Loss: 4.1977\n",
      "Epoch [1/1], Step [5231/7635], Loss: 4.1175\n",
      "Epoch [1/1], Step [5232/7635], Loss: 4.1551\n",
      "Epoch [1/1], Step [5233/7635], Loss: 4.2085\n",
      "Epoch [1/1], Step [5234/7635], Loss: 4.2124\n",
      "Epoch [1/1], Step [5235/7635], Loss: 4.1241\n",
      "Epoch [1/1], Step [5236/7635], Loss: 4.1847\n",
      "Epoch [1/1], Step [5237/7635], Loss: 4.1768\n",
      "Epoch [1/1], Step [5238/7635], Loss: 4.2128\n",
      "Epoch [1/1], Step [5239/7635], Loss: 4.1294\n",
      "Epoch [1/1], Step [5240/7635], Loss: 4.2000\n",
      "Epoch [1/1], Step [5241/7635], Loss: 4.2071\n",
      "Epoch [1/1], Step [5242/7635], Loss: 4.2199\n",
      "Epoch [1/1], Step [5243/7635], Loss: 4.1336\n",
      "Epoch [1/1], Step [5244/7635], Loss: 4.1670\n",
      "Epoch [1/1], Step [5245/7635], Loss: 4.1995\n",
      "Epoch [1/1], Step [5246/7635], Loss: 4.2167\n",
      "Epoch [1/1], Step [5247/7635], Loss: 4.1980\n",
      "Epoch [1/1], Step [5248/7635], Loss: 4.1529\n",
      "Epoch [1/1], Step [5249/7635], Loss: 4.2491\n",
      "Epoch [1/1], Step [5250/7635], Loss: 4.1626\n",
      "Epoch [1/1], Step [5251/7635], Loss: 4.1663\n",
      "Epoch [1/1], Step [5252/7635], Loss: 4.2116\n",
      "Epoch [1/1], Step [5253/7635], Loss: 4.2145\n",
      "Epoch [1/1], Step [5254/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [5255/7635], Loss: 4.1443\n",
      "Epoch [1/1], Step [5256/7635], Loss: 4.1739\n",
      "Epoch [1/1], Step [5257/7635], Loss: 4.2156\n",
      "Epoch [1/1], Step [5258/7635], Loss: 4.1033\n",
      "Epoch [1/1], Step [5259/7635], Loss: 4.1148\n",
      "Epoch [1/1], Step [5260/7635], Loss: 4.1775\n",
      "Epoch [1/1], Step [5261/7635], Loss: 4.1650\n",
      "Epoch [1/1], Step [5262/7635], Loss: 4.2054\n",
      "Epoch [1/1], Step [5263/7635], Loss: 4.2066\n",
      "Epoch [1/1], Step [5264/7635], Loss: 4.1023\n",
      "Epoch [1/1], Step [5265/7635], Loss: 4.2339\n",
      "Epoch [1/1], Step [5266/7635], Loss: 4.0938\n",
      "Epoch [1/1], Step [5267/7635], Loss: 4.1063\n",
      "Epoch [1/1], Step [5268/7635], Loss: 4.1946\n",
      "Epoch [1/1], Step [5269/7635], Loss: 4.1444\n",
      "Epoch [1/1], Step [5270/7635], Loss: 4.2319\n",
      "Epoch [1/1], Step [5271/7635], Loss: 4.2366\n",
      "Epoch [1/1], Step [5272/7635], Loss: 4.1547\n",
      "Epoch [1/1], Step [5273/7635], Loss: 4.1541\n",
      "Epoch [1/1], Step [5274/7635], Loss: 4.2244\n",
      "Epoch [1/1], Step [5275/7635], Loss: 4.2063\n",
      "Epoch [1/1], Step [5276/7635], Loss: 4.1857\n",
      "Epoch [1/1], Step [5277/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [5278/7635], Loss: 4.1890\n",
      "Epoch [1/1], Step [5279/7635], Loss: 4.1636\n",
      "Epoch [1/1], Step [5280/7635], Loss: 4.2162\n",
      "Epoch [1/1], Step [5281/7635], Loss: 4.1744\n",
      "Epoch [1/1], Step [5282/7635], Loss: 4.1583\n",
      "Epoch [1/1], Step [5283/7635], Loss: 4.0840\n",
      "Epoch [1/1], Step [5284/7635], Loss: 4.2026\n",
      "Epoch [1/1], Step [5285/7635], Loss: 4.2103\n",
      "Epoch [1/1], Step [5286/7635], Loss: 4.1270\n",
      "Epoch [1/1], Step [5287/7635], Loss: 4.1462\n",
      "Epoch [1/1], Step [5288/7635], Loss: 4.2230\n",
      "Epoch [1/1], Step [5289/7635], Loss: 4.1951\n",
      "Epoch [1/1], Step [5290/7635], Loss: 4.1031\n",
      "Epoch [1/1], Step [5291/7635], Loss: 4.1973\n",
      "Epoch [1/1], Step [5292/7635], Loss: 4.2241\n",
      "Epoch [1/1], Step [5293/7635], Loss: 4.1686\n",
      "Epoch [1/1], Step [5294/7635], Loss: 4.2552\n",
      "Epoch [1/1], Step [5295/7635], Loss: 4.1615\n",
      "Epoch [1/1], Step [5296/7635], Loss: 4.1467\n",
      "Epoch [1/1], Step [5297/7635], Loss: 4.1238\n",
      "Epoch [1/1], Step [5298/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [5299/7635], Loss: 4.1541\n",
      "Epoch [1/1], Step [5300/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [5301/7635], Loss: 4.0975\n",
      "Epoch [1/1], Step [5302/7635], Loss: 4.2300\n",
      "Epoch [1/1], Step [5303/7635], Loss: 4.1680\n",
      "Epoch [1/1], Step [5304/7635], Loss: 4.1196\n",
      "Epoch [1/1], Step [5305/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [5306/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [5307/7635], Loss: 4.1522\n",
      "Epoch [1/1], Step [5308/7635], Loss: 4.0788\n",
      "Epoch [1/1], Step [5309/7635], Loss: 4.2048\n",
      "Epoch [1/1], Step [5310/7635], Loss: 4.1514\n",
      "Epoch [1/1], Step [5311/7635], Loss: 4.2119\n",
      "Epoch [1/1], Step [5312/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [5313/7635], Loss: 4.1211\n",
      "Epoch [1/1], Step [5314/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [5315/7635], Loss: 4.2110\n",
      "Epoch [1/1], Step [5316/7635], Loss: 4.2145\n",
      "Epoch [1/1], Step [5317/7635], Loss: 4.2094\n",
      "Epoch [1/1], Step [5318/7635], Loss: 4.1879\n",
      "Epoch [1/1], Step [5319/7635], Loss: 4.0921\n",
      "Epoch [1/1], Step [5320/7635], Loss: 4.1678\n",
      "Epoch [1/1], Step [5321/7635], Loss: 4.1128\n",
      "Epoch [1/1], Step [5322/7635], Loss: 4.1403\n",
      "Epoch [1/1], Step [5323/7635], Loss: 4.1974\n",
      "Epoch [1/1], Step [5324/7635], Loss: 4.0574\n",
      "Epoch [1/1], Step [5325/7635], Loss: 4.1703\n",
      "Epoch [1/1], Step [5326/7635], Loss: 4.1685\n",
      "Epoch [1/1], Step [5327/7635], Loss: 4.1596\n",
      "Epoch [1/1], Step [5328/7635], Loss: 4.1377\n",
      "Epoch [1/1], Step [5329/7635], Loss: 4.1692\n",
      "Epoch [1/1], Step [5330/7635], Loss: 4.1519\n",
      "Epoch [1/1], Step [5331/7635], Loss: 4.1617\n",
      "Epoch [1/1], Step [5332/7635], Loss: 4.1067\n",
      "Epoch [1/1], Step [5333/7635], Loss: 4.1507\n",
      "Epoch [1/1], Step [5334/7635], Loss: 4.0649\n",
      "Epoch [1/1], Step [5335/7635], Loss: 4.1561\n",
      "Epoch [1/1], Step [5336/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [5337/7635], Loss: 4.1804\n",
      "Epoch [1/1], Step [5338/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [5339/7635], Loss: 4.2206\n",
      "Epoch [1/1], Step [5340/7635], Loss: 4.2396\n",
      "Epoch [1/1], Step [5341/7635], Loss: 4.1106\n",
      "Epoch [1/1], Step [5342/7635], Loss: 4.0888\n",
      "Epoch [1/1], Step [5343/7635], Loss: 4.2085\n",
      "Epoch [1/1], Step [5344/7635], Loss: 4.1767\n",
      "Epoch [1/1], Step [5345/7635], Loss: 4.1606\n",
      "Epoch [1/1], Step [5346/7635], Loss: 4.2108\n",
      "Epoch [1/1], Step [5347/7635], Loss: 4.1760\n",
      "Epoch [1/1], Step [5348/7635], Loss: 4.1037\n",
      "Epoch [1/1], Step [5349/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [5350/7635], Loss: 4.1248\n",
      "Epoch [1/1], Step [5351/7635], Loss: 4.1213\n",
      "Epoch [1/1], Step [5352/7635], Loss: 4.2001\n",
      "Epoch [1/1], Step [5353/7635], Loss: 4.2269\n",
      "Epoch [1/1], Step [5354/7635], Loss: 4.1950\n",
      "Epoch [1/1], Step [5355/7635], Loss: 4.1874\n",
      "Epoch [1/1], Step [5356/7635], Loss: 4.1546\n",
      "Epoch [1/1], Step [5357/7635], Loss: 4.1711\n",
      "Epoch [1/1], Step [5358/7635], Loss: 4.1517\n",
      "Epoch [1/1], Step [5359/7635], Loss: 4.1186\n",
      "Epoch [1/1], Step [5360/7635], Loss: 4.1605\n",
      "Epoch [1/1], Step [5361/7635], Loss: 4.2005\n",
      "Epoch [1/1], Step [5362/7635], Loss: 4.1843\n",
      "Epoch [1/1], Step [5363/7635], Loss: 4.1591\n",
      "Epoch [1/1], Step [5364/7635], Loss: 4.2019\n",
      "Epoch [1/1], Step [5365/7635], Loss: 4.1171\n",
      "Epoch [1/1], Step [5366/7635], Loss: 4.2124\n",
      "Epoch [1/1], Step [5367/7635], Loss: 4.1837\n",
      "Epoch [1/1], Step [5368/7635], Loss: 4.1100\n",
      "Epoch [1/1], Step [5369/7635], Loss: 4.1894\n",
      "Epoch [1/1], Step [5370/7635], Loss: 4.0488\n",
      "Epoch [1/1], Step [5371/7635], Loss: 4.1215\n",
      "Epoch [1/1], Step [5372/7635], Loss: 4.1961\n",
      "Epoch [1/1], Step [5373/7635], Loss: 4.1621\n",
      "Epoch [1/1], Step [5374/7635], Loss: 4.1172\n",
      "Epoch [1/1], Step [5375/7635], Loss: 4.0744\n",
      "Epoch [1/1], Step [5376/7635], Loss: 4.2265\n",
      "Epoch [1/1], Step [5377/7635], Loss: 4.2276\n",
      "Epoch [1/1], Step [5378/7635], Loss: 4.1793\n",
      "Epoch [1/1], Step [5379/7635], Loss: 4.1538\n",
      "Epoch [1/1], Step [5380/7635], Loss: 4.1562\n",
      "Epoch [1/1], Step [5381/7635], Loss: 4.0783\n",
      "Epoch [1/1], Step [5382/7635], Loss: 4.1250\n",
      "Epoch [1/1], Step [5383/7635], Loss: 4.1076\n",
      "Epoch [1/1], Step [5384/7635], Loss: 4.1343\n",
      "Epoch [1/1], Step [5385/7635], Loss: 4.1937\n",
      "Epoch [1/1], Step [5386/7635], Loss: 4.1783\n",
      "Epoch [1/1], Step [5387/7635], Loss: 4.1264\n",
      "Epoch [1/1], Step [5388/7635], Loss: 4.2032\n",
      "Epoch [1/1], Step [5389/7635], Loss: 4.1423\n",
      "Epoch [1/1], Step [5390/7635], Loss: 4.1483\n",
      "Epoch [1/1], Step [5391/7635], Loss: 4.1291\n",
      "Epoch [1/1], Step [5392/7635], Loss: 4.1416\n",
      "Epoch [1/1], Step [5393/7635], Loss: 4.1789\n",
      "Epoch [1/1], Step [5394/7635], Loss: 4.1703\n",
      "Epoch [1/1], Step [5395/7635], Loss: 4.2132\n",
      "Epoch [1/1], Step [5396/7635], Loss: 4.1379\n",
      "Epoch [1/1], Step [5397/7635], Loss: 4.1931\n",
      "Epoch [1/1], Step [5398/7635], Loss: 4.2011\n",
      "Epoch [1/1], Step [5399/7635], Loss: 4.1563\n",
      "Epoch [1/1], Step [5400/7635], Loss: 4.1789\n",
      "Epoch [1/1], Step [5401/7635], Loss: 4.1397\n",
      "Epoch [1/1], Step [5402/7635], Loss: 4.1462\n",
      "Epoch [1/1], Step [5403/7635], Loss: 4.2193\n",
      "Epoch [1/1], Step [5404/7635], Loss: 4.1484\n",
      "Epoch [1/1], Step [5405/7635], Loss: 4.1149\n",
      "Epoch [1/1], Step [5406/7635], Loss: 4.1671\n",
      "Epoch [1/1], Step [5407/7635], Loss: 4.2136\n",
      "Epoch [1/1], Step [5408/7635], Loss: 4.1724\n",
      "Epoch [1/1], Step [5409/7635], Loss: 4.1744\n",
      "Epoch [1/1], Step [5410/7635], Loss: 4.1637\n",
      "Epoch [1/1], Step [5411/7635], Loss: 4.1271\n",
      "Epoch [1/1], Step [5412/7635], Loss: 4.1681\n",
      "Epoch [1/1], Step [5413/7635], Loss: 4.1347\n",
      "Epoch [1/1], Step [5414/7635], Loss: 4.2288\n",
      "Epoch [1/1], Step [5415/7635], Loss: 4.1214\n",
      "Epoch [1/1], Step [5416/7635], Loss: 4.1416\n",
      "Epoch [1/1], Step [5417/7635], Loss: 4.1908\n",
      "Epoch [1/1], Step [5418/7635], Loss: 4.1341\n",
      "Epoch [1/1], Step [5419/7635], Loss: 4.1877\n",
      "Epoch [1/1], Step [5420/7635], Loss: 4.1292\n",
      "Epoch [1/1], Step [5421/7635], Loss: 4.1317\n",
      "Epoch [1/1], Step [5422/7635], Loss: 4.1674\n",
      "Epoch [1/1], Step [5423/7635], Loss: 4.1980\n",
      "Epoch [1/1], Step [5424/7635], Loss: 4.1657\n",
      "Epoch [1/1], Step [5425/7635], Loss: 4.1766\n",
      "Epoch [1/1], Step [5426/7635], Loss: 4.1601\n",
      "Epoch [1/1], Step [5427/7635], Loss: 4.0676\n",
      "Epoch [1/1], Step [5428/7635], Loss: 4.0421\n",
      "Epoch [1/1], Step [5429/7635], Loss: 4.1233\n",
      "Epoch [1/1], Step [5430/7635], Loss: 4.2123\n",
      "Epoch [1/1], Step [5431/7635], Loss: 4.1186\n",
      "Epoch [1/1], Step [5432/7635], Loss: 4.1288\n",
      "Epoch [1/1], Step [5433/7635], Loss: 4.2191\n",
      "Epoch [1/1], Step [5434/7635], Loss: 4.1941\n",
      "Epoch [1/1], Step [5435/7635], Loss: 4.1102\n",
      "Epoch [1/1], Step [5436/7635], Loss: 4.0800\n",
      "Epoch [1/1], Step [5437/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [5438/7635], Loss: 4.2022\n",
      "Epoch [1/1], Step [5439/7635], Loss: 4.1224\n",
      "Epoch [1/1], Step [5440/7635], Loss: 4.1891\n",
      "Epoch [1/1], Step [5441/7635], Loss: 4.1095\n",
      "Epoch [1/1], Step [5442/7635], Loss: 4.2081\n",
      "Epoch [1/1], Step [5443/7635], Loss: 4.1734\n",
      "Epoch [1/1], Step [5444/7635], Loss: 4.0791\n",
      "Epoch [1/1], Step [5445/7635], Loss: 4.2568\n",
      "Epoch [1/1], Step [5446/7635], Loss: 4.0987\n",
      "Epoch [1/1], Step [5447/7635], Loss: 4.1352\n",
      "Epoch [1/1], Step [5448/7635], Loss: 4.1585\n",
      "Epoch [1/1], Step [5449/7635], Loss: 4.1672\n",
      "Epoch [1/1], Step [5450/7635], Loss: 4.1841\n",
      "Epoch [1/1], Step [5451/7635], Loss: 4.1360\n",
      "Epoch [1/1], Step [5452/7635], Loss: 4.1924\n",
      "Epoch [1/1], Step [5453/7635], Loss: 4.1051\n",
      "Epoch [1/1], Step [5454/7635], Loss: 4.2733\n",
      "Epoch [1/1], Step [5455/7635], Loss: 4.1853\n",
      "Epoch [1/1], Step [5456/7635], Loss: 4.2455\n",
      "Epoch [1/1], Step [5457/7635], Loss: 4.1885\n",
      "Epoch [1/1], Step [5458/7635], Loss: 4.1714\n",
      "Epoch [1/1], Step [5459/7635], Loss: 4.1657\n",
      "Epoch [1/1], Step [5460/7635], Loss: 4.1869\n",
      "Epoch [1/1], Step [5461/7635], Loss: 4.1769\n",
      "Epoch [1/1], Step [5462/7635], Loss: 4.1845\n",
      "Epoch [1/1], Step [5463/7635], Loss: 4.2908\n",
      "Epoch [1/1], Step [5464/7635], Loss: 4.1787\n",
      "Epoch [1/1], Step [5465/7635], Loss: 4.1254\n",
      "Epoch [1/1], Step [5466/7635], Loss: 4.1740\n",
      "Epoch [1/1], Step [5467/7635], Loss: 4.1389\n",
      "Epoch [1/1], Step [5468/7635], Loss: 4.1055\n",
      "Epoch [1/1], Step [5469/7635], Loss: 4.1391\n",
      "Epoch [1/1], Step [5470/7635], Loss: 4.2286\n",
      "Epoch [1/1], Step [5471/7635], Loss: 4.1512\n",
      "Epoch [1/1], Step [5472/7635], Loss: 4.1452\n",
      "Epoch [1/1], Step [5473/7635], Loss: 4.1817\n",
      "Epoch [1/1], Step [5474/7635], Loss: 4.1368\n",
      "Epoch [1/1], Step [5475/7635], Loss: 4.2445\n",
      "Epoch [1/1], Step [5476/7635], Loss: 4.1807\n",
      "Epoch [1/1], Step [5477/7635], Loss: 4.1795\n",
      "Epoch [1/1], Step [5478/7635], Loss: 4.1959\n",
      "Epoch [1/1], Step [5479/7635], Loss: 4.2169\n",
      "Epoch [1/1], Step [5480/7635], Loss: 4.1977\n",
      "Epoch [1/1], Step [5481/7635], Loss: 4.1736\n",
      "Epoch [1/1], Step [5482/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [5483/7635], Loss: 4.1662\n",
      "Epoch [1/1], Step [5484/7635], Loss: 4.1553\n",
      "Epoch [1/1], Step [5485/7635], Loss: 4.1990\n",
      "Epoch [1/1], Step [5486/7635], Loss: 4.1230\n",
      "Epoch [1/1], Step [5487/7635], Loss: 4.1517\n",
      "Epoch [1/1], Step [5488/7635], Loss: 4.0981\n",
      "Epoch [1/1], Step [5489/7635], Loss: 4.0961\n",
      "Epoch [1/1], Step [5490/7635], Loss: 4.1547\n",
      "Epoch [1/1], Step [5491/7635], Loss: 4.1289\n",
      "Epoch [1/1], Step [5492/7635], Loss: 4.1765\n",
      "Epoch [1/1], Step [5493/7635], Loss: 4.0876\n",
      "Epoch [1/1], Step [5494/7635], Loss: 4.1767\n",
      "Epoch [1/1], Step [5495/7635], Loss: 4.1418\n",
      "Epoch [1/1], Step [5496/7635], Loss: 4.1593\n",
      "Epoch [1/1], Step [5497/7635], Loss: 4.1206\n",
      "Epoch [1/1], Step [5498/7635], Loss: 4.1955\n",
      "Epoch [1/1], Step [5499/7635], Loss: 4.1592\n",
      "Epoch [1/1], Step [5500/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [5501/7635], Loss: 4.2155\n",
      "Epoch [1/1], Step [5502/7635], Loss: 4.1558\n",
      "Epoch [1/1], Step [5503/7635], Loss: 4.1604\n",
      "Epoch [1/1], Step [5504/7635], Loss: 4.1406\n",
      "Epoch [1/1], Step [5505/7635], Loss: 4.1262\n",
      "Epoch [1/1], Step [5506/7635], Loss: 4.1862\n",
      "Epoch [1/1], Step [5507/7635], Loss: 4.1835\n",
      "Epoch [1/1], Step [5508/7635], Loss: 4.0695\n",
      "Epoch [1/1], Step [5509/7635], Loss: 4.1623\n",
      "Epoch [1/1], Step [5510/7635], Loss: 4.0865\n",
      "Epoch [1/1], Step [5511/7635], Loss: 4.2370\n",
      "Epoch [1/1], Step [5512/7635], Loss: 4.1628\n",
      "Epoch [1/1], Step [5513/7635], Loss: 4.1096\n",
      "Epoch [1/1], Step [5514/7635], Loss: 4.1519\n",
      "Epoch [1/1], Step [5515/7635], Loss: 4.1269\n",
      "Epoch [1/1], Step [5516/7635], Loss: 4.1868\n",
      "Epoch [1/1], Step [5517/7635], Loss: 4.2180\n",
      "Epoch [1/1], Step [5518/7635], Loss: 4.1737\n",
      "Epoch [1/1], Step [5519/7635], Loss: 4.1752\n",
      "Epoch [1/1], Step [5520/7635], Loss: 4.1184\n",
      "Epoch [1/1], Step [5521/7635], Loss: 4.2858\n",
      "Epoch [1/1], Step [5522/7635], Loss: 4.2267\n",
      "Epoch [1/1], Step [5523/7635], Loss: 4.1912\n",
      "Epoch [1/1], Step [5524/7635], Loss: 4.1559\n",
      "Epoch [1/1], Step [5525/7635], Loss: 4.0351\n",
      "Epoch [1/1], Step [5526/7635], Loss: 4.0903\n",
      "Epoch [1/1], Step [5527/7635], Loss: 4.1353\n",
      "Epoch [1/1], Step [5528/7635], Loss: 4.0756\n",
      "Epoch [1/1], Step [5529/7635], Loss: 4.1129\n",
      "Epoch [1/1], Step [5530/7635], Loss: 4.1208\n",
      "Epoch [1/1], Step [5531/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [5532/7635], Loss: 4.0893\n",
      "Epoch [1/1], Step [5533/7635], Loss: 4.1157\n",
      "Epoch [1/1], Step [5534/7635], Loss: 4.1971\n",
      "Epoch [1/1], Step [5535/7635], Loss: 4.1631\n",
      "Epoch [1/1], Step [5536/7635], Loss: 4.2315\n",
      "Epoch [1/1], Step [5537/7635], Loss: 4.1678\n",
      "Epoch [1/1], Step [5538/7635], Loss: 4.1215\n",
      "Epoch [1/1], Step [5539/7635], Loss: 4.2078\n",
      "Epoch [1/1], Step [5540/7635], Loss: 4.1741\n",
      "Epoch [1/1], Step [5541/7635], Loss: 4.1290\n",
      "Epoch [1/1], Step [5542/7635], Loss: 4.1672\n",
      "Epoch [1/1], Step [5543/7635], Loss: 4.1335\n",
      "Epoch [1/1], Step [5544/7635], Loss: 4.1747\n",
      "Epoch [1/1], Step [5545/7635], Loss: 4.1108\n",
      "Epoch [1/1], Step [5546/7635], Loss: 4.1263\n",
      "Epoch [1/1], Step [5547/7635], Loss: 4.1775\n",
      "Epoch [1/1], Step [5548/7635], Loss: 4.2000\n",
      "Epoch [1/1], Step [5549/7635], Loss: 4.1151\n",
      "Epoch [1/1], Step [5550/7635], Loss: 4.1860\n",
      "Epoch [1/1], Step [5551/7635], Loss: 4.1680\n",
      "Epoch [1/1], Step [5552/7635], Loss: 4.1963\n",
      "Epoch [1/1], Step [5553/7635], Loss: 4.1524\n",
      "Epoch [1/1], Step [5554/7635], Loss: 4.1372\n",
      "Epoch [1/1], Step [5555/7635], Loss: 4.1931\n",
      "Epoch [1/1], Step [5556/7635], Loss: 4.1294\n",
      "Epoch [1/1], Step [5557/7635], Loss: 4.2447\n",
      "Epoch [1/1], Step [5558/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [5559/7635], Loss: 4.1472\n",
      "Epoch [1/1], Step [5560/7635], Loss: 4.1322\n",
      "Epoch [1/1], Step [5561/7635], Loss: 4.2552\n",
      "Epoch [1/1], Step [5562/7635], Loss: 4.2170\n",
      "Epoch [1/1], Step [5563/7635], Loss: 4.1928\n",
      "Epoch [1/1], Step [5564/7635], Loss: 4.1551\n",
      "Epoch [1/1], Step [5565/7635], Loss: 4.1820\n",
      "Epoch [1/1], Step [5566/7635], Loss: 4.1823\n",
      "Epoch [1/1], Step [5567/7635], Loss: 4.1579\n",
      "Epoch [1/1], Step [5568/7635], Loss: 4.1074\n",
      "Epoch [1/1], Step [5569/7635], Loss: 4.1205\n",
      "Epoch [1/1], Step [5570/7635], Loss: 4.0645\n",
      "Epoch [1/1], Step [5571/7635], Loss: 4.1519\n",
      "Epoch [1/1], Step [5572/7635], Loss: 4.1687\n",
      "Epoch [1/1], Step [5573/7635], Loss: 4.1244\n",
      "Epoch [1/1], Step [5574/7635], Loss: 4.1469\n",
      "Epoch [1/1], Step [5575/7635], Loss: 4.0514\n",
      "Epoch [1/1], Step [5576/7635], Loss: 4.0973\n",
      "Epoch [1/1], Step [5577/7635], Loss: 4.1268\n",
      "Epoch [1/1], Step [5578/7635], Loss: 4.1691\n",
      "Epoch [1/1], Step [5579/7635], Loss: 4.1163\n",
      "Epoch [1/1], Step [5580/7635], Loss: 4.0993\n",
      "Epoch [1/1], Step [5581/7635], Loss: 4.1545\n",
      "Epoch [1/1], Step [5582/7635], Loss: 4.2291\n",
      "Epoch [1/1], Step [5583/7635], Loss: 4.1651\n",
      "Epoch [1/1], Step [5584/7635], Loss: 4.1986\n",
      "Epoch [1/1], Step [5585/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [5586/7635], Loss: 4.0892\n",
      "Epoch [1/1], Step [5587/7635], Loss: 4.1607\n",
      "Epoch [1/1], Step [5588/7635], Loss: 4.1293\n",
      "Epoch [1/1], Step [5589/7635], Loss: 4.1259\n",
      "Epoch [1/1], Step [5590/7635], Loss: 4.2113\n",
      "Epoch [1/1], Step [5591/7635], Loss: 4.1639\n",
      "Epoch [1/1], Step [5592/7635], Loss: 4.1165\n",
      "Epoch [1/1], Step [5593/7635], Loss: 4.0977\n",
      "Epoch [1/1], Step [5594/7635], Loss: 4.2314\n",
      "Epoch [1/1], Step [5595/7635], Loss: 4.1145\n",
      "Epoch [1/1], Step [5596/7635], Loss: 4.1356\n",
      "Epoch [1/1], Step [5597/7635], Loss: 4.0958\n",
      "Epoch [1/1], Step [5598/7635], Loss: 4.1888\n",
      "Epoch [1/1], Step [5599/7635], Loss: 4.2329\n",
      "Epoch [1/1], Step [5600/7635], Loss: 4.1883\n",
      "Epoch [1/1], Step [5601/7635], Loss: 4.2607\n",
      "Epoch [1/1], Step [5602/7635], Loss: 4.1511\n",
      "Epoch [1/1], Step [5603/7635], Loss: 4.1533\n",
      "Epoch [1/1], Step [5604/7635], Loss: 4.1315\n",
      "Epoch [1/1], Step [5605/7635], Loss: 4.1858\n",
      "Epoch [1/1], Step [5606/7635], Loss: 4.1681\n",
      "Epoch [1/1], Step [5607/7635], Loss: 4.1247\n",
      "Epoch [1/1], Step [5608/7635], Loss: 4.1397\n",
      "Epoch [1/1], Step [5609/7635], Loss: 4.1634\n",
      "Epoch [1/1], Step [5610/7635], Loss: 4.0890\n",
      "Epoch [1/1], Step [5611/7635], Loss: 4.1166\n",
      "Epoch [1/1], Step [5612/7635], Loss: 4.1580\n",
      "Epoch [1/1], Step [5613/7635], Loss: 4.1613\n",
      "Epoch [1/1], Step [5614/7635], Loss: 4.0881\n",
      "Epoch [1/1], Step [5615/7635], Loss: 4.1513\n",
      "Epoch [1/1], Step [5616/7635], Loss: 4.3011\n",
      "Epoch [1/1], Step [5617/7635], Loss: 4.2112\n",
      "Epoch [1/1], Step [5618/7635], Loss: 4.1737\n",
      "Epoch [1/1], Step [5619/7635], Loss: 4.1201\n",
      "Epoch [1/1], Step [5620/7635], Loss: 4.0928\n",
      "Epoch [1/1], Step [5621/7635], Loss: 4.1655\n",
      "Epoch [1/1], Step [5622/7635], Loss: 4.1097\n",
      "Epoch [1/1], Step [5623/7635], Loss: 4.2014\n",
      "Epoch [1/1], Step [5624/7635], Loss: 4.1214\n",
      "Epoch [1/1], Step [5625/7635], Loss: 4.1926\n",
      "Epoch [1/1], Step [5626/7635], Loss: 4.1947\n",
      "Epoch [1/1], Step [5627/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [5628/7635], Loss: 4.2042\n",
      "Epoch [1/1], Step [5629/7635], Loss: 4.0841\n",
      "Epoch [1/1], Step [5630/7635], Loss: 4.2409\n",
      "Epoch [1/1], Step [5631/7635], Loss: 4.1575\n",
      "Epoch [1/1], Step [5632/7635], Loss: 4.0890\n",
      "Epoch [1/1], Step [5633/7635], Loss: 4.1355\n",
      "Epoch [1/1], Step [5634/7635], Loss: 4.2195\n",
      "Epoch [1/1], Step [5635/7635], Loss: 4.1454\n",
      "Epoch [1/1], Step [5636/7635], Loss: 4.0872\n",
      "Epoch [1/1], Step [5637/7635], Loss: 4.1440\n",
      "Epoch [1/1], Step [5638/7635], Loss: 4.0849\n",
      "Epoch [1/1], Step [5639/7635], Loss: 4.1753\n",
      "Epoch [1/1], Step [5640/7635], Loss: 4.1340\n",
      "Epoch [1/1], Step [5641/7635], Loss: 4.1393\n",
      "Epoch [1/1], Step [5642/7635], Loss: 4.0838\n",
      "Epoch [1/1], Step [5643/7635], Loss: 4.1451\n",
      "Epoch [1/1], Step [5644/7635], Loss: 4.2463\n",
      "Epoch [1/1], Step [5645/7635], Loss: 4.1767\n",
      "Epoch [1/1], Step [5646/7635], Loss: 4.2026\n",
      "Epoch [1/1], Step [5647/7635], Loss: 4.1556\n",
      "Epoch [1/1], Step [5648/7635], Loss: 4.0119\n",
      "Epoch [1/1], Step [5649/7635], Loss: 4.1967\n",
      "Epoch [1/1], Step [5650/7635], Loss: 4.1747\n",
      "Epoch [1/1], Step [5651/7635], Loss: 4.0988\n",
      "Epoch [1/1], Step [5652/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [5653/7635], Loss: 4.1824\n",
      "Epoch [1/1], Step [5654/7635], Loss: 4.0954\n",
      "Epoch [1/1], Step [5655/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [5656/7635], Loss: 4.1588\n",
      "Epoch [1/1], Step [5657/7635], Loss: 4.2890\n",
      "Epoch [1/1], Step [5658/7635], Loss: 4.1178\n",
      "Epoch [1/1], Step [5659/7635], Loss: 4.0933\n",
      "Epoch [1/1], Step [5660/7635], Loss: 4.1232\n",
      "Epoch [1/1], Step [5661/7635], Loss: 4.1262\n",
      "Epoch [1/1], Step [5662/7635], Loss: 4.1328\n",
      "Epoch [1/1], Step [5663/7635], Loss: 4.1543\n",
      "Epoch [1/1], Step [5664/7635], Loss: 4.1809\n",
      "Epoch [1/1], Step [5665/7635], Loss: 4.1271\n",
      "Epoch [1/1], Step [5666/7635], Loss: 4.1867\n",
      "Epoch [1/1], Step [5667/7635], Loss: 4.0702\n",
      "Epoch [1/1], Step [5668/7635], Loss: 4.1926\n",
      "Epoch [1/1], Step [5669/7635], Loss: 4.1676\n",
      "Epoch [1/1], Step [5670/7635], Loss: 4.1280\n",
      "Epoch [1/1], Step [5671/7635], Loss: 4.2327\n",
      "Epoch [1/1], Step [5672/7635], Loss: 4.1393\n",
      "Epoch [1/1], Step [5673/7635], Loss: 4.1327\n",
      "Epoch [1/1], Step [5674/7635], Loss: 4.1270\n",
      "Epoch [1/1], Step [5675/7635], Loss: 4.1246\n",
      "Epoch [1/1], Step [5676/7635], Loss: 4.2059\n",
      "Epoch [1/1], Step [5677/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [5678/7635], Loss: 4.2001\n",
      "Epoch [1/1], Step [5679/7635], Loss: 4.1867\n",
      "Epoch [1/1], Step [5680/7635], Loss: 4.1745\n",
      "Epoch [1/1], Step [5681/7635], Loss: 4.0750\n",
      "Epoch [1/1], Step [5682/7635], Loss: 4.1305\n",
      "Epoch [1/1], Step [5683/7635], Loss: 4.1289\n",
      "Epoch [1/1], Step [5684/7635], Loss: 4.1882\n",
      "Epoch [1/1], Step [5685/7635], Loss: 4.1055\n",
      "Epoch [1/1], Step [5686/7635], Loss: 4.0925\n",
      "Epoch [1/1], Step [5687/7635], Loss: 4.1683\n",
      "Epoch [1/1], Step [5688/7635], Loss: 4.1490\n",
      "Epoch [1/1], Step [5689/7635], Loss: 4.1910\n",
      "Epoch [1/1], Step [5690/7635], Loss: 4.1707\n",
      "Epoch [1/1], Step [5691/7635], Loss: 4.1604\n",
      "Epoch [1/1], Step [5692/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [5693/7635], Loss: 4.1318\n",
      "Epoch [1/1], Step [5694/7635], Loss: 4.1008\n",
      "Epoch [1/1], Step [5695/7635], Loss: 4.1075\n",
      "Epoch [1/1], Step [5696/7635], Loss: 4.2067\n",
      "Epoch [1/1], Step [5697/7635], Loss: 4.2056\n",
      "Epoch [1/1], Step [5698/7635], Loss: 4.1190\n",
      "Epoch [1/1], Step [5699/7635], Loss: 4.1663\n",
      "Epoch [1/1], Step [5700/7635], Loss: 4.1937\n",
      "Epoch [1/1], Step [5701/7635], Loss: 4.0378\n",
      "Epoch [1/1], Step [5702/7635], Loss: 4.1568\n",
      "Epoch [1/1], Step [5703/7635], Loss: 4.0776\n",
      "Epoch [1/1], Step [5704/7635], Loss: 4.1725\n",
      "Epoch [1/1], Step [5705/7635], Loss: 4.1967\n",
      "Epoch [1/1], Step [5706/7635], Loss: 4.1888\n",
      "Epoch [1/1], Step [5707/7635], Loss: 4.0949\n",
      "Epoch [1/1], Step [5708/7635], Loss: 4.1337\n",
      "Epoch [1/1], Step [5709/7635], Loss: 4.2319\n",
      "Epoch [1/1], Step [5710/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [5711/7635], Loss: 4.1218\n",
      "Epoch [1/1], Step [5712/7635], Loss: 4.1796\n",
      "Epoch [1/1], Step [5713/7635], Loss: 4.1369\n",
      "Epoch [1/1], Step [5714/7635], Loss: 4.1157\n",
      "Epoch [1/1], Step [5715/7635], Loss: 4.0897\n",
      "Epoch [1/1], Step [5716/7635], Loss: 4.1434\n",
      "Epoch [1/1], Step [5717/7635], Loss: 4.1634\n",
      "Epoch [1/1], Step [5718/7635], Loss: 4.1567\n",
      "Epoch [1/1], Step [5719/7635], Loss: 4.0485\n",
      "Epoch [1/1], Step [5720/7635], Loss: 4.2087\n",
      "Epoch [1/1], Step [5721/7635], Loss: 4.1431\n",
      "Epoch [1/1], Step [5722/7635], Loss: 4.1624\n",
      "Epoch [1/1], Step [5723/7635], Loss: 4.1343\n",
      "Epoch [1/1], Step [5724/7635], Loss: 4.2521\n",
      "Epoch [1/1], Step [5725/7635], Loss: 4.1058\n",
      "Epoch [1/1], Step [5726/7635], Loss: 4.1124\n",
      "Epoch [1/1], Step [5727/7635], Loss: 4.1582\n",
      "Epoch [1/1], Step [5728/7635], Loss: 4.1642\n",
      "Epoch [1/1], Step [5729/7635], Loss: 4.0910\n",
      "Epoch [1/1], Step [5730/7635], Loss: 4.1781\n",
      "Epoch [1/1], Step [5731/7635], Loss: 4.2330\n",
      "Epoch [1/1], Step [5732/7635], Loss: 4.1451\n",
      "Epoch [1/1], Step [5733/7635], Loss: 4.1676\n",
      "Epoch [1/1], Step [5734/7635], Loss: 4.2747\n",
      "Epoch [1/1], Step [5735/7635], Loss: 4.1610\n",
      "Epoch [1/1], Step [5736/7635], Loss: 4.1183\n",
      "Epoch [1/1], Step [5737/7635], Loss: 4.1803\n",
      "Epoch [1/1], Step [5738/7635], Loss: 4.1197\n",
      "Epoch [1/1], Step [5739/7635], Loss: 4.1441\n",
      "Epoch [1/1], Step [5740/7635], Loss: 4.1991\n",
      "Epoch [1/1], Step [5741/7635], Loss: 4.1240\n",
      "Epoch [1/1], Step [5742/7635], Loss: 4.1907\n",
      "Epoch [1/1], Step [5743/7635], Loss: 4.1819\n",
      "Epoch [1/1], Step [5744/7635], Loss: 4.0526\n",
      "Epoch [1/1], Step [5745/7635], Loss: 4.0853\n",
      "Epoch [1/1], Step [5746/7635], Loss: 4.0846\n",
      "Epoch [1/1], Step [5747/7635], Loss: 4.1219\n",
      "Epoch [1/1], Step [5748/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [5749/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [5750/7635], Loss: 4.1997\n",
      "Epoch [1/1], Step [5751/7635], Loss: 4.1202\n",
      "Epoch [1/1], Step [5752/7635], Loss: 4.1554\n",
      "Epoch [1/1], Step [5753/7635], Loss: 4.1204\n",
      "Epoch [1/1], Step [5754/7635], Loss: 4.2229\n",
      "Epoch [1/1], Step [5755/7635], Loss: 4.1649\n",
      "Epoch [1/1], Step [5756/7635], Loss: 4.1830\n",
      "Epoch [1/1], Step [5757/7635], Loss: 4.1303\n",
      "Epoch [1/1], Step [5758/7635], Loss: 4.2130\n",
      "Epoch [1/1], Step [5759/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [5760/7635], Loss: 4.1731\n",
      "Epoch [1/1], Step [5761/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [5762/7635], Loss: 4.0785\n",
      "Epoch [1/1], Step [5763/7635], Loss: 4.1859\n",
      "Epoch [1/1], Step [5764/7635], Loss: 4.1218\n",
      "Epoch [1/1], Step [5765/7635], Loss: 4.1198\n",
      "Epoch [1/1], Step [5766/7635], Loss: 4.1649\n",
      "Epoch [1/1], Step [5767/7635], Loss: 4.2007\n",
      "Epoch [1/1], Step [5768/7635], Loss: 4.1949\n",
      "Epoch [1/1], Step [5769/7635], Loss: 4.1411\n",
      "Epoch [1/1], Step [5770/7635], Loss: 4.1279\n",
      "Epoch [1/1], Step [5771/7635], Loss: 4.1329\n",
      "Epoch [1/1], Step [5772/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [5773/7635], Loss: 4.1482\n",
      "Epoch [1/1], Step [5774/7635], Loss: 4.1425\n",
      "Epoch [1/1], Step [5775/7635], Loss: 4.1687\n",
      "Epoch [1/1], Step [5776/7635], Loss: 4.1058\n",
      "Epoch [1/1], Step [5777/7635], Loss: 4.1039\n",
      "Epoch [1/1], Step [5778/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [5779/7635], Loss: 4.1604\n",
      "Epoch [1/1], Step [5780/7635], Loss: 4.0891\n",
      "Epoch [1/1], Step [5781/7635], Loss: 4.1375\n",
      "Epoch [1/1], Step [5782/7635], Loss: 4.1126\n",
      "Epoch [1/1], Step [5783/7635], Loss: 4.1212\n",
      "Epoch [1/1], Step [5784/7635], Loss: 4.1381\n",
      "Epoch [1/1], Step [5785/7635], Loss: 4.0860\n",
      "Epoch [1/1], Step [5786/7635], Loss: 4.1112\n",
      "Epoch [1/1], Step [5787/7635], Loss: 4.1592\n",
      "Epoch [1/1], Step [5788/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [5789/7635], Loss: 4.1540\n",
      "Epoch [1/1], Step [5790/7635], Loss: 4.1822\n",
      "Epoch [1/1], Step [5791/7635], Loss: 4.1781\n",
      "Epoch [1/1], Step [5792/7635], Loss: 4.0893\n",
      "Epoch [1/1], Step [5793/7635], Loss: 4.1671\n",
      "Epoch [1/1], Step [5794/7635], Loss: 4.1654\n",
      "Epoch [1/1], Step [5795/7635], Loss: 4.1991\n",
      "Epoch [1/1], Step [5796/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [5797/7635], Loss: 4.1005\n",
      "Epoch [1/1], Step [5798/7635], Loss: 4.1468\n",
      "Epoch [1/1], Step [5799/7635], Loss: 4.1372\n",
      "Epoch [1/1], Step [5800/7635], Loss: 4.2263\n",
      "Epoch [1/1], Step [5801/7635], Loss: 4.2229\n",
      "Epoch [1/1], Step [5802/7635], Loss: 4.1373\n",
      "Epoch [1/1], Step [5803/7635], Loss: 4.1564\n",
      "Epoch [1/1], Step [5804/7635], Loss: 4.1524\n",
      "Epoch [1/1], Step [5805/7635], Loss: 4.1254\n",
      "Epoch [1/1], Step [5806/7635], Loss: 4.1742\n",
      "Epoch [1/1], Step [5807/7635], Loss: 4.1610\n",
      "Epoch [1/1], Step [5808/7635], Loss: 4.0810\n",
      "Epoch [1/1], Step [5809/7635], Loss: 4.1639\n",
      "Epoch [1/1], Step [5810/7635], Loss: 4.0932\n",
      "Epoch [1/1], Step [5811/7635], Loss: 4.1729\n",
      "Epoch [1/1], Step [5812/7635], Loss: 4.1255\n",
      "Epoch [1/1], Step [5813/7635], Loss: 4.1826\n",
      "Epoch [1/1], Step [5814/7635], Loss: 4.0902\n",
      "Epoch [1/1], Step [5815/7635], Loss: 4.1656\n",
      "Epoch [1/1], Step [5816/7635], Loss: 4.0746\n",
      "Epoch [1/1], Step [5817/7635], Loss: 4.1087\n",
      "Epoch [1/1], Step [5818/7635], Loss: 4.0925\n",
      "Epoch [1/1], Step [5819/7635], Loss: 4.1802\n",
      "Epoch [1/1], Step [5820/7635], Loss: 4.1267\n",
      "Epoch [1/1], Step [5821/7635], Loss: 4.1431\n",
      "Epoch [1/1], Step [5822/7635], Loss: 4.1384\n",
      "Epoch [1/1], Step [5823/7635], Loss: 4.1881\n",
      "Epoch [1/1], Step [5824/7635], Loss: 4.1274\n",
      "Epoch [1/1], Step [5825/7635], Loss: 4.1514\n",
      "Epoch [1/1], Step [5826/7635], Loss: 4.1912\n",
      "Epoch [1/1], Step [5827/7635], Loss: 4.1772\n",
      "Epoch [1/1], Step [5828/7635], Loss: 4.2119\n",
      "Epoch [1/1], Step [5829/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [5830/7635], Loss: 4.1474\n",
      "Epoch [1/1], Step [5831/7635], Loss: 4.0944\n",
      "Epoch [1/1], Step [5832/7635], Loss: 4.1122\n",
      "Epoch [1/1], Step [5833/7635], Loss: 4.1668\n",
      "Epoch [1/1], Step [5834/7635], Loss: 4.1715\n",
      "Epoch [1/1], Step [5835/7635], Loss: 4.1233\n",
      "Epoch [1/1], Step [5836/7635], Loss: 4.0796\n",
      "Epoch [1/1], Step [5837/7635], Loss: 4.1193\n",
      "Epoch [1/1], Step [5838/7635], Loss: 4.0949\n",
      "Epoch [1/1], Step [5839/7635], Loss: 4.1454\n",
      "Epoch [1/1], Step [5840/7635], Loss: 4.1447\n",
      "Epoch [1/1], Step [5841/7635], Loss: 4.0762\n",
      "Epoch [1/1], Step [5842/7635], Loss: 4.1674\n",
      "Epoch [1/1], Step [5843/7635], Loss: 4.0965\n",
      "Epoch [1/1], Step [5844/7635], Loss: 4.1329\n",
      "Epoch [1/1], Step [5845/7635], Loss: 4.1350\n",
      "Epoch [1/1], Step [5846/7635], Loss: 4.1512\n",
      "Epoch [1/1], Step [5847/7635], Loss: 4.1512\n",
      "Epoch [1/1], Step [5848/7635], Loss: 4.1639\n",
      "Epoch [1/1], Step [5849/7635], Loss: 4.0848\n",
      "Epoch [1/1], Step [5850/7635], Loss: 4.1687\n",
      "Epoch [1/1], Step [5851/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [5852/7635], Loss: 4.1443\n",
      "Epoch [1/1], Step [5853/7635], Loss: 4.1602\n",
      "Epoch [1/1], Step [5854/7635], Loss: 4.1184\n",
      "Epoch [1/1], Step [5855/7635], Loss: 4.1651\n",
      "Epoch [1/1], Step [5856/7635], Loss: 4.1546\n",
      "Epoch [1/1], Step [5857/7635], Loss: 4.0757\n",
      "Epoch [1/1], Step [5858/7635], Loss: 4.1781\n",
      "Epoch [1/1], Step [5859/7635], Loss: 4.1322\n",
      "Epoch [1/1], Step [5860/7635], Loss: 4.0943\n",
      "Epoch [1/1], Step [5861/7635], Loss: 4.1682\n",
      "Epoch [1/1], Step [5862/7635], Loss: 4.1016\n",
      "Epoch [1/1], Step [5863/7635], Loss: 4.1438\n",
      "Epoch [1/1], Step [5864/7635], Loss: 4.1326\n",
      "Epoch [1/1], Step [5865/7635], Loss: 4.2586\n",
      "Epoch [1/1], Step [5866/7635], Loss: 4.1306\n",
      "Epoch [1/1], Step [5867/7635], Loss: 4.1739\n",
      "Epoch [1/1], Step [5868/7635], Loss: 4.1531\n",
      "Epoch [1/1], Step [5869/7635], Loss: 4.2044\n",
      "Epoch [1/1], Step [5870/7635], Loss: 4.1497\n",
      "Epoch [1/1], Step [5871/7635], Loss: 4.0930\n",
      "Epoch [1/1], Step [5872/7635], Loss: 4.1365\n",
      "Epoch [1/1], Step [5873/7635], Loss: 4.1537\n",
      "Epoch [1/1], Step [5874/7635], Loss: 4.0288\n",
      "Epoch [1/1], Step [5875/7635], Loss: 4.0766\n",
      "Epoch [1/1], Step [5876/7635], Loss: 4.1249\n",
      "Epoch [1/1], Step [5877/7635], Loss: 4.1877\n",
      "Epoch [1/1], Step [5878/7635], Loss: 4.1361\n",
      "Epoch [1/1], Step [5879/7635], Loss: 4.0897\n",
      "Epoch [1/1], Step [5880/7635], Loss: 4.2213\n",
      "Epoch [1/1], Step [5881/7635], Loss: 4.1490\n",
      "Epoch [1/1], Step [5882/7635], Loss: 4.1209\n",
      "Epoch [1/1], Step [5883/7635], Loss: 4.0930\n",
      "Epoch [1/1], Step [5884/7635], Loss: 4.1898\n",
      "Epoch [1/1], Step [5885/7635], Loss: 4.0850\n",
      "Epoch [1/1], Step [5886/7635], Loss: 4.1678\n",
      "Epoch [1/1], Step [5887/7635], Loss: 4.2305\n",
      "Epoch [1/1], Step [5888/7635], Loss: 4.1708\n",
      "Epoch [1/1], Step [5889/7635], Loss: 4.1120\n",
      "Epoch [1/1], Step [5890/7635], Loss: 4.1692\n",
      "Epoch [1/1], Step [5891/7635], Loss: 4.1035\n",
      "Epoch [1/1], Step [5892/7635], Loss: 4.2727\n",
      "Epoch [1/1], Step [5893/7635], Loss: 4.1472\n",
      "Epoch [1/1], Step [5894/7635], Loss: 4.1181\n",
      "Epoch [1/1], Step [5895/7635], Loss: 4.1814\n",
      "Epoch [1/1], Step [5896/7635], Loss: 4.1970\n",
      "Epoch [1/1], Step [5897/7635], Loss: 4.1588\n",
      "Epoch [1/1], Step [5898/7635], Loss: 4.0773\n",
      "Epoch [1/1], Step [5899/7635], Loss: 4.1610\n",
      "Epoch [1/1], Step [5900/7635], Loss: 4.2129\n",
      "Epoch [1/1], Step [5901/7635], Loss: 4.1472\n",
      "Epoch [1/1], Step [5902/7635], Loss: 4.1830\n",
      "Epoch [1/1], Step [5903/7635], Loss: 4.2156\n",
      "Epoch [1/1], Step [5904/7635], Loss: 4.1280\n",
      "Epoch [1/1], Step [5905/7635], Loss: 4.1112\n",
      "Epoch [1/1], Step [5906/7635], Loss: 4.0841\n",
      "Epoch [1/1], Step [5907/7635], Loss: 4.1383\n",
      "Epoch [1/1], Step [5908/7635], Loss: 4.1441\n",
      "Epoch [1/1], Step [5909/7635], Loss: 4.1777\n",
      "Epoch [1/1], Step [5910/7635], Loss: 4.1101\n",
      "Epoch [1/1], Step [5911/7635], Loss: 4.0805\n",
      "Epoch [1/1], Step [5912/7635], Loss: 4.1363\n",
      "Epoch [1/1], Step [5913/7635], Loss: 4.0720\n",
      "Epoch [1/1], Step [5914/7635], Loss: 4.0686\n",
      "Epoch [1/1], Step [5915/7635], Loss: 4.1285\n",
      "Epoch [1/1], Step [5916/7635], Loss: 4.1668\n",
      "Epoch [1/1], Step [5917/7635], Loss: 4.1038\n",
      "Epoch [1/1], Step [5918/7635], Loss: 4.1343\n",
      "Epoch [1/1], Step [5919/7635], Loss: 4.1185\n",
      "Epoch [1/1], Step [5920/7635], Loss: 4.2476\n",
      "Epoch [1/1], Step [5921/7635], Loss: 4.1005\n",
      "Epoch [1/1], Step [5922/7635], Loss: 4.2326\n",
      "Epoch [1/1], Step [5923/7635], Loss: 4.0919\n",
      "Epoch [1/1], Step [5924/7635], Loss: 4.1127\n",
      "Epoch [1/1], Step [5925/7635], Loss: 4.1418\n",
      "Epoch [1/1], Step [5926/7635], Loss: 4.1836\n",
      "Epoch [1/1], Step [5927/7635], Loss: 4.1424\n",
      "Epoch [1/1], Step [5928/7635], Loss: 4.1468\n",
      "Epoch [1/1], Step [5929/7635], Loss: 4.1480\n",
      "Epoch [1/1], Step [5930/7635], Loss: 4.2099\n",
      "Epoch [1/1], Step [5931/7635], Loss: 4.2490\n",
      "Epoch [1/1], Step [5932/7635], Loss: 4.1499\n",
      "Epoch [1/1], Step [5933/7635], Loss: 4.1133\n",
      "Epoch [1/1], Step [5934/7635], Loss: 4.1928\n",
      "Epoch [1/1], Step [5935/7635], Loss: 4.1710\n",
      "Epoch [1/1], Step [5936/7635], Loss: 4.1639\n",
      "Epoch [1/1], Step [5937/7635], Loss: 4.1295\n",
      "Epoch [1/1], Step [5938/7635], Loss: 4.1526\n",
      "Epoch [1/1], Step [5939/7635], Loss: 4.0880\n",
      "Epoch [1/1], Step [5940/7635], Loss: 4.1158\n",
      "Epoch [1/1], Step [5941/7635], Loss: 4.0806\n",
      "Epoch [1/1], Step [5942/7635], Loss: 4.1162\n",
      "Epoch [1/1], Step [5943/7635], Loss: 4.1097\n",
      "Epoch [1/1], Step [5944/7635], Loss: 4.1416\n",
      "Epoch [1/1], Step [5945/7635], Loss: 4.2040\n",
      "Epoch [1/1], Step [5946/7635], Loss: 4.1228\n",
      "Epoch [1/1], Step [5947/7635], Loss: 4.1014\n",
      "Epoch [1/1], Step [5948/7635], Loss: 4.0947\n",
      "Epoch [1/1], Step [5949/7635], Loss: 4.0927\n",
      "Epoch [1/1], Step [5950/7635], Loss: 4.1599\n",
      "Epoch [1/1], Step [5951/7635], Loss: 4.1394\n",
      "Epoch [1/1], Step [5952/7635], Loss: 4.1277\n",
      "Epoch [1/1], Step [5953/7635], Loss: 4.1659\n",
      "Epoch [1/1], Step [5954/7635], Loss: 4.1258\n",
      "Epoch [1/1], Step [5955/7635], Loss: 4.0895\n",
      "Epoch [1/1], Step [5956/7635], Loss: 4.1391\n",
      "Epoch [1/1], Step [5957/7635], Loss: 4.1348\n",
      "Epoch [1/1], Step [5958/7635], Loss: 4.2079\n",
      "Epoch [1/1], Step [5959/7635], Loss: 4.1221\n",
      "Epoch [1/1], Step [5960/7635], Loss: 4.1411\n",
      "Epoch [1/1], Step [5961/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [5962/7635], Loss: 4.1531\n",
      "Epoch [1/1], Step [5963/7635], Loss: 4.1523\n",
      "Epoch [1/1], Step [5964/7635], Loss: 4.1415\n",
      "Epoch [1/1], Step [5965/7635], Loss: 4.1992\n",
      "Epoch [1/1], Step [5966/7635], Loss: 4.1906\n",
      "Epoch [1/1], Step [5967/7635], Loss: 4.0746\n",
      "Epoch [1/1], Step [5968/7635], Loss: 4.1825\n",
      "Epoch [1/1], Step [5969/7635], Loss: 4.1302\n",
      "Epoch [1/1], Step [5970/7635], Loss: 4.1150\n",
      "Epoch [1/1], Step [5971/7635], Loss: 4.1613\n",
      "Epoch [1/1], Step [5972/7635], Loss: 4.1909\n",
      "Epoch [1/1], Step [5973/7635], Loss: 4.1032\n",
      "Epoch [1/1], Step [5974/7635], Loss: 4.1154\n",
      "Epoch [1/1], Step [5975/7635], Loss: 4.0557\n",
      "Epoch [1/1], Step [5976/7635], Loss: 4.1547\n",
      "Epoch [1/1], Step [5977/7635], Loss: 4.1130\n",
      "Epoch [1/1], Step [5978/7635], Loss: 4.1452\n",
      "Epoch [1/1], Step [5979/7635], Loss: 4.1789\n",
      "Epoch [1/1], Step [5980/7635], Loss: 4.1594\n",
      "Epoch [1/1], Step [5981/7635], Loss: 4.0551\n",
      "Epoch [1/1], Step [5982/7635], Loss: 4.1509\n",
      "Epoch [1/1], Step [5983/7635], Loss: 4.1918\n",
      "Epoch [1/1], Step [5984/7635], Loss: 4.1199\n",
      "Epoch [1/1], Step [5985/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [5986/7635], Loss: 4.0686\n",
      "Epoch [1/1], Step [5987/7635], Loss: 4.0884\n",
      "Epoch [1/1], Step [5988/7635], Loss: 4.1624\n",
      "Epoch [1/1], Step [5989/7635], Loss: 4.0894\n",
      "Epoch [1/1], Step [5990/7635], Loss: 4.1547\n",
      "Epoch [1/1], Step [5991/7635], Loss: 4.1655\n",
      "Epoch [1/1], Step [5992/7635], Loss: 4.1570\n",
      "Epoch [1/1], Step [5993/7635], Loss: 4.0712\n",
      "Epoch [1/1], Step [5994/7635], Loss: 4.1695\n",
      "Epoch [1/1], Step [5995/7635], Loss: 4.1575\n",
      "Epoch [1/1], Step [5996/7635], Loss: 4.1107\n",
      "Epoch [1/1], Step [5997/7635], Loss: 4.1037\n",
      "Epoch [1/1], Step [5998/7635], Loss: 4.1290\n",
      "Epoch [1/1], Step [5999/7635], Loss: 4.1164\n",
      "Epoch [1/1], Step [6000/7635], Loss: 4.2049\n",
      "Epoch [1/1], Step [6001/7635], Loss: 4.1583\n",
      "Epoch [1/1], Step [6002/7635], Loss: 4.1412\n",
      "Epoch [1/1], Step [6003/7635], Loss: 4.1185\n",
      "Epoch [1/1], Step [6004/7635], Loss: 4.1051\n",
      "Epoch [1/1], Step [6005/7635], Loss: 4.0734\n",
      "Epoch [1/1], Step [6006/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [6007/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [6008/7635], Loss: 4.0528\n",
      "Epoch [1/1], Step [6009/7635], Loss: 4.1111\n",
      "Epoch [1/1], Step [6010/7635], Loss: 4.1604\n",
      "Epoch [1/1], Step [6011/7635], Loss: 4.1116\n",
      "Epoch [1/1], Step [6012/7635], Loss: 4.1161\n",
      "Epoch [1/1], Step [6013/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [6014/7635], Loss: 4.1103\n",
      "Epoch [1/1], Step [6015/7635], Loss: 4.0962\n",
      "Epoch [1/1], Step [6016/7635], Loss: 4.0842\n",
      "Epoch [1/1], Step [6017/7635], Loss: 4.1323\n",
      "Epoch [1/1], Step [6018/7635], Loss: 4.2023\n",
      "Epoch [1/1], Step [6019/7635], Loss: 4.0891\n",
      "Epoch [1/1], Step [6020/7635], Loss: 4.1208\n",
      "Epoch [1/1], Step [6021/7635], Loss: 4.1051\n",
      "Epoch [1/1], Step [6022/7635], Loss: 4.1468\n",
      "Epoch [1/1], Step [6023/7635], Loss: 4.1754\n",
      "Epoch [1/1], Step [6024/7635], Loss: 4.1236\n",
      "Epoch [1/1], Step [6025/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [6026/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [6027/7635], Loss: 4.1843\n",
      "Epoch [1/1], Step [6028/7635], Loss: 4.1465\n",
      "Epoch [1/1], Step [6029/7635], Loss: 4.2119\n",
      "Epoch [1/1], Step [6030/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [6031/7635], Loss: 4.1814\n",
      "Epoch [1/1], Step [6032/7635], Loss: 4.1055\n",
      "Epoch [1/1], Step [6033/7635], Loss: 4.0853\n",
      "Epoch [1/1], Step [6034/7635], Loss: 4.1412\n",
      "Epoch [1/1], Step [6035/7635], Loss: 4.1924\n",
      "Epoch [1/1], Step [6036/7635], Loss: 4.1829\n",
      "Epoch [1/1], Step [6037/7635], Loss: 4.1483\n",
      "Epoch [1/1], Step [6038/7635], Loss: 4.1731\n",
      "Epoch [1/1], Step [6039/7635], Loss: 4.1837\n",
      "Epoch [1/1], Step [6040/7635], Loss: 4.0655\n",
      "Epoch [1/1], Step [6041/7635], Loss: 4.1626\n",
      "Epoch [1/1], Step [6042/7635], Loss: 4.1655\n",
      "Epoch [1/1], Step [6043/7635], Loss: 4.1083\n",
      "Epoch [1/1], Step [6044/7635], Loss: 4.1554\n",
      "Epoch [1/1], Step [6045/7635], Loss: 4.1098\n",
      "Epoch [1/1], Step [6046/7635], Loss: 4.1428\n",
      "Epoch [1/1], Step [6047/7635], Loss: 4.1873\n",
      "Epoch [1/1], Step [6048/7635], Loss: 4.1560\n",
      "Epoch [1/1], Step [6049/7635], Loss: 4.0951\n",
      "Epoch [1/1], Step [6050/7635], Loss: 4.0744\n",
      "Epoch [1/1], Step [6051/7635], Loss: 4.1306\n",
      "Epoch [1/1], Step [6052/7635], Loss: 4.1099\n",
      "Epoch [1/1], Step [6053/7635], Loss: 4.1567\n",
      "Epoch [1/1], Step [6054/7635], Loss: 4.1237\n",
      "Epoch [1/1], Step [6055/7635], Loss: 4.0860\n",
      "Epoch [1/1], Step [6056/7635], Loss: 4.1138\n",
      "Epoch [1/1], Step [6057/7635], Loss: 4.1586\n",
      "Epoch [1/1], Step [6058/7635], Loss: 4.0798\n",
      "Epoch [1/1], Step [6059/7635], Loss: 4.0456\n",
      "Epoch [1/1], Step [6060/7635], Loss: 4.1884\n",
      "Epoch [1/1], Step [6061/7635], Loss: 4.0982\n",
      "Epoch [1/1], Step [6062/7635], Loss: 4.1056\n",
      "Epoch [1/1], Step [6063/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [6064/7635], Loss: 4.0630\n",
      "Epoch [1/1], Step [6065/7635], Loss: 4.0662\n",
      "Epoch [1/1], Step [6066/7635], Loss: 4.1572\n",
      "Epoch [1/1], Step [6067/7635], Loss: 4.1323\n",
      "Epoch [1/1], Step [6068/7635], Loss: 4.0813\n",
      "Epoch [1/1], Step [6069/7635], Loss: 4.1052\n",
      "Epoch [1/1], Step [6070/7635], Loss: 4.1646\n",
      "Epoch [1/1], Step [6071/7635], Loss: 4.1792\n",
      "Epoch [1/1], Step [6072/7635], Loss: 4.1242\n",
      "Epoch [1/1], Step [6073/7635], Loss: 4.0694\n",
      "Epoch [1/1], Step [6074/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [6075/7635], Loss: 4.1244\n",
      "Epoch [1/1], Step [6076/7635], Loss: 4.1300\n",
      "Epoch [1/1], Step [6077/7635], Loss: 4.1171\n",
      "Epoch [1/1], Step [6078/7635], Loss: 4.0686\n",
      "Epoch [1/1], Step [6079/7635], Loss: 4.2217\n",
      "Epoch [1/1], Step [6080/7635], Loss: 4.1560\n",
      "Epoch [1/1], Step [6081/7635], Loss: 4.1797\n",
      "Epoch [1/1], Step [6082/7635], Loss: 4.0980\n",
      "Epoch [1/1], Step [6083/7635], Loss: 4.1359\n",
      "Epoch [1/1], Step [6084/7635], Loss: 4.1045\n",
      "Epoch [1/1], Step [6085/7635], Loss: 4.1204\n",
      "Epoch [1/1], Step [6086/7635], Loss: 4.1688\n",
      "Epoch [1/1], Step [6087/7635], Loss: 4.1387\n",
      "Epoch [1/1], Step [6088/7635], Loss: 4.1252\n",
      "Epoch [1/1], Step [6089/7635], Loss: 4.0984\n",
      "Epoch [1/1], Step [6090/7635], Loss: 4.1354\n",
      "Epoch [1/1], Step [6091/7635], Loss: 4.1494\n",
      "Epoch [1/1], Step [6092/7635], Loss: 4.2144\n",
      "Epoch [1/1], Step [6093/7635], Loss: 4.1294\n",
      "Epoch [1/1], Step [6094/7635], Loss: 4.0986\n",
      "Epoch [1/1], Step [6095/7635], Loss: 4.1227\n",
      "Epoch [1/1], Step [6096/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [6097/7635], Loss: 4.1286\n",
      "Epoch [1/1], Step [6098/7635], Loss: 4.1168\n",
      "Epoch [1/1], Step [6099/7635], Loss: 4.1420\n",
      "Epoch [1/1], Step [6100/7635], Loss: 4.1065\n",
      "Epoch [1/1], Step [6101/7635], Loss: 4.0591\n",
      "Epoch [1/1], Step [6102/7635], Loss: 4.1026\n",
      "Epoch [1/1], Step [6103/7635], Loss: 4.1884\n",
      "Epoch [1/1], Step [6104/7635], Loss: 4.1320\n",
      "Epoch [1/1], Step [6105/7635], Loss: 4.1847\n",
      "Epoch [1/1], Step [6106/7635], Loss: 4.1997\n",
      "Epoch [1/1], Step [6107/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [6108/7635], Loss: 4.1709\n",
      "Epoch [1/1], Step [6109/7635], Loss: 4.0831\n",
      "Epoch [1/1], Step [6110/7635], Loss: 4.1082\n",
      "Epoch [1/1], Step [6111/7635], Loss: 4.1384\n",
      "Epoch [1/1], Step [6112/7635], Loss: 4.1272\n",
      "Epoch [1/1], Step [6113/7635], Loss: 4.1489\n",
      "Epoch [1/1], Step [6114/7635], Loss: 4.1231\n",
      "Epoch [1/1], Step [6115/7635], Loss: 4.1742\n",
      "Epoch [1/1], Step [6116/7635], Loss: 4.0657\n",
      "Epoch [1/1], Step [6117/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [6118/7635], Loss: 4.0932\n",
      "Epoch [1/1], Step [6119/7635], Loss: 4.1586\n",
      "Epoch [1/1], Step [6120/7635], Loss: 4.1167\n",
      "Epoch [1/1], Step [6121/7635], Loss: 4.0927\n",
      "Epoch [1/1], Step [6122/7635], Loss: 4.1383\n",
      "Epoch [1/1], Step [6123/7635], Loss: 4.1297\n",
      "Epoch [1/1], Step [6124/7635], Loss: 4.1690\n",
      "Epoch [1/1], Step [6125/7635], Loss: 4.1153\n",
      "Epoch [1/1], Step [6126/7635], Loss: 4.0362\n",
      "Epoch [1/1], Step [6127/7635], Loss: 4.0960\n",
      "Epoch [1/1], Step [6128/7635], Loss: 4.1674\n",
      "Epoch [1/1], Step [6129/7635], Loss: 4.1289\n",
      "Epoch [1/1], Step [6130/7635], Loss: 4.1090\n",
      "Epoch [1/1], Step [6131/7635], Loss: 4.1378\n",
      "Epoch [1/1], Step [6132/7635], Loss: 4.0596\n",
      "Epoch [1/1], Step [6133/7635], Loss: 4.1212\n",
      "Epoch [1/1], Step [6134/7635], Loss: 4.1293\n",
      "Epoch [1/1], Step [6135/7635], Loss: 4.0742\n",
      "Epoch [1/1], Step [6136/7635], Loss: 4.1274\n",
      "Epoch [1/1], Step [6137/7635], Loss: 4.1789\n",
      "Epoch [1/1], Step [6138/7635], Loss: 4.0899\n",
      "Epoch [1/1], Step [6139/7635], Loss: 4.0890\n",
      "Epoch [1/1], Step [6140/7635], Loss: 4.0967\n",
      "Epoch [1/1], Step [6141/7635], Loss: 4.1171\n",
      "Epoch [1/1], Step [6142/7635], Loss: 4.1171\n",
      "Epoch [1/1], Step [6143/7635], Loss: 4.0984\n",
      "Epoch [1/1], Step [6144/7635], Loss: 4.1317\n",
      "Epoch [1/1], Step [6145/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [6146/7635], Loss: 4.1212\n",
      "Epoch [1/1], Step [6147/7635], Loss: 4.1325\n",
      "Epoch [1/1], Step [6148/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [6149/7635], Loss: 4.0960\n",
      "Epoch [1/1], Step [6150/7635], Loss: 4.0707\n",
      "Epoch [1/1], Step [6151/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [6152/7635], Loss: 4.1015\n",
      "Epoch [1/1], Step [6153/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [6154/7635], Loss: 4.1055\n",
      "Epoch [1/1], Step [6155/7635], Loss: 4.0984\n",
      "Epoch [1/1], Step [6156/7635], Loss: 4.1948\n",
      "Epoch [1/1], Step [6157/7635], Loss: 4.1571\n",
      "Epoch [1/1], Step [6158/7635], Loss: 4.1123\n",
      "Epoch [1/1], Step [6159/7635], Loss: 4.1540\n",
      "Epoch [1/1], Step [6160/7635], Loss: 4.1117\n",
      "Epoch [1/1], Step [6161/7635], Loss: 4.0348\n",
      "Epoch [1/1], Step [6162/7635], Loss: 4.1140\n",
      "Epoch [1/1], Step [6163/7635], Loss: 4.1262\n",
      "Epoch [1/1], Step [6164/7635], Loss: 4.1252\n",
      "Epoch [1/1], Step [6165/7635], Loss: 4.2029\n",
      "Epoch [1/1], Step [6166/7635], Loss: 4.2202\n",
      "Epoch [1/1], Step [6167/7635], Loss: 4.1437\n",
      "Epoch [1/1], Step [6168/7635], Loss: 4.1080\n",
      "Epoch [1/1], Step [6169/7635], Loss: 4.1315\n",
      "Epoch [1/1], Step [6170/7635], Loss: 4.1021\n",
      "Epoch [1/1], Step [6171/7635], Loss: 4.0581\n",
      "Epoch [1/1], Step [6172/7635], Loss: 4.1145\n",
      "Epoch [1/1], Step [6173/7635], Loss: 4.2653\n",
      "Epoch [1/1], Step [6174/7635], Loss: 4.1038\n",
      "Epoch [1/1], Step [6175/7635], Loss: 4.0560\n",
      "Epoch [1/1], Step [6176/7635], Loss: 4.0790\n",
      "Epoch [1/1], Step [6177/7635], Loss: 4.1088\n",
      "Epoch [1/1], Step [6178/7635], Loss: 4.0675\n",
      "Epoch [1/1], Step [6179/7635], Loss: 4.1874\n",
      "Epoch [1/1], Step [6180/7635], Loss: 4.0735\n",
      "Epoch [1/1], Step [6181/7635], Loss: 4.0802\n",
      "Epoch [1/1], Step [6182/7635], Loss: 4.1464\n",
      "Epoch [1/1], Step [6183/7635], Loss: 4.1432\n",
      "Epoch [1/1], Step [6184/7635], Loss: 4.0677\n",
      "Epoch [1/1], Step [6185/7635], Loss: 4.0678\n",
      "Epoch [1/1], Step [6186/7635], Loss: 4.1570\n",
      "Epoch [1/1], Step [6187/7635], Loss: 4.1506\n",
      "Epoch [1/1], Step [6188/7635], Loss: 4.1186\n",
      "Epoch [1/1], Step [6189/7635], Loss: 4.0295\n",
      "Epoch [1/1], Step [6190/7635], Loss: 4.1198\n",
      "Epoch [1/1], Step [6191/7635], Loss: 4.1141\n",
      "Epoch [1/1], Step [6192/7635], Loss: 4.0809\n",
      "Epoch [1/1], Step [6193/7635], Loss: 4.1038\n",
      "Epoch [1/1], Step [6194/7635], Loss: 4.1751\n",
      "Epoch [1/1], Step [6195/7635], Loss: 4.1131\n",
      "Epoch [1/1], Step [6196/7635], Loss: 4.2123\n",
      "Epoch [1/1], Step [6197/7635], Loss: 4.0990\n",
      "Epoch [1/1], Step [6198/7635], Loss: 4.1109\n",
      "Epoch [1/1], Step [6199/7635], Loss: 4.1173\n",
      "Epoch [1/1], Step [6200/7635], Loss: 4.1033\n",
      "Epoch [1/1], Step [6201/7635], Loss: 4.0606\n",
      "Epoch [1/1], Step [6202/7635], Loss: 4.0856\n",
      "Epoch [1/1], Step [6203/7635], Loss: 4.2301\n",
      "Epoch [1/1], Step [6204/7635], Loss: 4.2229\n",
      "Epoch [1/1], Step [6205/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [6206/7635], Loss: 4.1852\n",
      "Epoch [1/1], Step [6207/7635], Loss: 4.2066\n",
      "Epoch [1/1], Step [6208/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [6209/7635], Loss: 4.1381\n",
      "Epoch [1/1], Step [6210/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [6211/7635], Loss: 4.1121\n",
      "Epoch [1/1], Step [6212/7635], Loss: 4.1297\n",
      "Epoch [1/1], Step [6213/7635], Loss: 4.0730\n",
      "Epoch [1/1], Step [6214/7635], Loss: 4.1114\n",
      "Epoch [1/1], Step [6215/7635], Loss: 4.1292\n",
      "Epoch [1/1], Step [6216/7635], Loss: 4.0805\n",
      "Epoch [1/1], Step [6217/7635], Loss: 4.0907\n",
      "Epoch [1/1], Step [6218/7635], Loss: 4.1291\n",
      "Epoch [1/1], Step [6219/7635], Loss: 4.1051\n",
      "Epoch [1/1], Step [6220/7635], Loss: 4.1408\n",
      "Epoch [1/1], Step [6221/7635], Loss: 4.1022\n",
      "Epoch [1/1], Step [6222/7635], Loss: 4.0793\n",
      "Epoch [1/1], Step [6223/7635], Loss: 4.1515\n",
      "Epoch [1/1], Step [6224/7635], Loss: 4.1206\n",
      "Epoch [1/1], Step [6225/7635], Loss: 4.1191\n",
      "Epoch [1/1], Step [6226/7635], Loss: 4.1662\n",
      "Epoch [1/1], Step [6227/7635], Loss: 4.1009\n",
      "Epoch [1/1], Step [6228/7635], Loss: 4.0942\n",
      "Epoch [1/1], Step [6229/7635], Loss: 4.1180\n",
      "Epoch [1/1], Step [6230/7635], Loss: 4.1601\n",
      "Epoch [1/1], Step [6231/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [6232/7635], Loss: 4.0933\n",
      "Epoch [1/1], Step [6233/7635], Loss: 4.0925\n",
      "Epoch [1/1], Step [6234/7635], Loss: 4.1359\n",
      "Epoch [1/1], Step [6235/7635], Loss: 4.1804\n",
      "Epoch [1/1], Step [6236/7635], Loss: 4.1160\n",
      "Epoch [1/1], Step [6237/7635], Loss: 4.1362\n",
      "Epoch [1/1], Step [6238/7635], Loss: 4.1442\n",
      "Epoch [1/1], Step [6239/7635], Loss: 4.0942\n",
      "Epoch [1/1], Step [6240/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [6241/7635], Loss: 4.1312\n",
      "Epoch [1/1], Step [6242/7635], Loss: 4.2093\n",
      "Epoch [1/1], Step [6243/7635], Loss: 4.1450\n",
      "Epoch [1/1], Step [6244/7635], Loss: 4.1853\n",
      "Epoch [1/1], Step [6245/7635], Loss: 4.1654\n",
      "Epoch [1/1], Step [6246/7635], Loss: 4.1370\n",
      "Epoch [1/1], Step [6247/7635], Loss: 4.1971\n",
      "Epoch [1/1], Step [6248/7635], Loss: 4.1489\n",
      "Epoch [1/1], Step [6249/7635], Loss: 4.1527\n",
      "Epoch [1/1], Step [6250/7635], Loss: 4.0831\n",
      "Epoch [1/1], Step [6251/7635], Loss: 4.1479\n",
      "Epoch [1/1], Step [6252/7635], Loss: 4.1529\n",
      "Epoch [1/1], Step [6253/7635], Loss: 4.1399\n",
      "Epoch [1/1], Step [6254/7635], Loss: 4.1207\n",
      "Epoch [1/1], Step [6255/7635], Loss: 4.1085\n",
      "Epoch [1/1], Step [6256/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [6257/7635], Loss: 4.1076\n",
      "Epoch [1/1], Step [6258/7635], Loss: 4.0917\n",
      "Epoch [1/1], Step [6259/7635], Loss: 4.1112\n",
      "Epoch [1/1], Step [6260/7635], Loss: 4.0417\n",
      "Epoch [1/1], Step [6261/7635], Loss: 4.1679\n",
      "Epoch [1/1], Step [6262/7635], Loss: 4.1144\n",
      "Epoch [1/1], Step [6263/7635], Loss: 4.1220\n",
      "Epoch [1/1], Step [6264/7635], Loss: 4.0839\n",
      "Epoch [1/1], Step [6265/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [6266/7635], Loss: 4.0778\n",
      "Epoch [1/1], Step [6267/7635], Loss: 4.0178\n",
      "Epoch [1/1], Step [6268/7635], Loss: 4.2100\n",
      "Epoch [1/1], Step [6269/7635], Loss: 4.2162\n",
      "Epoch [1/1], Step [6270/7635], Loss: 4.0724\n",
      "Epoch [1/1], Step [6271/7635], Loss: 4.1685\n",
      "Epoch [1/1], Step [6272/7635], Loss: 4.0433\n",
      "Epoch [1/1], Step [6273/7635], Loss: 4.1583\n",
      "Epoch [1/1], Step [6274/7635], Loss: 4.1447\n",
      "Epoch [1/1], Step [6275/7635], Loss: 4.1422\n",
      "Epoch [1/1], Step [6276/7635], Loss: 4.1571\n",
      "Epoch [1/1], Step [6277/7635], Loss: 4.0917\n",
      "Epoch [1/1], Step [6278/7635], Loss: 4.1062\n",
      "Epoch [1/1], Step [6279/7635], Loss: 4.1532\n",
      "Epoch [1/1], Step [6280/7635], Loss: 4.1274\n",
      "Epoch [1/1], Step [6281/7635], Loss: 4.1578\n",
      "Epoch [1/1], Step [6282/7635], Loss: 4.1763\n",
      "Epoch [1/1], Step [6283/7635], Loss: 4.0791\n",
      "Epoch [1/1], Step [6284/7635], Loss: 4.1266\n",
      "Epoch [1/1], Step [6285/7635], Loss: 4.1344\n",
      "Epoch [1/1], Step [6286/7635], Loss: 4.1713\n",
      "Epoch [1/1], Step [6287/7635], Loss: 4.1883\n",
      "Epoch [1/1], Step [6288/7635], Loss: 4.0670\n",
      "Epoch [1/1], Step [6289/7635], Loss: 4.0866\n",
      "Epoch [1/1], Step [6290/7635], Loss: 4.1788\n",
      "Epoch [1/1], Step [6291/7635], Loss: 4.0982\n",
      "Epoch [1/1], Step [6292/7635], Loss: 4.1457\n",
      "Epoch [1/1], Step [6293/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [6294/7635], Loss: 4.1169\n",
      "Epoch [1/1], Step [6295/7635], Loss: 4.1511\n",
      "Epoch [1/1], Step [6296/7635], Loss: 4.1712\n",
      "Epoch [1/1], Step [6297/7635], Loss: 4.1339\n",
      "Epoch [1/1], Step [6298/7635], Loss: 4.1823\n",
      "Epoch [1/1], Step [6299/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [6300/7635], Loss: 4.0516\n",
      "Epoch [1/1], Step [6301/7635], Loss: 4.1415\n",
      "Epoch [1/1], Step [6302/7635], Loss: 4.1123\n",
      "Epoch [1/1], Step [6303/7635], Loss: 4.1320\n",
      "Epoch [1/1], Step [6304/7635], Loss: 4.1518\n",
      "Epoch [1/1], Step [6305/7635], Loss: 4.1363\n",
      "Epoch [1/1], Step [6306/7635], Loss: 4.1339\n",
      "Epoch [1/1], Step [6307/7635], Loss: 4.0348\n",
      "Epoch [1/1], Step [6308/7635], Loss: 4.1892\n",
      "Epoch [1/1], Step [6309/7635], Loss: 4.0743\n",
      "Epoch [1/1], Step [6310/7635], Loss: 4.1622\n",
      "Epoch [1/1], Step [6311/7635], Loss: 4.1327\n",
      "Epoch [1/1], Step [6312/7635], Loss: 4.1940\n",
      "Epoch [1/1], Step [6313/7635], Loss: 4.1079\n",
      "Epoch [1/1], Step [6314/7635], Loss: 4.0595\n",
      "Epoch [1/1], Step [6315/7635], Loss: 4.1066\n",
      "Epoch [1/1], Step [6316/7635], Loss: 4.2209\n",
      "Epoch [1/1], Step [6317/7635], Loss: 4.1247\n",
      "Epoch [1/1], Step [6318/7635], Loss: 4.1446\n",
      "Epoch [1/1], Step [6319/7635], Loss: 4.1245\n",
      "Epoch [1/1], Step [6320/7635], Loss: 4.1041\n",
      "Epoch [1/1], Step [6321/7635], Loss: 4.1544\n",
      "Epoch [1/1], Step [6322/7635], Loss: 4.0505\n",
      "Epoch [1/1], Step [6323/7635], Loss: 4.1313\n",
      "Epoch [1/1], Step [6324/7635], Loss: 4.1201\n",
      "Epoch [1/1], Step [6325/7635], Loss: 4.1920\n",
      "Epoch [1/1], Step [6326/7635], Loss: 4.0462\n",
      "Epoch [1/1], Step [6327/7635], Loss: 4.0494\n",
      "Epoch [1/1], Step [6328/7635], Loss: 4.1007\n",
      "Epoch [1/1], Step [6329/7635], Loss: 4.1446\n",
      "Epoch [1/1], Step [6330/7635], Loss: 4.0907\n",
      "Epoch [1/1], Step [6331/7635], Loss: 4.1245\n",
      "Epoch [1/1], Step [6332/7635], Loss: 4.1357\n",
      "Epoch [1/1], Step [6333/7635], Loss: 4.0544\n",
      "Epoch [1/1], Step [6334/7635], Loss: 4.1123\n",
      "Epoch [1/1], Step [6335/7635], Loss: 4.1431\n",
      "Epoch [1/1], Step [6336/7635], Loss: 4.0971\n",
      "Epoch [1/1], Step [6337/7635], Loss: 4.1315\n",
      "Epoch [1/1], Step [6338/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [6339/7635], Loss: 4.0799\n",
      "Epoch [1/1], Step [6340/7635], Loss: 4.0728\n",
      "Epoch [1/1], Step [6341/7635], Loss: 4.0910\n",
      "Epoch [1/1], Step [6342/7635], Loss: 4.1265\n",
      "Epoch [1/1], Step [6343/7635], Loss: 4.1406\n",
      "Epoch [1/1], Step [6344/7635], Loss: 4.0245\n",
      "Epoch [1/1], Step [6345/7635], Loss: 4.1623\n",
      "Epoch [1/1], Step [6346/7635], Loss: 4.0847\n",
      "Epoch [1/1], Step [6347/7635], Loss: 4.1036\n",
      "Epoch [1/1], Step [6348/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [6349/7635], Loss: 4.1438\n",
      "Epoch [1/1], Step [6350/7635], Loss: 4.1458\n",
      "Epoch [1/1], Step [6351/7635], Loss: 4.0570\n",
      "Epoch [1/1], Step [6352/7635], Loss: 4.0700\n",
      "Epoch [1/1], Step [6353/7635], Loss: 4.1304\n",
      "Epoch [1/1], Step [6354/7635], Loss: 4.1199\n",
      "Epoch [1/1], Step [6355/7635], Loss: 4.1796\n",
      "Epoch [1/1], Step [6356/7635], Loss: 4.1346\n",
      "Epoch [1/1], Step [6357/7635], Loss: 4.1603\n",
      "Epoch [1/1], Step [6358/7635], Loss: 4.2079\n",
      "Epoch [1/1], Step [6359/7635], Loss: 4.1215\n",
      "Epoch [1/1], Step [6360/7635], Loss: 4.0718\n",
      "Epoch [1/1], Step [6361/7635], Loss: 4.1533\n",
      "Epoch [1/1], Step [6362/7635], Loss: 4.0990\n",
      "Epoch [1/1], Step [6363/7635], Loss: 4.1006\n",
      "Epoch [1/1], Step [6364/7635], Loss: 4.0970\n",
      "Epoch [1/1], Step [6365/7635], Loss: 4.1207\n",
      "Epoch [1/1], Step [6366/7635], Loss: 4.0706\n",
      "Epoch [1/1], Step [6367/7635], Loss: 4.1195\n",
      "Epoch [1/1], Step [6368/7635], Loss: 4.1867\n",
      "Epoch [1/1], Step [6369/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [6370/7635], Loss: 4.0564\n",
      "Epoch [1/1], Step [6371/7635], Loss: 4.0725\n",
      "Epoch [1/1], Step [6372/7635], Loss: 4.0479\n",
      "Epoch [1/1], Step [6373/7635], Loss: 4.1369\n",
      "Epoch [1/1], Step [6374/7635], Loss: 4.1360\n",
      "Epoch [1/1], Step [6375/7635], Loss: 4.0911\n",
      "Epoch [1/1], Step [6376/7635], Loss: 4.1729\n",
      "Epoch [1/1], Step [6377/7635], Loss: 4.0389\n",
      "Epoch [1/1], Step [6378/7635], Loss: 4.2400\n",
      "Epoch [1/1], Step [6379/7635], Loss: 4.0186\n",
      "Epoch [1/1], Step [6380/7635], Loss: 4.1364\n",
      "Epoch [1/1], Step [6381/7635], Loss: 4.1254\n",
      "Epoch [1/1], Step [6382/7635], Loss: 4.1357\n",
      "Epoch [1/1], Step [6383/7635], Loss: 4.1001\n",
      "Epoch [1/1], Step [6384/7635], Loss: 4.1623\n",
      "Epoch [1/1], Step [6385/7635], Loss: 4.1129\n",
      "Epoch [1/1], Step [6386/7635], Loss: 4.1286\n",
      "Epoch [1/1], Step [6387/7635], Loss: 4.1331\n",
      "Epoch [1/1], Step [6388/7635], Loss: 4.0991\n",
      "Epoch [1/1], Step [6389/7635], Loss: 4.1152\n",
      "Epoch [1/1], Step [6390/7635], Loss: 4.1145\n",
      "Epoch [1/1], Step [6391/7635], Loss: 4.1059\n",
      "Epoch [1/1], Step [6392/7635], Loss: 4.1697\n",
      "Epoch [1/1], Step [6393/7635], Loss: 4.1064\n",
      "Epoch [1/1], Step [6394/7635], Loss: 4.1297\n",
      "Epoch [1/1], Step [6395/7635], Loss: 4.0752\n",
      "Epoch [1/1], Step [6396/7635], Loss: 4.1409\n",
      "Epoch [1/1], Step [6397/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [6398/7635], Loss: 4.1439\n",
      "Epoch [1/1], Step [6399/7635], Loss: 4.1362\n",
      "Epoch [1/1], Step [6400/7635], Loss: 4.1736\n",
      "Epoch [1/1], Step [6401/7635], Loss: 4.2085\n",
      "Epoch [1/1], Step [6402/7635], Loss: 3.9304\n",
      "Epoch [1/1], Step [6403/7635], Loss: 4.0377\n",
      "Epoch [1/1], Step [6404/7635], Loss: 4.1665\n",
      "Epoch [1/1], Step [6405/7635], Loss: 4.0507\n",
      "Epoch [1/1], Step [6406/7635], Loss: 4.2177\n",
      "Epoch [1/1], Step [6407/7635], Loss: 4.0829\n",
      "Epoch [1/1], Step [6408/7635], Loss: 4.1796\n",
      "Epoch [1/1], Step [6409/7635], Loss: 4.0974\n",
      "Epoch [1/1], Step [6410/7635], Loss: 4.1414\n",
      "Epoch [1/1], Step [6411/7635], Loss: 4.1236\n",
      "Epoch [1/1], Step [6412/7635], Loss: 4.1591\n",
      "Epoch [1/1], Step [6413/7635], Loss: 4.2031\n",
      "Epoch [1/1], Step [6414/7635], Loss: 4.0771\n",
      "Epoch [1/1], Step [6415/7635], Loss: 4.1284\n",
      "Epoch [1/1], Step [6416/7635], Loss: 4.1368\n",
      "Epoch [1/1], Step [6417/7635], Loss: 4.1096\n",
      "Epoch [1/1], Step [6418/7635], Loss: 4.1115\n",
      "Epoch [1/1], Step [6419/7635], Loss: 4.0732\n",
      "Epoch [1/1], Step [6420/7635], Loss: 4.1811\n",
      "Epoch [1/1], Step [6421/7635], Loss: 4.1925\n",
      "Epoch [1/1], Step [6422/7635], Loss: 4.1740\n",
      "Epoch [1/1], Step [6423/7635], Loss: 4.1007\n",
      "Epoch [1/1], Step [6424/7635], Loss: 4.1374\n",
      "Epoch [1/1], Step [6425/7635], Loss: 4.1449\n",
      "Epoch [1/1], Step [6426/7635], Loss: 4.1386\n",
      "Epoch [1/1], Step [6427/7635], Loss: 4.0774\n",
      "Epoch [1/1], Step [6428/7635], Loss: 4.1987\n",
      "Epoch [1/1], Step [6429/7635], Loss: 4.0925\n",
      "Epoch [1/1], Step [6430/7635], Loss: 4.1412\n",
      "Epoch [1/1], Step [6431/7635], Loss: 4.1166\n",
      "Epoch [1/1], Step [6432/7635], Loss: 4.0678\n",
      "Epoch [1/1], Step [6433/7635], Loss: 4.0666\n",
      "Epoch [1/1], Step [6434/7635], Loss: 4.1702\n",
      "Epoch [1/1], Step [6435/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [6436/7635], Loss: 4.0903\n",
      "Epoch [1/1], Step [6437/7635], Loss: 4.1346\n",
      "Epoch [1/1], Step [6438/7635], Loss: 4.1016\n",
      "Epoch [1/1], Step [6439/7635], Loss: 4.0806\n",
      "Epoch [1/1], Step [6440/7635], Loss: 4.1106\n",
      "Epoch [1/1], Step [6441/7635], Loss: 4.1180\n",
      "Epoch [1/1], Step [6442/7635], Loss: 4.1068\n",
      "Epoch [1/1], Step [6443/7635], Loss: 4.1166\n",
      "Epoch [1/1], Step [6444/7635], Loss: 4.1843\n",
      "Epoch [1/1], Step [6445/7635], Loss: 4.0787\n",
      "Epoch [1/1], Step [6446/7635], Loss: 4.0618\n",
      "Epoch [1/1], Step [6447/7635], Loss: 4.1000\n",
      "Epoch [1/1], Step [6448/7635], Loss: 4.1419\n",
      "Epoch [1/1], Step [6449/7635], Loss: 4.1133\n",
      "Epoch [1/1], Step [6450/7635], Loss: 4.1520\n",
      "Epoch [1/1], Step [6451/7635], Loss: 4.1319\n",
      "Epoch [1/1], Step [6452/7635], Loss: 4.1424\n",
      "Epoch [1/1], Step [6453/7635], Loss: 4.1099\n",
      "Epoch [1/1], Step [6454/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [6455/7635], Loss: 4.2114\n",
      "Epoch [1/1], Step [6456/7635], Loss: 4.0582\n",
      "Epoch [1/1], Step [6457/7635], Loss: 4.0681\n",
      "Epoch [1/1], Step [6458/7635], Loss: 4.0713\n",
      "Epoch [1/1], Step [6459/7635], Loss: 4.1396\n",
      "Epoch [1/1], Step [6460/7635], Loss: 4.1265\n",
      "Epoch [1/1], Step [6461/7635], Loss: 4.1904\n",
      "Epoch [1/1], Step [6462/7635], Loss: 4.1445\n",
      "Epoch [1/1], Step [6463/7635], Loss: 4.0856\n",
      "Epoch [1/1], Step [6464/7635], Loss: 4.0236\n",
      "Epoch [1/1], Step [6465/7635], Loss: 4.1857\n",
      "Epoch [1/1], Step [6466/7635], Loss: 4.1795\n",
      "Epoch [1/1], Step [6467/7635], Loss: 4.1387\n",
      "Epoch [1/1], Step [6468/7635], Loss: 4.0912\n",
      "Epoch [1/1], Step [6469/7635], Loss: 4.1621\n",
      "Epoch [1/1], Step [6470/7635], Loss: 4.1854\n",
      "Epoch [1/1], Step [6471/7635], Loss: 4.0979\n",
      "Epoch [1/1], Step [6472/7635], Loss: 4.0440\n",
      "Epoch [1/1], Step [6473/7635], Loss: 4.0971\n",
      "Epoch [1/1], Step [6474/7635], Loss: 4.1408\n",
      "Epoch [1/1], Step [6475/7635], Loss: 4.0695\n",
      "Epoch [1/1], Step [6476/7635], Loss: 4.1496\n",
      "Epoch [1/1], Step [6477/7635], Loss: 4.1605\n",
      "Epoch [1/1], Step [6478/7635], Loss: 4.1133\n",
      "Epoch [1/1], Step [6479/7635], Loss: 4.0919\n",
      "Epoch [1/1], Step [6480/7635], Loss: 4.1279\n",
      "Epoch [1/1], Step [6481/7635], Loss: 4.0738\n",
      "Epoch [1/1], Step [6482/7635], Loss: 4.1024\n",
      "Epoch [1/1], Step [6483/7635], Loss: 4.0281\n",
      "Epoch [1/1], Step [6484/7635], Loss: 4.0933\n",
      "Epoch [1/1], Step [6485/7635], Loss: 4.1431\n",
      "Epoch [1/1], Step [6486/7635], Loss: 4.1157\n",
      "Epoch [1/1], Step [6487/7635], Loss: 4.1107\n",
      "Epoch [1/1], Step [6488/7635], Loss: 4.1282\n",
      "Epoch [1/1], Step [6489/7635], Loss: 4.0985\n",
      "Epoch [1/1], Step [6490/7635], Loss: 4.1765\n",
      "Epoch [1/1], Step [6491/7635], Loss: 4.1047\n",
      "Epoch [1/1], Step [6492/7635], Loss: 4.0791\n",
      "Epoch [1/1], Step [6493/7635], Loss: 4.2029\n",
      "Epoch [1/1], Step [6494/7635], Loss: 4.1067\n",
      "Epoch [1/1], Step [6495/7635], Loss: 4.0843\n",
      "Epoch [1/1], Step [6496/7635], Loss: 4.0467\n",
      "Epoch [1/1], Step [6497/7635], Loss: 4.2194\n",
      "Epoch [1/1], Step [6498/7635], Loss: 4.1443\n",
      "Epoch [1/1], Step [6499/7635], Loss: 4.1235\n",
      "Epoch [1/1], Step [6500/7635], Loss: 4.1630\n",
      "Epoch [1/1], Step [6501/7635], Loss: 4.0814\n",
      "Epoch [1/1], Step [6502/7635], Loss: 4.0532\n",
      "Epoch [1/1], Step [6503/7635], Loss: 4.1345\n",
      "Epoch [1/1], Step [6504/7635], Loss: 4.1568\n",
      "Epoch [1/1], Step [6505/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [6506/7635], Loss: 4.0916\n",
      "Epoch [1/1], Step [6507/7635], Loss: 4.1264\n",
      "Epoch [1/1], Step [6508/7635], Loss: 4.0606\n",
      "Epoch [1/1], Step [6509/7635], Loss: 4.1162\n",
      "Epoch [1/1], Step [6510/7635], Loss: 4.0976\n",
      "Epoch [1/1], Step [6511/7635], Loss: 4.1973\n",
      "Epoch [1/1], Step [6512/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [6513/7635], Loss: 4.0877\n",
      "Epoch [1/1], Step [6514/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [6515/7635], Loss: 4.1556\n",
      "Epoch [1/1], Step [6516/7635], Loss: 4.0978\n",
      "Epoch [1/1], Step [6517/7635], Loss: 4.1903\n",
      "Epoch [1/1], Step [6518/7635], Loss: 4.0763\n",
      "Epoch [1/1], Step [6519/7635], Loss: 4.1728\n",
      "Epoch [1/1], Step [6520/7635], Loss: 4.1339\n",
      "Epoch [1/1], Step [6521/7635], Loss: 4.1205\n",
      "Epoch [1/1], Step [6522/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [6523/7635], Loss: 4.1749\n",
      "Epoch [1/1], Step [6524/7635], Loss: 4.0539\n",
      "Epoch [1/1], Step [6525/7635], Loss: 4.1232\n",
      "Epoch [1/1], Step [6526/7635], Loss: 4.1386\n",
      "Epoch [1/1], Step [6527/7635], Loss: 4.0848\n",
      "Epoch [1/1], Step [6528/7635], Loss: 4.1229\n",
      "Epoch [1/1], Step [6529/7635], Loss: 4.1120\n",
      "Epoch [1/1], Step [6530/7635], Loss: 4.1238\n",
      "Epoch [1/1], Step [6531/7635], Loss: 4.0553\n",
      "Epoch [1/1], Step [6532/7635], Loss: 4.1304\n",
      "Epoch [1/1], Step [6533/7635], Loss: 4.2009\n",
      "Epoch [1/1], Step [6534/7635], Loss: 4.0982\n",
      "Epoch [1/1], Step [6535/7635], Loss: 4.0259\n",
      "Epoch [1/1], Step [6536/7635], Loss: 4.1506\n",
      "Epoch [1/1], Step [6537/7635], Loss: 4.0494\n",
      "Epoch [1/1], Step [6538/7635], Loss: 4.1876\n",
      "Epoch [1/1], Step [6539/7635], Loss: 4.2005\n",
      "Epoch [1/1], Step [6540/7635], Loss: 4.0786\n",
      "Epoch [1/1], Step [6541/7635], Loss: 4.1765\n",
      "Epoch [1/1], Step [6542/7635], Loss: 4.1503\n",
      "Epoch [1/1], Step [6543/7635], Loss: 4.2236\n",
      "Epoch [1/1], Step [6544/7635], Loss: 4.0391\n",
      "Epoch [1/1], Step [6545/7635], Loss: 4.1660\n",
      "Epoch [1/1], Step [6546/7635], Loss: 4.0868\n",
      "Epoch [1/1], Step [6547/7635], Loss: 4.1238\n",
      "Epoch [1/1], Step [6548/7635], Loss: 4.0998\n",
      "Epoch [1/1], Step [6549/7635], Loss: 4.1143\n",
      "Epoch [1/1], Step [6550/7635], Loss: 4.0826\n",
      "Epoch [1/1], Step [6551/7635], Loss: 4.1156\n",
      "Epoch [1/1], Step [6552/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [6553/7635], Loss: 4.1029\n",
      "Epoch [1/1], Step [6554/7635], Loss: 4.1033\n",
      "Epoch [1/1], Step [6555/7635], Loss: 4.1699\n",
      "Epoch [1/1], Step [6556/7635], Loss: 4.1122\n",
      "Epoch [1/1], Step [6557/7635], Loss: 4.1729\n",
      "Epoch [1/1], Step [6558/7635], Loss: 4.1683\n",
      "Epoch [1/1], Step [6559/7635], Loss: 4.0358\n",
      "Epoch [1/1], Step [6560/7635], Loss: 4.0140\n",
      "Epoch [1/1], Step [6561/7635], Loss: 4.1071\n",
      "Epoch [1/1], Step [6562/7635], Loss: 4.1396\n",
      "Epoch [1/1], Step [6563/7635], Loss: 4.0342\n",
      "Epoch [1/1], Step [6564/7635], Loss: 4.1559\n",
      "Epoch [1/1], Step [6565/7635], Loss: 4.0460\n",
      "Epoch [1/1], Step [6566/7635], Loss: 4.1628\n",
      "Epoch [1/1], Step [6567/7635], Loss: 4.0187\n",
      "Epoch [1/1], Step [6568/7635], Loss: 4.1763\n",
      "Epoch [1/1], Step [6569/7635], Loss: 4.0674\n",
      "Epoch [1/1], Step [6570/7635], Loss: 4.1515\n",
      "Epoch [1/1], Step [6571/7635], Loss: 4.1463\n",
      "Epoch [1/1], Step [6572/7635], Loss: 4.1307\n",
      "Epoch [1/1], Step [6573/7635], Loss: 4.1017\n",
      "Epoch [1/1], Step [6574/7635], Loss: 4.0886\n",
      "Epoch [1/1], Step [6575/7635], Loss: 4.1082\n",
      "Epoch [1/1], Step [6576/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [6577/7635], Loss: 4.1393\n",
      "Epoch [1/1], Step [6578/7635], Loss: 4.0588\n",
      "Epoch [1/1], Step [6579/7635], Loss: 4.0588\n",
      "Epoch [1/1], Step [6580/7635], Loss: 4.1183\n",
      "Epoch [1/1], Step [6581/7635], Loss: 4.1815\n",
      "Epoch [1/1], Step [6582/7635], Loss: 4.1188\n",
      "Epoch [1/1], Step [6583/7635], Loss: 4.1453\n",
      "Epoch [1/1], Step [6584/7635], Loss: 4.1567\n",
      "Epoch [1/1], Step [6585/7635], Loss: 4.0994\n",
      "Epoch [1/1], Step [6586/7635], Loss: 4.0860\n",
      "Epoch [1/1], Step [6587/7635], Loss: 4.1109\n",
      "Epoch [1/1], Step [6588/7635], Loss: 4.0857\n",
      "Epoch [1/1], Step [6589/7635], Loss: 4.1415\n",
      "Epoch [1/1], Step [6590/7635], Loss: 4.0826\n",
      "Epoch [1/1], Step [6591/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [6592/7635], Loss: 4.0571\n",
      "Epoch [1/1], Step [6593/7635], Loss: 4.1733\n",
      "Epoch [1/1], Step [6594/7635], Loss: 4.0690\n",
      "Epoch [1/1], Step [6595/7635], Loss: 4.1282\n",
      "Epoch [1/1], Step [6596/7635], Loss: 4.1062\n",
      "Epoch [1/1], Step [6597/7635], Loss: 4.1661\n",
      "Epoch [1/1], Step [6598/7635], Loss: 4.1564\n",
      "Epoch [1/1], Step [6599/7635], Loss: 4.0306\n",
      "Epoch [1/1], Step [6600/7635], Loss: 4.1412\n",
      "Epoch [1/1], Step [6601/7635], Loss: 4.1096\n",
      "Epoch [1/1], Step [6602/7635], Loss: 4.1076\n",
      "Epoch [1/1], Step [6603/7635], Loss: 4.0228\n",
      "Epoch [1/1], Step [6604/7635], Loss: 4.0972\n",
      "Epoch [1/1], Step [6605/7635], Loss: 4.1087\n",
      "Epoch [1/1], Step [6606/7635], Loss: 4.1032\n",
      "Epoch [1/1], Step [6607/7635], Loss: 4.1533\n",
      "Epoch [1/1], Step [6608/7635], Loss: 4.1175\n",
      "Epoch [1/1], Step [6609/7635], Loss: 4.0306\n",
      "Epoch [1/1], Step [6610/7635], Loss: 4.1021\n",
      "Epoch [1/1], Step [6611/7635], Loss: 4.0979\n",
      "Epoch [1/1], Step [6612/7635], Loss: 4.0490\n",
      "Epoch [1/1], Step [6613/7635], Loss: 4.1084\n",
      "Epoch [1/1], Step [6614/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [6615/7635], Loss: 4.1129\n",
      "Epoch [1/1], Step [6616/7635], Loss: 4.0634\n",
      "Epoch [1/1], Step [6617/7635], Loss: 4.0608\n",
      "Epoch [1/1], Step [6618/7635], Loss: 4.1492\n",
      "Epoch [1/1], Step [6619/7635], Loss: 4.1250\n",
      "Epoch [1/1], Step [6620/7635], Loss: 4.0326\n",
      "Epoch [1/1], Step [6621/7635], Loss: 4.1400\n",
      "Epoch [1/1], Step [6622/7635], Loss: 4.1172\n",
      "Epoch [1/1], Step [6623/7635], Loss: 4.1614\n",
      "Epoch [1/1], Step [6624/7635], Loss: 4.0844\n",
      "Epoch [1/1], Step [6625/7635], Loss: 4.0162\n",
      "Epoch [1/1], Step [6626/7635], Loss: 4.0988\n",
      "Epoch [1/1], Step [6627/7635], Loss: 4.1248\n",
      "Epoch [1/1], Step [6628/7635], Loss: 4.1404\n",
      "Epoch [1/1], Step [6629/7635], Loss: 4.0643\n",
      "Epoch [1/1], Step [6630/7635], Loss: 4.0892\n",
      "Epoch [1/1], Step [6631/7635], Loss: 4.1393\n",
      "Epoch [1/1], Step [6632/7635], Loss: 4.1088\n",
      "Epoch [1/1], Step [6633/7635], Loss: 4.1190\n",
      "Epoch [1/1], Step [6634/7635], Loss: 4.1207\n",
      "Epoch [1/1], Step [6635/7635], Loss: 4.1113\n",
      "Epoch [1/1], Step [6636/7635], Loss: 4.0689\n",
      "Epoch [1/1], Step [6637/7635], Loss: 4.0660\n",
      "Epoch [1/1], Step [6638/7635], Loss: 4.0674\n",
      "Epoch [1/1], Step [6639/7635], Loss: 4.1225\n",
      "Epoch [1/1], Step [6640/7635], Loss: 4.1907\n",
      "Epoch [1/1], Step [6641/7635], Loss: 4.1379\n",
      "Epoch [1/1], Step [6642/7635], Loss: 4.0669\n",
      "Epoch [1/1], Step [6643/7635], Loss: 4.1286\n",
      "Epoch [1/1], Step [6644/7635], Loss: 4.1199\n",
      "Epoch [1/1], Step [6645/7635], Loss: 4.0400\n",
      "Epoch [1/1], Step [6646/7635], Loss: 4.0712\n",
      "Epoch [1/1], Step [6647/7635], Loss: 4.0937\n",
      "Epoch [1/1], Step [6648/7635], Loss: 4.1214\n",
      "Epoch [1/1], Step [6649/7635], Loss: 4.1090\n",
      "Epoch [1/1], Step [6650/7635], Loss: 4.0518\n",
      "Epoch [1/1], Step [6651/7635], Loss: 4.1157\n",
      "Epoch [1/1], Step [6652/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [6653/7635], Loss: 4.0708\n",
      "Epoch [1/1], Step [6654/7635], Loss: 4.1440\n",
      "Epoch [1/1], Step [6655/7635], Loss: 4.1526\n",
      "Epoch [1/1], Step [6656/7635], Loss: 4.1521\n",
      "Epoch [1/1], Step [6657/7635], Loss: 4.1324\n",
      "Epoch [1/1], Step [6658/7635], Loss: 4.1284\n",
      "Epoch [1/1], Step [6659/7635], Loss: 4.0513\n",
      "Epoch [1/1], Step [6660/7635], Loss: 4.2331\n",
      "Epoch [1/1], Step [6661/7635], Loss: 4.1303\n",
      "Epoch [1/1], Step [6662/7635], Loss: 4.1541\n",
      "Epoch [1/1], Step [6663/7635], Loss: 4.0825\n",
      "Epoch [1/1], Step [6664/7635], Loss: 4.1322\n",
      "Epoch [1/1], Step [6665/7635], Loss: 4.1503\n",
      "Epoch [1/1], Step [6666/7635], Loss: 4.1196\n",
      "Epoch [1/1], Step [6667/7635], Loss: 4.1047\n",
      "Epoch [1/1], Step [6668/7635], Loss: 3.9923\n",
      "Epoch [1/1], Step [6669/7635], Loss: 4.1212\n",
      "Epoch [1/1], Step [6670/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [6671/7635], Loss: 4.1024\n",
      "Epoch [1/1], Step [6672/7635], Loss: 4.0924\n",
      "Epoch [1/1], Step [6673/7635], Loss: 4.0589\n",
      "Epoch [1/1], Step [6674/7635], Loss: 4.1471\n",
      "Epoch [1/1], Step [6675/7635], Loss: 4.1412\n",
      "Epoch [1/1], Step [6676/7635], Loss: 4.0379\n",
      "Epoch [1/1], Step [6677/7635], Loss: 4.1067\n",
      "Epoch [1/1], Step [6678/7635], Loss: 4.1432\n",
      "Epoch [1/1], Step [6679/7635], Loss: 4.1367\n",
      "Epoch [1/1], Step [6680/7635], Loss: 4.0243\n",
      "Epoch [1/1], Step [6681/7635], Loss: 4.0925\n",
      "Epoch [1/1], Step [6682/7635], Loss: 4.0735\n",
      "Epoch [1/1], Step [6683/7635], Loss: 4.0924\n",
      "Epoch [1/1], Step [6684/7635], Loss: 4.1007\n",
      "Epoch [1/1], Step [6685/7635], Loss: 4.1309\n",
      "Epoch [1/1], Step [6686/7635], Loss: 4.1495\n",
      "Epoch [1/1], Step [6687/7635], Loss: 4.0639\n",
      "Epoch [1/1], Step [6688/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [6689/7635], Loss: 4.0837\n",
      "Epoch [1/1], Step [6690/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [6691/7635], Loss: 4.1330\n",
      "Epoch [1/1], Step [6692/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [6693/7635], Loss: 4.1014\n",
      "Epoch [1/1], Step [6694/7635], Loss: 4.1556\n",
      "Epoch [1/1], Step [6695/7635], Loss: 4.0628\n",
      "Epoch [1/1], Step [6696/7635], Loss: 4.0387\n",
      "Epoch [1/1], Step [6697/7635], Loss: 4.1157\n",
      "Epoch [1/1], Step [6698/7635], Loss: 4.1249\n",
      "Epoch [1/1], Step [6699/7635], Loss: 4.0771\n",
      "Epoch [1/1], Step [6700/7635], Loss: 4.1142\n",
      "Epoch [1/1], Step [6701/7635], Loss: 4.1539\n",
      "Epoch [1/1], Step [6702/7635], Loss: 4.0737\n",
      "Epoch [1/1], Step [6703/7635], Loss: 4.0685\n",
      "Epoch [1/1], Step [6704/7635], Loss: 4.1493\n",
      "Epoch [1/1], Step [6705/7635], Loss: 4.1287\n",
      "Epoch [1/1], Step [6706/7635], Loss: 4.0667\n",
      "Epoch [1/1], Step [6707/7635], Loss: 4.0526\n",
      "Epoch [1/1], Step [6708/7635], Loss: 4.1207\n",
      "Epoch [1/1], Step [6709/7635], Loss: 4.1822\n",
      "Epoch [1/1], Step [6710/7635], Loss: 4.0683\n",
      "Epoch [1/1], Step [6711/7635], Loss: 4.1539\n",
      "Epoch [1/1], Step [6712/7635], Loss: 4.0394\n",
      "Epoch [1/1], Step [6713/7635], Loss: 4.0755\n",
      "Epoch [1/1], Step [6714/7635], Loss: 4.0770\n",
      "Epoch [1/1], Step [6715/7635], Loss: 4.0682\n",
      "Epoch [1/1], Step [6716/7635], Loss: 4.1050\n",
      "Epoch [1/1], Step [6717/7635], Loss: 4.1628\n",
      "Epoch [1/1], Step [6718/7635], Loss: 4.1423\n",
      "Epoch [1/1], Step [6719/7635], Loss: 4.0526\n",
      "Epoch [1/1], Step [6720/7635], Loss: 4.1041\n",
      "Epoch [1/1], Step [6721/7635], Loss: 4.1470\n",
      "Epoch [1/1], Step [6722/7635], Loss: 4.0296\n",
      "Epoch [1/1], Step [6723/7635], Loss: 4.1192\n",
      "Epoch [1/1], Step [6724/7635], Loss: 4.0538\n",
      "Epoch [1/1], Step [6725/7635], Loss: 4.1435\n",
      "Epoch [1/1], Step [6726/7635], Loss: 4.2157\n",
      "Epoch [1/1], Step [6727/7635], Loss: 4.1108\n",
      "Epoch [1/1], Step [6728/7635], Loss: 4.1319\n",
      "Epoch [1/1], Step [6729/7635], Loss: 4.0881\n",
      "Epoch [1/1], Step [6730/7635], Loss: 4.1259\n",
      "Epoch [1/1], Step [6731/7635], Loss: 4.1349\n",
      "Epoch [1/1], Step [6732/7635], Loss: 4.1143\n",
      "Epoch [1/1], Step [6733/7635], Loss: 4.0109\n",
      "Epoch [1/1], Step [6734/7635], Loss: 4.1507\n",
      "Epoch [1/1], Step [6735/7635], Loss: 4.1689\n",
      "Epoch [1/1], Step [6736/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [6737/7635], Loss: 4.1178\n",
      "Epoch [1/1], Step [6738/7635], Loss: 4.1161\n",
      "Epoch [1/1], Step [6739/7635], Loss: 4.1116\n",
      "Epoch [1/1], Step [6740/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [6741/7635], Loss: 4.0814\n",
      "Epoch [1/1], Step [6742/7635], Loss: 4.1234\n",
      "Epoch [1/1], Step [6743/7635], Loss: 4.0871\n",
      "Epoch [1/1], Step [6744/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [6745/7635], Loss: 4.0736\n",
      "Epoch [1/1], Step [6746/7635], Loss: 4.1454\n",
      "Epoch [1/1], Step [6747/7635], Loss: 4.0499\n",
      "Epoch [1/1], Step [6748/7635], Loss: 4.0869\n",
      "Epoch [1/1], Step [6749/7635], Loss: 4.2348\n",
      "Epoch [1/1], Step [6750/7635], Loss: 4.1468\n",
      "Epoch [1/1], Step [6751/7635], Loss: 3.9941\n",
      "Epoch [1/1], Step [6752/7635], Loss: 4.0420\n",
      "Epoch [1/1], Step [6753/7635], Loss: 4.1003\n",
      "Epoch [1/1], Step [6754/7635], Loss: 4.1394\n",
      "Epoch [1/1], Step [6755/7635], Loss: 4.1588\n",
      "Epoch [1/1], Step [6756/7635], Loss: 4.1098\n",
      "Epoch [1/1], Step [6757/7635], Loss: 4.2000\n",
      "Epoch [1/1], Step [6758/7635], Loss: 4.1126\n",
      "Epoch [1/1], Step [6759/7635], Loss: 4.0349\n",
      "Epoch [1/1], Step [6760/7635], Loss: 4.1015\n",
      "Epoch [1/1], Step [6761/7635], Loss: 4.0702\n",
      "Epoch [1/1], Step [6762/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [6763/7635], Loss: 4.0822\n",
      "Epoch [1/1], Step [6764/7635], Loss: 4.0817\n",
      "Epoch [1/1], Step [6765/7635], Loss: 4.1535\n",
      "Epoch [1/1], Step [6766/7635], Loss: 4.0204\n",
      "Epoch [1/1], Step [6767/7635], Loss: 4.0747\n",
      "Epoch [1/1], Step [6768/7635], Loss: 4.0882\n",
      "Epoch [1/1], Step [6769/7635], Loss: 4.1033\n",
      "Epoch [1/1], Step [6770/7635], Loss: 4.1180\n",
      "Epoch [1/1], Step [6771/7635], Loss: 4.0807\n",
      "Epoch [1/1], Step [6772/7635], Loss: 4.1774\n",
      "Epoch [1/1], Step [6773/7635], Loss: 4.0244\n",
      "Epoch [1/1], Step [6774/7635], Loss: 4.1021\n",
      "Epoch [1/1], Step [6775/7635], Loss: 4.0631\n",
      "Epoch [1/1], Step [6776/7635], Loss: 4.0588\n",
      "Epoch [1/1], Step [6777/7635], Loss: 4.0911\n",
      "Epoch [1/1], Step [6778/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [6779/7635], Loss: 4.1345\n",
      "Epoch [1/1], Step [6780/7635], Loss: 4.1249\n",
      "Epoch [1/1], Step [6781/7635], Loss: 4.1580\n",
      "Epoch [1/1], Step [6782/7635], Loss: 4.1354\n",
      "Epoch [1/1], Step [6783/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [6784/7635], Loss: 4.1553\n",
      "Epoch [1/1], Step [6785/7635], Loss: 4.1637\n",
      "Epoch [1/1], Step [6786/7635], Loss: 4.0811\n",
      "Epoch [1/1], Step [6787/7635], Loss: 4.0466\n",
      "Epoch [1/1], Step [6788/7635], Loss: 4.0569\n",
      "Epoch [1/1], Step [6789/7635], Loss: 4.1466\n",
      "Epoch [1/1], Step [6790/7635], Loss: 4.0851\n",
      "Epoch [1/1], Step [6791/7635], Loss: 4.1093\n",
      "Epoch [1/1], Step [6792/7635], Loss: 4.0271\n",
      "Epoch [1/1], Step [6793/7635], Loss: 4.2111\n",
      "Epoch [1/1], Step [6794/7635], Loss: 4.1191\n",
      "Epoch [1/1], Step [6795/7635], Loss: 4.0749\n",
      "Epoch [1/1], Step [6796/7635], Loss: 4.1332\n",
      "Epoch [1/1], Step [6797/7635], Loss: 4.0989\n",
      "Epoch [1/1], Step [6798/7635], Loss: 4.1006\n",
      "Epoch [1/1], Step [6799/7635], Loss: 4.0449\n",
      "Epoch [1/1], Step [6800/7635], Loss: 4.1213\n",
      "Epoch [1/1], Step [6801/7635], Loss: 4.0877\n",
      "Epoch [1/1], Step [6802/7635], Loss: 4.1059\n",
      "Epoch [1/1], Step [6803/7635], Loss: 4.0956\n",
      "Epoch [1/1], Step [6804/7635], Loss: 4.1204\n",
      "Epoch [1/1], Step [6805/7635], Loss: 4.0926\n",
      "Epoch [1/1], Step [6806/7635], Loss: 4.1015\n",
      "Epoch [1/1], Step [6807/7635], Loss: 4.1165\n",
      "Epoch [1/1], Step [6808/7635], Loss: 4.1414\n",
      "Epoch [1/1], Step [6809/7635], Loss: 4.1279\n",
      "Epoch [1/1], Step [6810/7635], Loss: 4.1415\n",
      "Epoch [1/1], Step [6811/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [6812/7635], Loss: 4.0914\n",
      "Epoch [1/1], Step [6813/7635], Loss: 4.1512\n",
      "Epoch [1/1], Step [6814/7635], Loss: 4.1945\n",
      "Epoch [1/1], Step [6815/7635], Loss: 4.1272\n",
      "Epoch [1/1], Step [6816/7635], Loss: 4.1534\n",
      "Epoch [1/1], Step [6817/7635], Loss: 4.1170\n",
      "Epoch [1/1], Step [6818/7635], Loss: 4.1354\n",
      "Epoch [1/1], Step [6819/7635], Loss: 4.1362\n",
      "Epoch [1/1], Step [6820/7635], Loss: 4.0985\n",
      "Epoch [1/1], Step [6821/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [6822/7635], Loss: 4.1357\n",
      "Epoch [1/1], Step [6823/7635], Loss: 4.1104\n",
      "Epoch [1/1], Step [6824/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [6825/7635], Loss: 4.0878\n",
      "Epoch [1/1], Step [6826/7635], Loss: 4.0443\n",
      "Epoch [1/1], Step [6827/7635], Loss: 4.0878\n",
      "Epoch [1/1], Step [6828/7635], Loss: 4.0986\n",
      "Epoch [1/1], Step [6829/7635], Loss: 4.0840\n",
      "Epoch [1/1], Step [6830/7635], Loss: 4.0638\n",
      "Epoch [1/1], Step [6831/7635], Loss: 4.0806\n",
      "Epoch [1/1], Step [6832/7635], Loss: 4.1816\n",
      "Epoch [1/1], Step [6833/7635], Loss: 4.0806\n",
      "Epoch [1/1], Step [6834/7635], Loss: 4.1450\n",
      "Epoch [1/1], Step [6835/7635], Loss: 4.0732\n",
      "Epoch [1/1], Step [6836/7635], Loss: 4.0831\n",
      "Epoch [1/1], Step [6837/7635], Loss: 4.1299\n",
      "Epoch [1/1], Step [6838/7635], Loss: 4.0720\n",
      "Epoch [1/1], Step [6839/7635], Loss: 4.1151\n",
      "Epoch [1/1], Step [6840/7635], Loss: 4.1190\n",
      "Epoch [1/1], Step [6841/7635], Loss: 4.0604\n",
      "Epoch [1/1], Step [6842/7635], Loss: 4.1248\n",
      "Epoch [1/1], Step [6843/7635], Loss: 4.1298\n",
      "Epoch [1/1], Step [6844/7635], Loss: 4.1589\n",
      "Epoch [1/1], Step [6845/7635], Loss: 4.0619\n",
      "Epoch [1/1], Step [6846/7635], Loss: 4.1354\n",
      "Epoch [1/1], Step [6847/7635], Loss: 4.1337\n",
      "Epoch [1/1], Step [6848/7635], Loss: 4.0631\n",
      "Epoch [1/1], Step [6849/7635], Loss: 4.0712\n",
      "Epoch [1/1], Step [6850/7635], Loss: 4.1133\n",
      "Epoch [1/1], Step [6851/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [6852/7635], Loss: 4.1101\n",
      "Epoch [1/1], Step [6853/7635], Loss: 4.0507\n",
      "Epoch [1/1], Step [6854/7635], Loss: 4.0678\n",
      "Epoch [1/1], Step [6855/7635], Loss: 4.0813\n",
      "Epoch [1/1], Step [6856/7635], Loss: 4.0254\n",
      "Epoch [1/1], Step [6857/7635], Loss: 4.0771\n",
      "Epoch [1/1], Step [6858/7635], Loss: 4.0537\n",
      "Epoch [1/1], Step [6859/7635], Loss: 4.1099\n",
      "Epoch [1/1], Step [6860/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [6861/7635], Loss: 4.1927\n",
      "Epoch [1/1], Step [6862/7635], Loss: 4.0827\n",
      "Epoch [1/1], Step [6863/7635], Loss: 4.1189\n",
      "Epoch [1/1], Step [6864/7635], Loss: 4.0707\n",
      "Epoch [1/1], Step [6865/7635], Loss: 4.1349\n",
      "Epoch [1/1], Step [6866/7635], Loss: 4.1040\n",
      "Epoch [1/1], Step [6867/7635], Loss: 4.0633\n",
      "Epoch [1/1], Step [6868/7635], Loss: 4.1061\n",
      "Epoch [1/1], Step [6869/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [6870/7635], Loss: 4.0852\n",
      "Epoch [1/1], Step [6871/7635], Loss: 4.1337\n",
      "Epoch [1/1], Step [6872/7635], Loss: 4.1148\n",
      "Epoch [1/1], Step [6873/7635], Loss: 4.1313\n",
      "Epoch [1/1], Step [6874/7635], Loss: 4.1713\n",
      "Epoch [1/1], Step [6875/7635], Loss: 4.0998\n",
      "Epoch [1/1], Step [6876/7635], Loss: 4.0539\n",
      "Epoch [1/1], Step [6877/7635], Loss: 4.0437\n",
      "Epoch [1/1], Step [6878/7635], Loss: 4.0437\n",
      "Epoch [1/1], Step [6879/7635], Loss: 4.0717\n",
      "Epoch [1/1], Step [6880/7635], Loss: 4.0351\n",
      "Epoch [1/1], Step [6881/7635], Loss: 4.0733\n",
      "Epoch [1/1], Step [6882/7635], Loss: 4.1209\n",
      "Epoch [1/1], Step [6883/7635], Loss: 4.0967\n",
      "Epoch [1/1], Step [6884/7635], Loss: 4.1580\n",
      "Epoch [1/1], Step [6885/7635], Loss: 4.1234\n",
      "Epoch [1/1], Step [6886/7635], Loss: 4.1326\n",
      "Epoch [1/1], Step [6887/7635], Loss: 4.0767\n",
      "Epoch [1/1], Step [6888/7635], Loss: 4.1462\n",
      "Epoch [1/1], Step [6889/7635], Loss: 4.0997\n",
      "Epoch [1/1], Step [6890/7635], Loss: 4.1265\n",
      "Epoch [1/1], Step [6891/7635], Loss: 3.9978\n",
      "Epoch [1/1], Step [6892/7635], Loss: 4.0139\n",
      "Epoch [1/1], Step [6893/7635], Loss: 4.0532\n",
      "Epoch [1/1], Step [6894/7635], Loss: 4.1861\n",
      "Epoch [1/1], Step [6895/7635], Loss: 4.1224\n",
      "Epoch [1/1], Step [6896/7635], Loss: 4.1617\n",
      "Epoch [1/1], Step [6897/7635], Loss: 4.0320\n",
      "Epoch [1/1], Step [6898/7635], Loss: 4.1093\n",
      "Epoch [1/1], Step [6899/7635], Loss: 4.1506\n",
      "Epoch [1/1], Step [6900/7635], Loss: 4.0732\n",
      "Epoch [1/1], Step [6901/7635], Loss: 4.0858\n",
      "Epoch [1/1], Step [6902/7635], Loss: 4.0930\n",
      "Epoch [1/1], Step [6903/7635], Loss: 4.1116\n",
      "Epoch [1/1], Step [6904/7635], Loss: 4.1104\n",
      "Epoch [1/1], Step [6905/7635], Loss: 4.0904\n",
      "Epoch [1/1], Step [6906/7635], Loss: 4.1425\n",
      "Epoch [1/1], Step [6907/7635], Loss: 4.1273\n",
      "Epoch [1/1], Step [6908/7635], Loss: 4.1120\n",
      "Epoch [1/1], Step [6909/7635], Loss: 4.1754\n",
      "Epoch [1/1], Step [6910/7635], Loss: 4.1813\n",
      "Epoch [1/1], Step [6911/7635], Loss: 4.1617\n",
      "Epoch [1/1], Step [6912/7635], Loss: 4.0382\n",
      "Epoch [1/1], Step [6913/7635], Loss: 4.1224\n",
      "Epoch [1/1], Step [6914/7635], Loss: 4.1712\n",
      "Epoch [1/1], Step [6915/7635], Loss: 4.1179\n",
      "Epoch [1/1], Step [6916/7635], Loss: 4.1615\n",
      "Epoch [1/1], Step [6917/7635], Loss: 4.1408\n",
      "Epoch [1/1], Step [6918/7635], Loss: 4.0982\n",
      "Epoch [1/1], Step [6919/7635], Loss: 4.0587\n",
      "Epoch [1/1], Step [6920/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [6921/7635], Loss: 4.0361\n",
      "Epoch [1/1], Step [6922/7635], Loss: 4.2255\n",
      "Epoch [1/1], Step [6923/7635], Loss: 4.0823\n",
      "Epoch [1/1], Step [6924/7635], Loss: 4.1252\n",
      "Epoch [1/1], Step [6925/7635], Loss: 4.0812\n",
      "Epoch [1/1], Step [6926/7635], Loss: 4.1169\n",
      "Epoch [1/1], Step [6927/7635], Loss: 4.1388\n",
      "Epoch [1/1], Step [6928/7635], Loss: 4.0628\n",
      "Epoch [1/1], Step [6929/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [6930/7635], Loss: 4.1331\n",
      "Epoch [1/1], Step [6931/7635], Loss: 4.0657\n",
      "Epoch [1/1], Step [6932/7635], Loss: 4.0256\n",
      "Epoch [1/1], Step [6933/7635], Loss: 4.1667\n",
      "Epoch [1/1], Step [6934/7635], Loss: 4.1312\n",
      "Epoch [1/1], Step [6935/7635], Loss: 4.0757\n",
      "Epoch [1/1], Step [6936/7635], Loss: 4.0702\n",
      "Epoch [1/1], Step [6937/7635], Loss: 4.0975\n",
      "Epoch [1/1], Step [6938/7635], Loss: 4.0644\n",
      "Epoch [1/1], Step [6939/7635], Loss: 4.1080\n",
      "Epoch [1/1], Step [6940/7635], Loss: 4.0927\n",
      "Epoch [1/1], Step [6941/7635], Loss: 4.1306\n",
      "Epoch [1/1], Step [6942/7635], Loss: 4.0843\n",
      "Epoch [1/1], Step [6943/7635], Loss: 4.1290\n",
      "Epoch [1/1], Step [6944/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [6945/7635], Loss: 4.1115\n",
      "Epoch [1/1], Step [6946/7635], Loss: 4.1330\n",
      "Epoch [1/1], Step [6947/7635], Loss: 4.1166\n",
      "Epoch [1/1], Step [6948/7635], Loss: 4.0795\n",
      "Epoch [1/1], Step [6949/7635], Loss: 4.1263\n",
      "Epoch [1/1], Step [6950/7635], Loss: 4.0694\n",
      "Epoch [1/1], Step [6951/7635], Loss: 4.1748\n",
      "Epoch [1/1], Step [6952/7635], Loss: 4.1479\n",
      "Epoch [1/1], Step [6953/7635], Loss: 4.1433\n",
      "Epoch [1/1], Step [6954/7635], Loss: 4.0300\n",
      "Epoch [1/1], Step [6955/7635], Loss: 4.0330\n",
      "Epoch [1/1], Step [6956/7635], Loss: 4.1717\n",
      "Epoch [1/1], Step [6957/7635], Loss: 4.1008\n",
      "Epoch [1/1], Step [6958/7635], Loss: 4.1684\n",
      "Epoch [1/1], Step [6959/7635], Loss: 4.1151\n",
      "Epoch [1/1], Step [6960/7635], Loss: 4.0620\n",
      "Epoch [1/1], Step [6961/7635], Loss: 4.1698\n",
      "Epoch [1/1], Step [6962/7635], Loss: 4.0906\n",
      "Epoch [1/1], Step [6963/7635], Loss: 4.1592\n",
      "Epoch [1/1], Step [6964/7635], Loss: 4.1377\n",
      "Epoch [1/1], Step [6965/7635], Loss: 4.0065\n",
      "Epoch [1/1], Step [6966/7635], Loss: 4.0792\n",
      "Epoch [1/1], Step [6967/7635], Loss: 4.1252\n",
      "Epoch [1/1], Step [6968/7635], Loss: 4.1193\n",
      "Epoch [1/1], Step [6969/7635], Loss: 4.1141\n",
      "Epoch [1/1], Step [6970/7635], Loss: 4.1441\n",
      "Epoch [1/1], Step [6971/7635], Loss: 3.9896\n",
      "Epoch [1/1], Step [6972/7635], Loss: 4.0775\n",
      "Epoch [1/1], Step [6973/7635], Loss: 4.1185\n",
      "Epoch [1/1], Step [6974/7635], Loss: 4.1144\n",
      "Epoch [1/1], Step [6975/7635], Loss: 4.1307\n",
      "Epoch [1/1], Step [6976/7635], Loss: 4.0627\n",
      "Epoch [1/1], Step [6977/7635], Loss: 4.0939\n",
      "Epoch [1/1], Step [6978/7635], Loss: 4.0299\n",
      "Epoch [1/1], Step [6979/7635], Loss: 4.0672\n",
      "Epoch [1/1], Step [6980/7635], Loss: 4.1069\n",
      "Epoch [1/1], Step [6981/7635], Loss: 4.0357\n",
      "Epoch [1/1], Step [6982/7635], Loss: 4.0491\n",
      "Epoch [1/1], Step [6983/7635], Loss: 4.0865\n",
      "Epoch [1/1], Step [6984/7635], Loss: 4.0983\n",
      "Epoch [1/1], Step [6985/7635], Loss: 4.0790\n",
      "Epoch [1/1], Step [6986/7635], Loss: 4.1168\n",
      "Epoch [1/1], Step [6987/7635], Loss: 4.0909\n",
      "Epoch [1/1], Step [6988/7635], Loss: 4.1367\n",
      "Epoch [1/1], Step [6989/7635], Loss: 4.1017\n",
      "Epoch [1/1], Step [6990/7635], Loss: 4.0587\n",
      "Epoch [1/1], Step [6991/7635], Loss: 4.1456\n",
      "Epoch [1/1], Step [6992/7635], Loss: 4.0375\n",
      "Epoch [1/1], Step [6993/7635], Loss: 4.0862\n",
      "Epoch [1/1], Step [6994/7635], Loss: 4.0818\n",
      "Epoch [1/1], Step [6995/7635], Loss: 4.1132\n",
      "Epoch [1/1], Step [6996/7635], Loss: 4.2268\n",
      "Epoch [1/1], Step [6997/7635], Loss: 4.0740\n",
      "Epoch [1/1], Step [6998/7635], Loss: 4.1312\n",
      "Epoch [1/1], Step [6999/7635], Loss: 4.0828\n",
      "Epoch [1/1], Step [7000/7635], Loss: 4.1250\n",
      "Epoch [1/1], Step [7001/7635], Loss: 4.0926\n",
      "Epoch [1/1], Step [7002/7635], Loss: 4.1071\n",
      "Epoch [1/1], Step [7003/7635], Loss: 4.0563\n",
      "Epoch [1/1], Step [7004/7635], Loss: 4.0851\n",
      "Epoch [1/1], Step [7005/7635], Loss: 4.0890\n",
      "Epoch [1/1], Step [7006/7635], Loss: 4.1500\n",
      "Epoch [1/1], Step [7007/7635], Loss: 4.1140\n",
      "Epoch [1/1], Step [7008/7635], Loss: 4.1355\n",
      "Epoch [1/1], Step [7009/7635], Loss: 4.0870\n",
      "Epoch [1/1], Step [7010/7635], Loss: 4.0964\n",
      "Epoch [1/1], Step [7011/7635], Loss: 4.0140\n",
      "Epoch [1/1], Step [7012/7635], Loss: 4.1761\n",
      "Epoch [1/1], Step [7013/7635], Loss: 4.0394\n",
      "Epoch [1/1], Step [7014/7635], Loss: 4.0773\n",
      "Epoch [1/1], Step [7015/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [7016/7635], Loss: 4.0841\n",
      "Epoch [1/1], Step [7017/7635], Loss: 4.1367\n",
      "Epoch [1/1], Step [7018/7635], Loss: 4.0420\n",
      "Epoch [1/1], Step [7019/7635], Loss: 4.1166\n",
      "Epoch [1/1], Step [7020/7635], Loss: 4.1022\n",
      "Epoch [1/1], Step [7021/7635], Loss: 4.0953\n",
      "Epoch [1/1], Step [7022/7635], Loss: 4.0941\n",
      "Epoch [1/1], Step [7023/7635], Loss: 4.1146\n",
      "Epoch [1/1], Step [7024/7635], Loss: 4.1483\n",
      "Epoch [1/1], Step [7025/7635], Loss: 4.0959\n",
      "Epoch [1/1], Step [7026/7635], Loss: 4.1002\n",
      "Epoch [1/1], Step [7027/7635], Loss: 4.1066\n",
      "Epoch [1/1], Step [7028/7635], Loss: 4.1117\n",
      "Epoch [1/1], Step [7029/7635], Loss: 4.0211\n",
      "Epoch [1/1], Step [7030/7635], Loss: 4.1032\n",
      "Epoch [1/1], Step [7031/7635], Loss: 4.1360\n",
      "Epoch [1/1], Step [7032/7635], Loss: 4.0671\n",
      "Epoch [1/1], Step [7033/7635], Loss: 4.0918\n",
      "Epoch [1/1], Step [7034/7635], Loss: 4.1159\n",
      "Epoch [1/1], Step [7035/7635], Loss: 4.0766\n",
      "Epoch [1/1], Step [7036/7635], Loss: 4.0337\n",
      "Epoch [1/1], Step [7037/7635], Loss: 4.0544\n",
      "Epoch [1/1], Step [7038/7635], Loss: 4.0520\n",
      "Epoch [1/1], Step [7039/7635], Loss: 4.0672\n",
      "Epoch [1/1], Step [7040/7635], Loss: 4.0706\n",
      "Epoch [1/1], Step [7041/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [7042/7635], Loss: 4.1761\n",
      "Epoch [1/1], Step [7043/7635], Loss: 4.0464\n",
      "Epoch [1/1], Step [7044/7635], Loss: 4.0448\n",
      "Epoch [1/1], Step [7045/7635], Loss: 4.0725\n",
      "Epoch [1/1], Step [7046/7635], Loss: 4.1120\n",
      "Epoch [1/1], Step [7047/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [7048/7635], Loss: 4.0445\n",
      "Epoch [1/1], Step [7049/7635], Loss: 4.1399\n",
      "Epoch [1/1], Step [7050/7635], Loss: 4.1271\n",
      "Epoch [1/1], Step [7051/7635], Loss: 4.1559\n",
      "Epoch [1/1], Step [7052/7635], Loss: 4.0619\n",
      "Epoch [1/1], Step [7053/7635], Loss: 4.1013\n",
      "Epoch [1/1], Step [7054/7635], Loss: 4.1353\n",
      "Epoch [1/1], Step [7055/7635], Loss: 3.9851\n",
      "Epoch [1/1], Step [7056/7635], Loss: 4.1298\n",
      "Epoch [1/1], Step [7057/7635], Loss: 4.1044\n",
      "Epoch [1/1], Step [7058/7635], Loss: 4.1077\n",
      "Epoch [1/1], Step [7059/7635], Loss: 4.1035\n",
      "Epoch [1/1], Step [7060/7635], Loss: 4.0740\n",
      "Epoch [1/1], Step [7061/7635], Loss: 4.0779\n",
      "Epoch [1/1], Step [7062/7635], Loss: 4.1576\n",
      "Epoch [1/1], Step [7063/7635], Loss: 4.0884\n",
      "Epoch [1/1], Step [7064/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [7065/7635], Loss: 4.0651\n",
      "Epoch [1/1], Step [7066/7635], Loss: 4.1063\n",
      "Epoch [1/1], Step [7067/7635], Loss: 4.1188\n",
      "Epoch [1/1], Step [7068/7635], Loss: 4.0665\n",
      "Epoch [1/1], Step [7069/7635], Loss: 4.0916\n",
      "Epoch [1/1], Step [7070/7635], Loss: 4.0655\n",
      "Epoch [1/1], Step [7071/7635], Loss: 4.1735\n",
      "Epoch [1/1], Step [7072/7635], Loss: 4.1124\n",
      "Epoch [1/1], Step [7073/7635], Loss: 4.0575\n",
      "Epoch [1/1], Step [7074/7635], Loss: 4.0587\n",
      "Epoch [1/1], Step [7075/7635], Loss: 4.1054\n",
      "Epoch [1/1], Step [7076/7635], Loss: 4.0968\n",
      "Epoch [1/1], Step [7077/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [7078/7635], Loss: 4.1446\n",
      "Epoch [1/1], Step [7079/7635], Loss: 4.0637\n",
      "Epoch [1/1], Step [7080/7635], Loss: 4.0947\n",
      "Epoch [1/1], Step [7081/7635], Loss: 4.0964\n",
      "Epoch [1/1], Step [7082/7635], Loss: 4.1507\n",
      "Epoch [1/1], Step [7083/7635], Loss: 4.1633\n",
      "Epoch [1/1], Step [7084/7635], Loss: 4.1804\n",
      "Epoch [1/1], Step [7085/7635], Loss: 4.0974\n",
      "Epoch [1/1], Step [7086/7635], Loss: 4.0330\n",
      "Epoch [1/1], Step [7087/7635], Loss: 4.1056\n",
      "Epoch [1/1], Step [7088/7635], Loss: 4.1581\n",
      "Epoch [1/1], Step [7089/7635], Loss: 4.0937\n",
      "Epoch [1/1], Step [7090/7635], Loss: 4.0560\n",
      "Epoch [1/1], Step [7091/7635], Loss: 4.1233\n",
      "Epoch [1/1], Step [7092/7635], Loss: 4.0872\n",
      "Epoch [1/1], Step [7093/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [7094/7635], Loss: 4.1145\n",
      "Epoch [1/1], Step [7095/7635], Loss: 4.0730\n",
      "Epoch [1/1], Step [7096/7635], Loss: 4.1829\n",
      "Epoch [1/1], Step [7097/7635], Loss: 4.1121\n",
      "Epoch [1/1], Step [7098/7635], Loss: 4.0396\n",
      "Epoch [1/1], Step [7099/7635], Loss: 4.1219\n",
      "Epoch [1/1], Step [7100/7635], Loss: 4.0711\n",
      "Epoch [1/1], Step [7101/7635], Loss: 4.1065\n",
      "Epoch [1/1], Step [7102/7635], Loss: 4.0273\n",
      "Epoch [1/1], Step [7103/7635], Loss: 4.0517\n",
      "Epoch [1/1], Step [7104/7635], Loss: 4.0417\n",
      "Epoch [1/1], Step [7105/7635], Loss: 4.1374\n",
      "Epoch [1/1], Step [7106/7635], Loss: 4.1751\n",
      "Epoch [1/1], Step [7107/7635], Loss: 4.0273\n",
      "Epoch [1/1], Step [7108/7635], Loss: 4.1022\n",
      "Epoch [1/1], Step [7109/7635], Loss: 4.0723\n",
      "Epoch [1/1], Step [7110/7635], Loss: 4.1537\n",
      "Epoch [1/1], Step [7111/7635], Loss: 4.0208\n",
      "Epoch [1/1], Step [7112/7635], Loss: 4.0874\n",
      "Epoch [1/1], Step [7113/7635], Loss: 4.2138\n",
      "Epoch [1/1], Step [7114/7635], Loss: 4.1044\n",
      "Epoch [1/1], Step [7115/7635], Loss: 4.1012\n",
      "Epoch [1/1], Step [7116/7635], Loss: 4.1391\n",
      "Epoch [1/1], Step [7117/7635], Loss: 4.1148\n",
      "Epoch [1/1], Step [7118/7635], Loss: 4.1261\n",
      "Epoch [1/1], Step [7119/7635], Loss: 4.0467\n",
      "Epoch [1/1], Step [7120/7635], Loss: 4.0828\n",
      "Epoch [1/1], Step [7121/7635], Loss: 4.1571\n",
      "Epoch [1/1], Step [7122/7635], Loss: 4.0827\n",
      "Epoch [1/1], Step [7123/7635], Loss: 4.0772\n",
      "Epoch [1/1], Step [7124/7635], Loss: 4.0487\n",
      "Epoch [1/1], Step [7125/7635], Loss: 4.0774\n",
      "Epoch [1/1], Step [7126/7635], Loss: 4.1049\n",
      "Epoch [1/1], Step [7127/7635], Loss: 4.1268\n",
      "Epoch [1/1], Step [7128/7635], Loss: 4.1332\n",
      "Epoch [1/1], Step [7129/7635], Loss: 4.0676\n",
      "Epoch [1/1], Step [7130/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [7131/7635], Loss: 4.1329\n",
      "Epoch [1/1], Step [7132/7635], Loss: 4.0731\n",
      "Epoch [1/1], Step [7133/7635], Loss: 4.1130\n",
      "Epoch [1/1], Step [7134/7635], Loss: 4.1719\n",
      "Epoch [1/1], Step [7135/7635], Loss: 4.0738\n",
      "Epoch [1/1], Step [7136/7635], Loss: 4.0937\n",
      "Epoch [1/1], Step [7137/7635], Loss: 4.1052\n",
      "Epoch [1/1], Step [7138/7635], Loss: 4.0197\n",
      "Epoch [1/1], Step [7139/7635], Loss: 4.1035\n",
      "Epoch [1/1], Step [7140/7635], Loss: 4.0775\n",
      "Epoch [1/1], Step [7141/7635], Loss: 4.0741\n",
      "Epoch [1/1], Step [7142/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [7143/7635], Loss: 4.0743\n",
      "Epoch [1/1], Step [7144/7635], Loss: 4.0789\n",
      "Epoch [1/1], Step [7145/7635], Loss: 4.1060\n",
      "Epoch [1/1], Step [7146/7635], Loss: 4.1000\n",
      "Epoch [1/1], Step [7147/7635], Loss: 4.1079\n",
      "Epoch [1/1], Step [7148/7635], Loss: 4.0674\n",
      "Epoch [1/1], Step [7149/7635], Loss: 4.0831\n",
      "Epoch [1/1], Step [7150/7635], Loss: 4.0230\n",
      "Epoch [1/1], Step [7151/7635], Loss: 4.0522\n",
      "Epoch [1/1], Step [7152/7635], Loss: 4.1415\n",
      "Epoch [1/1], Step [7153/7635], Loss: 4.0360\n",
      "Epoch [1/1], Step [7154/7635], Loss: 4.1122\n",
      "Epoch [1/1], Step [7155/7635], Loss: 4.1091\n",
      "Epoch [1/1], Step [7156/7635], Loss: 4.1102\n",
      "Epoch [1/1], Step [7157/7635], Loss: 4.0616\n",
      "Epoch [1/1], Step [7158/7635], Loss: 4.1766\n",
      "Epoch [1/1], Step [7159/7635], Loss: 4.1061\n",
      "Epoch [1/1], Step [7160/7635], Loss: 4.0706\n",
      "Epoch [1/1], Step [7161/7635], Loss: 4.0729\n",
      "Epoch [1/1], Step [7162/7635], Loss: 4.1093\n",
      "Epoch [1/1], Step [7163/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [7164/7635], Loss: 4.0660\n",
      "Epoch [1/1], Step [7165/7635], Loss: 4.0674\n",
      "Epoch [1/1], Step [7166/7635], Loss: 4.0069\n",
      "Epoch [1/1], Step [7167/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [7168/7635], Loss: 4.0843\n",
      "Epoch [1/1], Step [7169/7635], Loss: 4.0757\n",
      "Epoch [1/1], Step [7170/7635], Loss: 4.0731\n",
      "Epoch [1/1], Step [7171/7635], Loss: 4.1296\n",
      "Epoch [1/1], Step [7172/7635], Loss: 4.0532\n",
      "Epoch [1/1], Step [7173/7635], Loss: 4.0796\n",
      "Epoch [1/1], Step [7174/7635], Loss: 4.0902\n",
      "Epoch [1/1], Step [7175/7635], Loss: 4.1772\n",
      "Epoch [1/1], Step [7176/7635], Loss: 4.0185\n",
      "Epoch [1/1], Step [7177/7635], Loss: 4.1942\n",
      "Epoch [1/1], Step [7178/7635], Loss: 4.0074\n",
      "Epoch [1/1], Step [7179/7635], Loss: 4.1084\n",
      "Epoch [1/1], Step [7180/7635], Loss: 4.0361\n",
      "Epoch [1/1], Step [7181/7635], Loss: 4.0405\n",
      "Epoch [1/1], Step [7182/7635], Loss: 4.0654\n",
      "Epoch [1/1], Step [7183/7635], Loss: 4.0327\n",
      "Epoch [1/1], Step [7184/7635], Loss: 4.1308\n",
      "Epoch [1/1], Step [7185/7635], Loss: 4.0920\n",
      "Epoch [1/1], Step [7186/7635], Loss: 4.2446\n",
      "Epoch [1/1], Step [7187/7635], Loss: 4.0555\n",
      "Epoch [1/1], Step [7188/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [7189/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [7190/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [7191/7635], Loss: 4.1114\n",
      "Epoch [1/1], Step [7192/7635], Loss: 4.1189\n",
      "Epoch [1/1], Step [7193/7635], Loss: 4.0927\n",
      "Epoch [1/1], Step [7194/7635], Loss: 4.1010\n",
      "Epoch [1/1], Step [7195/7635], Loss: 4.1611\n",
      "Epoch [1/1], Step [7196/7635], Loss: 4.0778\n",
      "Epoch [1/1], Step [7197/7635], Loss: 4.1345\n",
      "Epoch [1/1], Step [7198/7635], Loss: 4.0708\n",
      "Epoch [1/1], Step [7199/7635], Loss: 4.1170\n",
      "Epoch [1/1], Step [7200/7635], Loss: 4.0786\n",
      "Epoch [1/1], Step [7201/7635], Loss: 4.1594\n",
      "Epoch [1/1], Step [7202/7635], Loss: 4.1006\n",
      "Epoch [1/1], Step [7203/7635], Loss: 4.1292\n",
      "Epoch [1/1], Step [7204/7635], Loss: 4.0571\n",
      "Epoch [1/1], Step [7205/7635], Loss: 4.0937\n",
      "Epoch [1/1], Step [7206/7635], Loss: 4.1092\n",
      "Epoch [1/1], Step [7207/7635], Loss: 4.0631\n",
      "Epoch [1/1], Step [7208/7635], Loss: 4.0852\n",
      "Epoch [1/1], Step [7209/7635], Loss: 4.1128\n",
      "Epoch [1/1], Step [7210/7635], Loss: 4.0837\n",
      "Epoch [1/1], Step [7211/7635], Loss: 4.1148\n",
      "Epoch [1/1], Step [7212/7635], Loss: 4.1050\n",
      "Epoch [1/1], Step [7213/7635], Loss: 4.1133\n",
      "Epoch [1/1], Step [7214/7635], Loss: 4.0928\n",
      "Epoch [1/1], Step [7215/7635], Loss: 4.1227\n",
      "Epoch [1/1], Step [7216/7635], Loss: 4.1216\n",
      "Epoch [1/1], Step [7217/7635], Loss: 4.0581\n",
      "Epoch [1/1], Step [7218/7635], Loss: 4.0931\n",
      "Epoch [1/1], Step [7219/7635], Loss: 4.0961\n",
      "Epoch [1/1], Step [7220/7635], Loss: 4.1010\n",
      "Epoch [1/1], Step [7221/7635], Loss: 4.0304\n",
      "Epoch [1/1], Step [7222/7635], Loss: 4.1039\n",
      "Epoch [1/1], Step [7223/7635], Loss: 4.0841\n",
      "Epoch [1/1], Step [7224/7635], Loss: 4.0104\n",
      "Epoch [1/1], Step [7225/7635], Loss: 4.1067\n",
      "Epoch [1/1], Step [7226/7635], Loss: 4.0693\n",
      "Epoch [1/1], Step [7227/7635], Loss: 4.2006\n",
      "Epoch [1/1], Step [7228/7635], Loss: 4.1020\n",
      "Epoch [1/1], Step [7229/7635], Loss: 4.1474\n",
      "Epoch [1/1], Step [7230/7635], Loss: 4.0809\n",
      "Epoch [1/1], Step [7231/7635], Loss: 4.0940\n",
      "Epoch [1/1], Step [7232/7635], Loss: 4.0651\n",
      "Epoch [1/1], Step [7233/7635], Loss: 4.1428\n",
      "Epoch [1/1], Step [7234/7635], Loss: 4.0526\n",
      "Epoch [1/1], Step [7235/7635], Loss: 4.0504\n",
      "Epoch [1/1], Step [7236/7635], Loss: 4.1381\n",
      "Epoch [1/1], Step [7237/7635], Loss: 4.0277\n",
      "Epoch [1/1], Step [7238/7635], Loss: 4.0835\n",
      "Epoch [1/1], Step [7239/7635], Loss: 4.0408\n",
      "Epoch [1/1], Step [7240/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [7241/7635], Loss: 4.0893\n",
      "Epoch [1/1], Step [7242/7635], Loss: 4.0868\n",
      "Epoch [1/1], Step [7243/7635], Loss: 4.0507\n",
      "Epoch [1/1], Step [7244/7635], Loss: 4.1478\n",
      "Epoch [1/1], Step [7245/7635], Loss: 4.0270\n",
      "Epoch [1/1], Step [7246/7635], Loss: 4.1020\n",
      "Epoch [1/1], Step [7247/7635], Loss: 4.1295\n",
      "Epoch [1/1], Step [7248/7635], Loss: 4.0941\n",
      "Epoch [1/1], Step [7249/7635], Loss: 4.1187\n",
      "Epoch [1/1], Step [7250/7635], Loss: 4.0969\n",
      "Epoch [1/1], Step [7251/7635], Loss: 4.0981\n",
      "Epoch [1/1], Step [7252/7635], Loss: 4.0815\n",
      "Epoch [1/1], Step [7253/7635], Loss: 4.0611\n",
      "Epoch [1/1], Step [7254/7635], Loss: 4.1345\n",
      "Epoch [1/1], Step [7255/7635], Loss: 4.0713\n",
      "Epoch [1/1], Step [7256/7635], Loss: 4.0510\n",
      "Epoch [1/1], Step [7257/7635], Loss: 4.1002\n",
      "Epoch [1/1], Step [7258/7635], Loss: 4.1764\n",
      "Epoch [1/1], Step [7259/7635], Loss: 4.0938\n",
      "Epoch [1/1], Step [7260/7635], Loss: 4.1131\n",
      "Epoch [1/1], Step [7261/7635], Loss: 4.0664\n",
      "Epoch [1/1], Step [7262/7635], Loss: 4.0772\n",
      "Epoch [1/1], Step [7263/7635], Loss: 4.1475\n",
      "Epoch [1/1], Step [7264/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [7265/7635], Loss: 4.1667\n",
      "Epoch [1/1], Step [7266/7635], Loss: 4.0479\n",
      "Epoch [1/1], Step [7267/7635], Loss: 4.0835\n",
      "Epoch [1/1], Step [7268/7635], Loss: 4.0590\n",
      "Epoch [1/1], Step [7269/7635], Loss: 4.0631\n",
      "Epoch [1/1], Step [7270/7635], Loss: 4.1594\n",
      "Epoch [1/1], Step [7271/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [7272/7635], Loss: 4.0531\n",
      "Epoch [1/1], Step [7273/7635], Loss: 4.1273\n",
      "Epoch [1/1], Step [7274/7635], Loss: 4.1250\n",
      "Epoch [1/1], Step [7275/7635], Loss: 4.1274\n",
      "Epoch [1/1], Step [7276/7635], Loss: 4.1516\n",
      "Epoch [1/1], Step [7277/7635], Loss: 4.1109\n",
      "Epoch [1/1], Step [7278/7635], Loss: 4.1377\n",
      "Epoch [1/1], Step [7279/7635], Loss: 4.1152\n",
      "Epoch [1/1], Step [7280/7635], Loss: 4.0511\n",
      "Epoch [1/1], Step [7281/7635], Loss: 4.0567\n",
      "Epoch [1/1], Step [7282/7635], Loss: 4.0953\n",
      "Epoch [1/1], Step [7283/7635], Loss: 4.0330\n",
      "Epoch [1/1], Step [7284/7635], Loss: 4.0806\n",
      "Epoch [1/1], Step [7285/7635], Loss: 4.1022\n",
      "Epoch [1/1], Step [7286/7635], Loss: 4.0703\n",
      "Epoch [1/1], Step [7287/7635], Loss: 4.1084\n",
      "Epoch [1/1], Step [7288/7635], Loss: 4.0942\n",
      "Epoch [1/1], Step [7289/7635], Loss: 4.1663\n",
      "Epoch [1/1], Step [7290/7635], Loss: 4.1368\n",
      "Epoch [1/1], Step [7291/7635], Loss: 4.0309\n",
      "Epoch [1/1], Step [7292/7635], Loss: 4.1291\n",
      "Epoch [1/1], Step [7293/7635], Loss: 4.0255\n",
      "Epoch [1/1], Step [7294/7635], Loss: 4.1407\n",
      "Epoch [1/1], Step [7295/7635], Loss: 4.0625\n",
      "Epoch [1/1], Step [7296/7635], Loss: 4.0782\n",
      "Epoch [1/1], Step [7297/7635], Loss: 4.0612\n",
      "Epoch [1/1], Step [7298/7635], Loss: 4.1370\n",
      "Epoch [1/1], Step [7299/7635], Loss: 4.0716\n",
      "Epoch [1/1], Step [7300/7635], Loss: 4.0376\n",
      "Epoch [1/1], Step [7301/7635], Loss: 4.0304\n",
      "Epoch [1/1], Step [7302/7635], Loss: 4.0557\n",
      "Epoch [1/1], Step [7303/7635], Loss: 4.0233\n",
      "Epoch [1/1], Step [7304/7635], Loss: 4.0776\n",
      "Epoch [1/1], Step [7305/7635], Loss: 4.0633\n",
      "Epoch [1/1], Step [7306/7635], Loss: 4.1495\n",
      "Epoch [1/1], Step [7307/7635], Loss: 4.1076\n",
      "Epoch [1/1], Step [7308/7635], Loss: 4.0738\n",
      "Epoch [1/1], Step [7309/7635], Loss: 4.1261\n",
      "Epoch [1/1], Step [7310/7635], Loss: 4.1066\n",
      "Epoch [1/1], Step [7311/7635], Loss: 4.1117\n",
      "Epoch [1/1], Step [7312/7635], Loss: 4.0562\n",
      "Epoch [1/1], Step [7313/7635], Loss: 4.0975\n",
      "Epoch [1/1], Step [7314/7635], Loss: 4.0778\n",
      "Epoch [1/1], Step [7315/7635], Loss: 4.0612\n",
      "Epoch [1/1], Step [7316/7635], Loss: 4.0035\n",
      "Epoch [1/1], Step [7317/7635], Loss: 4.0803\n",
      "Epoch [1/1], Step [7318/7635], Loss: 4.0399\n",
      "Epoch [1/1], Step [7319/7635], Loss: 4.1759\n",
      "Epoch [1/1], Step [7320/7635], Loss: 4.1125\n",
      "Epoch [1/1], Step [7321/7635], Loss: 4.1614\n",
      "Epoch [1/1], Step [7322/7635], Loss: 4.0481\n",
      "Epoch [1/1], Step [7323/7635], Loss: 4.0777\n",
      "Epoch [1/1], Step [7324/7635], Loss: 4.0651\n",
      "Epoch [1/1], Step [7325/7635], Loss: 4.1056\n",
      "Epoch [1/1], Step [7326/7635], Loss: 4.1275\n",
      "Epoch [1/1], Step [7327/7635], Loss: 4.0332\n",
      "Epoch [1/1], Step [7328/7635], Loss: 4.0673\n",
      "Epoch [1/1], Step [7329/7635], Loss: 4.1456\n",
      "Epoch [1/1], Step [7330/7635], Loss: 4.1429\n",
      "Epoch [1/1], Step [7331/7635], Loss: 4.0729\n",
      "Epoch [1/1], Step [7332/7635], Loss: 4.1784\n",
      "Epoch [1/1], Step [7333/7635], Loss: 4.1360\n",
      "Epoch [1/1], Step [7334/7635], Loss: 4.0972\n",
      "Epoch [1/1], Step [7335/7635], Loss: 4.1204\n",
      "Epoch [1/1], Step [7336/7635], Loss: 4.0481\n",
      "Epoch [1/1], Step [7337/7635], Loss: 4.0984\n",
      "Epoch [1/1], Step [7338/7635], Loss: 4.0717\n",
      "Epoch [1/1], Step [7339/7635], Loss: 4.0926\n",
      "Epoch [1/1], Step [7340/7635], Loss: 4.1248\n",
      "Epoch [1/1], Step [7341/7635], Loss: 4.1363\n",
      "Epoch [1/1], Step [7342/7635], Loss: 4.2091\n",
      "Epoch [1/1], Step [7343/7635], Loss: 4.1083\n",
      "Epoch [1/1], Step [7344/7635], Loss: 4.0305\n",
      "Epoch [1/1], Step [7345/7635], Loss: 4.0476\n",
      "Epoch [1/1], Step [7346/7635], Loss: 4.1672\n",
      "Epoch [1/1], Step [7347/7635], Loss: 4.0510\n",
      "Epoch [1/1], Step [7348/7635], Loss: 4.0983\n",
      "Epoch [1/1], Step [7349/7635], Loss: 4.1515\n",
      "Epoch [1/1], Step [7350/7635], Loss: 4.1016\n",
      "Epoch [1/1], Step [7351/7635], Loss: 4.1037\n",
      "Epoch [1/1], Step [7352/7635], Loss: 4.0199\n",
      "Epoch [1/1], Step [7353/7635], Loss: 4.0498\n",
      "Epoch [1/1], Step [7354/7635], Loss: 4.0848\n",
      "Epoch [1/1], Step [7355/7635], Loss: 4.0311\n",
      "Epoch [1/1], Step [7356/7635], Loss: 4.0671\n",
      "Epoch [1/1], Step [7357/7635], Loss: 4.1091\n",
      "Epoch [1/1], Step [7358/7635], Loss: 4.1014\n",
      "Epoch [1/1], Step [7359/7635], Loss: 4.0754\n",
      "Epoch [1/1], Step [7360/7635], Loss: 4.1539\n",
      "Epoch [1/1], Step [7361/7635], Loss: 4.1207\n",
      "Epoch [1/1], Step [7362/7635], Loss: 4.0798\n",
      "Epoch [1/1], Step [7363/7635], Loss: 4.1229\n",
      "Epoch [1/1], Step [7364/7635], Loss: 4.0837\n",
      "Epoch [1/1], Step [7365/7635], Loss: 4.0602\n",
      "Epoch [1/1], Step [7366/7635], Loss: 4.1089\n",
      "Epoch [1/1], Step [7367/7635], Loss: 4.0472\n",
      "Epoch [1/1], Step [7368/7635], Loss: 4.1410\n",
      "Epoch [1/1], Step [7369/7635], Loss: 4.1504\n",
      "Epoch [1/1], Step [7370/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [7371/7635], Loss: 4.0384\n",
      "Epoch [1/1], Step [7372/7635], Loss: 4.1534\n",
      "Epoch [1/1], Step [7373/7635], Loss: 4.0660\n",
      "Epoch [1/1], Step [7374/7635], Loss: 4.0338\n",
      "Epoch [1/1], Step [7375/7635], Loss: 4.1370\n",
      "Epoch [1/1], Step [7376/7635], Loss: 4.0916\n",
      "Epoch [1/1], Step [7377/7635], Loss: 4.0723\n",
      "Epoch [1/1], Step [7378/7635], Loss: 4.0821\n",
      "Epoch [1/1], Step [7379/7635], Loss: 4.0977\n",
      "Epoch [1/1], Step [7380/7635], Loss: 4.2311\n",
      "Epoch [1/1], Step [7381/7635], Loss: 4.1300\n",
      "Epoch [1/1], Step [7382/7635], Loss: 4.1517\n",
      "Epoch [1/1], Step [7383/7635], Loss: 4.1179\n",
      "Epoch [1/1], Step [7384/7635], Loss: 4.2153\n",
      "Epoch [1/1], Step [7385/7635], Loss: 4.0875\n",
      "Epoch [1/1], Step [7386/7635], Loss: 4.1309\n",
      "Epoch [1/1], Step [7387/7635], Loss: 4.0919\n",
      "Epoch [1/1], Step [7388/7635], Loss: 4.0385\n",
      "Epoch [1/1], Step [7389/7635], Loss: 4.1099\n",
      "Epoch [1/1], Step [7390/7635], Loss: 4.1369\n",
      "Epoch [1/1], Step [7391/7635], Loss: 4.1418\n",
      "Epoch [1/1], Step [7392/7635], Loss: 4.0275\n",
      "Epoch [1/1], Step [7393/7635], Loss: 4.1076\n",
      "Epoch [1/1], Step [7394/7635], Loss: 4.1056\n",
      "Epoch [1/1], Step [7395/7635], Loss: 4.1015\n",
      "Epoch [1/1], Step [7396/7635], Loss: 4.1734\n",
      "Epoch [1/1], Step [7397/7635], Loss: 3.9920\n",
      "Epoch [1/1], Step [7398/7635], Loss: 4.0365\n",
      "Epoch [1/1], Step [7399/7635], Loss: 4.1155\n",
      "Epoch [1/1], Step [7400/7635], Loss: 4.0979\n",
      "Epoch [1/1], Step [7401/7635], Loss: 4.1059\n",
      "Epoch [1/1], Step [7402/7635], Loss: 4.0995\n",
      "Epoch [1/1], Step [7403/7635], Loss: 4.0240\n",
      "Epoch [1/1], Step [7404/7635], Loss: 4.1063\n",
      "Epoch [1/1], Step [7405/7635], Loss: 4.0619\n",
      "Epoch [1/1], Step [7406/7635], Loss: 4.0915\n",
      "Epoch [1/1], Step [7407/7635], Loss: 4.0600\n",
      "Epoch [1/1], Step [7408/7635], Loss: 4.0926\n",
      "Epoch [1/1], Step [7409/7635], Loss: 4.0830\n",
      "Epoch [1/1], Step [7410/7635], Loss: 4.0379\n",
      "Epoch [1/1], Step [7411/7635], Loss: 4.1064\n",
      "Epoch [1/1], Step [7412/7635], Loss: 4.0964\n",
      "Epoch [1/1], Step [7413/7635], Loss: 4.0363\n",
      "Epoch [1/1], Step [7414/7635], Loss: 4.0999\n",
      "Epoch [1/1], Step [7415/7635], Loss: 4.0316\n",
      "Epoch [1/1], Step [7416/7635], Loss: 4.0978\n",
      "Epoch [1/1], Step [7417/7635], Loss: 4.1474\n",
      "Epoch [1/1], Step [7418/7635], Loss: 4.0605\n",
      "Epoch [1/1], Step [7419/7635], Loss: 4.0895\n",
      "Epoch [1/1], Step [7420/7635], Loss: 4.1266\n",
      "Epoch [1/1], Step [7421/7635], Loss: 4.0851\n",
      "Epoch [1/1], Step [7422/7635], Loss: 4.0709\n",
      "Epoch [1/1], Step [7423/7635], Loss: 4.0620\n",
      "Epoch [1/1], Step [7424/7635], Loss: 4.0673\n",
      "Epoch [1/1], Step [7425/7635], Loss: 4.0868\n",
      "Epoch [1/1], Step [7426/7635], Loss: 4.1159\n",
      "Epoch [1/1], Step [7427/7635], Loss: 4.1801\n",
      "Epoch [1/1], Step [7428/7635], Loss: 4.0858\n",
      "Epoch [1/1], Step [7429/7635], Loss: 4.0718\n",
      "Epoch [1/1], Step [7430/7635], Loss: 4.1064\n",
      "Epoch [1/1], Step [7431/7635], Loss: 4.0917\n",
      "Epoch [1/1], Step [7432/7635], Loss: 4.0399\n",
      "Epoch [1/1], Step [7433/7635], Loss: 4.0702\n",
      "Epoch [1/1], Step [7434/7635], Loss: 4.0038\n",
      "Epoch [1/1], Step [7435/7635], Loss: 4.0347\n",
      "Epoch [1/1], Step [7436/7635], Loss: 4.0978\n",
      "Epoch [1/1], Step [7437/7635], Loss: 4.0514\n",
      "Epoch [1/1], Step [7438/7635], Loss: 4.0923\n",
      "Epoch [1/1], Step [7439/7635], Loss: 4.0895\n",
      "Epoch [1/1], Step [7440/7635], Loss: 4.0858\n",
      "Epoch [1/1], Step [7441/7635], Loss: 4.1607\n",
      "Epoch [1/1], Step [7442/7635], Loss: 4.0163\n",
      "Epoch [1/1], Step [7443/7635], Loss: 4.0758\n",
      "Epoch [1/1], Step [7444/7635], Loss: 4.1074\n",
      "Epoch [1/1], Step [7445/7635], Loss: 4.0616\n",
      "Epoch [1/1], Step [7446/7635], Loss: 4.0661\n",
      "Epoch [1/1], Step [7447/7635], Loss: 4.0388\n",
      "Epoch [1/1], Step [7448/7635], Loss: 4.1168\n",
      "Epoch [1/1], Step [7449/7635], Loss: 4.0994\n",
      "Epoch [1/1], Step [7450/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [7451/7635], Loss: 4.0812\n",
      "Epoch [1/1], Step [7452/7635], Loss: 4.1033\n",
      "Epoch [1/1], Step [7453/7635], Loss: 4.0979\n",
      "Epoch [1/1], Step [7454/7635], Loss: 4.0334\n",
      "Epoch [1/1], Step [7455/7635], Loss: 4.0730\n",
      "Epoch [1/1], Step [7456/7635], Loss: 4.0769\n",
      "Epoch [1/1], Step [7457/7635], Loss: 4.0050\n",
      "Epoch [1/1], Step [7458/7635], Loss: 4.0352\n",
      "Epoch [1/1], Step [7459/7635], Loss: 4.0074\n",
      "Epoch [1/1], Step [7460/7635], Loss: 4.0339\n",
      "Epoch [1/1], Step [7461/7635], Loss: 4.0501\n",
      "Epoch [1/1], Step [7462/7635], Loss: 4.0707\n",
      "Epoch [1/1], Step [7463/7635], Loss: 4.1309\n",
      "Epoch [1/1], Step [7464/7635], Loss: 4.1178\n",
      "Epoch [1/1], Step [7465/7635], Loss: 4.0628\n",
      "Epoch [1/1], Step [7466/7635], Loss: 4.1307\n",
      "Epoch [1/1], Step [7467/7635], Loss: 4.1403\n",
      "Epoch [1/1], Step [7468/7635], Loss: 4.0755\n",
      "Epoch [1/1], Step [7469/7635], Loss: 4.1813\n",
      "Epoch [1/1], Step [7470/7635], Loss: 4.0830\n",
      "Epoch [1/1], Step [7471/7635], Loss: 4.0482\n",
      "Epoch [1/1], Step [7472/7635], Loss: 4.1660\n",
      "Epoch [1/1], Step [7473/7635], Loss: 4.1066\n",
      "Epoch [1/1], Step [7474/7635], Loss: 4.1001\n",
      "Epoch [1/1], Step [7475/7635], Loss: 4.0905\n",
      "Epoch [1/1], Step [7476/7635], Loss: 4.0456\n",
      "Epoch [1/1], Step [7477/7635], Loss: 4.1236\n",
      "Epoch [1/1], Step [7478/7635], Loss: 4.1344\n",
      "Epoch [1/1], Step [7479/7635], Loss: 4.1420\n",
      "Epoch [1/1], Step [7480/7635], Loss: 4.0948\n",
      "Epoch [1/1], Step [7481/7635], Loss: 4.0167\n",
      "Epoch [1/1], Step [7482/7635], Loss: 3.9966\n",
      "Epoch [1/1], Step [7483/7635], Loss: 4.1129\n",
      "Epoch [1/1], Step [7484/7635], Loss: 4.1503\n",
      "Epoch [1/1], Step [7485/7635], Loss: 4.0767\n",
      "Epoch [1/1], Step [7486/7635], Loss: 4.0513\n",
      "Epoch [1/1], Step [7487/7635], Loss: 4.1311\n",
      "Epoch [1/1], Step [7488/7635], Loss: 4.0095\n",
      "Epoch [1/1], Step [7489/7635], Loss: 4.1535\n",
      "Epoch [1/1], Step [7490/7635], Loss: 4.0341\n",
      "Epoch [1/1], Step [7491/7635], Loss: 4.0825\n",
      "Epoch [1/1], Step [7492/7635], Loss: 4.1035\n",
      "Epoch [1/1], Step [7493/7635], Loss: 4.0772\n",
      "Epoch [1/1], Step [7494/7635], Loss: 4.0919\n",
      "Epoch [1/1], Step [7495/7635], Loss: 4.1170\n",
      "Epoch [1/1], Step [7496/7635], Loss: 4.0892\n",
      "Epoch [1/1], Step [7497/7635], Loss: 4.0617\n",
      "Epoch [1/1], Step [7498/7635], Loss: 4.1111\n",
      "Epoch [1/1], Step [7499/7635], Loss: 4.0283\n",
      "Epoch [1/1], Step [7500/7635], Loss: 4.0539\n",
      "Epoch [1/1], Step [7501/7635], Loss: 4.0238\n",
      "Epoch [1/1], Step [7502/7635], Loss: 4.0808\n",
      "Epoch [1/1], Step [7503/7635], Loss: 4.0414\n",
      "Epoch [1/1], Step [7504/7635], Loss: 4.1032\n",
      "Epoch [1/1], Step [7505/7635], Loss: 4.1162\n",
      "Epoch [1/1], Step [7506/7635], Loss: 4.1266\n",
      "Epoch [1/1], Step [7507/7635], Loss: 4.0396\n",
      "Epoch [1/1], Step [7508/7635], Loss: 4.0769\n",
      "Epoch [1/1], Step [7509/7635], Loss: 4.0063\n",
      "Epoch [1/1], Step [7510/7635], Loss: 4.1090\n",
      "Epoch [1/1], Step [7511/7635], Loss: 4.0737\n",
      "Epoch [1/1], Step [7512/7635], Loss: 4.1094\n",
      "Epoch [1/1], Step [7513/7635], Loss: 4.0640\n",
      "Epoch [1/1], Step [7514/7635], Loss: 4.1058\n",
      "Epoch [1/1], Step [7515/7635], Loss: 4.0863\n",
      "Epoch [1/1], Step [7516/7635], Loss: 4.0453\n",
      "Epoch [1/1], Step [7517/7635], Loss: 4.0994\n",
      "Epoch [1/1], Step [7518/7635], Loss: 4.0766\n",
      "Epoch [1/1], Step [7519/7635], Loss: 4.0734\n",
      "Epoch [1/1], Step [7520/7635], Loss: 4.1520\n",
      "Epoch [1/1], Step [7521/7635], Loss: 4.1205\n",
      "Epoch [1/1], Step [7522/7635], Loss: 4.0773\n",
      "Epoch [1/1], Step [7523/7635], Loss: 4.0811\n",
      "Epoch [1/1], Step [7524/7635], Loss: 4.0446\n",
      "Epoch [1/1], Step [7525/7635], Loss: 4.0529\n",
      "Epoch [1/1], Step [7526/7635], Loss: 4.1088\n",
      "Epoch [1/1], Step [7527/7635], Loss: 4.0549\n",
      "Epoch [1/1], Step [7528/7635], Loss: 4.0703\n",
      "Epoch [1/1], Step [7529/7635], Loss: 4.0936\n",
      "Epoch [1/1], Step [7530/7635], Loss: 4.0772\n",
      "Epoch [1/1], Step [7531/7635], Loss: 4.0701\n",
      "Epoch [1/1], Step [7532/7635], Loss: 4.0978\n",
      "Epoch [1/1], Step [7533/7635], Loss: 4.1181\n",
      "Epoch [1/1], Step [7534/7635], Loss: 3.9991\n",
      "Epoch [1/1], Step [7535/7635], Loss: 4.0185\n",
      "Epoch [1/1], Step [7536/7635], Loss: 4.0614\n",
      "Epoch [1/1], Step [7537/7635], Loss: 4.0498\n",
      "Epoch [1/1], Step [7538/7635], Loss: 4.0940\n",
      "Epoch [1/1], Step [7539/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [7540/7635], Loss: 4.0154\n",
      "Epoch [1/1], Step [7541/7635], Loss: 4.0221\n",
      "Epoch [1/1], Step [7542/7635], Loss: 4.1411\n",
      "Epoch [1/1], Step [7543/7635], Loss: 4.0790\n",
      "Epoch [1/1], Step [7544/7635], Loss: 4.0233\n",
      "Epoch [1/1], Step [7545/7635], Loss: 4.1105\n",
      "Epoch [1/1], Step [7546/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [7547/7635], Loss: 4.1062\n",
      "Epoch [1/1], Step [7548/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [7549/7635], Loss: 4.1507\n",
      "Epoch [1/1], Step [7550/7635], Loss: 4.0031\n",
      "Epoch [1/1], Step [7551/7635], Loss: 4.0135\n",
      "Epoch [1/1], Step [7552/7635], Loss: 4.0661\n",
      "Epoch [1/1], Step [7553/7635], Loss: 4.0714\n",
      "Epoch [1/1], Step [7554/7635], Loss: 4.0916\n",
      "Epoch [1/1], Step [7555/7635], Loss: 4.0597\n",
      "Epoch [1/1], Step [7556/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [7557/7635], Loss: 4.0816\n",
      "Epoch [1/1], Step [7558/7635], Loss: 4.0794\n",
      "Epoch [1/1], Step [7559/7635], Loss: 4.1457\n",
      "Epoch [1/1], Step [7560/7635], Loss: 4.0660\n",
      "Epoch [1/1], Step [7561/7635], Loss: 4.0366\n",
      "Epoch [1/1], Step [7562/7635], Loss: 4.1168\n",
      "Epoch [1/1], Step [7563/7635], Loss: 4.0755\n",
      "Epoch [1/1], Step [7564/7635], Loss: 4.1620\n",
      "Epoch [1/1], Step [7565/7635], Loss: 4.1194\n",
      "Epoch [1/1], Step [7566/7635], Loss: 4.0904\n",
      "Epoch [1/1], Step [7567/7635], Loss: 4.0768\n",
      "Epoch [1/1], Step [7568/7635], Loss: 4.1258\n",
      "Epoch [1/1], Step [7569/7635], Loss: 4.0910\n",
      "Epoch [1/1], Step [7570/7635], Loss: 4.1442\n",
      "Epoch [1/1], Step [7571/7635], Loss: 4.0378\n",
      "Epoch [1/1], Step [7572/7635], Loss: 4.0749\n",
      "Epoch [1/1], Step [7573/7635], Loss: 4.0695\n",
      "Epoch [1/1], Step [7574/7635], Loss: 4.0409\n",
      "Epoch [1/1], Step [7575/7635], Loss: 4.1343\n",
      "Epoch [1/1], Step [7576/7635], Loss: 4.1092\n",
      "Epoch [1/1], Step [7577/7635], Loss: 4.1282\n",
      "Epoch [1/1], Step [7578/7635], Loss: 4.1308\n",
      "Epoch [1/1], Step [7579/7635], Loss: 4.0728\n",
      "Epoch [1/1], Step [7580/7635], Loss: 4.0399\n",
      "Epoch [1/1], Step [7581/7635], Loss: 4.0597\n",
      "Epoch [1/1], Step [7582/7635], Loss: 4.0812\n",
      "Epoch [1/1], Step [7583/7635], Loss: 3.9765\n",
      "Epoch [1/1], Step [7584/7635], Loss: 4.1053\n",
      "Epoch [1/1], Step [7585/7635], Loss: 4.0804\n",
      "Epoch [1/1], Step [7586/7635], Loss: 4.0319\n",
      "Epoch [1/1], Step [7587/7635], Loss: 4.1357\n",
      "Epoch [1/1], Step [7588/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [7589/7635], Loss: 4.1336\n",
      "Epoch [1/1], Step [7590/7635], Loss: 3.9895\n",
      "Epoch [1/1], Step [7591/7635], Loss: 4.1247\n",
      "Epoch [1/1], Step [7592/7635], Loss: 4.1688\n",
      "Epoch [1/1], Step [7593/7635], Loss: 4.1246\n",
      "Epoch [1/1], Step [7594/7635], Loss: 4.0743\n",
      "Epoch [1/1], Step [7595/7635], Loss: 4.0940\n",
      "Epoch [1/1], Step [7596/7635], Loss: 4.0793\n",
      "Epoch [1/1], Step [7597/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [7598/7635], Loss: 4.0743\n",
      "Epoch [1/1], Step [7599/7635], Loss: 3.9959\n",
      "Epoch [1/1], Step [7600/7635], Loss: 4.0261\n",
      "Epoch [1/1], Step [7601/7635], Loss: 4.0696\n",
      "Epoch [1/1], Step [7602/7635], Loss: 4.0823\n",
      "Epoch [1/1], Step [7603/7635], Loss: 4.1297\n",
      "Epoch [1/1], Step [7604/7635], Loss: 4.0794\n",
      "Epoch [1/1], Step [7605/7635], Loss: 4.0672\n",
      "Epoch [1/1], Step [7606/7635], Loss: 3.9878\n",
      "Epoch [1/1], Step [7607/7635], Loss: 4.0371\n",
      "Epoch [1/1], Step [7608/7635], Loss: 4.0388\n",
      "Epoch [1/1], Step [7609/7635], Loss: 4.0879\n",
      "Epoch [1/1], Step [7610/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [7611/7635], Loss: 4.0757\n",
      "Epoch [1/1], Step [7612/7635], Loss: 4.0756\n",
      "Epoch [1/1], Step [7613/7635], Loss: 4.0932\n",
      "Epoch [1/1], Step [7614/7635], Loss: 4.0674\n",
      "Epoch [1/1], Step [7615/7635], Loss: 4.0555\n",
      "Epoch [1/1], Step [7616/7635], Loss: 4.0610\n",
      "Epoch [1/1], Step [7617/7635], Loss: 3.9965\n",
      "Epoch [1/1], Step [7618/7635], Loss: 4.0409\n",
      "Epoch [1/1], Step [7619/7635], Loss: 4.1092\n",
      "Epoch [1/1], Step [7620/7635], Loss: 4.0474\n",
      "Epoch [1/1], Step [7621/7635], Loss: 4.0644\n",
      "Epoch [1/1], Step [7622/7635], Loss: 3.9954\n",
      "Epoch [1/1], Step [7623/7635], Loss: 4.1345\n",
      "Epoch [1/1], Step [7624/7635], Loss: 4.0727\n",
      "Epoch [1/1], Step [7625/7635], Loss: 4.0633\n",
      "Epoch [1/1], Step [7626/7635], Loss: 4.1040\n",
      "Epoch [1/1], Step [7627/7635], Loss: 4.0103\n",
      "Epoch [1/1], Step [7628/7635], Loss: 4.1010\n",
      "Epoch [1/1], Step [7629/7635], Loss: 4.0375\n",
      "Epoch [1/1], Step [7630/7635], Loss: 4.0809\n",
      "Epoch [1/1], Step [7631/7635], Loss: 4.0780\n",
      "Epoch [1/1], Step [7632/7635], Loss: 4.0486\n",
      "Epoch [1/1], Step [7633/7635], Loss: 4.0468\n",
      "Epoch [1/1], Step [7634/7635], Loss: 4.0706\n",
      "Epoch [1/1] Average Loss: 4.4651, Perplexity: 86.93\n"
     ]
    }
   ],
   "source": [
    "# train(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,train_run_label)\n",
    "#return (all_losses,train_losses,perplexities)\n",
    "from src.train import train\n",
    "num_epochs = 1\n",
    "\n",
    "model = RegularizedLanguageModel(vocab_size, embedding_dim, context_length, dropout=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "data_loader = train_dataloader\n",
    "train_run_label = \"1_droupot_model\"\n",
    "print_every = 1\n",
    "\n",
    "\n",
    "\n",
    "(all_losses,train_losses,perplexities) = train(model=model,num_epochs=num_epochs,optimizer=optimizer,criterion=criterion,data_loader=data_loader,path_to_save_folder=path_to_save_folder,\n",
    "                                               train_run_label=train_run_label,vocab_size=vocab_size,device=device,print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f996fc0-ce31-42c5-9012-8ccd2cf6ec1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#read_list_from_file(label,path_to_save_folder)\n",
    "from src.helper import read_list_from_file\n",
    "all_losses = read_list_from_file(\"0_final/dropout/1_droupot_model_step_losses\",path_to_save_folder)\n",
    "\n",
    "\n",
    "from src.plot import plot_two\n",
    "plot_two(all_losses,\"Loss For all Steps\",train_losses,\"la2\",axLabel1=(\"Steps\",\"Loss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930c125",
   "metadata": {},
   "source": [
    "### 4.3. Evaluating the dropout model\n",
    "\n",
    "Now, we'll compute the perplexity of our modified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39b8da71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Model Perplexity: 52.23\n"
     ]
    }
   ],
   "source": [
    "from src.train import evaluate\n",
    "path_to_model = path_to_save_folder +\"/\"+\"0_final/dropout/1_droupot_model_model\"\n",
    "model.load_state_dict(torch.load(path_to_model, weights_only=True))\n",
    "\n",
    "perplexity_regularized = evaluate(model, dev_dataloader,criterion,device,vocab_size)\n",
    "print(f\"Regularized Model Perplexity: {perplexity_regularized:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f9fae",
   "metadata": {},
   "source": [
    "## 5. Improving the Model\n",
    "\n",
    "Now, try to further improve the model. For example, you could:\n",
    "- Increase the model depth.\n",
    "- Increase the embedding dimension.\n",
    "- Introduce non-linear activation functions.\n",
    "- Adjust the `context_length`.\n",
    "- Adjust the parameters of the optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac05d28",
   "metadata": {},
   "source": [
    "## 6. Generating text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e3d78c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es  su papelisia de mayo con lo largo muchas anunci\n",
      "Esto es   PNR. En febre O principal almas positivas que Pner\n",
      "Esto es  Herb Deanroteen ser organiz su ltimismo se constit\n",
      "Esto es iz en el momento de Novela decepoca. Adapt a\n",
      "Esto es  tensos Mark Jenkins era su rojo ubicidad de las actuacin\n",
      "Esto es izando las puede soldado no a las acces enl I\n",
      "Esto es  mero qundicemite de 10 a cocina en la e\n",
      "Esto es . Energicionales la cualquir Talces es Nuest\n",
      "Esto es  propato sobre toda. Hay ense mismo era un hogar\n",
      "Esto es  en voluntar por ordencias ese encuideradas a rec\n",
      "\n",
      "\n",
      "New Beginning\n",
      "A los artistas se les infra en el compromiso a 000 como un curios del\n",
      "A los artistas se les hablico El material haba sido que p\n",
      "A los artistas se les pastidad ausin que se vida tarde f\n",
      "A los artistas se lesiones puesta y la Facultades para propiedades finan\n",
      "A los artistas se les tien lomo matrimon sufrir los pasarici\n",
      "A los artistas se les lbumes de mano respalda con los cerr\n",
      "A los artistas se les infancia llamaneses de oficial y dicho que\n",
      "A los artistas se les espaos otros esta esta dureccin\n",
      "A los artistas se les pr Orficiembre la a que k durante to\n",
      "A los artistas se les perros y la libro en conjunto La 2017 pasada\n"
     ]
    }
   ],
   "source": [
    "#def generate_text(model, tokenizer, start_text, context_length=15, temperature=1.0):\n",
    "from src.model import generate_text\n",
    "\n",
    "\n",
    "start_text = \"Esto es \"\n",
    "\n",
    "for x in range(10):\n",
    "    generated_text = generate_text(model, tokenizer, start_text, device=device, context_length=20)\n",
    "    print(generated_text)\n",
    "\n",
    "start_text = \"A los artistas se les\"\n",
    "print(\"\\n\\nNew Beginning\")\n",
    "\n",
    "for x in range(10):\n",
    "    generated_text = generate_text(model, tokenizer, start_text, device=device, context_length=20)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ea9bb-4dd8-4411-b995-457c3defdb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cebfd1b-9d09-462b-9333-fcdad84adaae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
