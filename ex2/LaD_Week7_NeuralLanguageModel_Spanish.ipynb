{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3c5c90",
   "metadata": {},
   "source": [
    "# Week 7 - Building and Improving a Simple Language Model\n",
    "\n",
    "Welcome back! In Week 6, we learned how to prepare textual data for training a language model. We generated input-target pairs using a DataLoader. This week, we'll build upon that foundation to implement and improve a simple neural network language model.\n",
    "\n",
    "This notebook was created by Qumeng Sun and Lisa Beinborn. It adapts parts from Sebastian Raschka's notebooks accompanying his book \"Build a Large Language Model (from Scratch)\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3aeb197-244c-42a7-8e70-8c240e6c091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'torch>=2.0.1' 'jupyterlab>=4.0' 'tiktoken>=0.5.1' 'numpy>=1.25,<2.0'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead8b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n",
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4ed7a",
   "metadata": {},
   "source": [
    "## 1. Review of data preparation\n",
    "\n",
    "First, let's revisit how we prepared our data last week. We'll load the text data, tokenize it using the GPT-2 tokenizer, and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0651954-feb9-4437-93a8-e838c807ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper  import get_cleaned_spanish_text_as_string\n",
    "text_path = \"content/spa_wikipedia_2021_30K-sentences.txt\"\n",
    "\n",
    "raw_text = get_cleaned_spanish_text_as_string(text_path)\n",
    "#enc_text = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788d6b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 1221649\n",
      "First 10 tokens: [1105, 390, 450, 22379, 390, 8235, 1658, 555, 64, 719]\n"
     ]
    }
   ],
   "source": [
    "# Load the text data\n",
    "raw_text = get_cleaned_spanish_text_as_string(text_path)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize the text\n",
    "token_ids = tokenizer.encode(raw_text)\n",
    "\n",
    "print(\"Total number of tokens:\", len(token_ids))\n",
    "print(\"First 10 tokens:\", token_ids[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddcddc",
   "metadata": {},
   "source": [
    "## 2. Preparing dataset and dataloader\n",
    "\n",
    "We'll use the same `GPTDataset` class and `create_dataloader` function that we defined in Week 6 to generate input-target pairs where the target is the input sequence shifted by one token to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7e4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, context_length):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of context_length\n",
    "        for i in range(0, len(token_ids) - context_length):\n",
    "            input_sequence = token_ids[i:i + context_length]\n",
    "            \n",
    "            #shift to the right\n",
    "            target_sequence = token_ids[i + 1: i + context_length + 1]\n",
    "\n",
    "            # input and output are represented as tensors\n",
    "            self.input_ids.append(torch.tensor(input_sequence))\n",
    "            self.target_ids.append(torch.tensor(target_sequence))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader(txt, batch_size=8, context_length=4, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDataset(txt, tokenizer, context_length)\n",
    "    train, dev, test = torch.utils.data.random_split(dataset, [0.8,0.1,0.1])\n",
    "    \n",
    "    # Create dataloader\n",
    "    train_dataloader = DataLoader(\n",
    "        train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    dev_dataloader = DataLoader(\n",
    "        dev,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_dataloader, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446d5b7",
   "metadata": {},
   "source": [
    "## 3. Training and evaluating a base model\n",
    "\n",
    "We'll start by defining and training a simplistic language model to understand the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf3e3d",
   "metadata": {},
   "source": [
    "### 3.1. Defining the model\n",
    "\n",
    "Our base model will consist of:\n",
    "- **Token Embeddings**: Convert token IDs to dense vectors.\n",
    "- **Positional Embeddings**: Incorporate positional information.\n",
    "- **Linear Layer**: Predict the next token in the sequence.\n",
    "\n",
    "We'll set an appropriate `context_length` during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e43abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_length):\n",
    "        super(SimpleLanguageModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "        \n",
    "        embeddings = token_embeds + position_embeds\n",
    "        logits = self.linear(embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34614df7",
   "metadata": {},
   "source": [
    "### 3.2. Setting up training parameters\n",
    "\n",
    "We'll initialize our model with an appropriate `context_length` and prepare for training.\n",
    "\n",
    "Check the torch documentation for the description of [CrossEntropyLoss](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CrossEntropyLoss) and try to understand what it means that it \"is equivalent to applying LogSoftmax on an input, followed by NLLLoss.\"\n",
    "\n",
    "Check the documentation for the [AdamOptimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) and make sure you understand the role of the lr parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537273b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 128\n",
    "context_length = 32  # Context size for training\n",
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 128\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    raw_text, batch_size=batch_size, \n",
    "    context_length=context_length, shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleLanguageModel(vocab_size, embedding_dim, context_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0728c",
   "metadata": {},
   "source": [
    "### 3.3. Training the model\n",
    "\n",
    "Let's train a very simple model and monitor the loss. This will take a while. \n",
    "Make sure you understand every step of the code at least conceptually and consult the pytorch documentation. If the training process takes too long, test it with a smaller portion of the dataset and/or fewer epochs first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99a43d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [0/7635], Loss: 10.4438\n",
      "Epoch [1/1] Average Loss: 0.0014, Perplexity: 1.00\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "perplexities = []\n",
    "\n",
    "# Go through learning epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    # Read in data in batches\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply the forward pass\n",
    "        logits = model(x)\n",
    "\n",
    "        # Reshape logits and labels\n",
    "        token_logits = logits.view(-1, vocab_size)\n",
    "        token_labels = y.view(-1)\n",
    "\n",
    "        # To understand what is happening during reshaping, print out logits.shape and token_logits.shape\n",
    "        # and the same for y\n",
    "        #print(logits.shape, token_logits.shape)\n",
    "        #print(y.shape, token_labels.shape)\n",
    "        #print(y[0])\n",
    "        #print(token_labels[0:10])\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(token_logits,token_labels)\n",
    "\n",
    "        # Apply the backward step (calculate the gradients) \n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss over batches\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Monitor progress every twenty batches\n",
    "        if batch_idx % 1 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate average cross-entropy loss and perplexity\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    # Monitor developments over learning process\n",
    "    train_losses.append(avg_loss)\n",
    "    perplexities.append(perplexity)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c60d7d-b4a0-4bae-9219-b08120f3976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the training data for later plotting \n",
    "# write_list_to_file(label,list_to_save,path_to_save_folder)\n",
    "from src.helper import write_list_to_file\n",
    "\n",
    "#read_list_from_file(label,path_to_save_folder)\n",
    "from src.helper import read_list_from_file\n",
    "\n",
    "#defining the path were the stuff should be saved\n",
    "#the folder needs to exist in order for this to work\n",
    "path_to_save_folder= \"model/train_data\"\n",
    "\n",
    "#Write list to File\n",
    "write_list_to_file(\"normal_model_train_losses\",train_losses,path_to_save_folder)\n",
    "\n",
    "write_list_to_file(\"normal_model_perplexities\",perplexities,path_to_save_folder)\n",
    "\n",
    "\n",
    "#To read just reverse \n",
    "\n",
    "train_losses=read_list_from_file(\"normal_model_train_losses\",path_to_save_folder)\n",
    "\n",
    "perplexities= read_list_from_file(\"normal_model_perplexities\",path_to_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Loss and Perplexity\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss', linestyle='dashed', marker=\"o\")\n",
    "plt.title('Simple Model - Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(perplexities, label='Perplexity', linestyle='dashed', marker=\"o\")\n",
    "plt.title('Simple Model - Perplexity over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0af7df",
   "metadata": {},
   "source": [
    "### 3.4. Evaluating the model\n",
    "\n",
    "Now, we'll compute the perplexity of our simplest model on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dev_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(dev_dataloader)\n",
    "perplexity_simple = math.exp(avg_loss)\n",
    "print(f\"Perplexity of base model: {perplexity_simple:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0d402",
   "metadata": {},
   "source": [
    "## 4. Training with dropout\n",
    "\n",
    "To prevent overfitting and improve generalization, we'll test dropout as a regularization strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5a2df",
   "metadata": {},
   "source": [
    "### 4.1. Adding dropout\n",
    "\n",
    "We'll modify our model to include a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bdc5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_length, dropout=0.2):\n",
    "        super(RegularizedLanguageModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        # This is new!\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "        \n",
    "        embeddings = token_embeds + position_embeds\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        logits = self.linear(embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182d5e3",
   "metadata": {},
   "source": [
    "### 4.2. Retraining the model with dropout\n",
    "\n",
    "We'll re-initialize the model and optimizer, then retrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_reg = []\n",
    "perplexities_reg = []\n",
    "\n",
    "# Re-initialize the model with dropout\n",
    "model = RegularizedLanguageModel(vocab_size, embedding_dim, context_length, dropout=0.2).to(device)\n",
    "\n",
    "# Re-initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Retrain the model\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        \n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    train_losses_reg.append(avg_loss)\n",
    "    perplexities_reg.append(perplexity)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss and perplexity for the model with dropout\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses_reg, label='Training Loss', linestyle=\"dashed\", marker=\"o\")\n",
    "plt.title('Dropout Model - Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(perplexities_reg, label='Perplexity', linestyle=\"dashed\", marker=\"o\")\n",
    "plt.title('Dropout Model - Perplexity over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930c125",
   "metadata": {},
   "source": [
    "### 4.3. Evaluating the dropout model\n",
    "\n",
    "Now, we'll compute the perplexity of our modified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model to evaluation turns off dropout\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dev_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(dev_dataloader)\n",
    "perplexity_regularized = math.exp(avg_loss)\n",
    "print(f\"Regularized Model Perplexity: {perplexity_regularized:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f9fae",
   "metadata": {},
   "source": [
    "## 5. Improving the Model\n",
    "\n",
    "Now, try to further improve the model. For example, you could:\n",
    "- Increase the model depth.\n",
    "- Increase the embedding dimension.\n",
    "- Introduce non-linear activation functions.\n",
    "- Adjust the `context_length`.\n",
    "- Adjust the parameters of the optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac05d28",
   "metadata": {},
   "source": [
    "## 6. Generating text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, start_text, context_length=15, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = tokenizer.encode(start_text)\n",
    "    context = torch.tensor(generated, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(context_length):\n",
    "            if context.size(1) >= context_length:\n",
    "                break\n",
    "            logits = model(context)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            context = torch.cat([context, next_token_id.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    generated_text = tokenizer.decode(context[0].tolist())\n",
    "    return generated_text\n",
    "\n",
    "start_text = \"Emma was\"\n",
    "generated_text = generate_text(model, tokenizer, start_text, context_length=20)\n",
    "print(\"Generated Text:\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ea9bb-4dd8-4411-b995-457c3defdb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cebfd1b-9d09-462b-9333-fcdad84adaae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
