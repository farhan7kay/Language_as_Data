{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cc0039",
   "metadata": {},
   "source": [
    "# Week 8 - Training a Language Model with Attention \n",
    "\n",
    "This week, we add an attention mechanism to our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5422cf",
   "metadata": {},
   "source": [
    "## 1. Setting up data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e93791b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n",
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85cfb2",
   "metadata": {},
   "source": [
    "### 1.1 Data preparation\n",
    "The steps are the same as last week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eafa541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text data\n",
    "with open(\"jane_austen_emma.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize the text\n",
    "token_ids = tokenizer.encode(raw_text)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, context_length):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of context_length\n",
    "        for i in range(0, len(token_ids) - context_length):\n",
    "            input_sequence = token_ids[i:i + context_length]\n",
    "            \n",
    "            #shift to the right\n",
    "            target_sequence = token_ids[i + 1: i + context_length + 1]\n",
    "\n",
    "            # input and output are represented as tensors\n",
    "            self.input_ids.append(torch.tensor(input_sequence))\n",
    "            self.target_ids.append(torch.tensor(target_sequence))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader(txt, batch_size=8, context_length=4, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDataset(txt, tokenizer, context_length)\n",
    "    train, dev, test = torch.utils.data.random_split(dataset, [0.8,0.1,0.1])\n",
    "    \n",
    "    # Create dataloader\n",
    "    train_dataloader = DataLoader(\n",
    "        train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    dev_dataloader = DataLoader(\n",
    "        dev,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_dataloader, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117fb24",
   "metadata": {},
   "source": [
    "### 1.2 Initializing a model\n",
    "\n",
    "We set up our model. We add a second layer and a non-linear activation function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9376cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_length, hidden_dim=256, dropout=0.2):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_length, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Adding feedforward layers and non-linear activation functions\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "        \n",
    "        embeddings = token_embeds + position_embeds\n",
    "        dropout_layer = self.dropout(embeddings)\n",
    "        \n",
    "        # Apply the feedforward layers\n",
    "        first_layer = self.fc1(dropout_layer)\n",
    "        non_linearity = self.relu(first_layer)\n",
    "        # Second layer\n",
    "        logits = self.fc2(non_linearity)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c75557",
   "metadata": {},
   "source": [
    "## 2. Implementing Self-Attention\n",
    "\n",
    "We'll implement self-attention step by step, starting with the basic computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095d505",
   "metadata": {},
   "source": [
    "### 3.1 Computing Attention Scores\n",
    "\n",
    "For each token in the sequence, we compute Query, Key, and Value vectors using learnable weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "820a2d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.W_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.W_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, embedding_dim)\n",
    "        Q = self.W_q(x)  # Queries\n",
    "        K = self.W_k(x)  # Keys\n",
    "        V = self.W_v(x)  # Values\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        scores = scores / math.sqrt(self.embedding_dim)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Compute the context vector\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca4f19",
   "metadata": {},
   "source": [
    "### 3.2 Masking for Causal Attention\n",
    "\n",
    "In language models, we often use causal (or masked) attention to prevent the model from attending to future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57c50ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_length):\n",
    "        super(CausalSelfAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.W_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.W_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Register a buffer for the causal mask\n",
    "        self.register_buffer(  'mask',torch.tril(torch.ones(max_length, max_length)).unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.embedding_dim)\n",
    "        \n",
    "        # Apply the causal mask\n",
    "        mask = self.mask[:, :seq_length, :seq_length]\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c805eff",
   "metadata": {},
   "source": [
    "## 3. Extending to multi-head attention\n",
    "\n",
    "Multi-head attention allows the model to attend to information from different representation subspaces at different positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "502b8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCausalAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, max_length, dropout=0.1):\n",
    "        super(MultiHeadCausalAttention, self).__init__()\n",
    "        assert embedding_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.W_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.W_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.W_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.fc_out = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.register_buffer( 'mask',torch.tril(torch.ones(max_length, max_length)).unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        \n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Split the embedding into self.num_heads different pieces\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        \n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply the causal mask\n",
    "        mask = self.mask[:, :seq_length, :seq_length]\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "        context = context.view(batch_size, seq_length, self.embedding_dim)\n",
    "        \n",
    "        out = self.fc_out(context)\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17dc895",
   "metadata": {},
   "source": [
    "## 4. Adding attention to the model\n",
    "\n",
    "Now, we'll update our model to include the multi-head causal attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de5d871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_length, hidden_dim=256, num_heads=8, dropout=0.2):\n",
    "        super(LanguageModelWithAttention, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_length, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attention = MultiHeadCausalAttention(embedding_dim, num_heads, max_length, dropout)\n",
    "        \n",
    "        # Adding more layers and non-linear activation functions\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "        \n",
    "        embeddings = token_embeds + position_embeds\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        # Apply the attention layer\n",
    "        context, attention_weights = self.attention(embeddings)\n",
    "        \n",
    "        # Apply the feedforward layers\n",
    "        out = self.fc1(context)\n",
    "        out = self.relu(out)\n",
    "        logits = self.fc2(out)\n",
    "        return logits, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27b881",
   "metadata": {},
   "source": [
    "## 5. Training and evaluation\n",
    "\n",
    "We'll train the modified model and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3317c7e",
   "metadata": {},
   "source": [
    "### 5.1 Setting up training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ae6c27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "context_length = 32  # Increased context size\n",
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_heads = 4\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    raw_text, batch_size=batch_size, \n",
    "    context_length=context_length,     shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, max_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bcb99c",
   "metadata": {},
   "source": [
    "### 5.2 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "347e3ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [0/5746], Loss: 10.8256\n",
      "Epoch [1/5], Step [10/5746], Loss: 9.4703\n",
      "Epoch [1/5], Step [20/5746], Loss: 7.3493\n",
      "Epoch [1/5], Step [30/5746], Loss: 6.7348\n",
      "Epoch [1/5], Step [40/5746], Loss: 6.6691\n",
      "Epoch [1/5], Step [50/5746], Loss: 6.5743\n",
      "Epoch [1/5], Step [60/5746], Loss: 6.3742\n",
      "Epoch [1/5], Step [70/5746], Loss: 6.4502\n",
      "Epoch [1/5], Step [80/5746], Loss: 6.3553\n",
      "Epoch [1/5], Step [90/5746], Loss: 6.4019\n",
      "Epoch [1/5], Step [100/5746], Loss: 6.3764\n",
      "Epoch [1/5], Step [110/5746], Loss: 6.2002\n",
      "Epoch [1/5], Step [120/5746], Loss: 6.2741\n",
      "Epoch [1/5], Step [130/5746], Loss: 6.0208\n",
      "Epoch [1/5], Step [140/5746], Loss: 6.1357\n",
      "Epoch [1/5], Step [150/5746], Loss: 6.1721\n",
      "Epoch [1/5], Step [160/5746], Loss: 6.0181\n",
      "Epoch [1/5], Step [170/5746], Loss: 6.1316\n",
      "Epoch [1/5], Step [180/5746], Loss: 6.0971\n",
      "Epoch [1/5], Step [190/5746], Loss: 5.9647\n",
      "Epoch [1/5], Step [200/5746], Loss: 6.0845\n",
      "Epoch [1/5], Step [210/5746], Loss: 6.0408\n",
      "Epoch [1/5], Step [220/5746], Loss: 5.9025\n",
      "Epoch [1/5], Step [230/5746], Loss: 5.9088\n",
      "Epoch [1/5], Step [240/5746], Loss: 5.8501\n",
      "Epoch [1/5], Step [250/5746], Loss: 5.9924\n",
      "Epoch [1/5], Step [260/5746], Loss: 5.7917\n",
      "Epoch [1/5], Step [270/5746], Loss: 6.1066\n",
      "Epoch [1/5], Step [280/5746], Loss: 5.7358\n",
      "Epoch [1/5], Step [290/5746], Loss: 5.6243\n",
      "Epoch [1/5], Step [300/5746], Loss: 5.8488\n",
      "Epoch [1/5], Step [310/5746], Loss: 5.6451\n",
      "Epoch [1/5], Step [320/5746], Loss: 5.8010\n",
      "Epoch [1/5], Step [330/5746], Loss: 5.7614\n",
      "Epoch [1/5], Step [340/5746], Loss: 5.7232\n",
      "Epoch [1/5], Step [350/5746], Loss: 5.7539\n",
      "Epoch [1/5], Step [360/5746], Loss: 5.5661\n",
      "Epoch [1/5], Step [370/5746], Loss: 5.2479\n",
      "Epoch [1/5], Step [380/5746], Loss: 5.5917\n",
      "Epoch [1/5], Step [390/5746], Loss: 5.6396\n",
      "Epoch [1/5], Step [400/5746], Loss: 5.4973\n",
      "Epoch [1/5], Step [410/5746], Loss: 5.4946\n",
      "Epoch [1/5], Step [420/5746], Loss: 5.3094\n",
      "Epoch [1/5], Step [430/5746], Loss: 5.6370\n",
      "Epoch [1/5], Step [440/5746], Loss: 5.4390\n",
      "Epoch [1/5], Step [450/5746], Loss: 5.5473\n",
      "Epoch [1/5], Step [460/5746], Loss: 5.4666\n",
      "Epoch [1/5], Step [470/5746], Loss: 5.3402\n",
      "Epoch [1/5], Step [480/5746], Loss: 5.5356\n",
      "Epoch [1/5], Step [490/5746], Loss: 5.5409\n",
      "Epoch [1/5], Step [500/5746], Loss: 5.3700\n",
      "Epoch [1/5], Step [510/5746], Loss: 5.1841\n",
      "Epoch [1/5], Step [520/5746], Loss: 5.2363\n",
      "Epoch [1/5], Step [530/5746], Loss: 5.3760\n",
      "Epoch [1/5], Step [540/5746], Loss: 5.3962\n",
      "Epoch [1/5], Step [550/5746], Loss: 5.4191\n",
      "Epoch [1/5], Step [560/5746], Loss: 5.4616\n",
      "Epoch [1/5], Step [570/5746], Loss: 5.4157\n",
      "Epoch [1/5], Step [580/5746], Loss: 5.3571\n",
      "Epoch [1/5], Step [590/5746], Loss: 5.4254\n",
      "Epoch [1/5], Step [600/5746], Loss: 5.2178\n",
      "Epoch [1/5], Step [610/5746], Loss: 5.3439\n",
      "Epoch [1/5], Step [620/5746], Loss: 5.2239\n",
      "Epoch [1/5], Step [630/5746], Loss: 5.2980\n",
      "Epoch [1/5], Step [640/5746], Loss: 5.1625\n",
      "Epoch [1/5], Step [650/5746], Loss: 5.2056\n",
      "Epoch [1/5], Step [660/5746], Loss: 5.0340\n",
      "Epoch [1/5], Step [670/5746], Loss: 5.2703\n",
      "Epoch [1/5], Step [680/5746], Loss: 5.2122\n",
      "Epoch [1/5], Step [690/5746], Loss: 4.8497\n",
      "Epoch [1/5], Step [700/5746], Loss: 5.2551\n",
      "Epoch [1/5], Step [710/5746], Loss: 5.2486\n",
      "Epoch [1/5], Step [720/5746], Loss: 5.2364\n",
      "Epoch [1/5], Step [730/5746], Loss: 5.0797\n",
      "Epoch [1/5], Step [740/5746], Loss: 5.0612\n",
      "Epoch [1/5], Step [750/5746], Loss: 5.0356\n",
      "Epoch [1/5], Step [760/5746], Loss: 4.9345\n",
      "Epoch [1/5], Step [770/5746], Loss: 4.9957\n",
      "Epoch [1/5], Step [780/5746], Loss: 5.0723\n",
      "Epoch [1/5], Step [790/5746], Loss: 5.2621\n",
      "Epoch [1/5], Step [800/5746], Loss: 5.0861\n",
      "Epoch [1/5], Step [810/5746], Loss: 4.9775\n",
      "Epoch [1/5], Step [820/5746], Loss: 5.0454\n",
      "Epoch [1/5], Step [830/5746], Loss: 5.2318\n",
      "Epoch [1/5], Step [840/5746], Loss: 5.2480\n",
      "Epoch [1/5], Step [850/5746], Loss: 4.9804\n",
      "Epoch [1/5], Step [860/5746], Loss: 4.8876\n",
      "Epoch [1/5], Step [870/5746], Loss: 5.4382\n",
      "Epoch [1/5], Step [880/5746], Loss: 5.0311\n",
      "Epoch [1/5], Step [890/5746], Loss: 4.9742\n",
      "Epoch [1/5], Step [900/5746], Loss: 4.8575\n",
      "Epoch [1/5], Step [910/5746], Loss: 4.9956\n",
      "Epoch [1/5], Step [920/5746], Loss: 5.1844\n",
      "Epoch [1/5], Step [930/5746], Loss: 4.9673\n",
      "Epoch [1/5], Step [940/5746], Loss: 5.1040\n",
      "Epoch [1/5], Step [950/5746], Loss: 4.6705\n",
      "Epoch [1/5], Step [960/5746], Loss: 4.9833\n",
      "Epoch [1/5], Step [970/5746], Loss: 5.1006\n",
      "Epoch [1/5], Step [980/5746], Loss: 4.7779\n",
      "Epoch [1/5], Step [990/5746], Loss: 5.0046\n",
      "Epoch [1/5], Step [1000/5746], Loss: 5.0469\n",
      "Epoch [1/5], Step [1010/5746], Loss: 5.0548\n",
      "Epoch [1/5], Step [1020/5746], Loss: 4.9285\n",
      "Epoch [1/5], Step [1030/5746], Loss: 5.0306\n",
      "Epoch [1/5], Step [1040/5746], Loss: 4.8039\n",
      "Epoch [1/5], Step [1050/5746], Loss: 4.9097\n",
      "Epoch [1/5], Step [1060/5746], Loss: 4.9403\n",
      "Epoch [1/5], Step [1070/5746], Loss: 5.1003\n",
      "Epoch [1/5], Step [1080/5746], Loss: 4.6030\n",
      "Epoch [1/5], Step [1090/5746], Loss: 4.8420\n",
      "Epoch [1/5], Step [1100/5746], Loss: 4.9809\n",
      "Epoch [1/5], Step [1110/5746], Loss: 4.7576\n",
      "Epoch [1/5], Step [1120/5746], Loss: 4.8945\n",
      "Epoch [1/5], Step [1130/5746], Loss: 5.0058\n",
      "Epoch [1/5], Step [1140/5746], Loss: 4.9246\n",
      "Epoch [1/5], Step [1150/5746], Loss: 5.0163\n",
      "Epoch [1/5], Step [1160/5746], Loss: 4.6596\n",
      "Epoch [1/5], Step [1170/5746], Loss: 4.9166\n",
      "Epoch [1/5], Step [1180/5746], Loss: 4.9436\n",
      "Epoch [1/5], Step [1190/5746], Loss: 4.9840\n",
      "Epoch [1/5], Step [1200/5746], Loss: 4.5449\n",
      "Epoch [1/5], Step [1210/5746], Loss: 4.6194\n",
      "Epoch [1/5], Step [1220/5746], Loss: 4.8615\n",
      "Epoch [1/5], Step [1230/5746], Loss: 4.8923\n",
      "Epoch [1/5], Step [1240/5746], Loss: 4.5782\n",
      "Epoch [1/5], Step [1250/5746], Loss: 4.8741\n",
      "Epoch [1/5], Step [1260/5746], Loss: 4.9302\n",
      "Epoch [1/5], Step [1270/5746], Loss: 4.8280\n",
      "Epoch [1/5], Step [1280/5746], Loss: 4.8974\n",
      "Epoch [1/5], Step [1290/5746], Loss: 4.7467\n",
      "Epoch [1/5], Step [1300/5746], Loss: 4.7201\n",
      "Epoch [1/5], Step [1310/5746], Loss: 4.7630\n",
      "Epoch [1/5], Step [1320/5746], Loss: 4.6544\n",
      "Epoch [1/5], Step [1330/5746], Loss: 4.8236\n",
      "Epoch [1/5], Step [1340/5746], Loss: 4.6769\n",
      "Epoch [1/5], Step [1350/5746], Loss: 4.5090\n",
      "Epoch [1/5], Step [1360/5746], Loss: 4.6918\n",
      "Epoch [1/5], Step [1370/5746], Loss: 4.6366\n",
      "Epoch [1/5], Step [1380/5746], Loss: 4.8749\n",
      "Epoch [1/5], Step [1390/5746], Loss: 4.7304\n",
      "Epoch [1/5], Step [1400/5746], Loss: 4.8704\n",
      "Epoch [1/5], Step [1410/5746], Loss: 4.7925\n",
      "Epoch [1/5], Step [1420/5746], Loss: 4.6432\n",
      "Epoch [1/5], Step [1430/5746], Loss: 4.7844\n",
      "Epoch [1/5], Step [1440/5746], Loss: 4.6882\n",
      "Epoch [1/5], Step [1450/5746], Loss: 4.6041\n",
      "Epoch [1/5], Step [1460/5746], Loss: 4.3588\n",
      "Epoch [1/5], Step [1470/5746], Loss: 4.6364\n",
      "Epoch [1/5], Step [1480/5746], Loss: 4.8164\n",
      "Epoch [1/5], Step [1490/5746], Loss: 4.6137\n",
      "Epoch [1/5], Step [1500/5746], Loss: 4.9197\n",
      "Epoch [1/5], Step [1510/5746], Loss: 4.4523\n",
      "Epoch [1/5], Step [1520/5746], Loss: 4.7588\n",
      "Epoch [1/5], Step [1530/5746], Loss: 4.8604\n",
      "Epoch [1/5], Step [1540/5746], Loss: 4.6473\n",
      "Epoch [1/5], Step [1550/5746], Loss: 4.4461\n",
      "Epoch [1/5], Step [1560/5746], Loss: 4.8094\n",
      "Epoch [1/5], Step [1570/5746], Loss: 4.5750\n",
      "Epoch [1/5], Step [1580/5746], Loss: 4.7034\n",
      "Epoch [1/5], Step [1590/5746], Loss: 4.5650\n",
      "Epoch [1/5], Step [1600/5746], Loss: 4.5758\n",
      "Epoch [1/5], Step [1610/5746], Loss: 4.5657\n",
      "Epoch [1/5], Step [1620/5746], Loss: 4.5260\n",
      "Epoch [1/5], Step [1630/5746], Loss: 4.6240\n",
      "Epoch [1/5], Step [1640/5746], Loss: 4.5828\n",
      "Epoch [1/5], Step [1650/5746], Loss: 4.6610\n",
      "Epoch [1/5], Step [1660/5746], Loss: 4.4644\n",
      "Epoch [1/5], Step [1670/5746], Loss: 4.7171\n",
      "Epoch [1/5], Step [1680/5746], Loss: 4.5544\n",
      "Epoch [1/5], Step [1690/5746], Loss: 4.4443\n",
      "Epoch [1/5], Step [1700/5746], Loss: 4.6911\n",
      "Epoch [1/5], Step [1710/5746], Loss: 4.5029\n",
      "Epoch [1/5], Step [1720/5746], Loss: 4.4330\n",
      "Epoch [1/5], Step [1730/5746], Loss: 4.5695\n",
      "Epoch [1/5], Step [1740/5746], Loss: 4.7266\n",
      "Epoch [1/5], Step [1750/5746], Loss: 4.4691\n",
      "Epoch [1/5], Step [1760/5746], Loss: 4.6613\n",
      "Epoch [1/5], Step [1770/5746], Loss: 4.7297\n",
      "Epoch [1/5], Step [1780/5746], Loss: 4.5148\n",
      "Epoch [1/5], Step [1790/5746], Loss: 4.5371\n",
      "Epoch [1/5], Step [1800/5746], Loss: 4.6063\n",
      "Epoch [1/5], Step [1810/5746], Loss: 4.5817\n",
      "Epoch [1/5], Step [1820/5746], Loss: 4.2852\n",
      "Epoch [1/5], Step [1830/5746], Loss: 4.5387\n",
      "Epoch [1/5], Step [1840/5746], Loss: 4.4557\n",
      "Epoch [1/5], Step [1850/5746], Loss: 4.4411\n",
      "Epoch [1/5], Step [1860/5746], Loss: 4.5315\n",
      "Epoch [1/5], Step [1870/5746], Loss: 4.6236\n",
      "Epoch [1/5], Step [1880/5746], Loss: 4.5713\n",
      "Epoch [1/5], Step [1890/5746], Loss: 4.4205\n",
      "Epoch [1/5], Step [1900/5746], Loss: 4.2813\n",
      "Epoch [1/5], Step [1910/5746], Loss: 4.5357\n",
      "Epoch [1/5], Step [1920/5746], Loss: 4.6514\n",
      "Epoch [1/5], Step [1930/5746], Loss: 4.4103\n",
      "Epoch [1/5], Step [1940/5746], Loss: 4.4010\n",
      "Epoch [1/5], Step [1950/5746], Loss: 4.4441\n",
      "Epoch [1/5], Step [1960/5746], Loss: 4.5131\n",
      "Epoch [1/5], Step [1970/5746], Loss: 4.6380\n",
      "Epoch [1/5], Step [1980/5746], Loss: 4.5422\n",
      "Epoch [1/5], Step [1990/5746], Loss: 4.3100\n",
      "Epoch [1/5], Step [2000/5746], Loss: 4.4321\n",
      "Epoch [1/5], Step [2010/5746], Loss: 4.3299\n",
      "Epoch [1/5], Step [2020/5746], Loss: 4.4362\n",
      "Epoch [1/5], Step [2030/5746], Loss: 4.5544\n",
      "Epoch [1/5], Step [2040/5746], Loss: 4.3236\n",
      "Epoch [1/5], Step [2050/5746], Loss: 4.4590\n",
      "Epoch [1/5], Step [2060/5746], Loss: 4.3450\n",
      "Epoch [1/5], Step [2070/5746], Loss: 4.2871\n",
      "Epoch [1/5], Step [2080/5746], Loss: 4.4359\n",
      "Epoch [1/5], Step [2090/5746], Loss: 4.4677\n",
      "Epoch [1/5], Step [2100/5746], Loss: 4.4426\n",
      "Epoch [1/5], Step [2110/5746], Loss: 4.3752\n",
      "Epoch [1/5], Step [2120/5746], Loss: 4.4488\n",
      "Epoch [1/5], Step [2130/5746], Loss: 4.3333\n",
      "Epoch [1/5], Step [2140/5746], Loss: 4.3326\n",
      "Epoch [1/5], Step [2150/5746], Loss: 4.3844\n",
      "Epoch [1/5], Step [2160/5746], Loss: 4.2643\n",
      "Epoch [1/5], Step [2170/5746], Loss: 4.2359\n",
      "Epoch [1/5], Step [2180/5746], Loss: 4.6542\n",
      "Epoch [1/5], Step [2190/5746], Loss: 4.5233\n",
      "Epoch [1/5], Step [2200/5746], Loss: 4.4303\n",
      "Epoch [1/5], Step [2210/5746], Loss: 4.3482\n",
      "Epoch [1/5], Step [2220/5746], Loss: 4.1991\n",
      "Epoch [1/5], Step [2230/5746], Loss: 4.2858\n",
      "Epoch [1/5], Step [2240/5746], Loss: 4.4193\n",
      "Epoch [1/5], Step [2250/5746], Loss: 4.3960\n",
      "Epoch [1/5], Step [2260/5746], Loss: 4.2763\n",
      "Epoch [1/5], Step [2270/5746], Loss: 4.4192\n",
      "Epoch [1/5], Step [2280/5746], Loss: 4.4103\n",
      "Epoch [1/5], Step [2290/5746], Loss: 4.3919\n",
      "Epoch [1/5], Step [2300/5746], Loss: 4.4073\n",
      "Epoch [1/5], Step [2310/5746], Loss: 4.4327\n",
      "Epoch [1/5], Step [2320/5746], Loss: 4.2936\n",
      "Epoch [1/5], Step [2330/5746], Loss: 4.1994\n",
      "Epoch [1/5], Step [2340/5746], Loss: 4.3364\n",
      "Epoch [1/5], Step [2350/5746], Loss: 4.3148\n",
      "Epoch [1/5], Step [2360/5746], Loss: 4.3611\n",
      "Epoch [1/5], Step [2370/5746], Loss: 4.3290\n",
      "Epoch [1/5], Step [2380/5746], Loss: 4.2769\n",
      "Epoch [1/5], Step [2390/5746], Loss: 4.3152\n",
      "Epoch [1/5], Step [2400/5746], Loss: 4.2395\n",
      "Epoch [1/5], Step [2410/5746], Loss: 4.2812\n",
      "Epoch [1/5], Step [2420/5746], Loss: 4.3693\n",
      "Epoch [1/5], Step [2430/5746], Loss: 4.3653\n",
      "Epoch [1/5], Step [2440/5746], Loss: 4.3194\n",
      "Epoch [1/5], Step [2450/5746], Loss: 4.4294\n",
      "Epoch [1/5], Step [2460/5746], Loss: 4.1374\n",
      "Epoch [1/5], Step [2470/5746], Loss: 4.4943\n",
      "Epoch [1/5], Step [2480/5746], Loss: 4.4570\n",
      "Epoch [1/5], Step [2490/5746], Loss: 4.1605\n",
      "Epoch [1/5], Step [2500/5746], Loss: 4.2742\n",
      "Epoch [1/5], Step [2510/5746], Loss: 4.3144\n",
      "Epoch [1/5], Step [2520/5746], Loss: 4.3148\n",
      "Epoch [1/5], Step [2530/5746], Loss: 4.4552\n",
      "Epoch [1/5], Step [2540/5746], Loss: 4.3583\n",
      "Epoch [1/5], Step [2550/5746], Loss: 4.3908\n",
      "Epoch [1/5], Step [2560/5746], Loss: 4.3637\n",
      "Epoch [1/5], Step [2570/5746], Loss: 4.3618\n",
      "Epoch [1/5], Step [2580/5746], Loss: 4.5374\n",
      "Epoch [1/5], Step [2590/5746], Loss: 4.1544\n",
      "Epoch [1/5], Step [2600/5746], Loss: 4.2427\n",
      "Epoch [1/5], Step [2610/5746], Loss: 4.2592\n",
      "Epoch [1/5], Step [2620/5746], Loss: 4.2949\n",
      "Epoch [1/5], Step [2630/5746], Loss: 4.4075\n",
      "Epoch [1/5], Step [2640/5746], Loss: 4.2413\n",
      "Epoch [1/5], Step [2650/5746], Loss: 4.2072\n",
      "Epoch [1/5], Step [2660/5746], Loss: 4.1593\n",
      "Epoch [1/5], Step [2670/5746], Loss: 4.1532\n",
      "Epoch [1/5], Step [2680/5746], Loss: 4.4835\n",
      "Epoch [1/5], Step [2690/5746], Loss: 4.2590\n",
      "Epoch [1/5], Step [2700/5746], Loss: 4.4433\n",
      "Epoch [1/5], Step [2710/5746], Loss: 4.1334\n",
      "Epoch [1/5], Step [2720/5746], Loss: 4.2836\n",
      "Epoch [1/5], Step [2730/5746], Loss: 4.4782\n",
      "Epoch [1/5], Step [2740/5746], Loss: 4.3009\n",
      "Epoch [1/5], Step [2750/5746], Loss: 4.2772\n",
      "Epoch [1/5], Step [2760/5746], Loss: 3.9950\n",
      "Epoch [1/5], Step [2770/5746], Loss: 4.3522\n",
      "Epoch [1/5], Step [2780/5746], Loss: 4.3062\n",
      "Epoch [1/5], Step [2790/5746], Loss: 4.2930\n",
      "Epoch [1/5], Step [2800/5746], Loss: 4.0529\n",
      "Epoch [1/5], Step [2810/5746], Loss: 4.2585\n",
      "Epoch [1/5], Step [2820/5746], Loss: 4.2190\n",
      "Epoch [1/5], Step [2830/5746], Loss: 4.4764\n",
      "Epoch [1/5], Step [2840/5746], Loss: 4.2088\n",
      "Epoch [1/5], Step [2850/5746], Loss: 4.4477\n",
      "Epoch [1/5], Step [2860/5746], Loss: 4.4432\n",
      "Epoch [1/5], Step [2870/5746], Loss: 4.1021\n",
      "Epoch [1/5], Step [2880/5746], Loss: 4.2595\n",
      "Epoch [1/5], Step [2890/5746], Loss: 4.1824\n",
      "Epoch [1/5], Step [2900/5746], Loss: 4.2459\n",
      "Epoch [1/5], Step [2910/5746], Loss: 4.0049\n",
      "Epoch [1/5], Step [2920/5746], Loss: 4.0384\n",
      "Epoch [1/5], Step [2930/5746], Loss: 4.1358\n",
      "Epoch [1/5], Step [2940/5746], Loss: 4.1290\n",
      "Epoch [1/5], Step [2950/5746], Loss: 4.1951\n",
      "Epoch [1/5], Step [2960/5746], Loss: 4.1752\n",
      "Epoch [1/5], Step [2970/5746], Loss: 4.2252\n",
      "Epoch [1/5], Step [2980/5746], Loss: 4.3481\n",
      "Epoch [1/5], Step [2990/5746], Loss: 4.1825\n",
      "Epoch [1/5], Step [3000/5746], Loss: 4.0672\n",
      "Epoch [1/5], Step [3010/5746], Loss: 4.1900\n",
      "Epoch [1/5], Step [3020/5746], Loss: 4.2111\n",
      "Epoch [1/5], Step [3030/5746], Loss: 4.1684\n",
      "Epoch [1/5], Step [3040/5746], Loss: 4.1772\n",
      "Epoch [1/5], Step [3050/5746], Loss: 4.1079\n",
      "Epoch [1/5], Step [3060/5746], Loss: 4.1619\n",
      "Epoch [1/5], Step [3070/5746], Loss: 4.2377\n",
      "Epoch [1/5], Step [3080/5746], Loss: 4.1348\n",
      "Epoch [1/5], Step [3090/5746], Loss: 4.4352\n",
      "Epoch [1/5], Step [3100/5746], Loss: 4.1154\n",
      "Epoch [1/5], Step [3110/5746], Loss: 4.0832\n",
      "Epoch [1/5], Step [3120/5746], Loss: 4.1463\n",
      "Epoch [1/5], Step [3130/5746], Loss: 4.0836\n",
      "Epoch [1/5], Step [3140/5746], Loss: 4.1879\n",
      "Epoch [1/5], Step [3150/5746], Loss: 4.3095\n",
      "Epoch [1/5], Step [3160/5746], Loss: 4.2956\n",
      "Epoch [1/5], Step [3170/5746], Loss: 4.2806\n",
      "Epoch [1/5], Step [3180/5746], Loss: 4.2207\n",
      "Epoch [1/5], Step [3190/5746], Loss: 4.1829\n",
      "Epoch [1/5], Step [3200/5746], Loss: 4.1219\n",
      "Epoch [1/5], Step [3210/5746], Loss: 4.1890\n",
      "Epoch [1/5], Step [3220/5746], Loss: 4.1267\n",
      "Epoch [1/5], Step [3230/5746], Loss: 4.2405\n",
      "Epoch [1/5], Step [3240/5746], Loss: 4.0393\n",
      "Epoch [1/5], Step [3250/5746], Loss: 4.0396\n",
      "Epoch [1/5], Step [3260/5746], Loss: 4.2021\n",
      "Epoch [1/5], Step [3270/5746], Loss: 4.1527\n",
      "Epoch [1/5], Step [3280/5746], Loss: 4.2485\n",
      "Epoch [1/5], Step [3290/5746], Loss: 3.9562\n",
      "Epoch [1/5], Step [3300/5746], Loss: 4.0550\n",
      "Epoch [1/5], Step [3310/5746], Loss: 4.0534\n",
      "Epoch [1/5], Step [3320/5746], Loss: 3.9144\n",
      "Epoch [1/5], Step [3330/5746], Loss: 4.2523\n",
      "Epoch [1/5], Step [3340/5746], Loss: 4.1067\n",
      "Epoch [1/5], Step [3350/5746], Loss: 4.0092\n",
      "Epoch [1/5], Step [3360/5746], Loss: 4.0064\n",
      "Epoch [1/5], Step [3370/5746], Loss: 3.9769\n",
      "Epoch [1/5], Step [3380/5746], Loss: 4.0362\n",
      "Epoch [1/5], Step [3390/5746], Loss: 4.0786\n",
      "Epoch [1/5], Step [3400/5746], Loss: 4.0800\n",
      "Epoch [1/5], Step [3410/5746], Loss: 3.9624\n",
      "Epoch [1/5], Step [3420/5746], Loss: 3.9493\n",
      "Epoch [1/5], Step [3430/5746], Loss: 4.2590\n",
      "Epoch [1/5], Step [3440/5746], Loss: 4.0730\n",
      "Epoch [1/5], Step [3450/5746], Loss: 3.9844\n",
      "Epoch [1/5], Step [3460/5746], Loss: 3.9374\n",
      "Epoch [1/5], Step [3470/5746], Loss: 4.1091\n",
      "Epoch [1/5], Step [3480/5746], Loss: 4.1049\n",
      "Epoch [1/5], Step [3490/5746], Loss: 4.1512\n",
      "Epoch [1/5], Step [3500/5746], Loss: 4.0722\n",
      "Epoch [1/5], Step [3510/5746], Loss: 3.9545\n",
      "Epoch [1/5], Step [3520/5746], Loss: 4.1773\n",
      "Epoch [1/5], Step [3530/5746], Loss: 4.1190\n",
      "Epoch [1/5], Step [3540/5746], Loss: 4.0646\n",
      "Epoch [1/5], Step [3550/5746], Loss: 4.0539\n",
      "Epoch [1/5], Step [3560/5746], Loss: 4.1312\n",
      "Epoch [1/5], Step [3570/5746], Loss: 3.9463\n",
      "Epoch [1/5], Step [3580/5746], Loss: 4.0770\n",
      "Epoch [1/5], Step [3590/5746], Loss: 4.0057\n",
      "Epoch [1/5], Step [3600/5746], Loss: 4.1216\n",
      "Epoch [1/5], Step [3610/5746], Loss: 3.7880\n",
      "Epoch [1/5], Step [3620/5746], Loss: 4.2329\n",
      "Epoch [1/5], Step [3630/5746], Loss: 4.2298\n",
      "Epoch [1/5], Step [3640/5746], Loss: 4.3340\n",
      "Epoch [1/5], Step [3650/5746], Loss: 4.2559\n",
      "Epoch [1/5], Step [3660/5746], Loss: 3.8908\n",
      "Epoch [1/5], Step [3670/5746], Loss: 3.9703\n",
      "Epoch [1/5], Step [3680/5746], Loss: 3.9376\n",
      "Epoch [1/5], Step [3690/5746], Loss: 3.7356\n",
      "Epoch [1/5], Step [3700/5746], Loss: 4.0220\n",
      "Epoch [1/5], Step [3710/5746], Loss: 4.0706\n",
      "Epoch [1/5], Step [3720/5746], Loss: 4.0346\n",
      "Epoch [1/5], Step [3730/5746], Loss: 4.1760\n",
      "Epoch [1/5], Step [3740/5746], Loss: 3.8788\n",
      "Epoch [1/5], Step [3750/5746], Loss: 4.2090\n",
      "Epoch [1/5], Step [3760/5746], Loss: 3.9840\n",
      "Epoch [1/5], Step [3770/5746], Loss: 4.2462\n",
      "Epoch [1/5], Step [3780/5746], Loss: 3.9655\n",
      "Epoch [1/5], Step [3790/5746], Loss: 4.1100\n",
      "Epoch [1/5], Step [3800/5746], Loss: 4.0641\n",
      "Epoch [1/5], Step [3810/5746], Loss: 3.9997\n",
      "Epoch [1/5], Step [3820/5746], Loss: 4.1191\n",
      "Epoch [1/5], Step [3830/5746], Loss: 4.1238\n",
      "Epoch [1/5], Step [3840/5746], Loss: 3.9014\n",
      "Epoch [1/5], Step [3850/5746], Loss: 3.9668\n",
      "Epoch [1/5], Step [3860/5746], Loss: 4.0684\n",
      "Epoch [1/5], Step [3870/5746], Loss: 3.8934\n",
      "Epoch [1/5], Step [3880/5746], Loss: 4.1966\n",
      "Epoch [1/5], Step [3890/5746], Loss: 4.1676\n",
      "Epoch [1/5], Step [3900/5746], Loss: 4.0064\n",
      "Epoch [1/5], Step [3910/5746], Loss: 3.9661\n",
      "Epoch [1/5], Step [3920/5746], Loss: 4.0577\n",
      "Epoch [1/5], Step [3930/5746], Loss: 3.9860\n",
      "Epoch [1/5], Step [3940/5746], Loss: 4.0002\n",
      "Epoch [1/5], Step [3950/5746], Loss: 4.1658\n",
      "Epoch [1/5], Step [3960/5746], Loss: 3.8915\n",
      "Epoch [1/5], Step [3970/5746], Loss: 4.0852\n",
      "Epoch [1/5], Step [3980/5746], Loss: 4.0672\n",
      "Epoch [1/5], Step [3990/5746], Loss: 3.9624\n",
      "Epoch [1/5], Step [4000/5746], Loss: 3.9947\n",
      "Epoch [1/5], Step [4010/5746], Loss: 4.1918\n",
      "Epoch [1/5], Step [4020/5746], Loss: 4.2543\n",
      "Epoch [1/5], Step [4030/5746], Loss: 4.2605\n",
      "Epoch [1/5], Step [4040/5746], Loss: 4.1243\n",
      "Epoch [1/5], Step [4050/5746], Loss: 4.0656\n",
      "Epoch [1/5], Step [4060/5746], Loss: 4.0904\n",
      "Epoch [1/5], Step [4070/5746], Loss: 4.0780\n",
      "Epoch [1/5], Step [4080/5746], Loss: 3.9640\n",
      "Epoch [1/5], Step [4090/5746], Loss: 4.0529\n",
      "Epoch [1/5], Step [4100/5746], Loss: 3.8391\n",
      "Epoch [1/5], Step [4110/5746], Loss: 3.8669\n",
      "Epoch [1/5], Step [4120/5746], Loss: 4.1011\n",
      "Epoch [1/5], Step [4130/5746], Loss: 4.0378\n",
      "Epoch [1/5], Step [4140/5746], Loss: 3.9636\n",
      "Epoch [1/5], Step [4150/5746], Loss: 4.0749\n",
      "Epoch [1/5], Step [4160/5746], Loss: 4.1409\n",
      "Epoch [1/5], Step [4170/5746], Loss: 4.0779\n",
      "Epoch [1/5], Step [4180/5746], Loss: 3.9713\n",
      "Epoch [1/5], Step [4190/5746], Loss: 3.9702\n",
      "Epoch [1/5], Step [4200/5746], Loss: 4.2093\n",
      "Epoch [1/5], Step [4210/5746], Loss: 3.9220\n",
      "Epoch [1/5], Step [4220/5746], Loss: 4.0463\n",
      "Epoch [1/5], Step [4230/5746], Loss: 4.0040\n",
      "Epoch [1/5], Step [4240/5746], Loss: 3.9875\n",
      "Epoch [1/5], Step [4250/5746], Loss: 3.9558\n",
      "Epoch [1/5], Step [4260/5746], Loss: 3.8486\n",
      "Epoch [1/5], Step [4270/5746], Loss: 4.0668\n",
      "Epoch [1/5], Step [4280/5746], Loss: 4.0431\n",
      "Epoch [1/5], Step [4290/5746], Loss: 4.1192\n",
      "Epoch [1/5], Step [4300/5746], Loss: 3.8773\n",
      "Epoch [1/5], Step [4310/5746], Loss: 3.9929\n",
      "Epoch [1/5], Step [4320/5746], Loss: 4.0039\n",
      "Epoch [1/5], Step [4330/5746], Loss: 4.0853\n",
      "Epoch [1/5], Step [4340/5746], Loss: 3.8943\n",
      "Epoch [1/5], Step [4350/5746], Loss: 3.9181\n",
      "Epoch [1/5], Step [4360/5746], Loss: 4.1425\n",
      "Epoch [1/5], Step [4370/5746], Loss: 3.8330\n",
      "Epoch [1/5], Step [4380/5746], Loss: 3.8622\n",
      "Epoch [1/5], Step [4390/5746], Loss: 3.8160\n",
      "Epoch [1/5], Step [4400/5746], Loss: 3.9596\n",
      "Epoch [1/5], Step [4410/5746], Loss: 3.7847\n",
      "Epoch [1/5], Step [4420/5746], Loss: 3.9361\n",
      "Epoch [1/5], Step [4430/5746], Loss: 3.9677\n",
      "Epoch [1/5], Step [4440/5746], Loss: 4.1196\n",
      "Epoch [1/5], Step [4450/5746], Loss: 4.0059\n",
      "Epoch [1/5], Step [4460/5746], Loss: 3.8881\n",
      "Epoch [1/5], Step [4470/5746], Loss: 3.8288\n",
      "Epoch [1/5], Step [4480/5746], Loss: 4.0568\n",
      "Epoch [1/5], Step [4490/5746], Loss: 4.0415\n",
      "Epoch [1/5], Step [4500/5746], Loss: 3.8357\n",
      "Epoch [1/5], Step [4510/5746], Loss: 4.0695\n",
      "Epoch [1/5], Step [4520/5746], Loss: 4.0002\n",
      "Epoch [1/5], Step [4530/5746], Loss: 3.8283\n",
      "Epoch [1/5], Step [4540/5746], Loss: 3.8943\n",
      "Epoch [1/5], Step [4550/5746], Loss: 4.0194\n",
      "Epoch [1/5], Step [4560/5746], Loss: 3.9720\n",
      "Epoch [1/5], Step [4570/5746], Loss: 3.9670\n",
      "Epoch [1/5], Step [4580/5746], Loss: 3.8543\n",
      "Epoch [1/5], Step [4590/5746], Loss: 3.7581\n",
      "Epoch [1/5], Step [4600/5746], Loss: 3.6662\n",
      "Epoch [1/5], Step [4610/5746], Loss: 4.1819\n",
      "Epoch [1/5], Step [4620/5746], Loss: 3.9615\n",
      "Epoch [1/5], Step [4630/5746], Loss: 3.6886\n",
      "Epoch [1/5], Step [4640/5746], Loss: 3.9513\n",
      "Epoch [1/5], Step [4650/5746], Loss: 3.9584\n",
      "Epoch [1/5], Step [4660/5746], Loss: 4.0278\n",
      "Epoch [1/5], Step [4670/5746], Loss: 4.0604\n",
      "Epoch [1/5], Step [4680/5746], Loss: 3.9576\n",
      "Epoch [1/5], Step [4690/5746], Loss: 3.9628\n",
      "Epoch [1/5], Step [4700/5746], Loss: 3.9138\n",
      "Epoch [1/5], Step [4710/5746], Loss: 3.8632\n",
      "Epoch [1/5], Step [4720/5746], Loss: 3.7768\n",
      "Epoch [1/5], Step [4730/5746], Loss: 3.9642\n",
      "Epoch [1/5], Step [4740/5746], Loss: 3.8761\n",
      "Epoch [1/5], Step [4750/5746], Loss: 3.9993\n",
      "Epoch [1/5], Step [4760/5746], Loss: 4.1088\n",
      "Epoch [1/5], Step [4770/5746], Loss: 3.6713\n",
      "Epoch [1/5], Step [4780/5746], Loss: 3.7528\n",
      "Epoch [1/5], Step [4790/5746], Loss: 4.0416\n",
      "Epoch [1/5], Step [4800/5746], Loss: 3.8031\n",
      "Epoch [1/5], Step [4810/5746], Loss: 3.8325\n",
      "Epoch [1/5], Step [4820/5746], Loss: 4.0284\n",
      "Epoch [1/5], Step [4830/5746], Loss: 3.8761\n",
      "Epoch [1/5], Step [4840/5746], Loss: 4.0540\n",
      "Epoch [1/5], Step [4850/5746], Loss: 4.0115\n",
      "Epoch [1/5], Step [4860/5746], Loss: 3.7475\n",
      "Epoch [1/5], Step [4870/5746], Loss: 3.9224\n",
      "Epoch [1/5], Step [4880/5746], Loss: 3.8950\n",
      "Epoch [1/5], Step [4890/5746], Loss: 3.8408\n",
      "Epoch [1/5], Step [4900/5746], Loss: 3.8210\n",
      "Epoch [1/5], Step [4910/5746], Loss: 4.0843\n",
      "Epoch [1/5], Step [4920/5746], Loss: 3.7794\n",
      "Epoch [1/5], Step [4930/5746], Loss: 4.0694\n",
      "Epoch [1/5], Step [4940/5746], Loss: 3.9240\n",
      "Epoch [1/5], Step [4950/5746], Loss: 3.9395\n",
      "Epoch [1/5], Step [4960/5746], Loss: 4.0360\n",
      "Epoch [1/5], Step [4970/5746], Loss: 3.7283\n",
      "Epoch [1/5], Step [4980/5746], Loss: 3.7578\n",
      "Epoch [1/5], Step [4990/5746], Loss: 3.7668\n",
      "Epoch [1/5], Step [5000/5746], Loss: 3.8996\n",
      "Epoch [1/5], Step [5010/5746], Loss: 3.9567\n",
      "Epoch [1/5], Step [5020/5746], Loss: 3.9434\n",
      "Epoch [1/5], Step [5030/5746], Loss: 3.9203\n",
      "Epoch [1/5], Step [5040/5746], Loss: 3.9899\n",
      "Epoch [1/5], Step [5050/5746], Loss: 3.8647\n",
      "Epoch [1/5], Step [5060/5746], Loss: 3.8710\n",
      "Epoch [1/5], Step [5070/5746], Loss: 3.9996\n",
      "Epoch [1/5], Step [5080/5746], Loss: 3.9580\n",
      "Epoch [1/5], Step [5090/5746], Loss: 3.9140\n",
      "Epoch [1/5], Step [5100/5746], Loss: 3.8703\n",
      "Epoch [1/5], Step [5110/5746], Loss: 4.0423\n",
      "Epoch [1/5], Step [5120/5746], Loss: 3.9454\n",
      "Epoch [1/5], Step [5130/5746], Loss: 3.8288\n",
      "Epoch [1/5], Step [5140/5746], Loss: 3.9778\n",
      "Epoch [1/5], Step [5150/5746], Loss: 3.8423\n",
      "Epoch [1/5], Step [5160/5746], Loss: 3.8158\n",
      "Epoch [1/5], Step [5170/5746], Loss: 3.8313\n",
      "Epoch [1/5], Step [5180/5746], Loss: 3.8354\n",
      "Epoch [1/5], Step [5190/5746], Loss: 4.0417\n",
      "Epoch [1/5], Step [5200/5746], Loss: 3.8235\n",
      "Epoch [1/5], Step [5210/5746], Loss: 3.7210\n",
      "Epoch [1/5], Step [5220/5746], Loss: 3.9284\n",
      "Epoch [1/5], Step [5230/5746], Loss: 3.9580\n",
      "Epoch [1/5], Step [5240/5746], Loss: 3.9396\n",
      "Epoch [1/5], Step [5250/5746], Loss: 3.9091\n",
      "Epoch [1/5], Step [5260/5746], Loss: 3.9401\n",
      "Epoch [1/5], Step [5270/5746], Loss: 3.9737\n",
      "Epoch [1/5], Step [5280/5746], Loss: 3.7733\n",
      "Epoch [1/5], Step [5290/5746], Loss: 3.8009\n",
      "Epoch [1/5], Step [5300/5746], Loss: 3.9545\n",
      "Epoch [1/5], Step [5310/5746], Loss: 3.9142\n",
      "Epoch [1/5], Step [5320/5746], Loss: 3.8116\n",
      "Epoch [1/5], Step [5330/5746], Loss: 3.7234\n",
      "Epoch [1/5], Step [5340/5746], Loss: 3.8501\n",
      "Epoch [1/5], Step [5350/5746], Loss: 3.8521\n",
      "Epoch [1/5], Step [5360/5746], Loss: 3.9045\n",
      "Epoch [1/5], Step [5370/5746], Loss: 3.8013\n",
      "Epoch [1/5], Step [5380/5746], Loss: 4.0618\n",
      "Epoch [1/5], Step [5390/5746], Loss: 3.7483\n",
      "Epoch [1/5], Step [5400/5746], Loss: 3.9276\n",
      "Epoch [1/5], Step [5410/5746], Loss: 3.7307\n",
      "Epoch [1/5], Step [5420/5746], Loss: 3.9544\n",
      "Epoch [1/5], Step [5430/5746], Loss: 3.7424\n",
      "Epoch [1/5], Step [5440/5746], Loss: 3.9470\n",
      "Epoch [1/5], Step [5450/5746], Loss: 3.9316\n",
      "Epoch [1/5], Step [5460/5746], Loss: 3.8300\n",
      "Epoch [1/5], Step [5470/5746], Loss: 3.9405\n",
      "Epoch [1/5], Step [5480/5746], Loss: 3.7702\n",
      "Epoch [1/5], Step [5490/5746], Loss: 3.9933\n",
      "Epoch [1/5], Step [5500/5746], Loss: 3.8823\n",
      "Epoch [1/5], Step [5510/5746], Loss: 3.6540\n",
      "Epoch [1/5], Step [5520/5746], Loss: 3.9462\n",
      "Epoch [1/5], Step [5530/5746], Loss: 3.9242\n",
      "Epoch [1/5], Step [5540/5746], Loss: 3.6746\n",
      "Epoch [1/5], Step [5550/5746], Loss: 3.8974\n",
      "Epoch [1/5], Step [5560/5746], Loss: 3.9863\n",
      "Epoch [1/5], Step [5570/5746], Loss: 3.8944\n",
      "Epoch [1/5], Step [5580/5746], Loss: 3.8921\n",
      "Epoch [1/5], Step [5590/5746], Loss: 3.8010\n",
      "Epoch [1/5], Step [5600/5746], Loss: 3.8225\n",
      "Epoch [1/5], Step [5610/5746], Loss: 3.8262\n",
      "Epoch [1/5], Step [5620/5746], Loss: 3.9364\n",
      "Epoch [1/5], Step [5630/5746], Loss: 3.7314\n",
      "Epoch [1/5], Step [5640/5746], Loss: 3.8730\n",
      "Epoch [1/5], Step [5650/5746], Loss: 3.6499\n",
      "Epoch [1/5], Step [5660/5746], Loss: 3.7350\n",
      "Epoch [1/5], Step [5670/5746], Loss: 3.7320\n",
      "Epoch [1/5], Step [5680/5746], Loss: 3.7830\n",
      "Epoch [1/5], Step [5690/5746], Loss: 3.6577\n",
      "Epoch [1/5], Step [5700/5746], Loss: 3.8570\n",
      "Epoch [1/5], Step [5710/5746], Loss: 3.7802\n",
      "Epoch [1/5], Step [5720/5746], Loss: 3.7788\n",
      "Epoch [1/5], Step [5730/5746], Loss: 3.6548\n",
      "Epoch [1/5], Step [5740/5746], Loss: 3.8136\n",
      "Epoch [1/5] Average Loss: 4.4453, Perplexity: 85.22\n",
      "Epoch [2/5], Step [0/5746], Loss: 3.7948\n",
      "Epoch [2/5], Step [10/5746], Loss: 3.9567\n",
      "Epoch [2/5], Step [20/5746], Loss: 3.6694\n",
      "Epoch [2/5], Step [30/5746], Loss: 3.6142\n",
      "Epoch [2/5], Step [40/5746], Loss: 3.8292\n",
      "Epoch [2/5], Step [50/5746], Loss: 3.7842\n",
      "Epoch [2/5], Step [60/5746], Loss: 3.8421\n",
      "Epoch [2/5], Step [70/5746], Loss: 3.7754\n",
      "Epoch [2/5], Step [80/5746], Loss: 3.8011\n",
      "Epoch [2/5], Step [90/5746], Loss: 3.9096\n",
      "Epoch [2/5], Step [100/5746], Loss: 3.8927\n",
      "Epoch [2/5], Step [110/5746], Loss: 3.7042\n",
      "Epoch [2/5], Step [120/5746], Loss: 3.7972\n",
      "Epoch [2/5], Step [130/5746], Loss: 3.9558\n",
      "Epoch [2/5], Step [140/5746], Loss: 3.9155\n",
      "Epoch [2/5], Step [150/5746], Loss: 3.8596\n",
      "Epoch [2/5], Step [160/5746], Loss: 3.8695\n",
      "Epoch [2/5], Step [170/5746], Loss: 3.7364\n",
      "Epoch [2/5], Step [180/5746], Loss: 3.7185\n",
      "Epoch [2/5], Step [190/5746], Loss: 3.7203\n",
      "Epoch [2/5], Step [200/5746], Loss: 3.7891\n",
      "Epoch [2/5], Step [210/5746], Loss: 3.8789\n",
      "Epoch [2/5], Step [220/5746], Loss: 3.8641\n",
      "Epoch [2/5], Step [230/5746], Loss: 3.9315\n",
      "Epoch [2/5], Step [240/5746], Loss: 3.7910\n",
      "Epoch [2/5], Step [250/5746], Loss: 3.7978\n",
      "Epoch [2/5], Step [260/5746], Loss: 3.6452\n",
      "Epoch [2/5], Step [270/5746], Loss: 3.7374\n",
      "Epoch [2/5], Step [280/5746], Loss: 3.7880\n",
      "Epoch [2/5], Step [290/5746], Loss: 3.8197\n",
      "Epoch [2/5], Step [300/5746], Loss: 3.6748\n",
      "Epoch [2/5], Step [310/5746], Loss: 3.8033\n",
      "Epoch [2/5], Step [320/5746], Loss: 3.7446\n",
      "Epoch [2/5], Step [330/5746], Loss: 3.7611\n",
      "Epoch [2/5], Step [340/5746], Loss: 3.5897\n",
      "Epoch [2/5], Step [350/5746], Loss: 3.9332\n",
      "Epoch [2/5], Step [360/5746], Loss: 3.8254\n",
      "Epoch [2/5], Step [370/5746], Loss: 3.7347\n",
      "Epoch [2/5], Step [380/5746], Loss: 3.8546\n",
      "Epoch [2/5], Step [390/5746], Loss: 3.8116\n",
      "Epoch [2/5], Step [400/5746], Loss: 3.7453\n",
      "Epoch [2/5], Step [410/5746], Loss: 3.7031\n",
      "Epoch [2/5], Step [420/5746], Loss: 3.7597\n",
      "Epoch [2/5], Step [430/5746], Loss: 3.7955\n",
      "Epoch [2/5], Step [440/5746], Loss: 3.8694\n",
      "Epoch [2/5], Step [450/5746], Loss: 3.7748\n",
      "Epoch [2/5], Step [460/5746], Loss: 3.6845\n",
      "Epoch [2/5], Step [470/5746], Loss: 3.8835\n",
      "Epoch [2/5], Step [480/5746], Loss: 3.6773\n",
      "Epoch [2/5], Step [490/5746], Loss: 3.8448\n",
      "Epoch [2/5], Step [500/5746], Loss: 3.7812\n",
      "Epoch [2/5], Step [510/5746], Loss: 3.8111\n",
      "Epoch [2/5], Step [520/5746], Loss: 3.7163\n",
      "Epoch [2/5], Step [530/5746], Loss: 3.6936\n",
      "Epoch [2/5], Step [540/5746], Loss: 3.8486\n",
      "Epoch [2/5], Step [550/5746], Loss: 3.7463\n",
      "Epoch [2/5], Step [560/5746], Loss: 3.8085\n",
      "Epoch [2/5], Step [570/5746], Loss: 3.9039\n",
      "Epoch [2/5], Step [580/5746], Loss: 3.8269\n",
      "Epoch [2/5], Step [590/5746], Loss: 3.6993\n",
      "Epoch [2/5], Step [600/5746], Loss: 3.6393\n",
      "Epoch [2/5], Step [610/5746], Loss: 3.8066\n",
      "Epoch [2/5], Step [620/5746], Loss: 3.6865\n",
      "Epoch [2/5], Step [630/5746], Loss: 3.6767\n",
      "Epoch [2/5], Step [640/5746], Loss: 3.8170\n",
      "Epoch [2/5], Step [650/5746], Loss: 3.6311\n",
      "Epoch [2/5], Step [660/5746], Loss: 3.8511\n",
      "Epoch [2/5], Step [670/5746], Loss: 3.7872\n",
      "Epoch [2/5], Step [680/5746], Loss: 3.7796\n",
      "Epoch [2/5], Step [690/5746], Loss: 3.7842\n",
      "Epoch [2/5], Step [700/5746], Loss: 3.8098\n",
      "Epoch [2/5], Step [710/5746], Loss: 3.7247\n",
      "Epoch [2/5], Step [720/5746], Loss: 3.7634\n",
      "Epoch [2/5], Step [730/5746], Loss: 3.7730\n",
      "Epoch [2/5], Step [740/5746], Loss: 3.6012\n",
      "Epoch [2/5], Step [750/5746], Loss: 3.7271\n",
      "Epoch [2/5], Step [760/5746], Loss: 3.8068\n",
      "Epoch [2/5], Step [770/5746], Loss: 3.7659\n",
      "Epoch [2/5], Step [780/5746], Loss: 3.8843\n",
      "Epoch [2/5], Step [790/5746], Loss: 3.7195\n",
      "Epoch [2/5], Step [800/5746], Loss: 3.7925\n",
      "Epoch [2/5], Step [810/5746], Loss: 3.8543\n",
      "Epoch [2/5], Step [820/5746], Loss: 3.7947\n",
      "Epoch [2/5], Step [830/5746], Loss: 3.7760\n",
      "Epoch [2/5], Step [840/5746], Loss: 3.7499\n",
      "Epoch [2/5], Step [850/5746], Loss: 3.7481\n",
      "Epoch [2/5], Step [860/5746], Loss: 3.8770\n",
      "Epoch [2/5], Step [870/5746], Loss: 3.7943\n",
      "Epoch [2/5], Step [880/5746], Loss: 3.7557\n",
      "Epoch [2/5], Step [890/5746], Loss: 3.7191\n",
      "Epoch [2/5], Step [900/5746], Loss: 3.8187\n",
      "Epoch [2/5], Step [910/5746], Loss: 3.9605\n",
      "Epoch [2/5], Step [920/5746], Loss: 3.8002\n",
      "Epoch [2/5], Step [930/5746], Loss: 3.7785\n",
      "Epoch [2/5], Step [940/5746], Loss: 3.6240\n",
      "Epoch [2/5], Step [950/5746], Loss: 3.7432\n",
      "Epoch [2/5], Step [960/5746], Loss: 3.8511\n",
      "Epoch [2/5], Step [970/5746], Loss: 3.6100\n",
      "Epoch [2/5], Step [980/5746], Loss: 3.7040\n",
      "Epoch [2/5], Step [990/5746], Loss: 3.5993\n",
      "Epoch [2/5], Step [1000/5746], Loss: 3.9483\n",
      "Epoch [2/5], Step [1010/5746], Loss: 3.8159\n",
      "Epoch [2/5], Step [1020/5746], Loss: 3.7857\n",
      "Epoch [2/5], Step [1030/5746], Loss: 3.7601\n",
      "Epoch [2/5], Step [1040/5746], Loss: 3.6708\n",
      "Epoch [2/5], Step [1050/5746], Loss: 3.8379\n",
      "Epoch [2/5], Step [1060/5746], Loss: 3.5687\n",
      "Epoch [2/5], Step [1070/5746], Loss: 3.6362\n",
      "Epoch [2/5], Step [1080/5746], Loss: 3.5720\n",
      "Epoch [2/5], Step [1090/5746], Loss: 3.7689\n",
      "Epoch [2/5], Step [1100/5746], Loss: 3.5598\n",
      "Epoch [2/5], Step [1110/5746], Loss: 3.6201\n",
      "Epoch [2/5], Step [1120/5746], Loss: 3.7111\n",
      "Epoch [2/5], Step [1130/5746], Loss: 3.7177\n",
      "Epoch [2/5], Step [1140/5746], Loss: 3.7606\n",
      "Epoch [2/5], Step [1150/5746], Loss: 3.6760\n",
      "Epoch [2/5], Step [1160/5746], Loss: 3.8470\n",
      "Epoch [2/5], Step [1170/5746], Loss: 3.7319\n",
      "Epoch [2/5], Step [1180/5746], Loss: 3.7802\n",
      "Epoch [2/5], Step [1190/5746], Loss: 3.6513\n",
      "Epoch [2/5], Step [1200/5746], Loss: 3.4071\n",
      "Epoch [2/5], Step [1210/5746], Loss: 3.8385\n",
      "Epoch [2/5], Step [1220/5746], Loss: 3.6694\n",
      "Epoch [2/5], Step [1230/5746], Loss: 3.6823\n",
      "Epoch [2/5], Step [1240/5746], Loss: 3.9505\n",
      "Epoch [2/5], Step [1250/5746], Loss: 3.7187\n",
      "Epoch [2/5], Step [1260/5746], Loss: 3.7442\n",
      "Epoch [2/5], Step [1270/5746], Loss: 3.7856\n",
      "Epoch [2/5], Step [1280/5746], Loss: 3.6202\n",
      "Epoch [2/5], Step [1290/5746], Loss: 3.6760\n",
      "Epoch [2/5], Step [1300/5746], Loss: 3.6037\n",
      "Epoch [2/5], Step [1310/5746], Loss: 3.7621\n",
      "Epoch [2/5], Step [1320/5746], Loss: 3.6143\n",
      "Epoch [2/5], Step [1330/5746], Loss: 3.6072\n",
      "Epoch [2/5], Step [1340/5746], Loss: 3.6198\n",
      "Epoch [2/5], Step [1350/5746], Loss: 3.4872\n",
      "Epoch [2/5], Step [1360/5746], Loss: 3.7579\n",
      "Epoch [2/5], Step [1370/5746], Loss: 3.6945\n",
      "Epoch [2/5], Step [1380/5746], Loss: 3.7232\n",
      "Epoch [2/5], Step [1390/5746], Loss: 3.8192\n",
      "Epoch [2/5], Step [1400/5746], Loss: 3.6343\n",
      "Epoch [2/5], Step [1410/5746], Loss: 3.6727\n",
      "Epoch [2/5], Step [1420/5746], Loss: 3.6251\n",
      "Epoch [2/5], Step [1430/5746], Loss: 3.6424\n",
      "Epoch [2/5], Step [1440/5746], Loss: 3.6724\n",
      "Epoch [2/5], Step [1450/5746], Loss: 3.8435\n",
      "Epoch [2/5], Step [1460/5746], Loss: 3.6091\n",
      "Epoch [2/5], Step [1470/5746], Loss: 3.6143\n",
      "Epoch [2/5], Step [1480/5746], Loss: 3.7016\n",
      "Epoch [2/5], Step [1490/5746], Loss: 3.6583\n",
      "Epoch [2/5], Step [1500/5746], Loss: 3.5936\n",
      "Epoch [2/5], Step [1510/5746], Loss: 3.7051\n",
      "Epoch [2/5], Step [1520/5746], Loss: 3.7944\n",
      "Epoch [2/5], Step [1530/5746], Loss: 3.5673\n",
      "Epoch [2/5], Step [1540/5746], Loss: 3.6775\n",
      "Epoch [2/5], Step [1550/5746], Loss: 3.6396\n",
      "Epoch [2/5], Step [1560/5746], Loss: 3.7879\n",
      "Epoch [2/5], Step [1570/5746], Loss: 3.6238\n",
      "Epoch [2/5], Step [1580/5746], Loss: 3.5904\n",
      "Epoch [2/5], Step [1590/5746], Loss: 3.5437\n",
      "Epoch [2/5], Step [1600/5746], Loss: 3.7422\n",
      "Epoch [2/5], Step [1610/5746], Loss: 3.7905\n",
      "Epoch [2/5], Step [1620/5746], Loss: 3.7194\n",
      "Epoch [2/5], Step [1630/5746], Loss: 3.7248\n",
      "Epoch [2/5], Step [1640/5746], Loss: 3.7226\n",
      "Epoch [2/5], Step [1650/5746], Loss: 3.7851\n",
      "Epoch [2/5], Step [1660/5746], Loss: 3.7156\n",
      "Epoch [2/5], Step [1670/5746], Loss: 3.8519\n",
      "Epoch [2/5], Step [1680/5746], Loss: 3.4724\n",
      "Epoch [2/5], Step [1690/5746], Loss: 3.6665\n",
      "Epoch [2/5], Step [1700/5746], Loss: 3.7301\n",
      "Epoch [2/5], Step [1710/5746], Loss: 3.7718\n",
      "Epoch [2/5], Step [1720/5746], Loss: 3.6712\n",
      "Epoch [2/5], Step [1730/5746], Loss: 3.6976\n",
      "Epoch [2/5], Step [1740/5746], Loss: 3.7701\n",
      "Epoch [2/5], Step [1750/5746], Loss: 3.6776\n",
      "Epoch [2/5], Step [1760/5746], Loss: 3.6346\n",
      "Epoch [2/5], Step [1770/5746], Loss: 3.5440\n",
      "Epoch [2/5], Step [1780/5746], Loss: 3.6171\n",
      "Epoch [2/5], Step [1790/5746], Loss: 3.6695\n",
      "Epoch [2/5], Step [1800/5746], Loss: 3.6578\n",
      "Epoch [2/5], Step [1810/5746], Loss: 3.6264\n",
      "Epoch [2/5], Step [1820/5746], Loss: 3.5614\n",
      "Epoch [2/5], Step [1830/5746], Loss: 3.6782\n",
      "Epoch [2/5], Step [1840/5746], Loss: 3.7044\n",
      "Epoch [2/5], Step [1850/5746], Loss: 3.7047\n",
      "Epoch [2/5], Step [1860/5746], Loss: 3.5795\n",
      "Epoch [2/5], Step [1870/5746], Loss: 3.5543\n",
      "Epoch [2/5], Step [1880/5746], Loss: 3.7631\n",
      "Epoch [2/5], Step [1890/5746], Loss: 3.5850\n",
      "Epoch [2/5], Step [1900/5746], Loss: 3.6734\n",
      "Epoch [2/5], Step [1910/5746], Loss: 3.6405\n",
      "Epoch [2/5], Step [1920/5746], Loss: 3.5932\n",
      "Epoch [2/5], Step [1930/5746], Loss: 3.6486\n",
      "Epoch [2/5], Step [1940/5746], Loss: 3.6147\n",
      "Epoch [2/5], Step [1950/5746], Loss: 3.6659\n",
      "Epoch [2/5], Step [1960/5746], Loss: 3.6678\n",
      "Epoch [2/5], Step [1970/5746], Loss: 3.6605\n",
      "Epoch [2/5], Step [1980/5746], Loss: 3.5455\n",
      "Epoch [2/5], Step [1990/5746], Loss: 3.5537\n",
      "Epoch [2/5], Step [2000/5746], Loss: 3.5963\n",
      "Epoch [2/5], Step [2010/5746], Loss: 3.7369\n",
      "Epoch [2/5], Step [2020/5746], Loss: 3.8115\n",
      "Epoch [2/5], Step [2030/5746], Loss: 3.6689\n",
      "Epoch [2/5], Step [2040/5746], Loss: 3.5310\n",
      "Epoch [2/5], Step [2050/5746], Loss: 3.5817\n",
      "Epoch [2/5], Step [2060/5746], Loss: 3.7648\n",
      "Epoch [2/5], Step [2070/5746], Loss: 3.6215\n",
      "Epoch [2/5], Step [2080/5746], Loss: 3.5756\n",
      "Epoch [2/5], Step [2090/5746], Loss: 3.7912\n",
      "Epoch [2/5], Step [2100/5746], Loss: 3.6235\n",
      "Epoch [2/5], Step [2110/5746], Loss: 3.5830\n",
      "Epoch [2/5], Step [2120/5746], Loss: 3.6493\n",
      "Epoch [2/5], Step [2130/5746], Loss: 3.8270\n",
      "Epoch [2/5], Step [2140/5746], Loss: 3.7776\n",
      "Epoch [2/5], Step [2150/5746], Loss: 3.5508\n",
      "Epoch [2/5], Step [2160/5746], Loss: 3.6727\n",
      "Epoch [2/5], Step [2170/5746], Loss: 3.7853\n",
      "Epoch [2/5], Step [2180/5746], Loss: 3.5752\n",
      "Epoch [2/5], Step [2190/5746], Loss: 3.5606\n",
      "Epoch [2/5], Step [2200/5746], Loss: 3.6379\n",
      "Epoch [2/5], Step [2210/5746], Loss: 3.8097\n",
      "Epoch [2/5], Step [2220/5746], Loss: 3.5949\n",
      "Epoch [2/5], Step [2230/5746], Loss: 3.6118\n",
      "Epoch [2/5], Step [2240/5746], Loss: 3.5119\n",
      "Epoch [2/5], Step [2250/5746], Loss: 3.5087\n",
      "Epoch [2/5], Step [2260/5746], Loss: 3.6891\n",
      "Epoch [2/5], Step [2270/5746], Loss: 3.7518\n",
      "Epoch [2/5], Step [2280/5746], Loss: 3.5699\n",
      "Epoch [2/5], Step [2290/5746], Loss: 3.5910\n",
      "Epoch [2/5], Step [2300/5746], Loss: 3.7176\n",
      "Epoch [2/5], Step [2310/5746], Loss: 3.7844\n",
      "Epoch [2/5], Step [2320/5746], Loss: 3.6426\n",
      "Epoch [2/5], Step [2330/5746], Loss: 3.5056\n",
      "Epoch [2/5], Step [2340/5746], Loss: 3.5894\n",
      "Epoch [2/5], Step [2350/5746], Loss: 3.5368\n",
      "Epoch [2/5], Step [2360/5746], Loss: 3.5148\n",
      "Epoch [2/5], Step [2370/5746], Loss: 3.5695\n",
      "Epoch [2/5], Step [2380/5746], Loss: 3.7083\n",
      "Epoch [2/5], Step [2390/5746], Loss: 3.6301\n",
      "Epoch [2/5], Step [2400/5746], Loss: 3.7036\n",
      "Epoch [2/5], Step [2410/5746], Loss: 3.6538\n",
      "Epoch [2/5], Step [2420/5746], Loss: 3.7301\n",
      "Epoch [2/5], Step [2430/5746], Loss: 3.7581\n",
      "Epoch [2/5], Step [2440/5746], Loss: 3.5298\n",
      "Epoch [2/5], Step [2450/5746], Loss: 3.7918\n",
      "Epoch [2/5], Step [2460/5746], Loss: 3.6240\n",
      "Epoch [2/5], Step [2470/5746], Loss: 3.6837\n",
      "Epoch [2/5], Step [2480/5746], Loss: 3.5912\n",
      "Epoch [2/5], Step [2490/5746], Loss: 3.5806\n",
      "Epoch [2/5], Step [2500/5746], Loss: 3.5454\n",
      "Epoch [2/5], Step [2510/5746], Loss: 3.5860\n",
      "Epoch [2/5], Step [2520/5746], Loss: 3.6433\n",
      "Epoch [2/5], Step [2530/5746], Loss: 3.5991\n",
      "Epoch [2/5], Step [2540/5746], Loss: 3.6001\n",
      "Epoch [2/5], Step [2550/5746], Loss: 3.4622\n",
      "Epoch [2/5], Step [2560/5746], Loss: 3.5502\n",
      "Epoch [2/5], Step [2570/5746], Loss: 3.6149\n",
      "Epoch [2/5], Step [2580/5746], Loss: 3.5181\n",
      "Epoch [2/5], Step [2590/5746], Loss: 3.6419\n",
      "Epoch [2/5], Step [2600/5746], Loss: 3.4945\n",
      "Epoch [2/5], Step [2610/5746], Loss: 3.7020\n",
      "Epoch [2/5], Step [2620/5746], Loss: 3.5699\n",
      "Epoch [2/5], Step [2630/5746], Loss: 3.6680\n",
      "Epoch [2/5], Step [2640/5746], Loss: 3.6038\n",
      "Epoch [2/5], Step [2650/5746], Loss: 3.5136\n",
      "Epoch [2/5], Step [2660/5746], Loss: 3.6335\n",
      "Epoch [2/5], Step [2670/5746], Loss: 3.6148\n",
      "Epoch [2/5], Step [2680/5746], Loss: 3.8181\n",
      "Epoch [2/5], Step [2690/5746], Loss: 3.6472\n",
      "Epoch [2/5], Step [2700/5746], Loss: 3.6497\n",
      "Epoch [2/5], Step [2710/5746], Loss: 3.6659\n",
      "Epoch [2/5], Step [2720/5746], Loss: 3.6249\n",
      "Epoch [2/5], Step [2730/5746], Loss: 3.4599\n",
      "Epoch [2/5], Step [2740/5746], Loss: 3.5710\n",
      "Epoch [2/5], Step [2750/5746], Loss: 3.6280\n",
      "Epoch [2/5], Step [2760/5746], Loss: 3.6968\n",
      "Epoch [2/5], Step [2770/5746], Loss: 3.6956\n",
      "Epoch [2/5], Step [2780/5746], Loss: 3.5982\n",
      "Epoch [2/5], Step [2790/5746], Loss: 3.4659\n",
      "Epoch [2/5], Step [2800/5746], Loss: 3.5644\n",
      "Epoch [2/5], Step [2810/5746], Loss: 3.5905\n",
      "Epoch [2/5], Step [2820/5746], Loss: 3.6079\n",
      "Epoch [2/5], Step [2830/5746], Loss: 3.7949\n",
      "Epoch [2/5], Step [2840/5746], Loss: 3.5339\n",
      "Epoch [2/5], Step [2850/5746], Loss: 3.8152\n",
      "Epoch [2/5], Step [2860/5746], Loss: 3.6365\n",
      "Epoch [2/5], Step [2870/5746], Loss: 3.4521\n",
      "Epoch [2/5], Step [2880/5746], Loss: 3.4062\n",
      "Epoch [2/5], Step [2890/5746], Loss: 3.5666\n",
      "Epoch [2/5], Step [2900/5746], Loss: 3.6533\n",
      "Epoch [2/5], Step [2910/5746], Loss: 3.5187\n",
      "Epoch [2/5], Step [2920/5746], Loss: 3.4601\n",
      "Epoch [2/5], Step [2930/5746], Loss: 3.7089\n",
      "Epoch [2/5], Step [2940/5746], Loss: 3.5788\n",
      "Epoch [2/5], Step [2950/5746], Loss: 3.5261\n",
      "Epoch [2/5], Step [2960/5746], Loss: 3.4678\n",
      "Epoch [2/5], Step [2970/5746], Loss: 3.5918\n",
      "Epoch [2/5], Step [2980/5746], Loss: 3.6218\n",
      "Epoch [2/5], Step [2990/5746], Loss: 3.6825\n",
      "Epoch [2/5], Step [3000/5746], Loss: 3.5360\n",
      "Epoch [2/5], Step [3010/5746], Loss: 3.6790\n",
      "Epoch [2/5], Step [3020/5746], Loss: 3.5816\n",
      "Epoch [2/5], Step [3030/5746], Loss: 3.6816\n",
      "Epoch [2/5], Step [3040/5746], Loss: 3.6484\n",
      "Epoch [2/5], Step [3050/5746], Loss: 3.3462\n",
      "Epoch [2/5], Step [3060/5746], Loss: 3.5579\n",
      "Epoch [2/5], Step [3070/5746], Loss: 3.6323\n",
      "Epoch [2/5], Step [3080/5746], Loss: 3.5073\n",
      "Epoch [2/5], Step [3090/5746], Loss: 3.5610\n",
      "Epoch [2/5], Step [3100/5746], Loss: 3.5845\n",
      "Epoch [2/5], Step [3110/5746], Loss: 3.4665\n",
      "Epoch [2/5], Step [3120/5746], Loss: 3.6928\n",
      "Epoch [2/5], Step [3130/5746], Loss: 3.6086\n",
      "Epoch [2/5], Step [3140/5746], Loss: 3.5968\n",
      "Epoch [2/5], Step [3150/5746], Loss: 3.6292\n",
      "Epoch [2/5], Step [3160/5746], Loss: 3.6148\n",
      "Epoch [2/5], Step [3170/5746], Loss: 3.6060\n",
      "Epoch [2/5], Step [3180/5746], Loss: 3.6106\n",
      "Epoch [2/5], Step [3190/5746], Loss: 3.4721\n",
      "Epoch [2/5], Step [3200/5746], Loss: 3.8093\n",
      "Epoch [2/5], Step [3210/5746], Loss: 3.6061\n",
      "Epoch [2/5], Step [3220/5746], Loss: 3.6707\n",
      "Epoch [2/5], Step [3230/5746], Loss: 3.5688\n",
      "Epoch [2/5], Step [3240/5746], Loss: 3.6859\n",
      "Epoch [2/5], Step [3250/5746], Loss: 3.6489\n",
      "Epoch [2/5], Step [3260/5746], Loss: 3.7156\n",
      "Epoch [2/5], Step [3270/5746], Loss: 3.6234\n",
      "Epoch [2/5], Step [3280/5746], Loss: 3.5788\n",
      "Epoch [2/5], Step [3290/5746], Loss: 3.4357\n",
      "Epoch [2/5], Step [3300/5746], Loss: 3.5340\n",
      "Epoch [2/5], Step [3310/5746], Loss: 3.6996\n",
      "Epoch [2/5], Step [3320/5746], Loss: 3.5895\n",
      "Epoch [2/5], Step [3330/5746], Loss: 3.6480\n",
      "Epoch [2/5], Step [3340/5746], Loss: 3.4534\n",
      "Epoch [2/5], Step [3350/5746], Loss: 3.5444\n",
      "Epoch [2/5], Step [3360/5746], Loss: 3.6567\n",
      "Epoch [2/5], Step [3370/5746], Loss: 3.5471\n",
      "Epoch [2/5], Step [3380/5746], Loss: 3.7497\n",
      "Epoch [2/5], Step [3390/5746], Loss: 3.5561\n",
      "Epoch [2/5], Step [3400/5746], Loss: 3.6755\n",
      "Epoch [2/5], Step [3410/5746], Loss: 3.6711\n",
      "Epoch [2/5], Step [3420/5746], Loss: 3.5439\n",
      "Epoch [2/5], Step [3430/5746], Loss: 3.5145\n",
      "Epoch [2/5], Step [3440/5746], Loss: 3.5965\n",
      "Epoch [2/5], Step [3450/5746], Loss: 3.5776\n",
      "Epoch [2/5], Step [3460/5746], Loss: 3.4157\n",
      "Epoch [2/5], Step [3470/5746], Loss: 3.6228\n",
      "Epoch [2/5], Step [3480/5746], Loss: 3.6045\n",
      "Epoch [2/5], Step [3490/5746], Loss: 3.5300\n",
      "Epoch [2/5], Step [3500/5746], Loss: 3.5095\n",
      "Epoch [2/5], Step [3510/5746], Loss: 3.5171\n",
      "Epoch [2/5], Step [3520/5746], Loss: 3.6813\n",
      "Epoch [2/5], Step [3530/5746], Loss: 3.6420\n",
      "Epoch [2/5], Step [3540/5746], Loss: 3.6444\n",
      "Epoch [2/5], Step [3550/5746], Loss: 3.5879\n",
      "Epoch [2/5], Step [3560/5746], Loss: 3.5417\n",
      "Epoch [2/5], Step [3570/5746], Loss: 3.5920\n",
      "Epoch [2/5], Step [3580/5746], Loss: 3.5052\n",
      "Epoch [2/5], Step [3590/5746], Loss: 3.6715\n",
      "Epoch [2/5], Step [3600/5746], Loss: 3.6029\n",
      "Epoch [2/5], Step [3610/5746], Loss: 3.5326\n",
      "Epoch [2/5], Step [3620/5746], Loss: 3.6560\n",
      "Epoch [2/5], Step [3630/5746], Loss: 3.6141\n",
      "Epoch [2/5], Step [3640/5746], Loss: 3.5623\n",
      "Epoch [2/5], Step [3650/5746], Loss: 3.6803\n",
      "Epoch [2/5], Step [3660/5746], Loss: 3.4827\n",
      "Epoch [2/5], Step [3670/5746], Loss: 3.6445\n",
      "Epoch [2/5], Step [3680/5746], Loss: 3.5972\n",
      "Epoch [2/5], Step [3690/5746], Loss: 3.3820\n",
      "Epoch [2/5], Step [3700/5746], Loss: 3.6317\n",
      "Epoch [2/5], Step [3710/5746], Loss: 3.4553\n",
      "Epoch [2/5], Step [3720/5746], Loss: 3.5057\n",
      "Epoch [2/5], Step [3730/5746], Loss: 3.6615\n",
      "Epoch [2/5], Step [3740/5746], Loss: 3.6114\n",
      "Epoch [2/5], Step [3750/5746], Loss: 3.5537\n",
      "Epoch [2/5], Step [3760/5746], Loss: 3.5889\n",
      "Epoch [2/5], Step [3770/5746], Loss: 3.4587\n",
      "Epoch [2/5], Step [3780/5746], Loss: 3.5930\n",
      "Epoch [2/5], Step [3790/5746], Loss: 3.6551\n",
      "Epoch [2/5], Step [3800/5746], Loss: 3.5244\n",
      "Epoch [2/5], Step [3810/5746], Loss: 3.5124\n",
      "Epoch [2/5], Step [3820/5746], Loss: 3.6028\n",
      "Epoch [2/5], Step [3830/5746], Loss: 3.6820\n",
      "Epoch [2/5], Step [3840/5746], Loss: 3.5143\n",
      "Epoch [2/5], Step [3850/5746], Loss: 3.6684\n",
      "Epoch [2/5], Step [3860/5746], Loss: 3.5035\n",
      "Epoch [2/5], Step [3870/5746], Loss: 3.5639\n",
      "Epoch [2/5], Step [3880/5746], Loss: 3.6026\n",
      "Epoch [2/5], Step [3890/5746], Loss: 3.4589\n",
      "Epoch [2/5], Step [3900/5746], Loss: 3.4753\n",
      "Epoch [2/5], Step [3910/5746], Loss: 3.5585\n",
      "Epoch [2/5], Step [3920/5746], Loss: 3.5940\n",
      "Epoch [2/5], Step [3930/5746], Loss: 3.7670\n",
      "Epoch [2/5], Step [3940/5746], Loss: 3.4835\n",
      "Epoch [2/5], Step [3950/5746], Loss: 3.5609\n",
      "Epoch [2/5], Step [3960/5746], Loss: 3.5008\n",
      "Epoch [2/5], Step [3970/5746], Loss: 3.6664\n",
      "Epoch [2/5], Step [3980/5746], Loss: 3.4540\n",
      "Epoch [2/5], Step [3990/5746], Loss: 3.5606\n",
      "Epoch [2/5], Step [4000/5746], Loss: 3.3923\n",
      "Epoch [2/5], Step [4010/5746], Loss: 3.3469\n",
      "Epoch [2/5], Step [4020/5746], Loss: 3.4645\n",
      "Epoch [2/5], Step [4030/5746], Loss: 3.5536\n",
      "Epoch [2/5], Step [4040/5746], Loss: 3.4947\n",
      "Epoch [2/5], Step [4050/5746], Loss: 3.4385\n",
      "Epoch [2/5], Step [4060/5746], Loss: 3.6527\n",
      "Epoch [2/5], Step [4070/5746], Loss: 3.5251\n",
      "Epoch [2/5], Step [4080/5746], Loss: 3.5326\n",
      "Epoch [2/5], Step [4090/5746], Loss: 3.5662\n",
      "Epoch [2/5], Step [4100/5746], Loss: 3.4678\n",
      "Epoch [2/5], Step [4110/5746], Loss: 3.4343\n",
      "Epoch [2/5], Step [4120/5746], Loss: 3.4220\n",
      "Epoch [2/5], Step [4130/5746], Loss: 3.5325\n",
      "Epoch [2/5], Step [4140/5746], Loss: 3.4379\n",
      "Epoch [2/5], Step [4150/5746], Loss: 3.4587\n",
      "Epoch [2/5], Step [4160/5746], Loss: 3.6020\n",
      "Epoch [2/5], Step [4170/5746], Loss: 3.5731\n",
      "Epoch [2/5], Step [4180/5746], Loss: 3.7408\n",
      "Epoch [2/5], Step [4190/5746], Loss: 3.4541\n",
      "Epoch [2/5], Step [4200/5746], Loss: 3.5989\n",
      "Epoch [2/5], Step [4210/5746], Loss: 3.5162\n",
      "Epoch [2/5], Step [4220/5746], Loss: 3.6111\n",
      "Epoch [2/5], Step [4230/5746], Loss: 3.5736\n",
      "Epoch [2/5], Step [4240/5746], Loss: 3.4740\n",
      "Epoch [2/5], Step [4250/5746], Loss: 3.6030\n",
      "Epoch [2/5], Step [4260/5746], Loss: 3.5321\n",
      "Epoch [2/5], Step [4270/5746], Loss: 3.4599\n",
      "Epoch [2/5], Step [4280/5746], Loss: 3.5509\n",
      "Epoch [2/5], Step [4290/5746], Loss: 3.4290\n",
      "Epoch [2/5], Step [4300/5746], Loss: 3.6026\n",
      "Epoch [2/5], Step [4310/5746], Loss: 3.6317\n",
      "Epoch [2/5], Step [4320/5746], Loss: 3.4535\n",
      "Epoch [2/5], Step [4330/5746], Loss: 3.4341\n",
      "Epoch [2/5], Step [4340/5746], Loss: 3.4954\n",
      "Epoch [2/5], Step [4350/5746], Loss: 3.5322\n",
      "Epoch [2/5], Step [4360/5746], Loss: 3.4293\n",
      "Epoch [2/5], Step [4370/5746], Loss: 3.6131\n",
      "Epoch [2/5], Step [4380/5746], Loss: 3.5104\n",
      "Epoch [2/5], Step [4390/5746], Loss: 3.6726\n",
      "Epoch [2/5], Step [4400/5746], Loss: 3.4287\n",
      "Epoch [2/5], Step [4410/5746], Loss: 3.5498\n",
      "Epoch [2/5], Step [4420/5746], Loss: 3.4870\n",
      "Epoch [2/5], Step [4430/5746], Loss: 3.6309\n",
      "Epoch [2/5], Step [4440/5746], Loss: 3.4700\n",
      "Epoch [2/5], Step [4450/5746], Loss: 3.5241\n",
      "Epoch [2/5], Step [4460/5746], Loss: 3.5792\n",
      "Epoch [2/5], Step [4470/5746], Loss: 3.5402\n",
      "Epoch [2/5], Step [4480/5746], Loss: 3.5864\n",
      "Epoch [2/5], Step [4490/5746], Loss: 3.6310\n",
      "Epoch [2/5], Step [4500/5746], Loss: 3.4070\n",
      "Epoch [2/5], Step [4510/5746], Loss: 3.5809\n",
      "Epoch [2/5], Step [4520/5746], Loss: 3.4270\n",
      "Epoch [2/5], Step [4530/5746], Loss: 3.4661\n",
      "Epoch [2/5], Step [4540/5746], Loss: 3.5967\n",
      "Epoch [2/5], Step [4550/5746], Loss: 3.5579\n",
      "Epoch [2/5], Step [4560/5746], Loss: 3.5116\n",
      "Epoch [2/5], Step [4570/5746], Loss: 3.4537\n",
      "Epoch [2/5], Step [4580/5746], Loss: 3.6754\n",
      "Epoch [2/5], Step [4590/5746], Loss: 3.5780\n",
      "Epoch [2/5], Step [4600/5746], Loss: 3.5038\n",
      "Epoch [2/5], Step [4610/5746], Loss: 3.4353\n",
      "Epoch [2/5], Step [4620/5746], Loss: 3.6015\n",
      "Epoch [2/5], Step [4630/5746], Loss: 3.6514\n",
      "Epoch [2/5], Step [4640/5746], Loss: 3.4723\n",
      "Epoch [2/5], Step [4650/5746], Loss: 3.6478\n",
      "Epoch [2/5], Step [4660/5746], Loss: 3.7163\n",
      "Epoch [2/5], Step [4670/5746], Loss: 3.6069\n",
      "Epoch [2/5], Step [4680/5746], Loss: 3.5955\n",
      "Epoch [2/5], Step [4690/5746], Loss: 3.5021\n",
      "Epoch [2/5], Step [4700/5746], Loss: 3.5333\n",
      "Epoch [2/5], Step [4710/5746], Loss: 3.5783\n",
      "Epoch [2/5], Step [4720/5746], Loss: 3.5853\n",
      "Epoch [2/5], Step [4730/5746], Loss: 3.4722\n",
      "Epoch [2/5], Step [4740/5746], Loss: 3.4507\n",
      "Epoch [2/5], Step [4750/5746], Loss: 3.5088\n",
      "Epoch [2/5], Step [4760/5746], Loss: 3.7040\n",
      "Epoch [2/5], Step [4770/5746], Loss: 3.5212\n",
      "Epoch [2/5], Step [4780/5746], Loss: 3.3473\n",
      "Epoch [2/5], Step [4790/5746], Loss: 3.5547\n",
      "Epoch [2/5], Step [4800/5746], Loss: 3.4529\n",
      "Epoch [2/5], Step [4810/5746], Loss: 3.4738\n",
      "Epoch [2/5], Step [4820/5746], Loss: 3.5603\n",
      "Epoch [2/5], Step [4830/5746], Loss: 3.4125\n",
      "Epoch [2/5], Step [4840/5746], Loss: 3.4687\n",
      "Epoch [2/5], Step [4850/5746], Loss: 3.4642\n",
      "Epoch [2/5], Step [4860/5746], Loss: 3.5642\n",
      "Epoch [2/5], Step [4870/5746], Loss: 3.4361\n",
      "Epoch [2/5], Step [4880/5746], Loss: 3.5896\n",
      "Epoch [2/5], Step [4890/5746], Loss: 3.5421\n",
      "Epoch [2/5], Step [4900/5746], Loss: 3.7324\n",
      "Epoch [2/5], Step [4910/5746], Loss: 3.4492\n",
      "Epoch [2/5], Step [4920/5746], Loss: 3.5645\n",
      "Epoch [2/5], Step [4930/5746], Loss: 3.5766\n",
      "Epoch [2/5], Step [4940/5746], Loss: 3.5766\n",
      "Epoch [2/5], Step [4950/5746], Loss: 3.5791\n",
      "Epoch [2/5], Step [4960/5746], Loss: 3.5777\n",
      "Epoch [2/5], Step [4970/5746], Loss: 3.4265\n",
      "Epoch [2/5], Step [4980/5746], Loss: 3.4236\n",
      "Epoch [2/5], Step [4990/5746], Loss: 3.6652\n",
      "Epoch [2/5], Step [5000/5746], Loss: 3.5082\n",
      "Epoch [2/5], Step [5010/5746], Loss: 3.5523\n",
      "Epoch [2/5], Step [5020/5746], Loss: 3.6080\n",
      "Epoch [2/5], Step [5030/5746], Loss: 3.3722\n",
      "Epoch [2/5], Step [5040/5746], Loss: 3.4389\n",
      "Epoch [2/5], Step [5050/5746], Loss: 3.3584\n",
      "Epoch [2/5], Step [5060/5746], Loss: 3.4194\n",
      "Epoch [2/5], Step [5070/5746], Loss: 3.4598\n",
      "Epoch [2/5], Step [5080/5746], Loss: 3.4534\n",
      "Epoch [2/5], Step [5090/5746], Loss: 3.5809\n",
      "Epoch [2/5], Step [5100/5746], Loss: 3.4728\n",
      "Epoch [2/5], Step [5110/5746], Loss: 3.5535\n",
      "Epoch [2/5], Step [5120/5746], Loss: 3.3927\n",
      "Epoch [2/5], Step [5130/5746], Loss: 3.4964\n",
      "Epoch [2/5], Step [5140/5746], Loss: 3.3685\n",
      "Epoch [2/5], Step [5150/5746], Loss: 3.4957\n",
      "Epoch [2/5], Step [5160/5746], Loss: 3.5512\n",
      "Epoch [2/5], Step [5170/5746], Loss: 3.3460\n",
      "Epoch [2/5], Step [5180/5746], Loss: 3.6538\n",
      "Epoch [2/5], Step [5190/5746], Loss: 3.3997\n",
      "Epoch [2/5], Step [5200/5746], Loss: 3.4257\n",
      "Epoch [2/5], Step [5210/5746], Loss: 3.3501\n",
      "Epoch [2/5], Step [5220/5746], Loss: 3.4304\n",
      "Epoch [2/5], Step [5230/5746], Loss: 3.4673\n",
      "Epoch [2/5], Step [5240/5746], Loss: 3.4993\n",
      "Epoch [2/5], Step [5250/5746], Loss: 3.4224\n",
      "Epoch [2/5], Step [5260/5746], Loss: 3.4296\n",
      "Epoch [2/5], Step [5270/5746], Loss: 3.4989\n",
      "Epoch [2/5], Step [5280/5746], Loss: 3.4204\n",
      "Epoch [2/5], Step [5290/5746], Loss: 3.4155\n",
      "Epoch [2/5], Step [5300/5746], Loss: 3.3259\n",
      "Epoch [2/5], Step [5310/5746], Loss: 3.5852\n",
      "Epoch [2/5], Step [5320/5746], Loss: 3.5791\n",
      "Epoch [2/5], Step [5330/5746], Loss: 3.5013\n",
      "Epoch [2/5], Step [5340/5746], Loss: 3.3591\n",
      "Epoch [2/5], Step [5350/5746], Loss: 3.4741\n",
      "Epoch [2/5], Step [5360/5746], Loss: 3.4888\n",
      "Epoch [2/5], Step [5370/5746], Loss: 3.5544\n",
      "Epoch [2/5], Step [5380/5746], Loss: 3.3871\n",
      "Epoch [2/5], Step [5390/5746], Loss: 3.5143\n",
      "Epoch [2/5], Step [5400/5746], Loss: 3.5487\n",
      "Epoch [2/5], Step [5410/5746], Loss: 3.4365\n",
      "Epoch [2/5], Step [5420/5746], Loss: 3.3882\n",
      "Epoch [2/5], Step [5430/5746], Loss: 3.5619\n",
      "Epoch [2/5], Step [5440/5746], Loss: 3.4293\n",
      "Epoch [2/5], Step [5450/5746], Loss: 3.3381\n",
      "Epoch [2/5], Step [5460/5746], Loss: 3.4193\n",
      "Epoch [2/5], Step [5470/5746], Loss: 3.5807\n",
      "Epoch [2/5], Step [5480/5746], Loss: 3.4777\n",
      "Epoch [2/5], Step [5490/5746], Loss: 3.3907\n",
      "Epoch [2/5], Step [5500/5746], Loss: 3.3710\n",
      "Epoch [2/5], Step [5510/5746], Loss: 3.4934\n",
      "Epoch [2/5], Step [5520/5746], Loss: 3.6119\n",
      "Epoch [2/5], Step [5530/5746], Loss: 3.3442\n",
      "Epoch [2/5], Step [5540/5746], Loss: 3.3509\n",
      "Epoch [2/5], Step [5550/5746], Loss: 3.4122\n",
      "Epoch [2/5], Step [5560/5746], Loss: 3.5561\n",
      "Epoch [2/5], Step [5570/5746], Loss: 3.4593\n",
      "Epoch [2/5], Step [5580/5746], Loss: 3.3114\n",
      "Epoch [2/5], Step [5590/5746], Loss: 3.1997\n",
      "Epoch [2/5], Step [5600/5746], Loss: 3.4833\n",
      "Epoch [2/5], Step [5610/5746], Loss: 3.4014\n",
      "Epoch [2/5], Step [5620/5746], Loss: 3.5348\n",
      "Epoch [2/5], Step [5630/5746], Loss: 3.5237\n",
      "Epoch [2/5], Step [5640/5746], Loss: 3.3667\n",
      "Epoch [2/5], Step [5650/5746], Loss: 3.6135\n",
      "Epoch [2/5], Step [5660/5746], Loss: 3.3179\n",
      "Epoch [2/5], Step [5670/5746], Loss: 3.4525\n",
      "Epoch [2/5], Step [5680/5746], Loss: 3.3524\n",
      "Epoch [2/5], Step [5690/5746], Loss: 3.3806\n",
      "Epoch [2/5], Step [5700/5746], Loss: 3.5189\n",
      "Epoch [2/5], Step [5710/5746], Loss: 3.3746\n",
      "Epoch [2/5], Step [5720/5746], Loss: 3.5338\n",
      "Epoch [2/5], Step [5730/5746], Loss: 3.5535\n",
      "Epoch [2/5], Step [5740/5746], Loss: 3.4682\n",
      "Epoch [2/5] Average Loss: 3.6118, Perplexity: 37.03\n",
      "Epoch [3/5], Step [0/5746], Loss: 3.4980\n",
      "Epoch [3/5], Step [10/5746], Loss: 3.3552\n",
      "Epoch [3/5], Step [20/5746], Loss: 3.5878\n",
      "Epoch [3/5], Step [30/5746], Loss: 3.5730\n",
      "Epoch [3/5], Step [40/5746], Loss: 3.4686\n",
      "Epoch [3/5], Step [50/5746], Loss: 3.3841\n",
      "Epoch [3/5], Step [60/5746], Loss: 3.4151\n",
      "Epoch [3/5], Step [70/5746], Loss: 3.2943\n",
      "Epoch [3/5], Step [80/5746], Loss: 3.1935\n",
      "Epoch [3/5], Step [90/5746], Loss: 3.2859\n",
      "Epoch [3/5], Step [100/5746], Loss: 3.4065\n",
      "Epoch [3/5], Step [110/5746], Loss: 3.5355\n",
      "Epoch [3/5], Step [120/5746], Loss: 3.4416\n",
      "Epoch [3/5], Step [130/5746], Loss: 3.4162\n",
      "Epoch [3/5], Step [140/5746], Loss: 3.3917\n",
      "Epoch [3/5], Step [150/5746], Loss: 3.5928\n",
      "Epoch [3/5], Step [160/5746], Loss: 3.5929\n",
      "Epoch [3/5], Step [170/5746], Loss: 3.4737\n",
      "Epoch [3/5], Step [180/5746], Loss: 3.5637\n",
      "Epoch [3/5], Step [190/5746], Loss: 3.4605\n",
      "Epoch [3/5], Step [200/5746], Loss: 3.5140\n",
      "Epoch [3/5], Step [210/5746], Loss: 3.5315\n",
      "Epoch [3/5], Step [220/5746], Loss: 3.3141\n",
      "Epoch [3/5], Step [230/5746], Loss: 3.2737\n",
      "Epoch [3/5], Step [240/5746], Loss: 3.5060\n",
      "Epoch [3/5], Step [250/5746], Loss: 3.3760\n",
      "Epoch [3/5], Step [260/5746], Loss: 3.3247\n",
      "Epoch [3/5], Step [270/5746], Loss: 3.4567\n",
      "Epoch [3/5], Step [280/5746], Loss: 3.3435\n",
      "Epoch [3/5], Step [290/5746], Loss: 3.5410\n",
      "Epoch [3/5], Step [300/5746], Loss: 3.5141\n",
      "Epoch [3/5], Step [310/5746], Loss: 3.4641\n",
      "Epoch [3/5], Step [320/5746], Loss: 3.5102\n",
      "Epoch [3/5], Step [330/5746], Loss: 3.3969\n",
      "Epoch [3/5], Step [340/5746], Loss: 3.3884\n",
      "Epoch [3/5], Step [350/5746], Loss: 3.4435\n",
      "Epoch [3/5], Step [360/5746], Loss: 3.2792\n",
      "Epoch [3/5], Step [370/5746], Loss: 3.3397\n",
      "Epoch [3/5], Step [380/5746], Loss: 3.4476\n",
      "Epoch [3/5], Step [390/5746], Loss: 3.6159\n",
      "Epoch [3/5], Step [400/5746], Loss: 3.3819\n",
      "Epoch [3/5], Step [410/5746], Loss: 3.3734\n",
      "Epoch [3/5], Step [420/5746], Loss: 3.3103\n",
      "Epoch [3/5], Step [430/5746], Loss: 3.2656\n",
      "Epoch [3/5], Step [440/5746], Loss: 3.5330\n",
      "Epoch [3/5], Step [450/5746], Loss: 3.4516\n",
      "Epoch [3/5], Step [460/5746], Loss: 3.5659\n",
      "Epoch [3/5], Step [470/5746], Loss: 3.4187\n",
      "Epoch [3/5], Step [480/5746], Loss: 3.5979\n",
      "Epoch [3/5], Step [490/5746], Loss: 3.4104\n",
      "Epoch [3/5], Step [500/5746], Loss: 3.4438\n",
      "Epoch [3/5], Step [510/5746], Loss: 3.2196\n",
      "Epoch [3/5], Step [520/5746], Loss: 3.4463\n",
      "Epoch [3/5], Step [530/5746], Loss: 3.3809\n",
      "Epoch [3/5], Step [540/5746], Loss: 3.4714\n",
      "Epoch [3/5], Step [550/5746], Loss: 3.4050\n",
      "Epoch [3/5], Step [560/5746], Loss: 3.4293\n",
      "Epoch [3/5], Step [570/5746], Loss: 3.3770\n",
      "Epoch [3/5], Step [580/5746], Loss: 3.4827\n",
      "Epoch [3/5], Step [590/5746], Loss: 3.3683\n",
      "Epoch [3/5], Step [600/5746], Loss: 3.3990\n",
      "Epoch [3/5], Step [610/5746], Loss: 3.3321\n",
      "Epoch [3/5], Step [620/5746], Loss: 3.4313\n",
      "Epoch [3/5], Step [630/5746], Loss: 3.4808\n",
      "Epoch [3/5], Step [640/5746], Loss: 3.5139\n",
      "Epoch [3/5], Step [650/5746], Loss: 3.3356\n",
      "Epoch [3/5], Step [660/5746], Loss: 3.2601\n",
      "Epoch [3/5], Step [670/5746], Loss: 3.4636\n",
      "Epoch [3/5], Step [680/5746], Loss: 3.4080\n",
      "Epoch [3/5], Step [690/5746], Loss: 3.3614\n",
      "Epoch [3/5], Step [700/5746], Loss: 3.3594\n",
      "Epoch [3/5], Step [710/5746], Loss: 3.5701\n",
      "Epoch [3/5], Step [720/5746], Loss: 3.3440\n",
      "Epoch [3/5], Step [730/5746], Loss: 3.4255\n",
      "Epoch [3/5], Step [740/5746], Loss: 3.4025\n",
      "Epoch [3/5], Step [750/5746], Loss: 3.4960\n",
      "Epoch [3/5], Step [760/5746], Loss: 3.1398\n",
      "Epoch [3/5], Step [770/5746], Loss: 3.5354\n",
      "Epoch [3/5], Step [780/5746], Loss: 3.3357\n",
      "Epoch [3/5], Step [790/5746], Loss: 3.3773\n",
      "Epoch [3/5], Step [800/5746], Loss: 3.3883\n",
      "Epoch [3/5], Step [810/5746], Loss: 3.4642\n",
      "Epoch [3/5], Step [820/5746], Loss: 3.4652\n",
      "Epoch [3/5], Step [830/5746], Loss: 3.2580\n",
      "Epoch [3/5], Step [840/5746], Loss: 3.3744\n",
      "Epoch [3/5], Step [850/5746], Loss: 3.5402\n",
      "Epoch [3/5], Step [860/5746], Loss: 3.4002\n",
      "Epoch [3/5], Step [870/5746], Loss: 3.4399\n",
      "Epoch [3/5], Step [880/5746], Loss: 3.3610\n",
      "Epoch [3/5], Step [890/5746], Loss: 3.3400\n",
      "Epoch [3/5], Step [900/5746], Loss: 3.4470\n",
      "Epoch [3/5], Step [910/5746], Loss: 3.5686\n",
      "Epoch [3/5], Step [920/5746], Loss: 3.2860\n",
      "Epoch [3/5], Step [930/5746], Loss: 3.4415\n",
      "Epoch [3/5], Step [940/5746], Loss: 3.4180\n",
      "Epoch [3/5], Step [950/5746], Loss: 3.4142\n",
      "Epoch [3/5], Step [960/5746], Loss: 3.3967\n",
      "Epoch [3/5], Step [970/5746], Loss: 3.2954\n",
      "Epoch [3/5], Step [980/5746], Loss: 3.4684\n",
      "Epoch [3/5], Step [990/5746], Loss: 3.5764\n",
      "Epoch [3/5], Step [1000/5746], Loss: 3.4037\n",
      "Epoch [3/5], Step [1010/5746], Loss: 3.3011\n",
      "Epoch [3/5], Step [1020/5746], Loss: 3.4960\n",
      "Epoch [3/5], Step [1030/5746], Loss: 3.5185\n",
      "Epoch [3/5], Step [1040/5746], Loss: 3.3418\n",
      "Epoch [3/5], Step [1050/5746], Loss: 3.3639\n",
      "Epoch [3/5], Step [1060/5746], Loss: 3.5072\n",
      "Epoch [3/5], Step [1070/5746], Loss: 3.3835\n",
      "Epoch [3/5], Step [1080/5746], Loss: 3.4360\n",
      "Epoch [3/5], Step [1090/5746], Loss: 3.4947\n",
      "Epoch [3/5], Step [1100/5746], Loss: 3.3360\n",
      "Epoch [3/5], Step [1110/5746], Loss: 3.2468\n",
      "Epoch [3/5], Step [1120/5746], Loss: 3.3977\n",
      "Epoch [3/5], Step [1130/5746], Loss: 3.4228\n",
      "Epoch [3/5], Step [1140/5746], Loss: 3.4176\n",
      "Epoch [3/5], Step [1150/5746], Loss: 3.5321\n",
      "Epoch [3/5], Step [1160/5746], Loss: 3.5181\n",
      "Epoch [3/5], Step [1170/5746], Loss: 3.5071\n",
      "Epoch [3/5], Step [1180/5746], Loss: 3.4211\n",
      "Epoch [3/5], Step [1190/5746], Loss: 3.3551\n",
      "Epoch [3/5], Step [1200/5746], Loss: 3.3959\n",
      "Epoch [3/5], Step [1210/5746], Loss: 3.5851\n",
      "Epoch [3/5], Step [1220/5746], Loss: 3.4334\n",
      "Epoch [3/5], Step [1230/5746], Loss: 3.4497\n",
      "Epoch [3/5], Step [1240/5746], Loss: 3.2754\n",
      "Epoch [3/5], Step [1250/5746], Loss: 3.4765\n",
      "Epoch [3/5], Step [1260/5746], Loss: 3.4684\n",
      "Epoch [3/5], Step [1270/5746], Loss: 3.3955\n",
      "Epoch [3/5], Step [1280/5746], Loss: 3.2418\n",
      "Epoch [3/5], Step [1290/5746], Loss: 3.2935\n",
      "Epoch [3/5], Step [1300/5746], Loss: 3.4828\n",
      "Epoch [3/5], Step [1310/5746], Loss: 3.2708\n",
      "Epoch [3/5], Step [1320/5746], Loss: 3.4641\n",
      "Epoch [3/5], Step [1330/5746], Loss: 3.4065\n",
      "Epoch [3/5], Step [1340/5746], Loss: 3.3313\n",
      "Epoch [3/5], Step [1350/5746], Loss: 3.3296\n",
      "Epoch [3/5], Step [1360/5746], Loss: 3.3611\n",
      "Epoch [3/5], Step [1370/5746], Loss: 3.4149\n",
      "Epoch [3/5], Step [1380/5746], Loss: 3.4459\n",
      "Epoch [3/5], Step [1390/5746], Loss: 3.2738\n",
      "Epoch [3/5], Step [1400/5746], Loss: 3.2890\n",
      "Epoch [3/5], Step [1410/5746], Loss: 3.4983\n",
      "Epoch [3/5], Step [1420/5746], Loss: 3.2951\n",
      "Epoch [3/5], Step [1430/5746], Loss: 3.3301\n",
      "Epoch [3/5], Step [1440/5746], Loss: 3.5104\n",
      "Epoch [3/5], Step [1450/5746], Loss: 3.3387\n",
      "Epoch [3/5], Step [1460/5746], Loss: 3.4963\n",
      "Epoch [3/5], Step [1470/5746], Loss: 3.4149\n",
      "Epoch [3/5], Step [1480/5746], Loss: 3.4500\n",
      "Epoch [3/5], Step [1490/5746], Loss: 3.2788\n",
      "Epoch [3/5], Step [1500/5746], Loss: 3.3214\n",
      "Epoch [3/5], Step [1510/5746], Loss: 3.4463\n",
      "Epoch [3/5], Step [1520/5746], Loss: 3.2816\n",
      "Epoch [3/5], Step [1530/5746], Loss: 3.3880\n",
      "Epoch [3/5], Step [1540/5746], Loss: 3.3695\n",
      "Epoch [3/5], Step [1550/5746], Loss: 3.4660\n",
      "Epoch [3/5], Step [1560/5746], Loss: 3.2345\n",
      "Epoch [3/5], Step [1570/5746], Loss: 3.3622\n",
      "Epoch [3/5], Step [1580/5746], Loss: 3.5541\n",
      "Epoch [3/5], Step [1590/5746], Loss: 3.3959\n",
      "Epoch [3/5], Step [1600/5746], Loss: 3.4828\n",
      "Epoch [3/5], Step [1610/5746], Loss: 3.4453\n",
      "Epoch [3/5], Step [1620/5746], Loss: 3.4912\n",
      "Epoch [3/5], Step [1630/5746], Loss: 3.5294\n",
      "Epoch [3/5], Step [1640/5746], Loss: 3.2891\n",
      "Epoch [3/5], Step [1650/5746], Loss: 3.3953\n",
      "Epoch [3/5], Step [1660/5746], Loss: 3.6121\n",
      "Epoch [3/5], Step [1670/5746], Loss: 3.3580\n",
      "Epoch [3/5], Step [1680/5746], Loss: 3.3459\n",
      "Epoch [3/5], Step [1690/5746], Loss: 3.4231\n",
      "Epoch [3/5], Step [1700/5746], Loss: 3.3151\n",
      "Epoch [3/5], Step [1710/5746], Loss: 3.2532\n",
      "Epoch [3/5], Step [1720/5746], Loss: 3.3448\n",
      "Epoch [3/5], Step [1730/5746], Loss: 3.2416\n",
      "Epoch [3/5], Step [1740/5746], Loss: 3.4095\n",
      "Epoch [3/5], Step [1750/5746], Loss: 3.3841\n",
      "Epoch [3/5], Step [1760/5746], Loss: 3.4721\n",
      "Epoch [3/5], Step [1770/5746], Loss: 3.2881\n",
      "Epoch [3/5], Step [1780/5746], Loss: 3.4195\n",
      "Epoch [3/5], Step [1790/5746], Loss: 3.3320\n",
      "Epoch [3/5], Step [1800/5746], Loss: 3.3855\n",
      "Epoch [3/5], Step [1810/5746], Loss: 3.2161\n",
      "Epoch [3/5], Step [1820/5746], Loss: 3.3303\n",
      "Epoch [3/5], Step [1830/5746], Loss: 3.4356\n",
      "Epoch [3/5], Step [1840/5746], Loss: 3.4142\n",
      "Epoch [3/5], Step [1850/5746], Loss: 3.4098\n",
      "Epoch [3/5], Step [1860/5746], Loss: 3.3740\n",
      "Epoch [3/5], Step [1870/5746], Loss: 3.2830\n",
      "Epoch [3/5], Step [1880/5746], Loss: 3.4395\n",
      "Epoch [3/5], Step [1890/5746], Loss: 3.5689\n",
      "Epoch [3/5], Step [1900/5746], Loss: 3.4807\n",
      "Epoch [3/5], Step [1910/5746], Loss: 3.3836\n",
      "Epoch [3/5], Step [1920/5746], Loss: 3.4628\n",
      "Epoch [3/5], Step [1930/5746], Loss: 3.2618\n",
      "Epoch [3/5], Step [1940/5746], Loss: 3.4309\n",
      "Epoch [3/5], Step [1950/5746], Loss: 3.2976\n",
      "Epoch [3/5], Step [1960/5746], Loss: 3.3548\n",
      "Epoch [3/5], Step [1970/5746], Loss: 3.3638\n",
      "Epoch [3/5], Step [1980/5746], Loss: 3.3393\n",
      "Epoch [3/5], Step [1990/5746], Loss: 3.3684\n",
      "Epoch [3/5], Step [2000/5746], Loss: 3.5264\n",
      "Epoch [3/5], Step [2010/5746], Loss: 3.4323\n",
      "Epoch [3/5], Step [2020/5746], Loss: 3.3887\n",
      "Epoch [3/5], Step [2030/5746], Loss: 3.4448\n",
      "Epoch [3/5], Step [2040/5746], Loss: 3.3289\n",
      "Epoch [3/5], Step [2050/5746], Loss: 3.3646\n",
      "Epoch [3/5], Step [2060/5746], Loss: 3.3771\n",
      "Epoch [3/5], Step [2070/5746], Loss: 3.2325\n",
      "Epoch [3/5], Step [2080/5746], Loss: 3.2729\n",
      "Epoch [3/5], Step [2090/5746], Loss: 3.3267\n",
      "Epoch [3/5], Step [2100/5746], Loss: 3.4453\n",
      "Epoch [3/5], Step [2110/5746], Loss: 3.3163\n",
      "Epoch [3/5], Step [2120/5746], Loss: 3.4214\n",
      "Epoch [3/5], Step [2130/5746], Loss: 3.3393\n",
      "Epoch [3/5], Step [2140/5746], Loss: 3.3666\n",
      "Epoch [3/5], Step [2150/5746], Loss: 3.4525\n",
      "Epoch [3/5], Step [2160/5746], Loss: 3.2761\n",
      "Epoch [3/5], Step [2170/5746], Loss: 3.3706\n",
      "Epoch [3/5], Step [2180/5746], Loss: 3.4240\n",
      "Epoch [3/5], Step [2190/5746], Loss: 3.4503\n",
      "Epoch [3/5], Step [2200/5746], Loss: 3.3933\n",
      "Epoch [3/5], Step [2210/5746], Loss: 3.4754\n",
      "Epoch [3/5], Step [2220/5746], Loss: 3.4048\n",
      "Epoch [3/5], Step [2230/5746], Loss: 3.4743\n",
      "Epoch [3/5], Step [2240/5746], Loss: 3.4488\n",
      "Epoch [3/5], Step [2250/5746], Loss: 3.4002\n",
      "Epoch [3/5], Step [2260/5746], Loss: 3.3966\n",
      "Epoch [3/5], Step [2270/5746], Loss: 3.3733\n",
      "Epoch [3/5], Step [2280/5746], Loss: 3.5492\n",
      "Epoch [3/5], Step [2290/5746], Loss: 3.4218\n",
      "Epoch [3/5], Step [2300/5746], Loss: 3.2080\n",
      "Epoch [3/5], Step [2310/5746], Loss: 3.2654\n",
      "Epoch [3/5], Step [2320/5746], Loss: 3.3325\n",
      "Epoch [3/5], Step [2330/5746], Loss: 3.3720\n",
      "Epoch [3/5], Step [2340/5746], Loss: 3.3245\n",
      "Epoch [3/5], Step [2350/5746], Loss: 3.3170\n",
      "Epoch [3/5], Step [2360/5746], Loss: 3.3723\n",
      "Epoch [3/5], Step [2370/5746], Loss: 3.4403\n",
      "Epoch [3/5], Step [2380/5746], Loss: 3.3325\n",
      "Epoch [3/5], Step [2390/5746], Loss: 3.2673\n",
      "Epoch [3/5], Step [2400/5746], Loss: 3.4461\n",
      "Epoch [3/5], Step [2410/5746], Loss: 3.2598\n",
      "Epoch [3/5], Step [2420/5746], Loss: 3.3493\n",
      "Epoch [3/5], Step [2430/5746], Loss: 3.4302\n",
      "Epoch [3/5], Step [2440/5746], Loss: 3.3460\n",
      "Epoch [3/5], Step [2450/5746], Loss: 3.3412\n",
      "Epoch [3/5], Step [2460/5746], Loss: 3.2380\n",
      "Epoch [3/5], Step [2470/5746], Loss: 3.3182\n",
      "Epoch [3/5], Step [2480/5746], Loss: 3.4267\n",
      "Epoch [3/5], Step [2490/5746], Loss: 3.2871\n",
      "Epoch [3/5], Step [2500/5746], Loss: 3.4686\n",
      "Epoch [3/5], Step [2510/5746], Loss: 3.4122\n",
      "Epoch [3/5], Step [2520/5746], Loss: 3.2960\n",
      "Epoch [3/5], Step [2530/5746], Loss: 3.2485\n",
      "Epoch [3/5], Step [2540/5746], Loss: 3.2655\n",
      "Epoch [3/5], Step [2550/5746], Loss: 3.1854\n",
      "Epoch [3/5], Step [2560/5746], Loss: 3.3556\n",
      "Epoch [3/5], Step [2570/5746], Loss: 3.3931\n",
      "Epoch [3/5], Step [2580/5746], Loss: 3.2783\n",
      "Epoch [3/5], Step [2590/5746], Loss: 3.3468\n",
      "Epoch [3/5], Step [2600/5746], Loss: 3.2815\n",
      "Epoch [3/5], Step [2610/5746], Loss: 3.3862\n",
      "Epoch [3/5], Step [2620/5746], Loss: 3.3367\n",
      "Epoch [3/5], Step [2630/5746], Loss: 3.1867\n",
      "Epoch [3/5], Step [2640/5746], Loss: 3.3200\n",
      "Epoch [3/5], Step [2650/5746], Loss: 3.3704\n",
      "Epoch [3/5], Step [2660/5746], Loss: 3.2798\n",
      "Epoch [3/5], Step [2670/5746], Loss: 3.3278\n",
      "Epoch [3/5], Step [2680/5746], Loss: 3.3431\n",
      "Epoch [3/5], Step [2690/5746], Loss: 3.3366\n",
      "Epoch [3/5], Step [2700/5746], Loss: 3.4092\n",
      "Epoch [3/5], Step [2710/5746], Loss: 3.4229\n",
      "Epoch [3/5], Step [2720/5746], Loss: 3.3842\n",
      "Epoch [3/5], Step [2730/5746], Loss: 3.1704\n",
      "Epoch [3/5], Step [2740/5746], Loss: 3.4082\n",
      "Epoch [3/5], Step [2750/5746], Loss: 3.1737\n",
      "Epoch [3/5], Step [2760/5746], Loss: 3.3926\n",
      "Epoch [3/5], Step [2770/5746], Loss: 3.2937\n",
      "Epoch [3/5], Step [2780/5746], Loss: 3.3118\n",
      "Epoch [3/5], Step [2790/5746], Loss: 3.3886\n",
      "Epoch [3/5], Step [2800/5746], Loss: 3.4302\n",
      "Epoch [3/5], Step [2810/5746], Loss: 3.3052\n",
      "Epoch [3/5], Step [2820/5746], Loss: 3.4169\n",
      "Epoch [3/5], Step [2830/5746], Loss: 3.2368\n",
      "Epoch [3/5], Step [2840/5746], Loss: 3.3973\n",
      "Epoch [3/5], Step [2850/5746], Loss: 3.4338\n",
      "Epoch [3/5], Step [2860/5746], Loss: 3.4193\n",
      "Epoch [3/5], Step [2870/5746], Loss: 3.2632\n",
      "Epoch [3/5], Step [2880/5746], Loss: 3.4024\n",
      "Epoch [3/5], Step [2890/5746], Loss: 3.4116\n",
      "Epoch [3/5], Step [2900/5746], Loss: 3.4592\n",
      "Epoch [3/5], Step [2910/5746], Loss: 3.2504\n",
      "Epoch [3/5], Step [2920/5746], Loss: 3.3743\n",
      "Epoch [3/5], Step [2930/5746], Loss: 3.3012\n",
      "Epoch [3/5], Step [2940/5746], Loss: 3.2728\n",
      "Epoch [3/5], Step [2950/5746], Loss: 3.3358\n",
      "Epoch [3/5], Step [2960/5746], Loss: 3.3692\n",
      "Epoch [3/5], Step [2970/5746], Loss: 3.3137\n",
      "Epoch [3/5], Step [2980/5746], Loss: 3.3917\n",
      "Epoch [3/5], Step [2990/5746], Loss: 3.4561\n",
      "Epoch [3/5], Step [3000/5746], Loss: 3.2984\n",
      "Epoch [3/5], Step [3010/5746], Loss: 3.2289\n",
      "Epoch [3/5], Step [3020/5746], Loss: 3.4287\n",
      "Epoch [3/5], Step [3030/5746], Loss: 3.2795\n",
      "Epoch [3/5], Step [3040/5746], Loss: 3.3430\n",
      "Epoch [3/5], Step [3050/5746], Loss: 3.3049\n",
      "Epoch [3/5], Step [3060/5746], Loss: 3.2809\n",
      "Epoch [3/5], Step [3070/5746], Loss: 3.4251\n",
      "Epoch [3/5], Step [3080/5746], Loss: 3.1779\n",
      "Epoch [3/5], Step [3090/5746], Loss: 3.4355\n",
      "Epoch [3/5], Step [3100/5746], Loss: 3.2743\n",
      "Epoch [3/5], Step [3110/5746], Loss: 3.4959\n",
      "Epoch [3/5], Step [3120/5746], Loss: 3.2907\n",
      "Epoch [3/5], Step [3130/5746], Loss: 3.2840\n",
      "Epoch [3/5], Step [3140/5746], Loss: 3.3476\n",
      "Epoch [3/5], Step [3150/5746], Loss: 3.3375\n",
      "Epoch [3/5], Step [3160/5746], Loss: 3.3449\n",
      "Epoch [3/5], Step [3170/5746], Loss: 3.4060\n",
      "Epoch [3/5], Step [3180/5746], Loss: 3.2704\n",
      "Epoch [3/5], Step [3190/5746], Loss: 3.3347\n",
      "Epoch [3/5], Step [3200/5746], Loss: 3.3405\n",
      "Epoch [3/5], Step [3210/5746], Loss: 3.4370\n",
      "Epoch [3/5], Step [3220/5746], Loss: 3.2008\n",
      "Epoch [3/5], Step [3230/5746], Loss: 3.2732\n",
      "Epoch [3/5], Step [3240/5746], Loss: 3.3839\n",
      "Epoch [3/5], Step [3250/5746], Loss: 3.2980\n",
      "Epoch [3/5], Step [3260/5746], Loss: 3.2159\n",
      "Epoch [3/5], Step [3270/5746], Loss: 3.4463\n",
      "Epoch [3/5], Step [3280/5746], Loss: 3.4519\n",
      "Epoch [3/5], Step [3290/5746], Loss: 3.3607\n",
      "Epoch [3/5], Step [3300/5746], Loss: 3.3953\n",
      "Epoch [3/5], Step [3310/5746], Loss: 3.3415\n",
      "Epoch [3/5], Step [3320/5746], Loss: 3.3520\n",
      "Epoch [3/5], Step [3330/5746], Loss: 3.2945\n",
      "Epoch [3/5], Step [3340/5746], Loss: 3.3651\n",
      "Epoch [3/5], Step [3350/5746], Loss: 3.3686\n",
      "Epoch [3/5], Step [3360/5746], Loss: 3.2641\n",
      "Epoch [3/5], Step [3370/5746], Loss: 3.2697\n",
      "Epoch [3/5], Step [3380/5746], Loss: 3.3030\n",
      "Epoch [3/5], Step [3390/5746], Loss: 3.3417\n",
      "Epoch [3/5], Step [3400/5746], Loss: 3.3208\n",
      "Epoch [3/5], Step [3410/5746], Loss: 3.3477\n",
      "Epoch [3/5], Step [3420/5746], Loss: 3.1981\n",
      "Epoch [3/5], Step [3430/5746], Loss: 3.3669\n",
      "Epoch [3/5], Step [3440/5746], Loss: 3.2634\n",
      "Epoch [3/5], Step [3450/5746], Loss: 3.4347\n",
      "Epoch [3/5], Step [3460/5746], Loss: 3.3769\n",
      "Epoch [3/5], Step [3470/5746], Loss: 3.3028\n",
      "Epoch [3/5], Step [3480/5746], Loss: 3.3685\n",
      "Epoch [3/5], Step [3490/5746], Loss: 3.2807\n",
      "Epoch [3/5], Step [3500/5746], Loss: 3.3618\n",
      "Epoch [3/5], Step [3510/5746], Loss: 3.2172\n",
      "Epoch [3/5], Step [3520/5746], Loss: 3.3653\n",
      "Epoch [3/5], Step [3530/5746], Loss: 3.2457\n",
      "Epoch [3/5], Step [3540/5746], Loss: 3.3222\n",
      "Epoch [3/5], Step [3550/5746], Loss: 3.3488\n",
      "Epoch [3/5], Step [3560/5746], Loss: 3.3364\n",
      "Epoch [3/5], Step [3570/5746], Loss: 3.2969\n",
      "Epoch [3/5], Step [3580/5746], Loss: 3.2766\n",
      "Epoch [3/5], Step [3590/5746], Loss: 3.3272\n",
      "Epoch [3/5], Step [3600/5746], Loss: 3.3825\n",
      "Epoch [3/5], Step [3610/5746], Loss: 3.3396\n",
      "Epoch [3/5], Step [3620/5746], Loss: 3.3492\n",
      "Epoch [3/5], Step [3630/5746], Loss: 3.3315\n",
      "Epoch [3/5], Step [3640/5746], Loss: 3.4522\n",
      "Epoch [3/5], Step [3650/5746], Loss: 3.2516\n",
      "Epoch [3/5], Step [3660/5746], Loss: 3.3069\n",
      "Epoch [3/5], Step [3670/5746], Loss: 3.1943\n",
      "Epoch [3/5], Step [3680/5746], Loss: 3.2362\n",
      "Epoch [3/5], Step [3690/5746], Loss: 3.2374\n",
      "Epoch [3/5], Step [3700/5746], Loss: 3.3692\n",
      "Epoch [3/5], Step [3710/5746], Loss: 3.2356\n",
      "Epoch [3/5], Step [3720/5746], Loss: 3.3367\n",
      "Epoch [3/5], Step [3730/5746], Loss: 3.3219\n",
      "Epoch [3/5], Step [3740/5746], Loss: 3.3929\n",
      "Epoch [3/5], Step [3750/5746], Loss: 3.2452\n",
      "Epoch [3/5], Step [3760/5746], Loss: 3.2565\n",
      "Epoch [3/5], Step [3770/5746], Loss: 3.4043\n",
      "Epoch [3/5], Step [3780/5746], Loss: 3.4326\n",
      "Epoch [3/5], Step [3790/5746], Loss: 3.3909\n",
      "Epoch [3/5], Step [3800/5746], Loss: 3.2677\n",
      "Epoch [3/5], Step [3810/5746], Loss: 3.3720\n",
      "Epoch [3/5], Step [3820/5746], Loss: 3.2167\n",
      "Epoch [3/5], Step [3830/5746], Loss: 3.2160\n",
      "Epoch [3/5], Step [3840/5746], Loss: 3.2934\n",
      "Epoch [3/5], Step [3850/5746], Loss: 3.2797\n",
      "Epoch [3/5], Step [3860/5746], Loss: 3.3180\n",
      "Epoch [3/5], Step [3870/5746], Loss: 3.2741\n",
      "Epoch [3/5], Step [3880/5746], Loss: 3.2988\n",
      "Epoch [3/5], Step [3890/5746], Loss: 3.2867\n",
      "Epoch [3/5], Step [3900/5746], Loss: 3.3910\n",
      "Epoch [3/5], Step [3910/5746], Loss: 3.2487\n",
      "Epoch [3/5], Step [3920/5746], Loss: 3.2944\n",
      "Epoch [3/5], Step [3930/5746], Loss: 3.2584\n",
      "Epoch [3/5], Step [3940/5746], Loss: 3.4175\n",
      "Epoch [3/5], Step [3950/5746], Loss: 3.1614\n",
      "Epoch [3/5], Step [3960/5746], Loss: 3.2864\n",
      "Epoch [3/5], Step [3970/5746], Loss: 3.2747\n",
      "Epoch [3/5], Step [3980/5746], Loss: 3.2790\n",
      "Epoch [3/5], Step [3990/5746], Loss: 3.2977\n",
      "Epoch [3/5], Step [4000/5746], Loss: 3.3615\n",
      "Epoch [3/5], Step [4010/5746], Loss: 3.2323\n",
      "Epoch [3/5], Step [4020/5746], Loss: 3.3275\n",
      "Epoch [3/5], Step [4030/5746], Loss: 3.2812\n",
      "Epoch [3/5], Step [4040/5746], Loss: 3.3617\n",
      "Epoch [3/5], Step [4050/5746], Loss: 3.3095\n",
      "Epoch [3/5], Step [4060/5746], Loss: 3.2344\n",
      "Epoch [3/5], Step [4070/5746], Loss: 3.1976\n",
      "Epoch [3/5], Step [4080/5746], Loss: 3.2291\n",
      "Epoch [3/5], Step [4090/5746], Loss: 3.2736\n",
      "Epoch [3/5], Step [4100/5746], Loss: 3.4089\n",
      "Epoch [3/5], Step [4110/5746], Loss: 3.2290\n",
      "Epoch [3/5], Step [4120/5746], Loss: 3.4293\n",
      "Epoch [3/5], Step [4130/5746], Loss: 3.3172\n",
      "Epoch [3/5], Step [4140/5746], Loss: 3.3693\n",
      "Epoch [3/5], Step [4150/5746], Loss: 3.3542\n",
      "Epoch [3/5], Step [4160/5746], Loss: 3.3725\n",
      "Epoch [3/5], Step [4170/5746], Loss: 3.3064\n",
      "Epoch [3/5], Step [4180/5746], Loss: 3.3353\n",
      "Epoch [3/5], Step [4190/5746], Loss: 3.1902\n",
      "Epoch [3/5], Step [4200/5746], Loss: 3.3548\n",
      "Epoch [3/5], Step [4210/5746], Loss: 3.0960\n",
      "Epoch [3/5], Step [4220/5746], Loss: 3.3488\n",
      "Epoch [3/5], Step [4230/5746], Loss: 3.3371\n",
      "Epoch [3/5], Step [4240/5746], Loss: 3.2098\n",
      "Epoch [3/5], Step [4250/5746], Loss: 3.2384\n",
      "Epoch [3/5], Step [4260/5746], Loss: 3.3918\n",
      "Epoch [3/5], Step [4270/5746], Loss: 3.3449\n",
      "Epoch [3/5], Step [4280/5746], Loss: 3.2540\n",
      "Epoch [3/5], Step [4290/5746], Loss: 3.3826\n",
      "Epoch [3/5], Step [4300/5746], Loss: 3.2616\n",
      "Epoch [3/5], Step [4310/5746], Loss: 3.1442\n",
      "Epoch [3/5], Step [4320/5746], Loss: 3.3928\n",
      "Epoch [3/5], Step [4330/5746], Loss: 3.4420\n",
      "Epoch [3/5], Step [4340/5746], Loss: 3.3127\n",
      "Epoch [3/5], Step [4350/5746], Loss: 3.3345\n",
      "Epoch [3/5], Step [4360/5746], Loss: 3.3569\n",
      "Epoch [3/5], Step [4370/5746], Loss: 3.2933\n",
      "Epoch [3/5], Step [4380/5746], Loss: 3.3011\n",
      "Epoch [3/5], Step [4390/5746], Loss: 3.3216\n",
      "Epoch [3/5], Step [4400/5746], Loss: 3.2484\n",
      "Epoch [3/5], Step [4410/5746], Loss: 3.0738\n",
      "Epoch [3/5], Step [4420/5746], Loss: 3.2956\n",
      "Epoch [3/5], Step [4430/5746], Loss: 3.1836\n",
      "Epoch [3/5], Step [4440/5746], Loss: 3.1373\n",
      "Epoch [3/5], Step [4450/5746], Loss: 3.3165\n",
      "Epoch [3/5], Step [4460/5746], Loss: 3.2760\n",
      "Epoch [3/5], Step [4470/5746], Loss: 3.3022\n",
      "Epoch [3/5], Step [4480/5746], Loss: 3.4490\n",
      "Epoch [3/5], Step [4490/5746], Loss: 3.2350\n",
      "Epoch [3/5], Step [4500/5746], Loss: 3.2162\n",
      "Epoch [3/5], Step [4510/5746], Loss: 3.2905\n",
      "Epoch [3/5], Step [4520/5746], Loss: 3.3884\n",
      "Epoch [3/5], Step [4530/5746], Loss: 3.1789\n",
      "Epoch [3/5], Step [4540/5746], Loss: 3.2857\n",
      "Epoch [3/5], Step [4550/5746], Loss: 3.2686\n",
      "Epoch [3/5], Step [4560/5746], Loss: 3.3469\n",
      "Epoch [3/5], Step [4570/5746], Loss: 3.4995\n",
      "Epoch [3/5], Step [4580/5746], Loss: 3.3881\n",
      "Epoch [3/5], Step [4590/5746], Loss: 3.2250\n",
      "Epoch [3/5], Step [4600/5746], Loss: 3.3947\n",
      "Epoch [3/5], Step [4610/5746], Loss: 3.3572\n",
      "Epoch [3/5], Step [4620/5746], Loss: 3.2672\n",
      "Epoch [3/5], Step [4630/5746], Loss: 3.3998\n",
      "Epoch [3/5], Step [4640/5746], Loss: 3.3172\n",
      "Epoch [3/5], Step [4650/5746], Loss: 3.2150\n",
      "Epoch [3/5], Step [4660/5746], Loss: 3.4415\n",
      "Epoch [3/5], Step [4670/5746], Loss: 3.2606\n",
      "Epoch [3/5], Step [4680/5746], Loss: 3.2805\n",
      "Epoch [3/5], Step [4690/5746], Loss: 3.2739\n",
      "Epoch [3/5], Step [4700/5746], Loss: 3.2869\n",
      "Epoch [3/5], Step [4710/5746], Loss: 3.3498\n",
      "Epoch [3/5], Step [4720/5746], Loss: 3.1860\n",
      "Epoch [3/5], Step [4730/5746], Loss: 3.3472\n",
      "Epoch [3/5], Step [4740/5746], Loss: 3.4250\n",
      "Epoch [3/5], Step [4750/5746], Loss: 3.3537\n",
      "Epoch [3/5], Step [4760/5746], Loss: 3.3027\n",
      "Epoch [3/5], Step [4770/5746], Loss: 3.1787\n",
      "Epoch [3/5], Step [4780/5746], Loss: 3.3748\n",
      "Epoch [3/5], Step [4790/5746], Loss: 3.4764\n",
      "Epoch [3/5], Step [4800/5746], Loss: 3.2864\n",
      "Epoch [3/5], Step [4810/5746], Loss: 3.3208\n",
      "Epoch [3/5], Step [4820/5746], Loss: 3.2293\n",
      "Epoch [3/5], Step [4830/5746], Loss: 3.3123\n",
      "Epoch [3/5], Step [4840/5746], Loss: 3.3126\n",
      "Epoch [3/5], Step [4850/5746], Loss: 3.3277\n",
      "Epoch [3/5], Step [4860/5746], Loss: 3.3439\n",
      "Epoch [3/5], Step [4870/5746], Loss: 3.1586\n",
      "Epoch [3/5], Step [4880/5746], Loss: 3.3643\n",
      "Epoch [3/5], Step [4890/5746], Loss: 3.1055\n",
      "Epoch [3/5], Step [4900/5746], Loss: 3.2155\n",
      "Epoch [3/5], Step [4910/5746], Loss: 3.1807\n",
      "Epoch [3/5], Step [4920/5746], Loss: 3.1686\n",
      "Epoch [3/5], Step [4930/5746], Loss: 3.3138\n",
      "Epoch [3/5], Step [4940/5746], Loss: 3.2927\n",
      "Epoch [3/5], Step [4950/5746], Loss: 3.3196\n",
      "Epoch [3/5], Step [4960/5746], Loss: 3.1627\n",
      "Epoch [3/5], Step [4970/5746], Loss: 3.1984\n",
      "Epoch [3/5], Step [4980/5746], Loss: 3.3338\n",
      "Epoch [3/5], Step [4990/5746], Loss: 3.3059\n",
      "Epoch [3/5], Step [5000/5746], Loss: 3.2608\n",
      "Epoch [3/5], Step [5010/5746], Loss: 3.1296\n",
      "Epoch [3/5], Step [5020/5746], Loss: 3.1717\n",
      "Epoch [3/5], Step [5030/5746], Loss: 3.1514\n",
      "Epoch [3/5], Step [5040/5746], Loss: 3.2955\n",
      "Epoch [3/5], Step [5050/5746], Loss: 3.2011\n",
      "Epoch [3/5], Step [5060/5746], Loss: 3.2505\n",
      "Epoch [3/5], Step [5070/5746], Loss: 3.3749\n",
      "Epoch [3/5], Step [5080/5746], Loss: 3.2645\n",
      "Epoch [3/5], Step [5090/5746], Loss: 3.3136\n",
      "Epoch [3/5], Step [5100/5746], Loss: 3.3861\n",
      "Epoch [3/5], Step [5110/5746], Loss: 3.3293\n",
      "Epoch [3/5], Step [5120/5746], Loss: 3.0929\n",
      "Epoch [3/5], Step [5130/5746], Loss: 3.3914\n",
      "Epoch [3/5], Step [5140/5746], Loss: 3.1313\n",
      "Epoch [3/5], Step [5150/5746], Loss: 3.3634\n",
      "Epoch [3/5], Step [5160/5746], Loss: 3.2953\n",
      "Epoch [3/5], Step [5170/5746], Loss: 3.2128\n",
      "Epoch [3/5], Step [5180/5746], Loss: 3.1304\n",
      "Epoch [3/5], Step [5190/5746], Loss: 3.3642\n",
      "Epoch [3/5], Step [5200/5746], Loss: 3.2587\n",
      "Epoch [3/5], Step [5210/5746], Loss: 3.1943\n",
      "Epoch [3/5], Step [5220/5746], Loss: 3.2681\n",
      "Epoch [3/5], Step [5230/5746], Loss: 3.2764\n",
      "Epoch [3/5], Step [5240/5746], Loss: 3.1748\n",
      "Epoch [3/5], Step [5250/5746], Loss: 3.3733\n",
      "Epoch [3/5], Step [5260/5746], Loss: 3.3398\n",
      "Epoch [3/5], Step [5270/5746], Loss: 3.1309\n",
      "Epoch [3/5], Step [5280/5746], Loss: 3.2655\n",
      "Epoch [3/5], Step [5290/5746], Loss: 3.2655\n",
      "Epoch [3/5], Step [5300/5746], Loss: 3.2444\n",
      "Epoch [3/5], Step [5310/5746], Loss: 3.3927\n",
      "Epoch [3/5], Step [5320/5746], Loss: 3.2003\n",
      "Epoch [3/5], Step [5330/5746], Loss: 3.3123\n",
      "Epoch [3/5], Step [5340/5746], Loss: 3.2531\n",
      "Epoch [3/5], Step [5350/5746], Loss: 3.2468\n",
      "Epoch [3/5], Step [5360/5746], Loss: 3.2798\n",
      "Epoch [3/5], Step [5370/5746], Loss: 3.3029\n",
      "Epoch [3/5], Step [5380/5746], Loss: 3.1430\n",
      "Epoch [3/5], Step [5390/5746], Loss: 3.1838\n",
      "Epoch [3/5], Step [5400/5746], Loss: 3.2590\n",
      "Epoch [3/5], Step [5410/5746], Loss: 3.2864\n",
      "Epoch [3/5], Step [5420/5746], Loss: 3.3444\n",
      "Epoch [3/5], Step [5430/5746], Loss: 3.2290\n",
      "Epoch [3/5], Step [5440/5746], Loss: 3.3266\n",
      "Epoch [3/5], Step [5450/5746], Loss: 3.3948\n",
      "Epoch [3/5], Step [5460/5746], Loss: 3.3413\n",
      "Epoch [3/5], Step [5470/5746], Loss: 3.2683\n",
      "Epoch [3/5], Step [5480/5746], Loss: 3.2354\n",
      "Epoch [3/5], Step [5490/5746], Loss: 3.0864\n",
      "Epoch [3/5], Step [5500/5746], Loss: 3.2822\n",
      "Epoch [3/5], Step [5510/5746], Loss: 3.2017\n",
      "Epoch [3/5], Step [5520/5746], Loss: 3.2349\n",
      "Epoch [3/5], Step [5530/5746], Loss: 3.3510\n",
      "Epoch [3/5], Step [5540/5746], Loss: 3.3384\n",
      "Epoch [3/5], Step [5550/5746], Loss: 3.1315\n",
      "Epoch [3/5], Step [5560/5746], Loss: 3.2290\n",
      "Epoch [3/5], Step [5570/5746], Loss: 3.1763\n",
      "Epoch [3/5], Step [5580/5746], Loss: 3.3054\n",
      "Epoch [3/5], Step [5590/5746], Loss: 3.1797\n",
      "Epoch [3/5], Step [5600/5746], Loss: 3.1311\n",
      "Epoch [3/5], Step [5610/5746], Loss: 3.2488\n",
      "Epoch [3/5], Step [5620/5746], Loss: 3.1911\n",
      "Epoch [3/5], Step [5630/5746], Loss: 3.3721\n",
      "Epoch [3/5], Step [5640/5746], Loss: 3.1948\n",
      "Epoch [3/5], Step [5650/5746], Loss: 3.3182\n",
      "Epoch [3/5], Step [5660/5746], Loss: 3.3437\n",
      "Epoch [3/5], Step [5670/5746], Loss: 3.2430\n",
      "Epoch [3/5], Step [5680/5746], Loss: 3.2146\n",
      "Epoch [3/5], Step [5690/5746], Loss: 3.3161\n",
      "Epoch [3/5], Step [5700/5746], Loss: 3.2248\n",
      "Epoch [3/5], Step [5710/5746], Loss: 3.2276\n",
      "Epoch [3/5], Step [5720/5746], Loss: 3.2439\n",
      "Epoch [3/5], Step [5730/5746], Loss: 3.2401\n",
      "Epoch [3/5], Step [5740/5746], Loss: 3.2534\n",
      "Epoch [3/5] Average Loss: 3.3410, Perplexity: 28.25\n",
      "Epoch [4/5], Step [0/5746], Loss: 3.2458\n",
      "Epoch [4/5], Step [10/5746], Loss: 3.1742\n",
      "Epoch [4/5], Step [20/5746], Loss: 3.1733\n",
      "Epoch [4/5], Step [30/5746], Loss: 3.2749\n",
      "Epoch [4/5], Step [40/5746], Loss: 3.3039\n",
      "Epoch [4/5], Step [50/5746], Loss: 3.0280\n",
      "Epoch [4/5], Step [60/5746], Loss: 3.2086\n",
      "Epoch [4/5], Step [70/5746], Loss: 3.1903\n",
      "Epoch [4/5], Step [80/5746], Loss: 3.2324\n",
      "Epoch [4/5], Step [90/5746], Loss: 3.1154\n",
      "Epoch [4/5], Step [100/5746], Loss: 3.1291\n",
      "Epoch [4/5], Step [110/5746], Loss: 3.2291\n",
      "Epoch [4/5], Step [120/5746], Loss: 3.3786\n",
      "Epoch [4/5], Step [130/5746], Loss: 3.2377\n",
      "Epoch [4/5], Step [140/5746], Loss: 3.2274\n",
      "Epoch [4/5], Step [150/5746], Loss: 3.3691\n",
      "Epoch [4/5], Step [160/5746], Loss: 3.2067\n",
      "Epoch [4/5], Step [170/5746], Loss: 3.1422\n",
      "Epoch [4/5], Step [180/5746], Loss: 3.1284\n",
      "Epoch [4/5], Step [190/5746], Loss: 3.1251\n",
      "Epoch [4/5], Step [200/5746], Loss: 3.1715\n",
      "Epoch [4/5], Step [210/5746], Loss: 3.2829\n",
      "Epoch [4/5], Step [220/5746], Loss: 3.2583\n",
      "Epoch [4/5], Step [230/5746], Loss: 3.1604\n",
      "Epoch [4/5], Step [240/5746], Loss: 3.2212\n",
      "Epoch [4/5], Step [250/5746], Loss: 3.2330\n",
      "Epoch [4/5], Step [260/5746], Loss: 3.0990\n",
      "Epoch [4/5], Step [270/5746], Loss: 3.2461\n",
      "Epoch [4/5], Step [280/5746], Loss: 3.2163\n",
      "Epoch [4/5], Step [290/5746], Loss: 3.2028\n",
      "Epoch [4/5], Step [300/5746], Loss: 3.2686\n",
      "Epoch [4/5], Step [310/5746], Loss: 3.1061\n",
      "Epoch [4/5], Step [320/5746], Loss: 3.2639\n",
      "Epoch [4/5], Step [330/5746], Loss: 3.1401\n",
      "Epoch [4/5], Step [340/5746], Loss: 3.1741\n",
      "Epoch [4/5], Step [350/5746], Loss: 3.1508\n",
      "Epoch [4/5], Step [360/5746], Loss: 3.1327\n",
      "Epoch [4/5], Step [370/5746], Loss: 3.2532\n",
      "Epoch [4/5], Step [380/5746], Loss: 3.0594\n",
      "Epoch [4/5], Step [390/5746], Loss: 3.2081\n",
      "Epoch [4/5], Step [400/5746], Loss: 3.1481\n",
      "Epoch [4/5], Step [410/5746], Loss: 3.3071\n",
      "Epoch [4/5], Step [420/5746], Loss: 3.3364\n",
      "Epoch [4/5], Step [430/5746], Loss: 3.2040\n",
      "Epoch [4/5], Step [440/5746], Loss: 3.1414\n",
      "Epoch [4/5], Step [450/5746], Loss: 3.1584\n",
      "Epoch [4/5], Step [460/5746], Loss: 3.2436\n",
      "Epoch [4/5], Step [470/5746], Loss: 3.1204\n",
      "Epoch [4/5], Step [480/5746], Loss: 3.2680\n",
      "Epoch [4/5], Step [490/5746], Loss: 3.1856\n",
      "Epoch [4/5], Step [500/5746], Loss: 3.1440\n",
      "Epoch [4/5], Step [510/5746], Loss: 3.1380\n",
      "Epoch [4/5], Step [520/5746], Loss: 3.2023\n",
      "Epoch [4/5], Step [530/5746], Loss: 3.1384\n",
      "Epoch [4/5], Step [540/5746], Loss: 3.2177\n",
      "Epoch [4/5], Step [550/5746], Loss: 3.0493\n",
      "Epoch [4/5], Step [560/5746], Loss: 3.4107\n",
      "Epoch [4/5], Step [570/5746], Loss: 3.2945\n",
      "Epoch [4/5], Step [580/5746], Loss: 3.1975\n",
      "Epoch [4/5], Step [590/5746], Loss: 3.3968\n",
      "Epoch [4/5], Step [600/5746], Loss: 3.3365\n",
      "Epoch [4/5], Step [610/5746], Loss: 3.3187\n",
      "Epoch [4/5], Step [620/5746], Loss: 3.2727\n",
      "Epoch [4/5], Step [630/5746], Loss: 3.0547\n",
      "Epoch [4/5], Step [640/5746], Loss: 3.1623\n",
      "Epoch [4/5], Step [650/5746], Loss: 3.2373\n",
      "Epoch [4/5], Step [660/5746], Loss: 3.1672\n",
      "Epoch [4/5], Step [670/5746], Loss: 3.3184\n",
      "Epoch [4/5], Step [680/5746], Loss: 3.1074\n",
      "Epoch [4/5], Step [690/5746], Loss: 3.0910\n",
      "Epoch [4/5], Step [700/5746], Loss: 3.3498\n",
      "Epoch [4/5], Step [710/5746], Loss: 3.2635\n",
      "Epoch [4/5], Step [720/5746], Loss: 3.1856\n",
      "Epoch [4/5], Step [730/5746], Loss: 3.2493\n",
      "Epoch [4/5], Step [740/5746], Loss: 3.2255\n",
      "Epoch [4/5], Step [750/5746], Loss: 3.2625\n",
      "Epoch [4/5], Step [760/5746], Loss: 3.1869\n",
      "Epoch [4/5], Step [770/5746], Loss: 3.3291\n",
      "Epoch [4/5], Step [780/5746], Loss: 3.1583\n",
      "Epoch [4/5], Step [790/5746], Loss: 3.0890\n",
      "Epoch [4/5], Step [800/5746], Loss: 3.2117\n",
      "Epoch [4/5], Step [810/5746], Loss: 3.2682\n",
      "Epoch [4/5], Step [820/5746], Loss: 3.1743\n",
      "Epoch [4/5], Step [830/5746], Loss: 3.1004\n",
      "Epoch [4/5], Step [840/5746], Loss: 3.2667\n",
      "Epoch [4/5], Step [850/5746], Loss: 3.3038\n",
      "Epoch [4/5], Step [860/5746], Loss: 3.1616\n",
      "Epoch [4/5], Step [870/5746], Loss: 3.0735\n",
      "Epoch [4/5], Step [880/5746], Loss: 3.1529\n",
      "Epoch [4/5], Step [890/5746], Loss: 3.1921\n",
      "Epoch [4/5], Step [900/5746], Loss: 3.2583\n",
      "Epoch [4/5], Step [910/5746], Loss: 3.2134\n",
      "Epoch [4/5], Step [920/5746], Loss: 3.2925\n",
      "Epoch [4/5], Step [930/5746], Loss: 3.2126\n",
      "Epoch [4/5], Step [940/5746], Loss: 3.1620\n",
      "Epoch [4/5], Step [950/5746], Loss: 3.1522\n",
      "Epoch [4/5], Step [960/5746], Loss: 3.2502\n",
      "Epoch [4/5], Step [970/5746], Loss: 3.2185\n",
      "Epoch [4/5], Step [980/5746], Loss: 3.2857\n",
      "Epoch [4/5], Step [990/5746], Loss: 3.2462\n",
      "Epoch [4/5], Step [1000/5746], Loss: 3.1439\n",
      "Epoch [4/5], Step [1010/5746], Loss: 3.3146\n",
      "Epoch [4/5], Step [1020/5746], Loss: 3.2076\n",
      "Epoch [4/5], Step [1030/5746], Loss: 3.1668\n",
      "Epoch [4/5], Step [1040/5746], Loss: 3.3192\n",
      "Epoch [4/5], Step [1050/5746], Loss: 3.1541\n",
      "Epoch [4/5], Step [1060/5746], Loss: 3.1972\n",
      "Epoch [4/5], Step [1070/5746], Loss: 3.3291\n",
      "Epoch [4/5], Step [1080/5746], Loss: 3.0906\n",
      "Epoch [4/5], Step [1090/5746], Loss: 3.3120\n",
      "Epoch [4/5], Step [1100/5746], Loss: 3.1500\n",
      "Epoch [4/5], Step [1110/5746], Loss: 3.2542\n",
      "Epoch [4/5], Step [1120/5746], Loss: 3.1500\n",
      "Epoch [4/5], Step [1130/5746], Loss: 3.3520\n",
      "Epoch [4/5], Step [1140/5746], Loss: 3.2434\n",
      "Epoch [4/5], Step [1150/5746], Loss: 3.2851\n",
      "Epoch [4/5], Step [1160/5746], Loss: 3.2058\n",
      "Epoch [4/5], Step [1170/5746], Loss: 3.3845\n",
      "Epoch [4/5], Step [1180/5746], Loss: 3.1562\n",
      "Epoch [4/5], Step [1190/5746], Loss: 3.1989\n",
      "Epoch [4/5], Step [1200/5746], Loss: 3.1680\n",
      "Epoch [4/5], Step [1210/5746], Loss: 3.1568\n",
      "Epoch [4/5], Step [1220/5746], Loss: 3.2310\n",
      "Epoch [4/5], Step [1230/5746], Loss: 3.0691\n",
      "Epoch [4/5], Step [1240/5746], Loss: 3.1354\n",
      "Epoch [4/5], Step [1250/5746], Loss: 3.2629\n",
      "Epoch [4/5], Step [1260/5746], Loss: 3.2594\n",
      "Epoch [4/5], Step [1270/5746], Loss: 3.0646\n",
      "Epoch [4/5], Step [1280/5746], Loss: 3.2845\n",
      "Epoch [4/5], Step [1290/5746], Loss: 3.1461\n",
      "Epoch [4/5], Step [1300/5746], Loss: 3.1194\n",
      "Epoch [4/5], Step [1310/5746], Loss: 3.2807\n",
      "Epoch [4/5], Step [1320/5746], Loss: 3.2681\n",
      "Epoch [4/5], Step [1330/5746], Loss: 3.2826\n",
      "Epoch [4/5], Step [1340/5746], Loss: 3.2003\n",
      "Epoch [4/5], Step [1350/5746], Loss: 3.1932\n",
      "Epoch [4/5], Step [1360/5746], Loss: 3.2639\n",
      "Epoch [4/5], Step [1370/5746], Loss: 3.3043\n",
      "Epoch [4/5], Step [1380/5746], Loss: 3.2142\n",
      "Epoch [4/5], Step [1390/5746], Loss: 3.2353\n",
      "Epoch [4/5], Step [1400/5746], Loss: 3.2444\n",
      "Epoch [4/5], Step [1410/5746], Loss: 3.1316\n",
      "Epoch [4/5], Step [1420/5746], Loss: 3.2108\n",
      "Epoch [4/5], Step [1430/5746], Loss: 3.0452\n",
      "Epoch [4/5], Step [1440/5746], Loss: 3.3119\n",
      "Epoch [4/5], Step [1450/5746], Loss: 3.2002\n",
      "Epoch [4/5], Step [1460/5746], Loss: 3.2272\n",
      "Epoch [4/5], Step [1470/5746], Loss: 3.3050\n",
      "Epoch [4/5], Step [1480/5746], Loss: 3.1798\n",
      "Epoch [4/5], Step [1490/5746], Loss: 3.0909\n",
      "Epoch [4/5], Step [1500/5746], Loss: 3.1712\n",
      "Epoch [4/5], Step [1510/5746], Loss: 3.3288\n",
      "Epoch [4/5], Step [1520/5746], Loss: 3.2130\n",
      "Epoch [4/5], Step [1530/5746], Loss: 3.2368\n",
      "Epoch [4/5], Step [1540/5746], Loss: 3.2472\n",
      "Epoch [4/5], Step [1550/5746], Loss: 3.0616\n",
      "Epoch [4/5], Step [1560/5746], Loss: 3.2772\n",
      "Epoch [4/5], Step [1570/5746], Loss: 3.1766\n",
      "Epoch [4/5], Step [1580/5746], Loss: 3.1563\n",
      "Epoch [4/5], Step [1590/5746], Loss: 3.1933\n",
      "Epoch [4/5], Step [1600/5746], Loss: 3.3320\n",
      "Epoch [4/5], Step [1610/5746], Loss: 3.1317\n",
      "Epoch [4/5], Step [1620/5746], Loss: 3.2894\n",
      "Epoch [4/5], Step [1630/5746], Loss: 3.2913\n",
      "Epoch [4/5], Step [1640/5746], Loss: 3.2320\n",
      "Epoch [4/5], Step [1650/5746], Loss: 3.1978\n",
      "Epoch [4/5], Step [1660/5746], Loss: 3.1596\n",
      "Epoch [4/5], Step [1670/5746], Loss: 3.2429\n",
      "Epoch [4/5], Step [1680/5746], Loss: 3.0781\n",
      "Epoch [4/5], Step [1690/5746], Loss: 3.2218\n",
      "Epoch [4/5], Step [1700/5746], Loss: 3.1614\n",
      "Epoch [4/5], Step [1710/5746], Loss: 3.1729\n",
      "Epoch [4/5], Step [1720/5746], Loss: 3.3428\n",
      "Epoch [4/5], Step [1730/5746], Loss: 3.2558\n",
      "Epoch [4/5], Step [1740/5746], Loss: 3.1841\n",
      "Epoch [4/5], Step [1750/5746], Loss: 3.1357\n",
      "Epoch [4/5], Step [1760/5746], Loss: 3.2496\n",
      "Epoch [4/5], Step [1770/5746], Loss: 3.1432\n",
      "Epoch [4/5], Step [1780/5746], Loss: 3.3055\n",
      "Epoch [4/5], Step [1790/5746], Loss: 3.4187\n",
      "Epoch [4/5], Step [1800/5746], Loss: 3.1502\n",
      "Epoch [4/5], Step [1810/5746], Loss: 3.1266\n",
      "Epoch [4/5], Step [1820/5746], Loss: 3.1255\n",
      "Epoch [4/5], Step [1830/5746], Loss: 3.2926\n",
      "Epoch [4/5], Step [1840/5746], Loss: 3.2083\n",
      "Epoch [4/5], Step [1850/5746], Loss: 3.2250\n",
      "Epoch [4/5], Step [1860/5746], Loss: 3.0439\n",
      "Epoch [4/5], Step [1870/5746], Loss: 3.1657\n",
      "Epoch [4/5], Step [1880/5746], Loss: 3.3624\n",
      "Epoch [4/5], Step [1890/5746], Loss: 3.0808\n",
      "Epoch [4/5], Step [1900/5746], Loss: 3.1981\n",
      "Epoch [4/5], Step [1910/5746], Loss: 3.3254\n",
      "Epoch [4/5], Step [1920/5746], Loss: 3.2533\n",
      "Epoch [4/5], Step [1930/5746], Loss: 3.1242\n",
      "Epoch [4/5], Step [1940/5746], Loss: 3.2793\n",
      "Epoch [4/5], Step [1950/5746], Loss: 3.1680\n",
      "Epoch [4/5], Step [1960/5746], Loss: 3.1680\n",
      "Epoch [4/5], Step [1970/5746], Loss: 3.3111\n",
      "Epoch [4/5], Step [1980/5746], Loss: 3.0776\n",
      "Epoch [4/5], Step [1990/5746], Loss: 3.3209\n",
      "Epoch [4/5], Step [2000/5746], Loss: 3.1128\n",
      "Epoch [4/5], Step [2010/5746], Loss: 3.2766\n",
      "Epoch [4/5], Step [2020/5746], Loss: 3.1624\n",
      "Epoch [4/5], Step [2030/5746], Loss: 3.0796\n",
      "Epoch [4/5], Step [2040/5746], Loss: 3.1249\n",
      "Epoch [4/5], Step [2050/5746], Loss: 3.1727\n",
      "Epoch [4/5], Step [2060/5746], Loss: 3.0474\n",
      "Epoch [4/5], Step [2070/5746], Loss: 3.1458\n",
      "Epoch [4/5], Step [2080/5746], Loss: 3.2274\n",
      "Epoch [4/5], Step [2090/5746], Loss: 3.0614\n",
      "Epoch [4/5], Step [2100/5746], Loss: 3.1766\n",
      "Epoch [4/5], Step [2110/5746], Loss: 3.2264\n",
      "Epoch [4/5], Step [2120/5746], Loss: 3.2395\n",
      "Epoch [4/5], Step [2130/5746], Loss: 3.2460\n",
      "Epoch [4/5], Step [2140/5746], Loss: 3.2504\n",
      "Epoch [4/5], Step [2150/5746], Loss: 3.3271\n",
      "Epoch [4/5], Step [2160/5746], Loss: 3.1835\n",
      "Epoch [4/5], Step [2170/5746], Loss: 3.3701\n",
      "Epoch [4/5], Step [2180/5746], Loss: 3.0703\n",
      "Epoch [4/5], Step [2190/5746], Loss: 3.2555\n",
      "Epoch [4/5], Step [2200/5746], Loss: 3.0504\n",
      "Epoch [4/5], Step [2210/5746], Loss: 3.2034\n",
      "Epoch [4/5], Step [2220/5746], Loss: 3.1716\n",
      "Epoch [4/5], Step [2230/5746], Loss: 3.2296\n",
      "Epoch [4/5], Step [2240/5746], Loss: 3.2931\n",
      "Epoch [4/5], Step [2250/5746], Loss: 3.0913\n",
      "Epoch [4/5], Step [2260/5746], Loss: 3.1604\n",
      "Epoch [4/5], Step [2270/5746], Loss: 2.9605\n",
      "Epoch [4/5], Step [2280/5746], Loss: 3.1929\n",
      "Epoch [4/5], Step [2290/5746], Loss: 3.1714\n",
      "Epoch [4/5], Step [2300/5746], Loss: 3.0883\n",
      "Epoch [4/5], Step [2310/5746], Loss: 3.1422\n",
      "Epoch [4/5], Step [2320/5746], Loss: 3.2979\n",
      "Epoch [4/5], Step [2330/5746], Loss: 3.2565\n",
      "Epoch [4/5], Step [2340/5746], Loss: 3.1527\n",
      "Epoch [4/5], Step [2350/5746], Loss: 3.2411\n",
      "Epoch [4/5], Step [2360/5746], Loss: 3.1817\n",
      "Epoch [4/5], Step [2370/5746], Loss: 3.2043\n",
      "Epoch [4/5], Step [2380/5746], Loss: 3.2992\n",
      "Epoch [4/5], Step [2390/5746], Loss: 3.1709\n",
      "Epoch [4/5], Step [2400/5746], Loss: 3.1671\n",
      "Epoch [4/5], Step [2410/5746], Loss: 3.1550\n",
      "Epoch [4/5], Step [2420/5746], Loss: 3.0753\n",
      "Epoch [4/5], Step [2430/5746], Loss: 3.0993\n",
      "Epoch [4/5], Step [2440/5746], Loss: 3.2415\n",
      "Epoch [4/5], Step [2450/5746], Loss: 3.2799\n",
      "Epoch [4/5], Step [2460/5746], Loss: 3.2802\n",
      "Epoch [4/5], Step [2470/5746], Loss: 3.2661\n",
      "Epoch [4/5], Step [2480/5746], Loss: 3.0975\n",
      "Epoch [4/5], Step [2490/5746], Loss: 3.2103\n",
      "Epoch [4/5], Step [2500/5746], Loss: 3.1028\n",
      "Epoch [4/5], Step [2510/5746], Loss: 3.2551\n",
      "Epoch [4/5], Step [2520/5746], Loss: 3.0219\n",
      "Epoch [4/5], Step [2530/5746], Loss: 3.1591\n",
      "Epoch [4/5], Step [2540/5746], Loss: 3.1435\n",
      "Epoch [4/5], Step [2550/5746], Loss: 3.1122\n",
      "Epoch [4/5], Step [2560/5746], Loss: 3.2320\n",
      "Epoch [4/5], Step [2570/5746], Loss: 3.0907\n",
      "Epoch [4/5], Step [2580/5746], Loss: 3.3618\n",
      "Epoch [4/5], Step [2590/5746], Loss: 3.2855\n",
      "Epoch [4/5], Step [2600/5746], Loss: 3.1795\n",
      "Epoch [4/5], Step [2610/5746], Loss: 3.1179\n",
      "Epoch [4/5], Step [2620/5746], Loss: 3.1904\n",
      "Epoch [4/5], Step [2630/5746], Loss: 3.3031\n",
      "Epoch [4/5], Step [2640/5746], Loss: 3.1764\n",
      "Epoch [4/5], Step [2650/5746], Loss: 3.1859\n",
      "Epoch [4/5], Step [2660/5746], Loss: 3.1897\n",
      "Epoch [4/5], Step [2670/5746], Loss: 3.2720\n",
      "Epoch [4/5], Step [2680/5746], Loss: 3.1973\n",
      "Epoch [4/5], Step [2690/5746], Loss: 3.1175\n",
      "Epoch [4/5], Step [2700/5746], Loss: 3.1697\n",
      "Epoch [4/5], Step [2710/5746], Loss: 3.1658\n",
      "Epoch [4/5], Step [2720/5746], Loss: 3.1526\n",
      "Epoch [4/5], Step [2730/5746], Loss: 3.1587\n",
      "Epoch [4/5], Step [2740/5746], Loss: 3.2510\n",
      "Epoch [4/5], Step [2750/5746], Loss: 3.2155\n",
      "Epoch [4/5], Step [2760/5746], Loss: 3.0774\n",
      "Epoch [4/5], Step [2770/5746], Loss: 3.2947\n",
      "Epoch [4/5], Step [2780/5746], Loss: 3.2026\n",
      "Epoch [4/5], Step [2790/5746], Loss: 3.2408\n",
      "Epoch [4/5], Step [2800/5746], Loss: 3.0931\n",
      "Epoch [4/5], Step [2810/5746], Loss: 3.2861\n",
      "Epoch [4/5], Step [2820/5746], Loss: 3.1341\n",
      "Epoch [4/5], Step [2830/5746], Loss: 3.1686\n",
      "Epoch [4/5], Step [2840/5746], Loss: 3.1634\n",
      "Epoch [4/5], Step [2850/5746], Loss: 3.1749\n",
      "Epoch [4/5], Step [2860/5746], Loss: 3.1754\n",
      "Epoch [4/5], Step [2870/5746], Loss: 3.2777\n",
      "Epoch [4/5], Step [2880/5746], Loss: 3.0760\n",
      "Epoch [4/5], Step [2890/5746], Loss: 3.2637\n",
      "Epoch [4/5], Step [2900/5746], Loss: 3.0698\n",
      "Epoch [4/5], Step [2910/5746], Loss: 3.2575\n",
      "Epoch [4/5], Step [2920/5746], Loss: 3.1965\n",
      "Epoch [4/5], Step [2930/5746], Loss: 3.0249\n",
      "Epoch [4/5], Step [2940/5746], Loss: 3.3764\n",
      "Epoch [4/5], Step [2950/5746], Loss: 3.1495\n",
      "Epoch [4/5], Step [2960/5746], Loss: 3.2667\n",
      "Epoch [4/5], Step [2970/5746], Loss: 3.0887\n",
      "Epoch [4/5], Step [2980/5746], Loss: 3.2558\n",
      "Epoch [4/5], Step [2990/5746], Loss: 3.1661\n",
      "Epoch [4/5], Step [3000/5746], Loss: 3.1735\n",
      "Epoch [4/5], Step [3010/5746], Loss: 2.9376\n",
      "Epoch [4/5], Step [3020/5746], Loss: 3.2176\n",
      "Epoch [4/5], Step [3030/5746], Loss: 3.1446\n",
      "Epoch [4/5], Step [3040/5746], Loss: 3.1610\n",
      "Epoch [4/5], Step [3050/5746], Loss: 3.2123\n",
      "Epoch [4/5], Step [3060/5746], Loss: 3.2350\n",
      "Epoch [4/5], Step [3070/5746], Loss: 3.2018\n",
      "Epoch [4/5], Step [3080/5746], Loss: 3.2128\n",
      "Epoch [4/5], Step [3090/5746], Loss: 3.0967\n",
      "Epoch [4/5], Step [3100/5746], Loss: 3.1964\n",
      "Epoch [4/5], Step [3110/5746], Loss: 3.0796\n",
      "Epoch [4/5], Step [3120/5746], Loss: 3.0651\n",
      "Epoch [4/5], Step [3130/5746], Loss: 3.3200\n",
      "Epoch [4/5], Step [3140/5746], Loss: 3.2081\n",
      "Epoch [4/5], Step [3150/5746], Loss: 3.2324\n",
      "Epoch [4/5], Step [3160/5746], Loss: 3.2665\n",
      "Epoch [4/5], Step [3170/5746], Loss: 2.9619\n",
      "Epoch [4/5], Step [3180/5746], Loss: 2.9650\n",
      "Epoch [4/5], Step [3190/5746], Loss: 3.1570\n",
      "Epoch [4/5], Step [3200/5746], Loss: 3.0568\n",
      "Epoch [4/5], Step [3210/5746], Loss: 3.1486\n",
      "Epoch [4/5], Step [3220/5746], Loss: 3.1095\n",
      "Epoch [4/5], Step [3230/5746], Loss: 3.2719\n",
      "Epoch [4/5], Step [3240/5746], Loss: 3.2197\n",
      "Epoch [4/5], Step [3250/5746], Loss: 3.1153\n",
      "Epoch [4/5], Step [3260/5746], Loss: 3.3737\n",
      "Epoch [4/5], Step [3270/5746], Loss: 3.2806\n",
      "Epoch [4/5], Step [3280/5746], Loss: 3.2451\n",
      "Epoch [4/5], Step [3290/5746], Loss: 3.1281\n",
      "Epoch [4/5], Step [3300/5746], Loss: 3.2220\n",
      "Epoch [4/5], Step [3310/5746], Loss: 3.2449\n",
      "Epoch [4/5], Step [3320/5746], Loss: 3.0264\n",
      "Epoch [4/5], Step [3330/5746], Loss: 3.1481\n",
      "Epoch [4/5], Step [3340/5746], Loss: 3.0954\n",
      "Epoch [4/5], Step [3350/5746], Loss: 2.9632\n",
      "Epoch [4/5], Step [3360/5746], Loss: 3.2641\n",
      "Epoch [4/5], Step [3370/5746], Loss: 3.1905\n",
      "Epoch [4/5], Step [3380/5746], Loss: 3.1043\n",
      "Epoch [4/5], Step [3390/5746], Loss: 3.1566\n",
      "Epoch [4/5], Step [3400/5746], Loss: 3.1848\n",
      "Epoch [4/5], Step [3410/5746], Loss: 3.2687\n",
      "Epoch [4/5], Step [3420/5746], Loss: 3.0934\n",
      "Epoch [4/5], Step [3430/5746], Loss: 3.1250\n",
      "Epoch [4/5], Step [3440/5746], Loss: 3.2165\n",
      "Epoch [4/5], Step [3450/5746], Loss: 3.1087\n",
      "Epoch [4/5], Step [3460/5746], Loss: 3.1710\n",
      "Epoch [4/5], Step [3470/5746], Loss: 3.0880\n",
      "Epoch [4/5], Step [3480/5746], Loss: 3.2507\n",
      "Epoch [4/5], Step [3490/5746], Loss: 3.3495\n",
      "Epoch [4/5], Step [3500/5746], Loss: 3.2627\n",
      "Epoch [4/5], Step [3510/5746], Loss: 3.3210\n",
      "Epoch [4/5], Step [3520/5746], Loss: 3.1190\n",
      "Epoch [4/5], Step [3530/5746], Loss: 3.2994\n",
      "Epoch [4/5], Step [3540/5746], Loss: 3.1319\n",
      "Epoch [4/5], Step [3550/5746], Loss: 3.1565\n",
      "Epoch [4/5], Step [3560/5746], Loss: 3.1455\n",
      "Epoch [4/5], Step [3570/5746], Loss: 3.1076\n",
      "Epoch [4/5], Step [3580/5746], Loss: 3.2960\n",
      "Epoch [4/5], Step [3590/5746], Loss: 3.1887\n",
      "Epoch [4/5], Step [3600/5746], Loss: 3.1652\n",
      "Epoch [4/5], Step [3610/5746], Loss: 3.0823\n",
      "Epoch [4/5], Step [3620/5746], Loss: 3.2363\n",
      "Epoch [4/5], Step [3630/5746], Loss: 3.0830\n",
      "Epoch [4/5], Step [3640/5746], Loss: 3.1627\n",
      "Epoch [4/5], Step [3650/5746], Loss: 3.0940\n",
      "Epoch [4/5], Step [3660/5746], Loss: 3.1551\n",
      "Epoch [4/5], Step [3670/5746], Loss: 3.1885\n",
      "Epoch [4/5], Step [3680/5746], Loss: 3.1609\n",
      "Epoch [4/5], Step [3690/5746], Loss: 3.1415\n",
      "Epoch [4/5], Step [3700/5746], Loss: 3.2033\n",
      "Epoch [4/5], Step [3710/5746], Loss: 3.2476\n",
      "Epoch [4/5], Step [3720/5746], Loss: 3.2082\n",
      "Epoch [4/5], Step [3730/5746], Loss: 3.3164\n",
      "Epoch [4/5], Step [3740/5746], Loss: 3.2533\n",
      "Epoch [4/5], Step [3750/5746], Loss: 3.1634\n",
      "Epoch [4/5], Step [3760/5746], Loss: 3.0625\n",
      "Epoch [4/5], Step [3770/5746], Loss: 3.1109\n",
      "Epoch [4/5], Step [3780/5746], Loss: 3.1294\n",
      "Epoch [4/5], Step [3790/5746], Loss: 3.2099\n",
      "Epoch [4/5], Step [3800/5746], Loss: 3.0410\n",
      "Epoch [4/5], Step [3810/5746], Loss: 3.1939\n",
      "Epoch [4/5], Step [3820/5746], Loss: 3.0247\n",
      "Epoch [4/5], Step [3830/5746], Loss: 3.1802\n",
      "Epoch [4/5], Step [3840/5746], Loss: 3.0814\n",
      "Epoch [4/5], Step [3850/5746], Loss: 3.1546\n",
      "Epoch [4/5], Step [3860/5746], Loss: 3.1133\n",
      "Epoch [4/5], Step [3870/5746], Loss: 3.2632\n",
      "Epoch [4/5], Step [3880/5746], Loss: 3.0501\n",
      "Epoch [4/5], Step [3890/5746], Loss: 3.0932\n",
      "Epoch [4/5], Step [3900/5746], Loss: 3.1096\n",
      "Epoch [4/5], Step [3910/5746], Loss: 3.1015\n",
      "Epoch [4/5], Step [3920/5746], Loss: 3.1965\n",
      "Epoch [4/5], Step [3930/5746], Loss: 2.9703\n",
      "Epoch [4/5], Step [3940/5746], Loss: 3.1701\n",
      "Epoch [4/5], Step [3950/5746], Loss: 3.1857\n",
      "Epoch [4/5], Step [3960/5746], Loss: 3.2332\n",
      "Epoch [4/5], Step [3970/5746], Loss: 3.2806\n",
      "Epoch [4/5], Step [3980/5746], Loss: 3.2586\n",
      "Epoch [4/5], Step [3990/5746], Loss: 3.1644\n",
      "Epoch [4/5], Step [4000/5746], Loss: 3.1018\n",
      "Epoch [4/5], Step [4010/5746], Loss: 3.1759\n",
      "Epoch [4/5], Step [4020/5746], Loss: 3.2399\n",
      "Epoch [4/5], Step [4030/5746], Loss: 3.1908\n",
      "Epoch [4/5], Step [4040/5746], Loss: 3.2285\n",
      "Epoch [4/5], Step [4050/5746], Loss: 3.1342\n",
      "Epoch [4/5], Step [4060/5746], Loss: 3.3252\n",
      "Epoch [4/5], Step [4070/5746], Loss: 3.3627\n",
      "Epoch [4/5], Step [4080/5746], Loss: 3.0035\n",
      "Epoch [4/5], Step [4090/5746], Loss: 3.3853\n",
      "Epoch [4/5], Step [4100/5746], Loss: 3.1497\n",
      "Epoch [4/5], Step [4110/5746], Loss: 2.9962\n",
      "Epoch [4/5], Step [4120/5746], Loss: 3.0439\n",
      "Epoch [4/5], Step [4130/5746], Loss: 3.2805\n",
      "Epoch [4/5], Step [4140/5746], Loss: 3.0225\n",
      "Epoch [4/5], Step [4150/5746], Loss: 3.1849\n",
      "Epoch [4/5], Step [4160/5746], Loss: 3.1361\n",
      "Epoch [4/5], Step [4170/5746], Loss: 3.3347\n",
      "Epoch [4/5], Step [4180/5746], Loss: 3.0837\n",
      "Epoch [4/5], Step [4190/5746], Loss: 3.2740\n",
      "Epoch [4/5], Step [4200/5746], Loss: 3.0729\n",
      "Epoch [4/5], Step [4210/5746], Loss: 3.2352\n",
      "Epoch [4/5], Step [4220/5746], Loss: 3.1679\n",
      "Epoch [4/5], Step [4230/5746], Loss: 3.1986\n",
      "Epoch [4/5], Step [4240/5746], Loss: 3.2642\n",
      "Epoch [4/5], Step [4250/5746], Loss: 3.2806\n",
      "Epoch [4/5], Step [4260/5746], Loss: 3.1467\n",
      "Epoch [4/5], Step [4270/5746], Loss: 3.1320\n",
      "Epoch [4/5], Step [4280/5746], Loss: 3.1757\n",
      "Epoch [4/5], Step [4290/5746], Loss: 3.1941\n",
      "Epoch [4/5], Step [4300/5746], Loss: 3.1872\n",
      "Epoch [4/5], Step [4310/5746], Loss: 3.0608\n",
      "Epoch [4/5], Step [4320/5746], Loss: 3.3221\n",
      "Epoch [4/5], Step [4330/5746], Loss: 3.1334\n",
      "Epoch [4/5], Step [4340/5746], Loss: 3.1603\n",
      "Epoch [4/5], Step [4350/5746], Loss: 3.1903\n",
      "Epoch [4/5], Step [4360/5746], Loss: 3.1553\n",
      "Epoch [4/5], Step [4370/5746], Loss: 3.0493\n",
      "Epoch [4/5], Step [4380/5746], Loss: 3.0966\n",
      "Epoch [4/5], Step [4390/5746], Loss: 3.0409\n",
      "Epoch [4/5], Step [4400/5746], Loss: 3.1046\n",
      "Epoch [4/5], Step [4410/5746], Loss: 3.2780\n",
      "Epoch [4/5], Step [4420/5746], Loss: 3.0844\n",
      "Epoch [4/5], Step [4430/5746], Loss: 3.0797\n",
      "Epoch [4/5], Step [4440/5746], Loss: 3.2324\n",
      "Epoch [4/5], Step [4450/5746], Loss: 3.2447\n",
      "Epoch [4/5], Step [4460/5746], Loss: 3.0979\n",
      "Epoch [4/5], Step [4470/5746], Loss: 2.9996\n",
      "Epoch [4/5], Step [4480/5746], Loss: 3.1817\n",
      "Epoch [4/5], Step [4490/5746], Loss: 3.0363\n",
      "Epoch [4/5], Step [4500/5746], Loss: 3.0958\n",
      "Epoch [4/5], Step [4510/5746], Loss: 3.1602\n",
      "Epoch [4/5], Step [4520/5746], Loss: 3.0462\n",
      "Epoch [4/5], Step [4530/5746], Loss: 3.1729\n",
      "Epoch [4/5], Step [4540/5746], Loss: 3.1240\n",
      "Epoch [4/5], Step [4550/5746], Loss: 3.0940\n",
      "Epoch [4/5], Step [4560/5746], Loss: 3.0338\n",
      "Epoch [4/5], Step [4570/5746], Loss: 3.0960\n",
      "Epoch [4/5], Step [4580/5746], Loss: 3.1635\n",
      "Epoch [4/5], Step [4590/5746], Loss: 3.1290\n",
      "Epoch [4/5], Step [4600/5746], Loss: 3.0466\n",
      "Epoch [4/5], Step [4610/5746], Loss: 3.0691\n",
      "Epoch [4/5], Step [4620/5746], Loss: 3.1103\n",
      "Epoch [4/5], Step [4630/5746], Loss: 3.0850\n",
      "Epoch [4/5], Step [4640/5746], Loss: 3.2279\n",
      "Epoch [4/5], Step [4650/5746], Loss: 3.0903\n",
      "Epoch [4/5], Step [4660/5746], Loss: 2.9838\n",
      "Epoch [4/5], Step [4670/5746], Loss: 3.2127\n",
      "Epoch [4/5], Step [4680/5746], Loss: 3.2168\n",
      "Epoch [4/5], Step [4690/5746], Loss: 3.1269\n",
      "Epoch [4/5], Step [4700/5746], Loss: 3.1067\n",
      "Epoch [4/5], Step [4710/5746], Loss: 3.1466\n",
      "Epoch [4/5], Step [4720/5746], Loss: 3.2101\n",
      "Epoch [4/5], Step [4730/5746], Loss: 3.0648\n",
      "Epoch [4/5], Step [4740/5746], Loss: 3.0983\n",
      "Epoch [4/5], Step [4750/5746], Loss: 3.1108\n",
      "Epoch [4/5], Step [4760/5746], Loss: 3.1069\n",
      "Epoch [4/5], Step [4770/5746], Loss: 3.0912\n",
      "Epoch [4/5], Step [4780/5746], Loss: 3.1980\n",
      "Epoch [4/5], Step [4790/5746], Loss: 3.0960\n",
      "Epoch [4/5], Step [4800/5746], Loss: 3.0832\n",
      "Epoch [4/5], Step [4810/5746], Loss: 3.1264\n",
      "Epoch [4/5], Step [4820/5746], Loss: 3.2764\n",
      "Epoch [4/5], Step [4830/5746], Loss: 3.2080\n",
      "Epoch [4/5], Step [4840/5746], Loss: 3.1222\n",
      "Epoch [4/5], Step [4850/5746], Loss: 3.2481\n",
      "Epoch [4/5], Step [4860/5746], Loss: 3.0582\n",
      "Epoch [4/5], Step [4870/5746], Loss: 3.0392\n",
      "Epoch [4/5], Step [4880/5746], Loss: 3.1244\n",
      "Epoch [4/5], Step [4890/5746], Loss: 3.0757\n",
      "Epoch [4/5], Step [4900/5746], Loss: 3.0921\n",
      "Epoch [4/5], Step [4910/5746], Loss: 3.1474\n",
      "Epoch [4/5], Step [4920/5746], Loss: 3.1193\n",
      "Epoch [4/5], Step [4930/5746], Loss: 3.0619\n",
      "Epoch [4/5], Step [4940/5746], Loss: 3.2010\n",
      "Epoch [4/5], Step [4950/5746], Loss: 3.2057\n",
      "Epoch [4/5], Step [4960/5746], Loss: 3.1955\n",
      "Epoch [4/5], Step [4970/5746], Loss: 3.0468\n",
      "Epoch [4/5], Step [4980/5746], Loss: 3.1448\n",
      "Epoch [4/5], Step [4990/5746], Loss: 3.0617\n",
      "Epoch [4/5], Step [5000/5746], Loss: 3.0751\n",
      "Epoch [4/5], Step [5010/5746], Loss: 3.2380\n",
      "Epoch [4/5], Step [5020/5746], Loss: 2.8571\n",
      "Epoch [4/5], Step [5030/5746], Loss: 3.0548\n",
      "Epoch [4/5], Step [5040/5746], Loss: 3.2566\n",
      "Epoch [4/5], Step [5050/5746], Loss: 3.1978\n",
      "Epoch [4/5], Step [5060/5746], Loss: 3.1305\n",
      "Epoch [4/5], Step [5070/5746], Loss: 3.1716\n",
      "Epoch [4/5], Step [5080/5746], Loss: 3.1605\n",
      "Epoch [4/5], Step [5090/5746], Loss: 2.9731\n",
      "Epoch [4/5], Step [5100/5746], Loss: 3.1959\n",
      "Epoch [4/5], Step [5110/5746], Loss: 3.2248\n",
      "Epoch [4/5], Step [5120/5746], Loss: 3.1755\n",
      "Epoch [4/5], Step [5130/5746], Loss: 3.1724\n",
      "Epoch [4/5], Step [5140/5746], Loss: 3.1047\n",
      "Epoch [4/5], Step [5150/5746], Loss: 3.0672\n",
      "Epoch [4/5], Step [5160/5746], Loss: 3.0619\n",
      "Epoch [4/5], Step [5170/5746], Loss: 2.9892\n",
      "Epoch [4/5], Step [5180/5746], Loss: 3.0360\n",
      "Epoch [4/5], Step [5190/5746], Loss: 3.2271\n",
      "Epoch [4/5], Step [5200/5746], Loss: 3.0744\n",
      "Epoch [4/5], Step [5210/5746], Loss: 3.1319\n",
      "Epoch [4/5], Step [5220/5746], Loss: 3.1302\n",
      "Epoch [4/5], Step [5230/5746], Loss: 3.1366\n",
      "Epoch [4/5], Step [5240/5746], Loss: 3.1946\n",
      "Epoch [4/5], Step [5250/5746], Loss: 2.8862\n",
      "Epoch [4/5], Step [5260/5746], Loss: 3.0648\n",
      "Epoch [4/5], Step [5270/5746], Loss: 3.1399\n",
      "Epoch [4/5], Step [5280/5746], Loss: 2.9900\n",
      "Epoch [4/5], Step [5290/5746], Loss: 3.1067\n",
      "Epoch [4/5], Step [5300/5746], Loss: 3.2777\n",
      "Epoch [4/5], Step [5310/5746], Loss: 3.0280\n",
      "Epoch [4/5], Step [5320/5746], Loss: 3.2343\n",
      "Epoch [4/5], Step [5330/5746], Loss: 3.1921\n",
      "Epoch [4/5], Step [5340/5746], Loss: 3.1914\n",
      "Epoch [4/5], Step [5350/5746], Loss: 3.1556\n",
      "Epoch [4/5], Step [5360/5746], Loss: 3.1898\n",
      "Epoch [4/5], Step [5370/5746], Loss: 3.0227\n",
      "Epoch [4/5], Step [5380/5746], Loss: 3.1356\n",
      "Epoch [4/5], Step [5390/5746], Loss: 2.9789\n",
      "Epoch [4/5], Step [5400/5746], Loss: 3.0237\n",
      "Epoch [4/5], Step [5410/5746], Loss: 3.0850\n",
      "Epoch [4/5], Step [5420/5746], Loss: 3.0527\n",
      "Epoch [4/5], Step [5430/5746], Loss: 3.2101\n",
      "Epoch [4/5], Step [5440/5746], Loss: 3.0663\n",
      "Epoch [4/5], Step [5450/5746], Loss: 3.2418\n",
      "Epoch [4/5], Step [5460/5746], Loss: 3.1733\n",
      "Epoch [4/5], Step [5470/5746], Loss: 3.2044\n",
      "Epoch [4/5], Step [5480/5746], Loss: 3.0760\n",
      "Epoch [4/5], Step [5490/5746], Loss: 3.1768\n",
      "Epoch [4/5], Step [5500/5746], Loss: 3.0680\n",
      "Epoch [4/5], Step [5510/5746], Loss: 3.1548\n",
      "Epoch [4/5], Step [5520/5746], Loss: 3.0043\n",
      "Epoch [4/5], Step [5530/5746], Loss: 3.0582\n",
      "Epoch [4/5], Step [5540/5746], Loss: 3.0196\n",
      "Epoch [4/5], Step [5550/5746], Loss: 3.1727\n",
      "Epoch [4/5], Step [5560/5746], Loss: 2.9940\n",
      "Epoch [4/5], Step [5570/5746], Loss: 3.0978\n",
      "Epoch [4/5], Step [5580/5746], Loss: 3.1468\n",
      "Epoch [4/5], Step [5590/5746], Loss: 3.1921\n",
      "Epoch [4/5], Step [5600/5746], Loss: 3.0619\n",
      "Epoch [4/5], Step [5610/5746], Loss: 2.9887\n",
      "Epoch [4/5], Step [5620/5746], Loss: 3.1440\n",
      "Epoch [4/5], Step [5630/5746], Loss: 3.1261\n",
      "Epoch [4/5], Step [5640/5746], Loss: 2.9220\n",
      "Epoch [4/5], Step [5650/5746], Loss: 2.9950\n",
      "Epoch [4/5], Step [5660/5746], Loss: 3.0951\n",
      "Epoch [4/5], Step [5670/5746], Loss: 3.1198\n",
      "Epoch [4/5], Step [5680/5746], Loss: 3.0519\n",
      "Epoch [4/5], Step [5690/5746], Loss: 3.0235\n",
      "Epoch [4/5], Step [5700/5746], Loss: 3.1414\n",
      "Epoch [4/5], Step [5710/5746], Loss: 3.1336\n",
      "Epoch [4/5], Step [5720/5746], Loss: 3.1493\n",
      "Epoch [4/5], Step [5730/5746], Loss: 3.1309\n",
      "Epoch [4/5], Step [5740/5746], Loss: 3.2147\n",
      "Epoch [4/5] Average Loss: 3.1723, Perplexity: 23.86\n",
      "Epoch [5/5], Step [0/5746], Loss: 3.1415\n",
      "Epoch [5/5], Step [10/5746], Loss: 3.1459\n",
      "Epoch [5/5], Step [20/5746], Loss: 3.1601\n",
      "Epoch [5/5], Step [30/5746], Loss: 3.0632\n",
      "Epoch [5/5], Step [40/5746], Loss: 3.1197\n",
      "Epoch [5/5], Step [50/5746], Loss: 2.9552\n",
      "Epoch [5/5], Step [60/5746], Loss: 2.8431\n",
      "Epoch [5/5], Step [70/5746], Loss: 3.0873\n",
      "Epoch [5/5], Step [80/5746], Loss: 3.0595\n",
      "Epoch [5/5], Step [90/5746], Loss: 3.0753\n",
      "Epoch [5/5], Step [100/5746], Loss: 2.9616\n",
      "Epoch [5/5], Step [110/5746], Loss: 3.1835\n",
      "Epoch [5/5], Step [120/5746], Loss: 3.1292\n",
      "Epoch [5/5], Step [130/5746], Loss: 3.0616\n",
      "Epoch [5/5], Step [140/5746], Loss: 3.0203\n",
      "Epoch [5/5], Step [150/5746], Loss: 3.0718\n",
      "Epoch [5/5], Step [160/5746], Loss: 3.1403\n",
      "Epoch [5/5], Step [170/5746], Loss: 3.0946\n",
      "Epoch [5/5], Step [180/5746], Loss: 3.1017\n",
      "Epoch [5/5], Step [190/5746], Loss: 3.1049\n",
      "Epoch [5/5], Step [200/5746], Loss: 3.0478\n",
      "Epoch [5/5], Step [210/5746], Loss: 3.0454\n",
      "Epoch [5/5], Step [220/5746], Loss: 3.2014\n",
      "Epoch [5/5], Step [230/5746], Loss: 3.1205\n",
      "Epoch [5/5], Step [240/5746], Loss: 3.0654\n",
      "Epoch [5/5], Step [250/5746], Loss: 3.0093\n",
      "Epoch [5/5], Step [260/5746], Loss: 3.0643\n",
      "Epoch [5/5], Step [270/5746], Loss: 3.1397\n",
      "Epoch [5/5], Step [280/5746], Loss: 2.8846\n",
      "Epoch [5/5], Step [290/5746], Loss: 2.9759\n",
      "Epoch [5/5], Step [300/5746], Loss: 3.0464\n",
      "Epoch [5/5], Step [310/5746], Loss: 3.2672\n",
      "Epoch [5/5], Step [320/5746], Loss: 3.1126\n",
      "Epoch [5/5], Step [330/5746], Loss: 2.9085\n",
      "Epoch [5/5], Step [340/5746], Loss: 3.2392\n",
      "Epoch [5/5], Step [350/5746], Loss: 3.1037\n",
      "Epoch [5/5], Step [360/5746], Loss: 3.0015\n",
      "Epoch [5/5], Step [370/5746], Loss: 2.8175\n",
      "Epoch [5/5], Step [380/5746], Loss: 3.0669\n",
      "Epoch [5/5], Step [390/5746], Loss: 3.1696\n",
      "Epoch [5/5], Step [400/5746], Loss: 3.2053\n",
      "Epoch [5/5], Step [410/5746], Loss: 3.1586\n",
      "Epoch [5/5], Step [420/5746], Loss: 3.1223\n",
      "Epoch [5/5], Step [430/5746], Loss: 2.9860\n",
      "Epoch [5/5], Step [440/5746], Loss: 3.0129\n",
      "Epoch [5/5], Step [450/5746], Loss: 3.0627\n",
      "Epoch [5/5], Step [460/5746], Loss: 3.1922\n",
      "Epoch [5/5], Step [470/5746], Loss: 3.0596\n",
      "Epoch [5/5], Step [480/5746], Loss: 3.1193\n",
      "Epoch [5/5], Step [490/5746], Loss: 3.1484\n",
      "Epoch [5/5], Step [500/5746], Loss: 3.1192\n",
      "Epoch [5/5], Step [510/5746], Loss: 3.2257\n",
      "Epoch [5/5], Step [520/5746], Loss: 3.1085\n",
      "Epoch [5/5], Step [530/5746], Loss: 3.1155\n",
      "Epoch [5/5], Step [540/5746], Loss: 3.0632\n",
      "Epoch [5/5], Step [550/5746], Loss: 3.1787\n",
      "Epoch [5/5], Step [560/5746], Loss: 3.1918\n",
      "Epoch [5/5], Step [570/5746], Loss: 3.0728\n",
      "Epoch [5/5], Step [580/5746], Loss: 3.1687\n",
      "Epoch [5/5], Step [590/5746], Loss: 3.1424\n",
      "Epoch [5/5], Step [600/5746], Loss: 2.9410\n",
      "Epoch [5/5], Step [610/5746], Loss: 3.1693\n",
      "Epoch [5/5], Step [620/5746], Loss: 3.1330\n",
      "Epoch [5/5], Step [630/5746], Loss: 3.1206\n",
      "Epoch [5/5], Step [640/5746], Loss: 3.2760\n",
      "Epoch [5/5], Step [650/5746], Loss: 3.0751\n",
      "Epoch [5/5], Step [660/5746], Loss: 3.1388\n",
      "Epoch [5/5], Step [670/5746], Loss: 3.0939\n",
      "Epoch [5/5], Step [680/5746], Loss: 3.1698\n",
      "Epoch [5/5], Step [690/5746], Loss: 3.0821\n",
      "Epoch [5/5], Step [700/5746], Loss: 3.0440\n",
      "Epoch [5/5], Step [710/5746], Loss: 3.2085\n",
      "Epoch [5/5], Step [720/5746], Loss: 3.0165\n",
      "Epoch [5/5], Step [730/5746], Loss: 3.1887\n",
      "Epoch [5/5], Step [740/5746], Loss: 3.0617\n",
      "Epoch [5/5], Step [750/5746], Loss: 3.0431\n",
      "Epoch [5/5], Step [760/5746], Loss: 3.1076\n",
      "Epoch [5/5], Step [770/5746], Loss: 3.1224\n",
      "Epoch [5/5], Step [780/5746], Loss: 3.0564\n",
      "Epoch [5/5], Step [790/5746], Loss: 3.1175\n",
      "Epoch [5/5], Step [800/5746], Loss: 3.0758\n",
      "Epoch [5/5], Step [810/5746], Loss: 2.9767\n",
      "Epoch [5/5], Step [820/5746], Loss: 2.9796\n",
      "Epoch [5/5], Step [830/5746], Loss: 3.0833\n",
      "Epoch [5/5], Step [840/5746], Loss: 3.0286\n",
      "Epoch [5/5], Step [850/5746], Loss: 3.0572\n",
      "Epoch [5/5], Step [860/5746], Loss: 3.1196\n",
      "Epoch [5/5], Step [870/5746], Loss: 3.0547\n",
      "Epoch [5/5], Step [880/5746], Loss: 3.0786\n",
      "Epoch [5/5], Step [890/5746], Loss: 3.0485\n",
      "Epoch [5/5], Step [900/5746], Loss: 2.9980\n",
      "Epoch [5/5], Step [910/5746], Loss: 3.1269\n",
      "Epoch [5/5], Step [920/5746], Loss: 3.0919\n",
      "Epoch [5/5], Step [930/5746], Loss: 3.1000\n",
      "Epoch [5/5], Step [940/5746], Loss: 3.1995\n",
      "Epoch [5/5], Step [950/5746], Loss: 3.1246\n",
      "Epoch [5/5], Step [960/5746], Loss: 3.1077\n",
      "Epoch [5/5], Step [970/5746], Loss: 3.0882\n",
      "Epoch [5/5], Step [980/5746], Loss: 2.9907\n",
      "Epoch [5/5], Step [990/5746], Loss: 3.0804\n",
      "Epoch [5/5], Step [1000/5746], Loss: 3.1194\n",
      "Epoch [5/5], Step [1010/5746], Loss: 3.0874\n",
      "Epoch [5/5], Step [1020/5746], Loss: 3.1336\n",
      "Epoch [5/5], Step [1030/5746], Loss: 3.0921\n",
      "Epoch [5/5], Step [1040/5746], Loss: 3.1224\n",
      "Epoch [5/5], Step [1050/5746], Loss: 3.0824\n",
      "Epoch [5/5], Step [1060/5746], Loss: 3.0416\n",
      "Epoch [5/5], Step [1070/5746], Loss: 3.2353\n",
      "Epoch [5/5], Step [1080/5746], Loss: 3.1007\n",
      "Epoch [5/5], Step [1090/5746], Loss: 3.1061\n",
      "Epoch [5/5], Step [1100/5746], Loss: 3.0317\n",
      "Epoch [5/5], Step [1110/5746], Loss: 2.9977\n",
      "Epoch [5/5], Step [1120/5746], Loss: 3.0281\n",
      "Epoch [5/5], Step [1130/5746], Loss: 2.9857\n",
      "Epoch [5/5], Step [1140/5746], Loss: 3.0464\n",
      "Epoch [5/5], Step [1150/5746], Loss: 3.0663\n",
      "Epoch [5/5], Step [1160/5746], Loss: 3.0874\n",
      "Epoch [5/5], Step [1170/5746], Loss: 3.0094\n",
      "Epoch [5/5], Step [1180/5746], Loss: 3.0533\n",
      "Epoch [5/5], Step [1190/5746], Loss: 2.9750\n",
      "Epoch [5/5], Step [1200/5746], Loss: 3.0900\n",
      "Epoch [5/5], Step [1210/5746], Loss: 3.0261\n",
      "Epoch [5/5], Step [1220/5746], Loss: 3.1675\n",
      "Epoch [5/5], Step [1230/5746], Loss: 3.0050\n",
      "Epoch [5/5], Step [1240/5746], Loss: 3.1709\n",
      "Epoch [5/5], Step [1250/5746], Loss: 3.1475\n",
      "Epoch [5/5], Step [1260/5746], Loss: 3.1908\n",
      "Epoch [5/5], Step [1270/5746], Loss: 3.2149\n",
      "Epoch [5/5], Step [1280/5746], Loss: 3.1927\n",
      "Epoch [5/5], Step [1290/5746], Loss: 2.9678\n",
      "Epoch [5/5], Step [1300/5746], Loss: 3.1576\n",
      "Epoch [5/5], Step [1310/5746], Loss: 3.0051\n",
      "Epoch [5/5], Step [1320/5746], Loss: 3.0457\n",
      "Epoch [5/5], Step [1330/5746], Loss: 3.2061\n",
      "Epoch [5/5], Step [1340/5746], Loss: 3.0211\n",
      "Epoch [5/5], Step [1350/5746], Loss: 3.1778\n",
      "Epoch [5/5], Step [1360/5746], Loss: 2.9696\n",
      "Epoch [5/5], Step [1370/5746], Loss: 3.1358\n",
      "Epoch [5/5], Step [1380/5746], Loss: 3.0264\n",
      "Epoch [5/5], Step [1390/5746], Loss: 2.9380\n",
      "Epoch [5/5], Step [1400/5746], Loss: 3.0266\n",
      "Epoch [5/5], Step [1410/5746], Loss: 3.2360\n",
      "Epoch [5/5], Step [1420/5746], Loss: 3.0591\n",
      "Epoch [5/5], Step [1430/5746], Loss: 3.0084\n",
      "Epoch [5/5], Step [1440/5746], Loss: 3.0999\n",
      "Epoch [5/5], Step [1450/5746], Loss: 2.9966\n",
      "Epoch [5/5], Step [1460/5746], Loss: 3.0354\n",
      "Epoch [5/5], Step [1470/5746], Loss: 3.0522\n",
      "Epoch [5/5], Step [1480/5746], Loss: 2.9964\n",
      "Epoch [5/5], Step [1490/5746], Loss: 2.9971\n",
      "Epoch [5/5], Step [1500/5746], Loss: 3.0654\n",
      "Epoch [5/5], Step [1510/5746], Loss: 3.0462\n",
      "Epoch [5/5], Step [1520/5746], Loss: 3.1247\n",
      "Epoch [5/5], Step [1530/5746], Loss: 3.1897\n",
      "Epoch [5/5], Step [1540/5746], Loss: 3.2102\n",
      "Epoch [5/5], Step [1550/5746], Loss: 3.2113\n",
      "Epoch [5/5], Step [1560/5746], Loss: 3.1169\n",
      "Epoch [5/5], Step [1570/5746], Loss: 3.0950\n",
      "Epoch [5/5], Step [1580/5746], Loss: 3.1387\n",
      "Epoch [5/5], Step [1590/5746], Loss: 3.0851\n",
      "Epoch [5/5], Step [1600/5746], Loss: 3.1721\n",
      "Epoch [5/5], Step [1610/5746], Loss: 2.9488\n",
      "Epoch [5/5], Step [1620/5746], Loss: 3.0680\n",
      "Epoch [5/5], Step [1630/5746], Loss: 2.9941\n",
      "Epoch [5/5], Step [1640/5746], Loss: 3.3249\n",
      "Epoch [5/5], Step [1650/5746], Loss: 3.0611\n",
      "Epoch [5/5], Step [1660/5746], Loss: 2.9614\n",
      "Epoch [5/5], Step [1670/5746], Loss: 2.9996\n",
      "Epoch [5/5], Step [1680/5746], Loss: 3.1305\n",
      "Epoch [5/5], Step [1690/5746], Loss: 3.1235\n",
      "Epoch [5/5], Step [1700/5746], Loss: 3.1049\n",
      "Epoch [5/5], Step [1710/5746], Loss: 3.0551\n",
      "Epoch [5/5], Step [1720/5746], Loss: 3.0733\n",
      "Epoch [5/5], Step [1730/5746], Loss: 2.9724\n",
      "Epoch [5/5], Step [1740/5746], Loss: 3.1287\n",
      "Epoch [5/5], Step [1750/5746], Loss: 2.9906\n",
      "Epoch [5/5], Step [1760/5746], Loss: 3.1456\n",
      "Epoch [5/5], Step [1770/5746], Loss: 2.9284\n",
      "Epoch [5/5], Step [1780/5746], Loss: 3.0817\n",
      "Epoch [5/5], Step [1790/5746], Loss: 3.0849\n",
      "Epoch [5/5], Step [1800/5746], Loss: 3.0317\n",
      "Epoch [5/5], Step [1810/5746], Loss: 3.0383\n",
      "Epoch [5/5], Step [1820/5746], Loss: 2.9479\n",
      "Epoch [5/5], Step [1830/5746], Loss: 3.0497\n",
      "Epoch [5/5], Step [1840/5746], Loss: 3.0659\n",
      "Epoch [5/5], Step [1850/5746], Loss: 3.1778\n",
      "Epoch [5/5], Step [1860/5746], Loss: 2.9762\n",
      "Epoch [5/5], Step [1870/5746], Loss: 3.0343\n",
      "Epoch [5/5], Step [1880/5746], Loss: 3.0645\n",
      "Epoch [5/5], Step [1890/5746], Loss: 2.9611\n",
      "Epoch [5/5], Step [1900/5746], Loss: 2.9802\n",
      "Epoch [5/5], Step [1910/5746], Loss: 3.0132\n",
      "Epoch [5/5], Step [1920/5746], Loss: 3.0596\n",
      "Epoch [5/5], Step [1930/5746], Loss: 3.0561\n",
      "Epoch [5/5], Step [1940/5746], Loss: 3.0572\n",
      "Epoch [5/5], Step [1950/5746], Loss: 3.0993\n",
      "Epoch [5/5], Step [1960/5746], Loss: 3.0845\n",
      "Epoch [5/5], Step [1970/5746], Loss: 2.9964\n",
      "Epoch [5/5], Step [1980/5746], Loss: 3.1103\n",
      "Epoch [5/5], Step [1990/5746], Loss: 3.1935\n",
      "Epoch [5/5], Step [2000/5746], Loss: 3.1788\n",
      "Epoch [5/5], Step [2010/5746], Loss: 3.0646\n",
      "Epoch [5/5], Step [2020/5746], Loss: 3.0606\n",
      "Epoch [5/5], Step [2030/5746], Loss: 3.1349\n",
      "Epoch [5/5], Step [2040/5746], Loss: 3.1408\n",
      "Epoch [5/5], Step [2050/5746], Loss: 3.0828\n",
      "Epoch [5/5], Step [2060/5746], Loss: 2.7908\n",
      "Epoch [5/5], Step [2070/5746], Loss: 3.1793\n",
      "Epoch [5/5], Step [2080/5746], Loss: 3.1012\n",
      "Epoch [5/5], Step [2090/5746], Loss: 2.9110\n",
      "Epoch [5/5], Step [2100/5746], Loss: 3.0407\n",
      "Epoch [5/5], Step [2110/5746], Loss: 2.9519\n",
      "Epoch [5/5], Step [2120/5746], Loss: 2.9700\n",
      "Epoch [5/5], Step [2130/5746], Loss: 3.1535\n",
      "Epoch [5/5], Step [2140/5746], Loss: 2.9911\n",
      "Epoch [5/5], Step [2150/5746], Loss: 3.1018\n",
      "Epoch [5/5], Step [2160/5746], Loss: 2.9946\n",
      "Epoch [5/5], Step [2170/5746], Loss: 3.1086\n",
      "Epoch [5/5], Step [2180/5746], Loss: 3.0072\n",
      "Epoch [5/5], Step [2190/5746], Loss: 3.0008\n",
      "Epoch [5/5], Step [2200/5746], Loss: 3.0705\n",
      "Epoch [5/5], Step [2210/5746], Loss: 3.0071\n",
      "Epoch [5/5], Step [2220/5746], Loss: 3.1723\n",
      "Epoch [5/5], Step [2230/5746], Loss: 3.0211\n",
      "Epoch [5/5], Step [2240/5746], Loss: 2.9910\n",
      "Epoch [5/5], Step [2250/5746], Loss: 3.1891\n",
      "Epoch [5/5], Step [2260/5746], Loss: 3.1065\n",
      "Epoch [5/5], Step [2270/5746], Loss: 2.9204\n",
      "Epoch [5/5], Step [2280/5746], Loss: 3.0792\n",
      "Epoch [5/5], Step [2290/5746], Loss: 2.9332\n",
      "Epoch [5/5], Step [2300/5746], Loss: 3.1053\n",
      "Epoch [5/5], Step [2310/5746], Loss: 2.9477\n",
      "Epoch [5/5], Step [2320/5746], Loss: 3.0856\n",
      "Epoch [5/5], Step [2330/5746], Loss: 3.0240\n",
      "Epoch [5/5], Step [2340/5746], Loss: 3.1288\n",
      "Epoch [5/5], Step [2350/5746], Loss: 3.1062\n",
      "Epoch [5/5], Step [2360/5746], Loss: 3.0013\n",
      "Epoch [5/5], Step [2370/5746], Loss: 3.1046\n",
      "Epoch [5/5], Step [2380/5746], Loss: 3.0773\n",
      "Epoch [5/5], Step [2390/5746], Loss: 3.0237\n",
      "Epoch [5/5], Step [2400/5746], Loss: 3.1530\n",
      "Epoch [5/5], Step [2410/5746], Loss: 3.1920\n",
      "Epoch [5/5], Step [2420/5746], Loss: 2.9971\n",
      "Epoch [5/5], Step [2430/5746], Loss: 3.1525\n",
      "Epoch [5/5], Step [2440/5746], Loss: 3.1433\n",
      "Epoch [5/5], Step [2450/5746], Loss: 3.0383\n",
      "Epoch [5/5], Step [2460/5746], Loss: 3.0265\n",
      "Epoch [5/5], Step [2470/5746], Loss: 2.9426\n",
      "Epoch [5/5], Step [2480/5746], Loss: 3.2132\n",
      "Epoch [5/5], Step [2490/5746], Loss: 3.1028\n",
      "Epoch [5/5], Step [2500/5746], Loss: 2.9144\n",
      "Epoch [5/5], Step [2510/5746], Loss: 3.0668\n",
      "Epoch [5/5], Step [2520/5746], Loss: 2.9729\n",
      "Epoch [5/5], Step [2530/5746], Loss: 3.0685\n",
      "Epoch [5/5], Step [2540/5746], Loss: 2.9948\n",
      "Epoch [5/5], Step [2550/5746], Loss: 3.1130\n",
      "Epoch [5/5], Step [2560/5746], Loss: 2.9823\n",
      "Epoch [5/5], Step [2570/5746], Loss: 3.0543\n",
      "Epoch [5/5], Step [2580/5746], Loss: 3.1619\n",
      "Epoch [5/5], Step [2590/5746], Loss: 2.9902\n",
      "Epoch [5/5], Step [2600/5746], Loss: 3.0361\n",
      "Epoch [5/5], Step [2610/5746], Loss: 3.0861\n",
      "Epoch [5/5], Step [2620/5746], Loss: 3.1145\n",
      "Epoch [5/5], Step [2630/5746], Loss: 2.9637\n",
      "Epoch [5/5], Step [2640/5746], Loss: 3.1386\n",
      "Epoch [5/5], Step [2650/5746], Loss: 3.1298\n",
      "Epoch [5/5], Step [2660/5746], Loss: 3.1945\n",
      "Epoch [5/5], Step [2670/5746], Loss: 3.1511\n",
      "Epoch [5/5], Step [2680/5746], Loss: 2.9634\n",
      "Epoch [5/5], Step [2690/5746], Loss: 3.2112\n",
      "Epoch [5/5], Step [2700/5746], Loss: 3.1784\n",
      "Epoch [5/5], Step [2710/5746], Loss: 3.2109\n",
      "Epoch [5/5], Step [2720/5746], Loss: 3.1297\n",
      "Epoch [5/5], Step [2730/5746], Loss: 3.1003\n",
      "Epoch [5/5], Step [2740/5746], Loss: 3.0762\n",
      "Epoch [5/5], Step [2750/5746], Loss: 2.9714\n",
      "Epoch [5/5], Step [2760/5746], Loss: 3.0563\n",
      "Epoch [5/5], Step [2770/5746], Loss: 3.0132\n",
      "Epoch [5/5], Step [2780/5746], Loss: 3.0274\n",
      "Epoch [5/5], Step [2790/5746], Loss: 3.0075\n",
      "Epoch [5/5], Step [2800/5746], Loss: 2.9351\n",
      "Epoch [5/5], Step [2810/5746], Loss: 3.1673\n",
      "Epoch [5/5], Step [2820/5746], Loss: 3.0681\n",
      "Epoch [5/5], Step [2830/5746], Loss: 3.1613\n",
      "Epoch [5/5], Step [2840/5746], Loss: 3.0130\n",
      "Epoch [5/5], Step [2850/5746], Loss: 3.0703\n",
      "Epoch [5/5], Step [2860/5746], Loss: 3.0967\n",
      "Epoch [5/5], Step [2870/5746], Loss: 2.9704\n",
      "Epoch [5/5], Step [2880/5746], Loss: 3.1061\n",
      "Epoch [5/5], Step [2890/5746], Loss: 3.0461\n",
      "Epoch [5/5], Step [2900/5746], Loss: 3.0601\n",
      "Epoch [5/5], Step [2910/5746], Loss: 3.0546\n",
      "Epoch [5/5], Step [2920/5746], Loss: 3.0360\n",
      "Epoch [5/5], Step [2930/5746], Loss: 3.0217\n",
      "Epoch [5/5], Step [2940/5746], Loss: 2.9960\n",
      "Epoch [5/5], Step [2950/5746], Loss: 3.0558\n",
      "Epoch [5/5], Step [2960/5746], Loss: 2.9890\n",
      "Epoch [5/5], Step [2970/5746], Loss: 3.0440\n",
      "Epoch [5/5], Step [2980/5746], Loss: 2.9190\n",
      "Epoch [5/5], Step [2990/5746], Loss: 3.0960\n",
      "Epoch [5/5], Step [3000/5746], Loss: 3.0621\n",
      "Epoch [5/5], Step [3010/5746], Loss: 3.1113\n",
      "Epoch [5/5], Step [3020/5746], Loss: 2.9784\n",
      "Epoch [5/5], Step [3030/5746], Loss: 2.9896\n",
      "Epoch [5/5], Step [3040/5746], Loss: 2.9297\n",
      "Epoch [5/5], Step [3050/5746], Loss: 3.1749\n",
      "Epoch [5/5], Step [3060/5746], Loss: 3.1276\n",
      "Epoch [5/5], Step [3070/5746], Loss: 3.0358\n",
      "Epoch [5/5], Step [3080/5746], Loss: 3.1861\n",
      "Epoch [5/5], Step [3090/5746], Loss: 3.1084\n",
      "Epoch [5/5], Step [3100/5746], Loss: 2.9572\n",
      "Epoch [5/5], Step [3110/5746], Loss: 3.0971\n",
      "Epoch [5/5], Step [3120/5746], Loss: 2.8894\n",
      "Epoch [5/5], Step [3130/5746], Loss: 3.2053\n",
      "Epoch [5/5], Step [3140/5746], Loss: 3.0073\n",
      "Epoch [5/5], Step [3150/5746], Loss: 2.9449\n",
      "Epoch [5/5], Step [3160/5746], Loss: 2.9895\n",
      "Epoch [5/5], Step [3170/5746], Loss: 3.0109\n",
      "Epoch [5/5], Step [3180/5746], Loss: 3.0879\n",
      "Epoch [5/5], Step [3190/5746], Loss: 3.2118\n",
      "Epoch [5/5], Step [3200/5746], Loss: 3.0193\n",
      "Epoch [5/5], Step [3210/5746], Loss: 3.0218\n",
      "Epoch [5/5], Step [3220/5746], Loss: 3.1491\n",
      "Epoch [5/5], Step [3230/5746], Loss: 2.8766\n",
      "Epoch [5/5], Step [3240/5746], Loss: 3.2016\n",
      "Epoch [5/5], Step [3250/5746], Loss: 3.0672\n",
      "Epoch [5/5], Step [3260/5746], Loss: 2.9497\n",
      "Epoch [5/5], Step [3270/5746], Loss: 3.0433\n",
      "Epoch [5/5], Step [3280/5746], Loss: 2.9799\n",
      "Epoch [5/5], Step [3290/5746], Loss: 3.1072\n",
      "Epoch [5/5], Step [3300/5746], Loss: 3.0152\n",
      "Epoch [5/5], Step [3310/5746], Loss: 3.1367\n",
      "Epoch [5/5], Step [3320/5746], Loss: 2.9496\n",
      "Epoch [5/5], Step [3330/5746], Loss: 2.9586\n",
      "Epoch [5/5], Step [3340/5746], Loss: 3.0254\n",
      "Epoch [5/5], Step [3350/5746], Loss: 2.9882\n",
      "Epoch [5/5], Step [3360/5746], Loss: 3.0654\n",
      "Epoch [5/5], Step [3370/5746], Loss: 2.8700\n",
      "Epoch [5/5], Step [3380/5746], Loss: 3.0688\n",
      "Epoch [5/5], Step [3390/5746], Loss: 2.9889\n",
      "Epoch [5/5], Step [3400/5746], Loss: 2.9935\n",
      "Epoch [5/5], Step [3410/5746], Loss: 2.8653\n",
      "Epoch [5/5], Step [3420/5746], Loss: 3.1267\n",
      "Epoch [5/5], Step [3430/5746], Loss: 3.2415\n",
      "Epoch [5/5], Step [3440/5746], Loss: 3.1252\n",
      "Epoch [5/5], Step [3450/5746], Loss: 3.0781\n",
      "Epoch [5/5], Step [3460/5746], Loss: 3.0014\n",
      "Epoch [5/5], Step [3470/5746], Loss: 3.1649\n",
      "Epoch [5/5], Step [3480/5746], Loss: 2.9314\n",
      "Epoch [5/5], Step [3490/5746], Loss: 3.1550\n",
      "Epoch [5/5], Step [3500/5746], Loss: 3.1596\n",
      "Epoch [5/5], Step [3510/5746], Loss: 3.0404\n",
      "Epoch [5/5], Step [3520/5746], Loss: 3.1012\n",
      "Epoch [5/5], Step [3530/5746], Loss: 3.0851\n",
      "Epoch [5/5], Step [3540/5746], Loss: 3.1322\n",
      "Epoch [5/5], Step [3550/5746], Loss: 2.9782\n",
      "Epoch [5/5], Step [3560/5746], Loss: 2.9993\n",
      "Epoch [5/5], Step [3570/5746], Loss: 3.0508\n",
      "Epoch [5/5], Step [3580/5746], Loss: 3.1301\n",
      "Epoch [5/5], Step [3590/5746], Loss: 3.0417\n",
      "Epoch [5/5], Step [3600/5746], Loss: 3.1386\n",
      "Epoch [5/5], Step [3610/5746], Loss: 3.1287\n",
      "Epoch [5/5], Step [3620/5746], Loss: 3.0231\n",
      "Epoch [5/5], Step [3630/5746], Loss: 2.9669\n",
      "Epoch [5/5], Step [3640/5746], Loss: 2.9266\n",
      "Epoch [5/5], Step [3650/5746], Loss: 3.1005\n",
      "Epoch [5/5], Step [3660/5746], Loss: 3.0547\n",
      "Epoch [5/5], Step [3670/5746], Loss: 3.1237\n",
      "Epoch [5/5], Step [3680/5746], Loss: 2.9448\n",
      "Epoch [5/5], Step [3690/5746], Loss: 3.0430\n",
      "Epoch [5/5], Step [3700/5746], Loss: 2.9715\n",
      "Epoch [5/5], Step [3710/5746], Loss: 2.9800\n",
      "Epoch [5/5], Step [3720/5746], Loss: 3.0710\n",
      "Epoch [5/5], Step [3730/5746], Loss: 2.8614\n",
      "Epoch [5/5], Step [3740/5746], Loss: 2.9563\n",
      "Epoch [5/5], Step [3750/5746], Loss: 3.0625\n",
      "Epoch [5/5], Step [3760/5746], Loss: 3.1800\n",
      "Epoch [5/5], Step [3770/5746], Loss: 3.0649\n",
      "Epoch [5/5], Step [3780/5746], Loss: 3.0592\n",
      "Epoch [5/5], Step [3790/5746], Loss: 2.9998\n",
      "Epoch [5/5], Step [3800/5746], Loss: 2.9757\n",
      "Epoch [5/5], Step [3810/5746], Loss: 3.1403\n",
      "Epoch [5/5], Step [3820/5746], Loss: 3.1199\n",
      "Epoch [5/5], Step [3830/5746], Loss: 3.1114\n",
      "Epoch [5/5], Step [3840/5746], Loss: 2.9488\n",
      "Epoch [5/5], Step [3850/5746], Loss: 3.0527\n",
      "Epoch [5/5], Step [3860/5746], Loss: 2.9879\n",
      "Epoch [5/5], Step [3870/5746], Loss: 2.9672\n",
      "Epoch [5/5], Step [3880/5746], Loss: 3.1453\n",
      "Epoch [5/5], Step [3890/5746], Loss: 2.9561\n",
      "Epoch [5/5], Step [3900/5746], Loss: 3.1498\n",
      "Epoch [5/5], Step [3910/5746], Loss: 3.0131\n",
      "Epoch [5/5], Step [3920/5746], Loss: 3.0109\n",
      "Epoch [5/5], Step [3930/5746], Loss: 3.1294\n",
      "Epoch [5/5], Step [3940/5746], Loss: 2.9267\n",
      "Epoch [5/5], Step [3950/5746], Loss: 2.9348\n",
      "Epoch [5/5], Step [3960/5746], Loss: 3.0344\n",
      "Epoch [5/5], Step [3970/5746], Loss: 3.1461\n",
      "Epoch [5/5], Step [3980/5746], Loss: 3.0321\n",
      "Epoch [5/5], Step [3990/5746], Loss: 3.1077\n",
      "Epoch [5/5], Step [4000/5746], Loss: 3.0939\n",
      "Epoch [5/5], Step [4010/5746], Loss: 3.0471\n",
      "Epoch [5/5], Step [4020/5746], Loss: 3.0645\n",
      "Epoch [5/5], Step [4030/5746], Loss: 3.1429\n",
      "Epoch [5/5], Step [4040/5746], Loss: 2.8982\n",
      "Epoch [5/5], Step [4050/5746], Loss: 2.9384\n",
      "Epoch [5/5], Step [4060/5746], Loss: 3.1018\n",
      "Epoch [5/5], Step [4070/5746], Loss: 3.1487\n",
      "Epoch [5/5], Step [4080/5746], Loss: 2.9950\n",
      "Epoch [5/5], Step [4090/5746], Loss: 3.0818\n",
      "Epoch [5/5], Step [4100/5746], Loss: 3.1516\n",
      "Epoch [5/5], Step [4110/5746], Loss: 2.9419\n",
      "Epoch [5/5], Step [4120/5746], Loss: 3.0601\n",
      "Epoch [5/5], Step [4130/5746], Loss: 3.1170\n",
      "Epoch [5/5], Step [4140/5746], Loss: 3.0245\n",
      "Epoch [5/5], Step [4150/5746], Loss: 3.0596\n",
      "Epoch [5/5], Step [4160/5746], Loss: 3.1571\n",
      "Epoch [5/5], Step [4170/5746], Loss: 3.0914\n",
      "Epoch [5/5], Step [4180/5746], Loss: 3.0159\n",
      "Epoch [5/5], Step [4190/5746], Loss: 3.0505\n",
      "Epoch [5/5], Step [4200/5746], Loss: 3.0568\n",
      "Epoch [5/5], Step [4210/5746], Loss: 3.0715\n",
      "Epoch [5/5], Step [4220/5746], Loss: 3.0190\n",
      "Epoch [5/5], Step [4230/5746], Loss: 2.9982\n",
      "Epoch [5/5], Step [4240/5746], Loss: 2.8914\n",
      "Epoch [5/5], Step [4250/5746], Loss: 3.1381\n",
      "Epoch [5/5], Step [4260/5746], Loss: 3.0494\n",
      "Epoch [5/5], Step [4270/5746], Loss: 2.9900\n",
      "Epoch [5/5], Step [4280/5746], Loss: 3.0139\n",
      "Epoch [5/5], Step [4290/5746], Loss: 3.1052\n",
      "Epoch [5/5], Step [4300/5746], Loss: 3.1574\n",
      "Epoch [5/5], Step [4310/5746], Loss: 3.1028\n",
      "Epoch [5/5], Step [4320/5746], Loss: 3.0822\n",
      "Epoch [5/5], Step [4330/5746], Loss: 2.8802\n",
      "Epoch [5/5], Step [4340/5746], Loss: 3.2112\n",
      "Epoch [5/5], Step [4350/5746], Loss: 2.9328\n",
      "Epoch [5/5], Step [4360/5746], Loss: 2.9548\n",
      "Epoch [5/5], Step [4370/5746], Loss: 3.0520\n",
      "Epoch [5/5], Step [4380/5746], Loss: 2.9563\n",
      "Epoch [5/5], Step [4390/5746], Loss: 2.9983\n",
      "Epoch [5/5], Step [4400/5746], Loss: 2.9487\n",
      "Epoch [5/5], Step [4410/5746], Loss: 3.1390\n",
      "Epoch [5/5], Step [4420/5746], Loss: 2.9245\n",
      "Epoch [5/5], Step [4430/5746], Loss: 3.0108\n",
      "Epoch [5/5], Step [4440/5746], Loss: 2.8539\n",
      "Epoch [5/5], Step [4450/5746], Loss: 3.0542\n",
      "Epoch [5/5], Step [4460/5746], Loss: 3.1004\n",
      "Epoch [5/5], Step [4470/5746], Loss: 2.8806\n",
      "Epoch [5/5], Step [4480/5746], Loss: 2.9464\n",
      "Epoch [5/5], Step [4490/5746], Loss: 3.0405\n",
      "Epoch [5/5], Step [4500/5746], Loss: 2.9151\n",
      "Epoch [5/5], Step [4510/5746], Loss: 3.0099\n",
      "Epoch [5/5], Step [4520/5746], Loss: 3.1257\n",
      "Epoch [5/5], Step [4530/5746], Loss: 3.1604\n",
      "Epoch [5/5], Step [4540/5746], Loss: 3.0303\n",
      "Epoch [5/5], Step [4550/5746], Loss: 3.0674\n",
      "Epoch [5/5], Step [4560/5746], Loss: 3.1261\n",
      "Epoch [5/5], Step [4570/5746], Loss: 2.8845\n",
      "Epoch [5/5], Step [4580/5746], Loss: 3.1358\n",
      "Epoch [5/5], Step [4590/5746], Loss: 3.1022\n",
      "Epoch [5/5], Step [4600/5746], Loss: 3.0427\n",
      "Epoch [5/5], Step [4610/5746], Loss: 3.0376\n",
      "Epoch [5/5], Step [4620/5746], Loss: 3.0279\n",
      "Epoch [5/5], Step [4630/5746], Loss: 3.0050\n",
      "Epoch [5/5], Step [4640/5746], Loss: 3.1434\n",
      "Epoch [5/5], Step [4650/5746], Loss: 2.9794\n",
      "Epoch [5/5], Step [4660/5746], Loss: 3.0105\n",
      "Epoch [5/5], Step [4670/5746], Loss: 2.9751\n",
      "Epoch [5/5], Step [4680/5746], Loss: 3.1197\n",
      "Epoch [5/5], Step [4690/5746], Loss: 3.1491\n",
      "Epoch [5/5], Step [4700/5746], Loss: 2.9270\n",
      "Epoch [5/5], Step [4710/5746], Loss: 2.8681\n",
      "Epoch [5/5], Step [4720/5746], Loss: 3.0534\n",
      "Epoch [5/5], Step [4730/5746], Loss: 3.0000\n",
      "Epoch [5/5], Step [4740/5746], Loss: 3.0185\n",
      "Epoch [5/5], Step [4750/5746], Loss: 3.1033\n",
      "Epoch [5/5], Step [4760/5746], Loss: 3.0767\n",
      "Epoch [5/5], Step [4770/5746], Loss: 2.9876\n",
      "Epoch [5/5], Step [4780/5746], Loss: 2.9318\n",
      "Epoch [5/5], Step [4790/5746], Loss: 3.2129\n",
      "Epoch [5/5], Step [4800/5746], Loss: 2.9793\n",
      "Epoch [5/5], Step [4810/5746], Loss: 3.0394\n",
      "Epoch [5/5], Step [4820/5746], Loss: 2.9967\n",
      "Epoch [5/5], Step [4830/5746], Loss: 3.0400\n",
      "Epoch [5/5], Step [4840/5746], Loss: 2.9643\n",
      "Epoch [5/5], Step [4850/5746], Loss: 2.9573\n",
      "Epoch [5/5], Step [4860/5746], Loss: 2.9759\n",
      "Epoch [5/5], Step [4870/5746], Loss: 3.0533\n",
      "Epoch [5/5], Step [4880/5746], Loss: 3.0787\n",
      "Epoch [5/5], Step [4890/5746], Loss: 2.9719\n",
      "Epoch [5/5], Step [4900/5746], Loss: 3.0316\n",
      "Epoch [5/5], Step [4910/5746], Loss: 3.0849\n",
      "Epoch [5/5], Step [4920/5746], Loss: 3.1337\n",
      "Epoch [5/5], Step [4930/5746], Loss: 2.9361\n",
      "Epoch [5/5], Step [4940/5746], Loss: 2.9550\n",
      "Epoch [5/5], Step [4950/5746], Loss: 2.8580\n",
      "Epoch [5/5], Step [4960/5746], Loss: 2.9465\n",
      "Epoch [5/5], Step [4970/5746], Loss: 3.0198\n",
      "Epoch [5/5], Step [4980/5746], Loss: 3.0939\n",
      "Epoch [5/5], Step [4990/5746], Loss: 3.1307\n",
      "Epoch [5/5], Step [5000/5746], Loss: 2.9180\n",
      "Epoch [5/5], Step [5010/5746], Loss: 3.1438\n",
      "Epoch [5/5], Step [5020/5746], Loss: 3.1106\n",
      "Epoch [5/5], Step [5030/5746], Loss: 2.9770\n",
      "Epoch [5/5], Step [5040/5746], Loss: 3.0465\n",
      "Epoch [5/5], Step [5050/5746], Loss: 2.9411\n",
      "Epoch [5/5], Step [5060/5746], Loss: 2.9412\n",
      "Epoch [5/5], Step [5070/5746], Loss: 2.9863\n",
      "Epoch [5/5], Step [5080/5746], Loss: 2.9250\n",
      "Epoch [5/5], Step [5090/5746], Loss: 3.0743\n",
      "Epoch [5/5], Step [5100/5746], Loss: 3.0337\n",
      "Epoch [5/5], Step [5110/5746], Loss: 2.9849\n",
      "Epoch [5/5], Step [5120/5746], Loss: 2.9061\n",
      "Epoch [5/5], Step [5130/5746], Loss: 3.1071\n",
      "Epoch [5/5], Step [5140/5746], Loss: 3.0296\n",
      "Epoch [5/5], Step [5150/5746], Loss: 3.1102\n",
      "Epoch [5/5], Step [5160/5746], Loss: 3.1101\n",
      "Epoch [5/5], Step [5170/5746], Loss: 3.1158\n",
      "Epoch [5/5], Step [5180/5746], Loss: 2.9355\n",
      "Epoch [5/5], Step [5190/5746], Loss: 2.8986\n",
      "Epoch [5/5], Step [5200/5746], Loss: 2.8452\n",
      "Epoch [5/5], Step [5210/5746], Loss: 3.0280\n",
      "Epoch [5/5], Step [5220/5746], Loss: 3.1139\n",
      "Epoch [5/5], Step [5230/5746], Loss: 2.8856\n",
      "Epoch [5/5], Step [5240/5746], Loss: 2.9977\n",
      "Epoch [5/5], Step [5250/5746], Loss: 3.1219\n",
      "Epoch [5/5], Step [5260/5746], Loss: 2.9783\n",
      "Epoch [5/5], Step [5270/5746], Loss: 2.8319\n",
      "Epoch [5/5], Step [5280/5746], Loss: 3.0331\n",
      "Epoch [5/5], Step [5290/5746], Loss: 3.0941\n",
      "Epoch [5/5], Step [5300/5746], Loss: 3.0990\n",
      "Epoch [5/5], Step [5310/5746], Loss: 2.9370\n",
      "Epoch [5/5], Step [5320/5746], Loss: 2.9894\n",
      "Epoch [5/5], Step [5330/5746], Loss: 3.0067\n",
      "Epoch [5/5], Step [5340/5746], Loss: 2.9109\n",
      "Epoch [5/5], Step [5350/5746], Loss: 2.9836\n",
      "Epoch [5/5], Step [5360/5746], Loss: 2.9960\n",
      "Epoch [5/5], Step [5370/5746], Loss: 3.0643\n",
      "Epoch [5/5], Step [5380/5746], Loss: 2.9703\n",
      "Epoch [5/5], Step [5390/5746], Loss: 3.0507\n",
      "Epoch [5/5], Step [5400/5746], Loss: 3.0292\n",
      "Epoch [5/5], Step [5410/5746], Loss: 3.0151\n",
      "Epoch [5/5], Step [5420/5746], Loss: 3.0828\n",
      "Epoch [5/5], Step [5430/5746], Loss: 2.8340\n",
      "Epoch [5/5], Step [5440/5746], Loss: 3.0297\n",
      "Epoch [5/5], Step [5450/5746], Loss: 3.0624\n",
      "Epoch [5/5], Step [5460/5746], Loss: 3.0914\n",
      "Epoch [5/5], Step [5470/5746], Loss: 3.0035\n",
      "Epoch [5/5], Step [5480/5746], Loss: 3.0701\n",
      "Epoch [5/5], Step [5490/5746], Loss: 3.0390\n",
      "Epoch [5/5], Step [5500/5746], Loss: 3.0254\n",
      "Epoch [5/5], Step [5510/5746], Loss: 3.1338\n",
      "Epoch [5/5], Step [5520/5746], Loss: 3.1741\n",
      "Epoch [5/5], Step [5530/5746], Loss: 2.8884\n",
      "Epoch [5/5], Step [5540/5746], Loss: 2.9489\n",
      "Epoch [5/5], Step [5550/5746], Loss: 2.8843\n",
      "Epoch [5/5], Step [5560/5746], Loss: 3.0060\n",
      "Epoch [5/5], Step [5570/5746], Loss: 2.9528\n",
      "Epoch [5/5], Step [5580/5746], Loss: 2.9583\n",
      "Epoch [5/5], Step [5590/5746], Loss: 3.1018\n",
      "Epoch [5/5], Step [5600/5746], Loss: 2.9562\n",
      "Epoch [5/5], Step [5610/5746], Loss: 3.0547\n",
      "Epoch [5/5], Step [5620/5746], Loss: 3.1785\n",
      "Epoch [5/5], Step [5630/5746], Loss: 2.9249\n",
      "Epoch [5/5], Step [5640/5746], Loss: 3.0508\n",
      "Epoch [5/5], Step [5650/5746], Loss: 2.9568\n",
      "Epoch [5/5], Step [5660/5746], Loss: 2.9667\n",
      "Epoch [5/5], Step [5670/5746], Loss: 3.0408\n",
      "Epoch [5/5], Step [5680/5746], Loss: 2.9757\n",
      "Epoch [5/5], Step [5690/5746], Loss: 2.9782\n",
      "Epoch [5/5], Step [5700/5746], Loss: 3.0464\n",
      "Epoch [5/5], Step [5710/5746], Loss: 2.9148\n",
      "Epoch [5/5], Step [5720/5746], Loss: 3.0112\n",
      "Epoch [5/5], Step [5730/5746], Loss: 2.9658\n",
      "Epoch [5/5], Step [5740/5746], Loss: 2.9512\n",
      "Epoch [5/5] Average Loss: 3.0532, Perplexity: 21.18\n"
     ]
    }
   ],
   "source": [
    "train_losses_attention = []\n",
    "perplexities_attention = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(x)\n",
    "        \n",
    "        # Reshape logits and targets for loss computation\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step \"\n",
    "                  f\"[{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    train_losses_attention.append(avg_loss)\n",
    "    perplexities_attention.append(perplexity)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f03185cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting loss and perplexity for the attention model\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses_attention, label='Training Loss')\n",
    "plt.title('Model with attention: Training loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(perplexities_attention, label='Perplexity')\n",
    "plt.title('Model with attention: Perplexity over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f66357",
   "metadata": {},
   "source": [
    "### 5.3 Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24f467f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for the model with attention: 13.65\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dev_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(dev_dataloader)\n",
    "perplexity_attention = math.exp(avg_loss)\n",
    "print(f\"Perplexity for the model with attention: {perplexity_attention:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50310932",
   "metadata": {},
   "source": [
    "### 5.4 Generating text\n",
    "\n",
    "Generate text and analyse how different temperature settings affect the generated text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52d34fe5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\n\u001b[1;32m     22\u001b[0m start_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJim was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m generate_text(model, tokenizer, start_text, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, start_text, max_length, temperature)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_length:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m model(context)\n\u001b[1;32m     12\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m/\u001b[39m temperature\n\u001b[1;32m     13\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m, in \u001b[0;36mLanguageModelWithAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m token_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding(x)\n\u001b[0;32m---> 18\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(positions)\n\u001b[1;32m     20\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m token_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[1;32m     21\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, start_text, max_length=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = tokenizer.encode(start_text)\n",
    "    context = torch.tensor(generated, dtype=torch.long,\n",
    "                          device=device).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            if context.size(1) >= max_length:\n",
    "                break\n",
    "            logits, _ = model(context)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            context = torch.cat(\n",
    "                [context, next_token_id.unsqueeze(0)], dim=1\n",
    "            )\n",
    "    \n",
    "    generated_text = tokenizer.decode(context[0].tolist())\n",
    "    return generated_text\n",
    "\n",
    "start_text = \"Jim was\"\n",
    "generated_text = generate_text(model, tokenizer, start_text, max_length=32)\n",
    "print(\"Generated Text:\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ad960-f641-4709-9977-890c117a1979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "display_name": "Python 3",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
